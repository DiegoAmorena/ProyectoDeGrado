{"text": " La clase pasada estuvimos viendo una metodolog\u00eda de clasificaci\u00f3n en general, as\u00ed para cualquier problema de clasificaci\u00f3n, especialmente c\u00f3mo separar el corpus, qu\u00e9 medidas utilizar. Una cantidad de aspectos metodol\u00f3gicos que son muy importantes y que lo hicimos independiente del dominio en el que estamos, que es el depresamiento de lenguajas natural porque aplica para cualquier problema de clasificaci\u00f3n. Problemas de clasificaci\u00f3n y los m\u00e9todos a aplicar a utilizar se pueden definir en general. De hecho, en el curso de aprendizaje autom\u00e1tico, ustedes aprenden con m\u00e1s detalle lo que vimos en parte del curso de aprendizaje autom\u00e1tico, aprenden con m\u00e1s detalle lo que ayer vimos en una clase sola. Porque se ven diferentes m\u00e9todos, excluimos cu\u00e1l era el m\u00e9todo en particular y hablamos en general un clasificador, un clasificador supervisado, dijimos aquel caso donde yo tengo un conjunto de instancias de cosas, un conjunto de clases discreto y tengo que asignarle a cada instancia, la tarea de desasignarle a cada una de esas instancias uno del grupo de clases. Si yo tengo un conjunto de documentos y quiero saber en qu\u00e9 idioma est\u00e1 lo que estoy haciendo es un problema de clasificaci\u00f3n. Tengo el conjunto de los documentos, tengo las clases que son los idiomas posibles y yo tengo que a cada uno asociarle una clase. Podr\u00eda eventualmente ser m\u00e1s de una clase, podemos tener un problema multiclase, es decir hay variantes \u00bfno? Yo podr\u00eda decir que a cada documento era signo m\u00e1s de una clase, por ejemplo si lo quiero clasificar el t\u00f3pico de un documento, esto puede ser de espect\u00e1culos y de deportes o estamos hablando de guandanar a poner. En la clase de hoy lo que vamos a ver es vamos a hablar de los m\u00e9todos que hay de clasificaci\u00f3n de algunos m\u00e9todos y de c\u00f3mo se aplican algunas tareas del procesamiento de lenguaje natural. Vamos a hablar un poco de las caracter\u00edsticas del m\u00e9todo y de c\u00f3mo intanciarlo en alg\u00fan caso de ejemplo. Como yo dec\u00eda en la clase pasada, los m\u00e9todos de clasificaci\u00f3n est\u00e1n muy difundidos en todos los diferentes an\u00e1lisis porque generalmente los elementos de dominio con lo que trabajamos son discretos, las palabras, las oraciones, los documentos, los tweets son todas cosas discretas. Entonces en general vamos a ver m\u00e9todos de clasificaci\u00f3n supervisadas. Si yo quisiera por ejemplo un ejemplo concreto, un proyecto que tuvimos el a\u00f1o pasado que era que clasificaba un tweet si era un chiste o no, esa era una tarea de clasificaci\u00f3n, una tarea que tambi\u00e9n encaramos aunque no con demasiado \u00e9xito, era la de calificar el chiste en un rango, en un, por vacinar un valor de qu\u00e9 tan bueno estaba, digamos, si se pod\u00eda llegar a capturar eso y ah\u00ed si yo como lo planteamos nosotros era que vos le pod\u00edas poner una, dos, tres, cuatro, cinco estrellas, eso sigue siendo un problema de clasificaci\u00f3n supervisada, pero si esto yo lo considerara un continuo, ah\u00ed tendr\u00edamos un problema de regresi\u00f3n, no son usuales, los problemas de regresi\u00f3n de pasamiento no van a que natural porque nuestros ni\u00f1os generalmente son discretos. Bueno, pero vamos a m\u00e9todo de clasificaci\u00f3n supervisada y en particular vamos a hablar de m\u00e9todos probabilistas. Los m\u00e9todos probabilistas en general tenemos la instancia, o sea yo no voy a volver sobre la terminolog\u00eda que vimos hace pasada, tenemos la instancia representada por atributos y queremos asignarlo a una clase, pero adem\u00e1s los m\u00e9todos probabilistas lo que hacen es asignarle una probabilidad a cada clase posible. Entonces yo no solo te digo esta instancia, esta instancia, este tweet es humor\u00edstico, sino que te digo este tweet tiene un 85% de chances en humor\u00edstico y no humor\u00edstico un 15%. Y esto por supuesto tiene que ser una distribuci\u00f3n de probabilidad, sumar uno y tal, mayor que cero. Entonces y adem\u00e1s los m\u00e9todos probabilistas intentan obtener una distribuci\u00f3n sobre las clases dado en los atributos. Y por supuesto clasificar en general va a ser, uno va a elegir la clase con la probabilidad m\u00e1s alta. As\u00ed es que no quiere simplemente dejar de volver esa distribuci\u00f3n para que otra etapa del proceso lo utilice. Yo tengo la posibilidad de hacer eso. Los m\u00e9todos generativos, que son uno de los tipos de m\u00e9todos que hay, lo que intentan es, son los que hemos estado viendo hasta ahora en general y es lo que tratan de modelar la distribuci\u00f3n conjunta, es decir, la clase junto con los atributos, \u00bfs\u00ed? Y las etiquetas, \u00bfde acuerdo? \u00bfPor qu\u00e9? Porque es lo que necesitan para, a partir de la regla de Valle. Es decir, yo quiero la clase dada del conjunto de features, \u00bfse acuerdan que la feature era nuestra representaci\u00f3n del documento, \u00bfno? Caracter\u00edstica que, Valle a la redundancia caracterizaban al documento. Entonces, la probabilidad de la clase dada de los atributos es igual, la probabilidad conjunta dividida de la probabilidad de los atributos, \u00bfs\u00ed? Por definici\u00f3n, por la definici\u00f3n de probabilidad condicional. \u00bfDe acuerdo? Entonces lo que tratan de modelar es esto. \u00bfPor qu\u00e9 lo hacen? \u00bfPor qu\u00e9 esta probabilidad generalmente son m\u00e1s f\u00e1ciles de estimar que las otras? \u00bfPor qu\u00e9 la puedo estimar contando m\u00e1s f\u00e1cilmente? \u00bfPor qu\u00e9? Porque f\u00edjense que yo como condiciono en dada de la clase, digamos, yo, por ejemplo, puedo asumir independencia entre las variables aleatorias esta, o sea, entre los atributos y puedo decir, si estas son independientes, p de x1 dado c por p de x2 dado c, \u00bfno? \u00bfEsto no lo puedo hacer de este lado? Yo no puedo decir p de c dado x1, porque no funciona as\u00ed la probabilidad, digamos. La independencia la puedo dejar de ac\u00e1 al lado. Y cualquier propiedad de dependencia entre variables aleatorias se mira de este lado, \u00bfno? Eso genera toda una teor\u00eda que se llama la de los modelos gr\u00e1ficos, que por supuesto no vamos a hablar ac\u00e1, pero que me dicen, bueno, \u00bfcu\u00e1l es la estructura que yo supongo en t\u00e9rminos de dependencia? Es decir, esta variable depende de esta, esta no, y as\u00ed. Y puedo modelarlo con un gr\u00e1fico, como va a tomar. Entonces, llegan a esto, \u00bfno? La probabilidad de la clase, dado los atributos, la probabilidad de la clase por la probabilidad de los atributos a la clase. Esto es valles, \u00bfno? Y ya lo hemos visto varias veces en el curso, no estamos inventando nada. Dividido la probabilidad de los atributos. Y bueno, y nada, lo que hemos hecho hasta ahora, tanto la probabilidad priori, la PC como la probabilidad de verosimilitud, esta la puedo estimar a partir de los datos. Esto ya lo hemos hecho. Pero vamos a tener que simplificar el problema. El m\u00e9todo nai valles lo que hace es asumir que los atributos son independientes entre s\u00ed, lo cual es una barbaridad conceptual, si por ejemplo estamos hablando de un texto y los atributos son las palabras que tiene. Realmente las palabras vienen acompa\u00f1adas, se hacen amigas entre ellas, digamos, \u00bfno? Si hay muy palabras positivas, muy probable que haya otras palabras positivas. Bueno, valles dice, bueno, no s\u00e9, no s\u00e9. La probabilidad de una palabra solo depende de la clase. Y por lo tanto eso hace que pueda partir la probabilidad, porque como son independientes, la probabilidad de x1 dado x1 por xn dado c, la probabilidad de x1 dado c por la probabilidad de x2 dado c, bla, bla. Y bueno, \u00bfy c\u00f3mo construye un clasificador a partir de esto? Y bueno, maximizo lo de arriba, busco la clase que maximice lo de arriba. Lo de abajo es independiente de la clase. Entonces busco la clase que maximice lo de arriba y ah\u00ed tengo un clasificador. \u00bfDe acuerdo? Es muy sencillo, tomo todos los atributos que se me ocurren, los considero independientes. Ahora lo moveremos en alg\u00fan ejemplo y busco la clase que maximiza. El m\u00e9todo Ney Valle funciona muy bien como base para un clasificador y por poca plata uno hace un clasificador como la gente que capaz que hasta le pueden llamar un AI en la prensa. Yo no s\u00e9 de cu\u00e1ndo es el m\u00e9todo de Ney Valle, me suena como de los a\u00f1os 60, si bien se basa en el teor\u00eda de Valle que de 1700, pero funciona muy bien. En general, como primera aproximaci\u00f3n r\u00e1pida o algo, uno puede usar Ney Valle sin mucho cargo de conciencia y funciona en general muy bien. El m\u00e9todo de Ney Valle es aplicado a la clasificaci\u00f3n de documentos. Utiliza una de las formas de darlo es utilizando lo que se llama una aproximaci\u00f3n vago words. Es el ejemplo, es como el ejemplo can\u00f3nico de clasificaci\u00f3n, digamos, el vago word. Yo digo tengo todo esto, es un documento que tiene una estructura, que tiene un orden entre las palabras, que tiene una sintaxis, que tiene relaciones bien formadas, con una sem\u00e1ntica, yo no le hago caso a nada de eso. Y lo que hago solamente es considero que esto es una bolsa de palabras. La bolsa de se acuerdan, bolsa es como un set, pero que puede tener elemento repetido. Una bolsa de palabras y tengo el conteo de cantidad de veces que una palabra aparece en ese documento. Mi representaci\u00f3n del documento es esto. Me features son estos. Entonces, c\u00f3mo hago clasificaci\u00f3n, esto fue lo que hubo en la laboratoria del a\u00f1o pasado. Entonces, c\u00f3mo se instancia Ney Valle para el problema de clasificaci\u00f3n de documento? Bueno, las posiciones son todas las posiciones que tengo en el documento que quiero evaluar. Yo quiero evaluar en la clase. Aguardo, quiero evaluar la clase en un documento, entonces tengo las posiciones, que son todos los tokens que aparecen en cada palabra, en el documento. Y la clase, seg\u00fan Ney Valle, es la clase que maximiza, quer\u00eda comentar algo ac\u00e1. Esto en realidad es un conteo, pero yo ac\u00e1 la voy a contar seis veces. Por eso es un bug of words. En las posiciones considero todas las posiciones posibles, como dec\u00eda, y calculo la clase como la clase que maximiza la probabilidad de cada palabra que aparece en el documento dado a esa clase. \u00bfSe entiende? Es la clase que hace m\u00e1s probable, considerando independencia, que esa palabra es T en ese documento, digamos, \u00bfno? La probabilidad de W subida o C. \u00bfY c\u00f3mo hago para hacer eso? Y bueno, para calcular esos valores, para estimar esos valores, yo digo, bueno, nuestro mejor estimador, este corrito quiere decir nuestro estimador, nuestro mejor estimador de la clase, de la probabilidad priori, de la probabilidad, estamos hablando de la probabilidad de la clase, si no tuvieramos la palabra, es decir, yo puedo tener una distribuci\u00f3n, yo tengo documentos que son o de deporte o de m\u00fasica, vamos a suponer que son exclusivos, \u00bft\u00e1? La probabilidad de la clase es el n\u00famero de documentos de deporte sobre el total, o sea, mi probabilidad priori, \u00bfse acuerdan de Valle, \u00bfno? Yo tengo una probabilidad priori que lo que pienso antes de empezar a ver el documento y antes de ver el documento yo puedo decir, bueno, el 90% de los documentos son de deporte, entonces mi probabilidad a priori es 0.9, \u00bfte acuerdo? Es mucho m\u00e1s probable a priori que sea un documento de deporte, yo voy a ajustar esa probabilidad con la probabilidad de las palabras de cada una, \u00bfte acuerdo? Entonces, yo estimo esa probabilidad priori con el n\u00famero de clase de documentos, que tienen la clase dividido el total de documentos. Y, similarmente, estimo por conteo la probabilidad de cada palabra de la clase contando del total de veces que aparecen todas las palabras en los documentos de esa clase, o sea, de todas las palabras que aparecen en los documentos de deporte, \u00bfcu\u00e1ntas veces aparece esa palabra en la de deporte? Tiene sentido, \u00bfno? Es una palabra com\u00fan en un dominio de deportes, esta es lo que se pregunta, y multiplica a todas esas probabilidades, que seguramente operativamente tengamos que usar un logaritmo y sumar, porque si no nos va a dar todav\u00eda muy chiquita, pero conceptualmente lo mismo. \u00bfSe entiende? \u00bfPor qu\u00e9, en vez de usar esto, tengo que usar esto? \u00bfPor qu\u00e9 tengo que hacer eso? \u00bfPor qu\u00e9 tengo que hacer esto? \u00bfQu\u00e9 es esto? La plaza, le agrego uno, acaba contador para que no tenga el problema de que, porque si una de estas probabilidades, lo mismo que nos pas\u00f3 con los engramas, si le suena conocido, porque es lo mismo, si una de aquella probabilidad de da cero, se me cancela toda la clase, la probabilidad de clase va a ser cero. Entonces para eso hacemos la plaza, hacemos smoothing, suavizado, agreg\u00e1ndole uno a cada contador. Por ejemplo, bueno todo esto que yo estoy diciendo est\u00e1 en el cap\u00edtulo 7, m\u00e1s o menos, que es general, del cap\u00edtulo 7 del libro de Martin Yurashki. El libro de Martin Yurashki est\u00e1 online, los cap\u00edtulos nuevos, de hecho todos los cap\u00edtulos correspondientes a clases que hemos dado est\u00e1n online, yo realmente les recomiendo leerlos un libro que est\u00e1 muy claro, no va a tener mucha m\u00e1s dificultad que lo que vemos en la clase, por lo menos no se, uno pierde perspectiva, no? Est\u00e1 claro, pero, pero... \u00bfQu\u00e9 le pasa? Le agrajo de si, si me giro nada, si, si, y ah\u00ed pueden chequear y hay algunos detalles m\u00e1s que me parecen muy interesantes, si a ustedes les interesa. Bueno, supongamos que nosotros tenemos el cuerpo de entrenamiento que tenemos arriba, las oraciones que est\u00e1n arriba y con una categor\u00eda negativa o positiva, alg\u00fan tipo, en este caso estamos haciendo sentimenta an\u00e1lisis, es decir, analizar si la percepci\u00f3n es positiva o negativa sobre un documento. En el cuerpo de los tweets hac\u00edamos algo as\u00ed, algo parecido, es decir, yo necesito saber si la clase del cuerpo del tweet es de humor o no humor. Bueno, y ah\u00ed tenemos algunos ejemplos negativos y otros positivos y queremos saber qu\u00e9 pasa con predictable with no originality. Entonces, la probabilidad priori de la clase cu\u00e1l es y es el total de documentos hay 1, 2, 3, 4, 5, de los cuales tres son negativas y dos son positivas, o sea, que estas son nuestra probabilidad priori. Y luego entramos a buscar la probabilidad de cada palabra. La probabilidad de predictable, dado que la clase es negativa, es 1 que es la ocurrencia de predictable, predictable solo aparece en la segunda oraci\u00f3n y en un contexto negativo. Entonces, a cada 1 y a cada tenemos el m\u00e1s 20 es para normalizar, para la plaza, o sea, 1 m\u00e1s 1 y 14, que es el total de palabras m\u00e1s 20, 14 es el total de palabras diferente. \u00bfDe acuerdo? De las palabras diferentes. \u00bfLa palabras? \u00bfC\u00f3mo f\u00e1ciles verlo ac\u00e1? S\u00ed, es la clase, \u00bfno? De la clase. \u00bfLa cantidad de palabras que hay en la clase? No, no son diferentes, son todas. \u00bfDel total de palabras que hay? \u00bfVoy a contar? Positivo, 1, 2, 3, 4, 5, 6, 7, 8, 9. Son todas, porque ac\u00e1 yo estoy considerando todas las ocurrencias. Es una de las cosas que se le critican, ah\u00ed vayan, es general, es eso, que si yo repito muchas veces algo, le sumo probabilidad. Que a veces no es lo que se quiere, digamos. Si hay atributos que reiteran cosas, es como que est\u00e1n muy relacionados y no est\u00e1n aportando informaci\u00f3n. Entonces, ac\u00e1 est\u00e1n todas las probabilidades de las diferentes palabras. F\u00edjense, bueno, ahora nos fijamos en el ejemplo. Con esas probabilidades, esa es nuestra, es como entrenamos nuestro clasificador, esencialmente. \u00bfDe acuerdo? Es decir, a partir del cuerpo de entrenamiento, yo calculo esta probabilidad y lo que estoy haciendo es entrenar. Como ustedes ven, son cuentas muy sencillas de hacer. El clasificador, no hay vaya, la ventaja que tiene, es que es muy r\u00e1pido, muy, muy r\u00e1pido. Tanto para entrenar como para evaluar. Entonces, cuando uno quiera acercarse a un problema y ver, \u00bfqu\u00e9 tan dif\u00edcil es clasificar un cuerpo de humor? Entonces, se le arrima con un m\u00e9todo de esto, que lo entrenan dos patadas, y m\u00e1s o menos tiene una idea. Dice, ah, mirad, que pude clasificar el 75, 80% del olor. O sea, lo que es un problema que tiene para mejorar un poco, tampoco es que es horrible y dif\u00edcil. Y luego, s\u00ed, empieza a afinar, a ajustar par\u00e1metros, a cambiar el m\u00e9todo, capaz que le mete una red o agregarle datos, le mete una red neuronal que est\u00e1 una semana entrenando. Pero con esto tiene una primera aproximaci\u00f3n, por lo menos. A m\u00ed se alcanza, pasa alguien en los medios. Porque depende la tarea que estamos haciendo. Bueno, \u00bfpero qu\u00e9 pasa? Entonces, \u00bfc\u00f3mo clasifico? Y bueno, si la palabra es prevista, volvi\u00f3 en no originality, yo tengo la probabilidad de la oraci\u00f3n dada la categor\u00eda negativa por la probabilidad de la categor\u00eda negativa. O sea, que es 3 dividido 5 por las diferentes probabilidades de las palabras que aparecen en la categor\u00eda negativa. Si se fijan ac\u00e1 estos 1 es porque no aparec\u00edan. Y ac\u00e1, f\u00edjense que originality es una palabra montinando a positiva, \u00bfno? Es 1 sobre 29 contra 1 sobre 34. O sea, que est\u00e1 mejor en la positiva que en la negativa. \u00bfS\u00ed? \u00bfPor qu\u00e9? Porque aparecen contextos positivos, realmente. Ac\u00e1 el problema que tienen no, adelante. Que es uno de los problemas que ahora vamos a ver. Pero de todos modos, multiplicando las probabilidades de cada palabra, llega que es m\u00e1s probable que sea negativa. \u00bfY por qu\u00e9? Porque dice pred\u00edctabel, seguramente. \u00bfPor qu\u00e9 dice no? En realidad esto es number crunching, \u00bfno? Es porque hay un motivo, digamos, uno de las aplicaciones son siempre aposteriores en estas cosas, \u00bfno? Es decir, bueno, pas\u00f3 esto, pero en realidad esto es un motivo de sus cuentas, inicialmente. \u00bfSe entiende? \u00bfSe entiende ac\u00e1? Si nosotros queremos hacer sentimentan\u00e1lisis, para el caso particular de clasificaci\u00f3n de documentos que se llama sentimentan\u00e1lisis, que es ver la impresi\u00f3n respecto a algo, a un documento, hay algunas reglas que permiten mejorar la performance. Es lo mismo, es exactamente lo mismo, las clases son las mismas, pero se puede hacer alguna modificaci\u00f3n. Por ejemplo, no contar m\u00faltiples ocurrencias en la palabra en el mismo documento. Esto que yo les dec\u00eda hoy, cuento una vez olas. Y se dice, es muy, muy linda, linda, linda, cuento una vez olas. Eso se llama binary navages. El manejo de la innovaci\u00f3n es todo un tema, es todo un tema, el manejo de la innovaci\u00f3n. Y una aproximaci\u00f3n muy, muy sencilla, muy na\u00ed, pero que mejora las cosas, pues bueno, yo a todo lo que dice despu\u00e9s de didn't, lo clasifico no como like, sino como not like, invento una palabra nueva. Podr\u00eda llegar a hacer alguna cosa un poco m\u00e1s elaborada si tuviera un parser, porque si yo tengo un parser, tengo el \u00e1rbol y tengo una rama que dice no todo lo que hay abajo. Entonces yo s\u00e9 el alcance de no. Ah\u00ed igual el problema est\u00e1 en c\u00f3mo hacer el parsing, pero si yo le agrego informaci\u00f3n de parsing, la cosa puede mejorar. De parsing vamos a hablar la semana que viene, pero yo dir\u00eda que... H\u00e1ganme acordar que hable el final de esto, del parsing. Esta, pero esta es una primera aproximaci\u00f3n, \u00bfentiendes? Creo unas palabras nuevas ah\u00ed y ahora el like se cuenta como not like. Es muy na\u00ed porque dice todo lo que est\u00e1 despu\u00e9s de didn't, pero podr\u00eda haber otras cosas en el medio. No, no es tan sencillo, digamos, pues las oraciones son m\u00e1s complicadas. No, creo que pienses que, y hay un que ah\u00ed con oraci\u00f3n subordinada, puede ser m\u00e1s complejo que esto, pero no da rimamos. Y otra aproximaci\u00f3n, por supuesto, es usar lo que se llama lexicones de sentimiento, que son listas de palabras positivas y listas de palabras negativas. Tengo una lista recolectada, \u00bfs\u00ed? En el caso de que est\u00e1 mostrando como se llena la palabra, \u00bfno? S\u00ed. Como no estaba, no ten\u00eda supexicon, cualquier cosa que pusiera, de originales, \u00bfno? Cualquier cosa que pusiera sento para originales y que no estuvieran entre niches, la primera, \u00bfno? Ah, s\u00ed, s\u00ed, claro, claro, claro, claro, claro. De todos modos se supone que vos, en todo este tipo de m\u00e9todos, justamente lo que supone es que como vos ten\u00e9s grandes vol\u00famenes, si no, no funcionan. Claro, claro, claro. Es decir, que lo que hacen es capturar algo a partir de muchas ocurrencias. Pero s\u00ed, si no parece, si ten\u00e9s cero, es la misma para todas. En el lexic\u00f3n, entonces vos lo que pod\u00e9s hacer es agregar, en tu clasificador, simplemente una fitur que dice la cantidad de palabras en un lexic\u00f3n positivo y la cantidad de palabras en un lexic\u00f3n negativo. Es decir, tiene tres palabras negativas, es un X, Xn m\u00e1s 1 y Xn m\u00e1s 2, son dos atributos nom\u00e1s, \u00bfs\u00ed? Y la cantidad de palabras en un lexic\u00f3n negativo. Le agrego dos atributos que, si recordamos en la clase pasada, van a seguramente estar m\u00e1s correlacionados con la clase y nos van a poder dar una pista de su comportamiento. Si llegara a hacer un m\u00e9todo de regla que dice, bueno, el que tiene m\u00e1s palabra positiva gana, porque juegan todas, intervienen mucho en la clasificaci\u00f3n, \u00bfs\u00ed? \u00bfDe acuerdo? Otro ejemplo, \u00bfc\u00f3mo puedo hacer para calcular un tag de part of pitch si tengo la palabra y los postage de las palabras anteriores y siguientes? Y bueno, de la misma forma, \u00bfno? La probabilidad de que sea un adjetivo, dado que la anterior es un determinante, el siguiente es un nombre y la palabra es blanco, es la probabilidad de que la clase sea un adjetivo a priori, esto lo hago por conteo, la probabilidad de que una palabra sea blanco como adjetivo, es decir, de todas las veces que hubo blanco, cu\u00e1ntas veces, miento, de todos los adjetivos cual era blanco, cu\u00e1ntas veces pas\u00f3 que antes de un adjetivo hubieron determinantes por la probabilidad de que el siguiente sea un nombre si este es un adjetivo. \u00bfSe entiende? Simplemente hago conteo de todas las veces que aparecieron cosas antes y las considero independiente entre ellas, lo cual sabemos que no es cierto, pero es lo que hay, es lo que puedo computar. Y bueno, y como yo estoy calculando la probabilidad conjunta de esto, podr\u00eda llegar a generar ejemplos con la distribuci\u00f3n calculada, eso me puede ser \u00fatil para hacer generaci\u00f3n de texto, todos estos m\u00e9todos me permiten, los m\u00e9todos de, por ejemplo, de engrama me permiten generar tambi\u00e9n texto, que es la forma que hacen los generadores, que escriben parecido a alguien, digamos. Bueno, bueno, atacar los m\u00e9todos generativos que son estos, es, como nadie valles. Un m\u00e9todo generativo es ese que busca una distribuci\u00f3n de todas las clases, prueba todas las clases y computa la distribuci\u00f3n conjunta con los atributos. Los m\u00e9todos discriminativos son un poco diferentes porque en lugar de, en lugar de calcular la probabilidad de la conjunta dicen, bueno, no, de todo ejemplo, cu\u00e1l de los dos es mejor, cu\u00e1l clase es mejor para este ejemplo, sin tratar de modelar todas las clases posibles. Es decir, modelamos directamente la probabilidad, intento modelar directamente la probabilidad condicional, la probabilidad de la clase daba los atributos, \u00bfs\u00ed? Voy derecho a eso, \u00bfqu\u00e9 es m\u00e1s probable dado de todos estos atributos? Nada m\u00e1s. Y hay varias aproximaciones, algunas que son probabil\u00edsticas como entrop\u00eda m\u00e1xima y otras no. A ver, el preceptor de su porvector machine, ahora vamos a ver, no, vamos a ver la definici\u00f3n de su porvector machine. Pero esencialmente lo que te dicen es, bueno, esto est\u00e1 de tal lado. Si yo tengo estos puntos as\u00ed, entreno y despu\u00e9s te digo, bueno, este est\u00e1 de este, si este punto est\u00e1 del lado de los redonditos. No s\u00e9 qu\u00e9 tan del lado est\u00e1 de los, esto no es probabilista, por ejemplo. O puedo hacer lo probabilista, pero igual lo \u00fanico que respondo es ac\u00e1 y de qu\u00e9 lado est\u00e1. Bueno, entonces vamos a ver uno que es el modelo de entrop\u00eda m\u00e1xima, que es como lo que vamos a ver, es como la versi\u00f3n discriminativa del m\u00e9todo de Ney Valle. O tambi\u00e9n conocido como regresi\u00f3n multinomial log\u00edstica, que vamos a ver por qu\u00e9 se llama as\u00ed, y son modelos lo lineales para clasificaci\u00f3n, es decir, yo quiero la clase, si tengo una serie de tributos. Hago. E, ahora vamos a ver qu\u00e9 es esto, \u00bfno? F su I, son las features, son como un indicador de algunas, son features a partir de los atributos, ahora vamos a ver c\u00f3mo lo abrimos eso, pero son derivadas de estos atributos. Los W son los pesos, una serie de pesos que yo voy a intentar calcular, son los pesos de mi modelo, lo que yo voy a entrenar, aprender son los pesos de mi modelo. Y este es el producto, el dot product de ambos, es decir, esto va a ser W1 por F1 m\u00e1s W2 por F2 m\u00e1s W3 por F3, etc\u00e9tera. \u00bfDe acuerdo? Entonces yo, la feature esta, que seg\u00fan mi ejemplo, cuando yo vaya a evaluar, seg\u00fan mi ejemplo, esto va a valer algo, lo multiplico por un n\u00famero fijo que va a depender de lo que yo entren\u00e9, es decir, lo que yo quiero aprender es W, \u00bfde acuerdo? Eso, elev\u00f3 E a la suma de eso, ahora vamos a ver por qu\u00e9 hago esto. Y esta Z es simplemente un factor de normalizaci\u00f3n, es decir, de todos los W sub\u00ed que tengo, o sea, esto no necesariamente genera una distribuci\u00f3n de probabilidad, entonces este Z es como la suma de todos los casos posibles para llevarlo a una probabilidad, a que la suma me d\u00e9 uno, aquellos que habl\u00e1bamos unas clases atr\u00e1s, bueno, generalizado ac\u00e1. Esa es la famosa Z, que parece una pavada, pero es lo m\u00e1s dif\u00edcil de computar, porque yo tengo que calcular este valor para todos los atributos posibles para que me d\u00e9 una distribuci\u00f3n. Entonces, los modelos de entrop\u00eda al m\u00e1ximo calculan la probabilidad de la clase utilizando esta f\u00f3rmula. Ahora vamos a ver por qu\u00e9. Pero antes vamos a hablar de otra cosa para llegar a eso, y es de regresi\u00f3n lineal. Un problema de regresi\u00f3n lineal, que era lo que yo le dec\u00eda hoy, es cuando uno intenta, un problema de regresi\u00f3n es cuando uno intenta calcular un valor, de alg\u00fan valor real, un valor real, \u00bfs\u00ed? Entonces yo, si yo quiero saber, supongamos que yo tengo estos puntos ac\u00e1, \u00bfs\u00ed? Cuando yo hago regresi\u00f3n lineal, lo que hago es trazar, buscar una l\u00ednea que separe los ejemplos. Eso esencialmente es, si esto es, va a ser una cosa como, si yo supongo que pasa por el origen, esta recta, vamos a suponer que pasa por el origen, y si no, vemos c\u00f3mo se corrige. Vamos a llamarle X1, X2. Esto es la recta que representa esto, \u00bfno? Es decir, el W1 y W2 me van a determinar la legislaci\u00f3n de la recta. Ac\u00e1 est\u00e1 pasando por el origen porque no tiene elemento independiente, yo puedo inventar un W0 con un X0 que vale siempre 1, para agregarle, vamos a moverla a la recta. \u00bfDe acuerdo? Entonces, lo que yo, cuando digo que hago regresi\u00f3n lineal, lo que digo es bueno, mis puntos yo asumo que son separables por una recta. Ah, perd\u00f3n, yo quiero estimar X2 dado X1, \u00bfde acuerdo? Entonces yo obtengo la recta para un nuevo X, vengo ac\u00e1 y calculo el I. Entonces, I va a ser igual al WI por FI, que es esto, la sumatoria de los WI por FI es el dot product de WI con F. \u00bfDe acuerdo? Simplemente estoy haciendo un estimador lineal de esto. \u00bfY c\u00f3mo hago para, como encuentro esta recta? Bueno, una de las formas m\u00e1s usuales es la que minimiza la suma de los cuadrados de la diferencia entre valores y predicciones, o sea, esta recta minimiza esta distancia, \u00bfde acuerdo? La distancia, yo busco la recta que tenga la distancia m\u00ednima de esto al cuadrado y esto al cuadrado, \u00bfs\u00ed? Lo hago al cuadrado para que la suma sea positiva, para que no me afecte si estoy de un lado o del otro. La vieja regresa en un lineal, \u00bfs\u00ed? Entonces, no voy a entrar en detalles, pero yo calculo la f\u00f3rmula de los m\u00ednimos cuadrados que son, si lo piensan son todo multiplicaciones de cosas al cuadrado, m\u00e1s cosas al cuadrado. O sea, que esto es positivo, es una funci\u00f3n positiva y convexa y entonces yo lo que trato de buscar es el m\u00ednimo de esa funci\u00f3n. El a\u00f1o que viene lo voy a escribir a eso, porque no s\u00e9 si queda claro. Este, a ustedes no les importa. Pero la cuesti\u00f3n es que yo termino minimizando una funci\u00f3n convexa, una funci\u00f3n convexa y una funci\u00f3n que es as\u00ed. As\u00ed es una funci\u00f3n convexa, \u00bfno? Que cualquier, cualquier par de puntos que yo una pasan todo por adentro del, no? Vamos, ac\u00e1, esto es una funci\u00f3n convexa. S\u00ed, yo puedo unir ac\u00e1, \u00bfde acuerdo? \u00bfC\u00f3mo es una funci\u00f3n convexa? \u00bfQu\u00e9 caracter\u00edstica tiene las funciones convexas? \u00bfCu\u00e1l es qu\u00e9 caracter\u00edstica tiene la funci\u00f3n convexa? Ah, no se acuerdo. Las funciones convexas tienen el tema de que cuando yo encuentro un m\u00ednimo local es un m\u00ednimo global. Si una funci\u00f3n es as\u00ed, yo puedo quedarme, buscar el m\u00ednimo ac\u00e1 y encontrarme con este m\u00ednimo local y buscar ac\u00e1. S\u00ed, s\u00ed, claro, puedo llegar a quedar atascado ac\u00e1. Si yo tengo una funci\u00f3n convexa, esto es informativo, si quieren hacer curso de prensa autom\u00e1tico, esto lo ven en detalle. Este, si yo tengo una funci\u00f3n convexa, yo puedo buscar un punto cualquiera y empezar a calcular en la derivada y avanzar en la, en la direcci\u00f3n de la derivada y al final, al final del d\u00eda voy a encontrar si hago las cosas bien el m\u00ednimo de funci\u00f3n. Eso llama descenso por gradiente, \u00bfs\u00ed? Y deber\u00eda ense\u00f1arse en primer a\u00f1o. Hay otro m\u00e9todo de minimizaci\u00f3n. Son m\u00e9todos de minimizaci\u00f3n num\u00e9rica, \u00bfno? Son calculos num\u00e9ricos. Quiero decir, no hay una f\u00f3rmula cerrada para eso. Para el m\u00e9todo de m\u00ednimo cuadrado s\u00ed hay una f\u00f3rmula cerrada, es decir, una f\u00f3rmula calcular, pero es m\u00e1s f\u00e1cil de hacer descenso por gradiente, pues m\u00e1s r\u00e1pido. Bueno, cuesti\u00f3n, que nosotros podemos saber c\u00f3mo hacer esto, es decir, que yo puedo aprender los WB, o sea, los WB que minimizan, esos son los WB que queremos, est\u00e1 claro, \u00bfno? Es decir, yo calculo a partir del cuerpo de entrenamiento, esos WB, y lo uso luego. Eso se trata de aprender. Entonces, este es un problema de regresi\u00f3n, donde yo quiero calcular un n\u00famero, pero ac\u00e1 estoy en un problema de clasificaci\u00f3n, o sea, que lo que yo quiero aprender es una categor\u00eda, una probabilidad. Entonces, mi primera aproximaci\u00f3n es, perd\u00f3n, es bueno, yo digo esto, la probabilidad, el n\u00famero que yo quiero estimar es la probabilidad de que sea clase, ac\u00e1 tenemos un caso positivo o negativo, \u00bfno? La probabilidad de que I va a ir a true, o sea, de la clase dado mi X. Entonces, yo lo que digo es bueno, hago regresi\u00f3n, hago regresi\u00f3n, pero en lugar de calcular un n\u00famero, o sea, sigo calculando un n\u00famero que es el valor de la probabilidad. Ahora, \u00bfqu\u00e9 problema tiene esto? El problema que tiene esto es que no es una distribuci\u00f3n de probabilidad, no es un valor de probabilidad, porque la probabilidad tiene que estar entre 0 y 1, \u00bfde acuerdo? Entonces, esto no me sirve a aplicarlo directamente, porque me puede dar cualquier cosa, yo quiero una probabilidad. Entonces, lo que digo es bueno, pruebo con los odds, los odds que no s\u00e9 c\u00f3mo se traduce, los odds son como las chances, como las apuestas, \u00bfno? Ten\u00e9s 2 a 1, 1 a 2, que es, esencialmente, la probabilidad de que sea verdadero comparado con la probabilidad de que no lo sea. Esto est\u00e1 un poco mejor, porque este resultado est\u00e1 entre 0 e infinito, pero si es sin estar entre 0 y 1, entre, perd\u00f3n, yo quiero llevarlo a algo que est\u00e9 entre menos infinito y m\u00e1s infinito que es esto, \u00bfno? Est\u00e1 claro, est\u00e1 claro. Esto est\u00e1 entre menos infinito y m\u00e1s infinito, el W por F, cualquier cosa. Ac\u00e1 yo lo reduzco a una cosa que est\u00e1 entre 0 y infinito, mejor. Bueno, pero para que esto quede entre, entre 0 y 1, lo que hago es, le aplico el logaritmo, para que quede, perd\u00f3n, dije al rev\u00e9s, para que quede entre 1 y infinito y m\u00e1s infinito, le aplico el logaritmo. Implico el logaritmo y entonces digo, bueno, esto es lo que busc\u00e1bamos, yo quiero estimar el logaritmo de la probabilidad de las odds, y por eso lleg\u00f3 a esa f\u00f3rmula tan rara con E, porque cuando yo despejo, y esto se lo dejo de ver, cuando yo despejo P igual true, es f\u00e1cil, \u00bfno? Digamos, este logaritmo se transforma en un E a la W por F, \u00bfs\u00ed? Bueno, se lo dejo de ver. Cuesti\u00f3n, \u00bfqu\u00e9 queda de s\u00ed? E a la W por F dividido 1 m\u00e1s E a la W por F. Las odds, se transforma en que es esta funci\u00f3n, \u00bfs\u00ed? Entonces, llegu\u00e9 a una funci\u00f3n que me dice la probabilidad de que sea verdadero da la clase, a partir de haciendo unas cosas raras con las features, nada menos. O sea, algo parecido al lineal, pero que la corrijo con esta funci\u00f3n. Esto es lo mismo que hace la red neuronal. No mismo. Esa funci\u00f3n se llama funci\u00f3n log\u00edstica y tiene este aspecto. \u00bfCu\u00e1l es la caracter\u00edstica de la funci\u00f3n log\u00edstica? Y bueno, que parece un escal\u00f3n, es parecida una cosa que vale cero, si es negativo y uno si es positivo, pero que es continua. Es una linda funci\u00f3n, es una funci\u00f3n smooth. Pero sigue pareciendo un escal\u00f3n. Si yo logro, si estoy de este lado, m\u00e1s seguramente sea negativo y si estoy de este lado sea positivo. Pero puedo derivar a las esas cosas. Las red neuronal usan mucho eso. Y hacemos el chiste de no se puede entrar. No, es para que se sientan m\u00e1s. Bueno. Y bueno, \u00bfy c\u00f3mo clasificamos? Muy es f\u00e1cil. Si la probabilidad de que sea verdadero mayor que la probabilidad que sea falso, dada el atributo, es lo mismo que decir que e a la w por f es mayor que 1. Por esto. \u00bfS\u00ed? Que es lo mismo que decir que w por f sea mayor que 0. Entonces clasificar es muy f\u00e1cil con este m\u00e9todo, porque lo \u00fanico que toca hacer es multiplicar w por f con los pesos que calcul\u00e9 por la feature y si me da mayor que 0 quiere decir que positivo y sino negativo. Eso es la regresi\u00f3n log\u00edstica. Se llama regresi\u00f3n, aunque se llama regresi\u00f3n es un m\u00e9todo de clasificaci\u00f3n. \u00bfDe acuerdo? Y la pregunta es bueno, pero ac\u00e1 yo todav\u00eda no respond\u00ed. \u00bfC\u00f3mo estimaba los pesos que me iba all\u00ed? Se era contando. Ac\u00e1 tengo que hacer algunas cosas un poco m\u00e1s raras. Digo que mi w estimado es el que maximiza este producto de probabilidades de las diferentes clases. Y me queda esta funci\u00f3n s\u00faper rara, s\u00faper fea, s\u00faper complicada, pero que adivinen que es convexa. Y como es convexa, bueno, yo quiero buscar el m\u00e1ximo de una funci\u00f3n convexa, lo mismo que le dec\u00eda hoy. Aplico desde eso por la diente o alg\u00fan otro m\u00e9todo de num\u00e9rico. Entonces tengo una forma de estimar esos w, el asunto que tengo es la forma de estimar. Y \u00bfqu\u00e9 pasa si tengo m\u00e1s de dos clases? Y bueno, tengo que hacer una cosa as\u00ed, calcular la feature a partir de cada clase con cada tributo. Metarlo dentro de la f\u00f3rmula y volver a normalizar. Y por eso nuestro m\u00e9todo se llama multinomial logistic regression, porque es una extensi\u00f3n de la regresi\u00f3n log\u00edstica a un caso de m\u00faltiple clases. Y por supuesto no vamos a quedar con la clase que maximiza la probabilidad de este tributo. \u00bfDe acuerdo? Esta clase es un poquito m\u00e1s, entra m\u00e1s en detalles matem\u00e1ticos que el resto. Me parece importante entender por qu\u00e9 esos atributos aparecen y por qu\u00e9 aparecen todas esas cosas con e. Y las cosas con e generalmente son para cambiar la curva. \u00bfQu\u00e9 pasa si es paiguiente? Y por \u00faltimo, como comentario, \u00bfpor qu\u00e9 se llaman modelos de entrop\u00eda m\u00e1xima? La entrop\u00eda, no s\u00e9 si hablamos algo de entrop\u00eda en alguna clase. La entrop\u00eda es una medida que trata de ver qu\u00e9 tan parecido son los elementos de algo. Entonces, el principio de entrop\u00eda m\u00e1xima dice, bueno, yo si tengo muchas distribuciones posibles, candidatas, algo, el hijo, la que tiene entrop\u00eda m\u00e1xima, es decir, la que da dos mil datos, la que solo asume lo que los datos te dicen. \u00bfQu\u00e9 quiero decir eso? Si yo no conozco nada sobre un documento en el caso del 90-20, 90-10, asumo 90-10 porque puedo asumir a partir de los datos. Yo podr\u00eda asumir 0802 por alg\u00fan motivo, pero si yo no s\u00e9 m\u00e1s que eso, estoy utilizando, o si no s\u00e9 nada, si yo no s\u00e9 nada sobre un documento, no s\u00e9 nada, no tengo ninguna informaci\u00f3n a priori. Y te doy un documento y te digo de qu\u00e9 clase es, es de deporte o es de espect\u00e1culo. \u00bfQu\u00e9 har\u00edan ustedes? Si yo te digo 50-50, eso es aplicar el principio de entrop\u00eda m\u00e1xima, es decir, bueno, yo no tengo informaci\u00f3n, es todo equiprobable. \u00bfSe acuerdan que la entrop\u00eda es m\u00e1xima cuando son todo equiprobable? Si yo agrego un poco de informaci\u00f3n y yo tengo un dado, pero yo te aseguro que el 6 no sale nunca. \u00bfCu\u00e1l es la probabilidad de sacar un 1? Un quinto. O sea, paso de ser un sexto, un quinto, porque tengo m\u00e1s informaci\u00f3n, pero siempre no debo un cuarto, porque no puedo sacarlo de ning\u00fan dato. Eso es el principio de entrop\u00eda m\u00e1xima. Si yo, cuando elijo estas distribuciones posibles, aplico solo lo que los atributos me dicen, aplicando el principio, el que tenga m\u00e1xima entrop\u00eda, a lo que llego es exactamente al mismo modelo que present\u00e9 antes. Por eso tambi\u00e9n los modelos se llaman modelos de entrop\u00eda m\u00e1xima, es porque son dos formas diferentes de llegar a los mismos. Si usted quiere en el detalle, en la literatura est\u00e1 eso. No s\u00e9 si les interesa, pero al que les voy a interesar est\u00e1 muy bien. Coinciden con eso. Coinciden con una distribuci\u00f3n de probabilidad para un modelo log\u00edstico lupinional cuyo peso maximiza la verosimilitud en los datos de entrenamiento. Por ejemplo, si yo quiero aplicar un modelo de entrop\u00eda m\u00e1xima al ejemplo del post time, la feature van a lucir as\u00ed. Tengo una feature 1 que dice vale f1, vale 1 si la palabra es reis y la clase es nombre y si no vale 0. Otra feature va a ser 1 si la anterior es t\u00fa y la clase es verbo y si no es 0. Otra feature y como se imaginar\u00e1n la feature son, estamos hablando de miles o de millones de features, pues son todas las posibles, las relevantes. As\u00ed lucen las features un modelo de entrop\u00eda m\u00e1xima, de un mont\u00f3n de features y yo lo que voy a hacer y adem\u00e1s son indicadores, eso generalmente vale 1 o 0, usualmente. Y lo que yo voy a hacer es, calculando a trav\u00e9s de contando, s\u00ed, voy a calcular los W, con aquello que hablamos hoy de la f\u00f3rmula de minimizarla, o sea que cada feature va a tener un peso indicando qu\u00e9 tanto afecta la feature corresponde para el tag ese. Por ejemplo, si yo tengo aquello, se acuerdan que quer\u00edamos saber qu\u00e9 era reis en el post tag, \u00bfno? Que era la \u00fanica palabra que no sab\u00edamos lo que era. Entonces, estos son los pesos que yo entren\u00e9, lo que me dicen es que, f\u00edjense que los pesos que tenemos ac\u00e1, lo que me dicen es que la feature m\u00e1s importante es la size, en el caso, para que sea, si es un nombre, esta es muy negativa, o sea que resta valor y esta es muy positiva, f2 para un verbo, no me pregunten si son n\u00fameros reales o no m\u00e1s guardas, f2 es, ah, si la previa es t\u00fa, si, la previa es t\u00fa, pesa muy positivamente para que eso sea un verbo, t\u00fa reis, \u00bfno? Y la f6, \u00bfqu\u00e9 es? Y pesa muy negativamente para un nombre, f\u00edjense que hay features diferentes seg\u00fan la clase, porque son m\u00e1s de 1, m\u00e1s de 2, \u00bfde acuerdo? Entonces, yo hago las cuentas, la probabilidad de que sea un nombre dado las features es, es, es exactamente aplicar las features relevantes, ac\u00e1 es 0,8 porque multiplica la prim, la segunda y la sexta, porque son las que aplican a nn, \u00bfno? Si no valen 0. La 2, dijimos, y la 6, son las de t\u00fa, 0,8. No, es la 1 y la 6, la 1 y la 6, correcto. O sea, si reis la palabra, porque la otra no aplica, f\u00edjense que como valen 0 no pasa nada con la multiplicaci\u00f3n, porque yo estoy diciendo e al a eso, \u00bfno? No me molesta el 0 en este caso. Y este es el factor de normalizaci\u00f3n, es simplemente para que esto de 0,20 y 0,8. Sumo esto m\u00e1s esto, sumo todas y bueno, entonces yo busco la clase que m\u00e1s se inicia que en este caso es verbo, \u00bfde acuerdo? Bueno, esos son los modelos de entropia m\u00e1xima. Hay otros modelos discriminativos que lo voy a mencionar r\u00e1pidamente, porque en la forma de aplicarlos es la misma, lo \u00fanico que hay ac\u00e1 de diferente es que es diferente la forma de elegir clasificador, es decir, si ac\u00e1 lo hac\u00edamos por regresi\u00f3n log\u00edstica, el support vector machines, que es un m\u00e9todo que se puso muy de moda en principio de este siglo, en la d\u00e9cada pasada digamos, es un m\u00e9todo que lo que hace es buscar separar linealmente, pero en vez de hacerlo por m\u00ednimos cuadrados, lo que dice es buscar la recta que separa m\u00e1s, que queda m\u00e1s en el medio digamos, la intuici\u00f3n atr\u00e1s de support vector machines que yo busco, si yo tengo los ejemplos as\u00ed, tengo muchas rectas que pasan, \u00bfs\u00ed? Yo trato de encontrar la que maximiza el margen de los que est\u00e1n m\u00e1s cerca y queda en el medio, \u00bfs\u00ed, por eso se llama, los support vectors son estos, son los que est\u00e1n m\u00e1s cerca, los dem\u00e1s, si se fijan, no importan para el clasificador. \u00bfCu\u00e1l es la hip\u00f3tesis del support vector machines y por qu\u00e9 son tan robustos y por qu\u00e9, como est\u00e1n justo en el medio? Quiero decir, si yo meto uno que est\u00e1 ac\u00e1, si un ejemplo a clasificar queda muy cerquita del borde, me puedo equivocar, \u00bfse entiende? Es m\u00e1s probable que me est\u00e9 equivocando, en cambio yo le pongo en el medio y bueno, quedan bastante lejos digamos, y de hecho funcionan muy bien clasificando. Fueron toda una revoluci\u00f3n en la support vector machines, ahora como ahora est\u00e1n de moda la red neuronal en la support vector machines, hicieron lo mismo a principios, agarraron, fueron los primeros m\u00e9todos de discriminativo clasificaci\u00f3n que empezaron a batir todos los r\u00e9cords digamos de diferentes tareas, hasta que pasaron de moda con el tema de las, si bien se usan mucho pasaron de moda con el tema de las red neuronales que volvieron a batirle los r\u00e9cords, pero esencialmente el m\u00e9todo c\u00f3mo se aplica es el mismo, as\u00ed la diferencia es como te\u00f3ricamente como se calcula que est\u00e1 ah\u00ed, hay otros m\u00e9todos de clasificaci\u00f3n, hacen el aprendizaje autom\u00e1tico, los aprenden, vecinos m\u00e1s cercanos, los can\u00edres, que es, clasifico un documento buscando los que est\u00e1n m\u00e1s cerca del punto de vista tribu, calculo una distancia entre documentos y me quedo con los que est\u00e1n m\u00e1s cercanos, \u00bfno? Es como la idea de, bueno si este est\u00e1 ac\u00e1, \u00bfqui\u00e9nes son los vecinos m\u00e1s cercanos? Y bueno, supongamos que los tres vecinos m\u00e1s cercanos son estos, en este caso los tres son circulitos, o sea que eso seguramente sea un circulito, podemos tener problemas cuando estamos los m\u00e9todos de vecinos m\u00e1s cercanos tienen la ventaja, obviamente, de que pueden reconocer, pueden reconocer cl\u00e1steres, los m\u00e9todos de vecinos m\u00e1s cercanos definen una cosa as\u00ed, no, pero, la cosa as\u00ed, pueden reconocer cosas que no son lineales, tienen el problema de que a veces sobreajustan demasiado, \u00e1rboles de decisi\u00f3n que no son muy realizados en el procesamiento de la imaginaci\u00f3n, rando fores que son como una, hay muchos, muchos m\u00e9todos de clasificaci\u00f3n, pero en todos lo que tienen en com\u00fan es que las medidas para realizar, para el m\u00e9todolog\u00eda es la que hay en la clasificaci\u00f3n pasada. Y eso desde el punto de vista de los m\u00e9todos de clasificaci\u00f3n puros, pero tambi\u00e9n se acuerdan que hab\u00edamos visto los m\u00e9todos de clasificaci\u00f3n secuencial, cuando yo quiero asignar una secuencia de tangs, de clases, asumo que mi atributo tiene una secuencia, mi instancia es una secuencia, por ejemplo, una oraci\u00f3n, que es una secuencia de palabra, y quiero asignar una secuencia de tangs, bueno, hay versiones generativas, en el caso de los, la versi\u00f3n generativa de Naive Bayes son los hidden Marco Models, que lo vimos bastante en detalle en alguna clase anterior, y hay una versi\u00f3n tambi\u00e9n de clasificadores secuenciales, que estos son por lejos los que bandan mejor, que son los temas secuenciales, que son los conditions al random fields, los conditions al random field tambi\u00e9n fueron una novedad en los temas de clasificaci\u00f3n secuencial, porque andan mucho mejor el general que los hidden Marco Models, y tienen una, son como una versi\u00f3n, una versi\u00f3n secuencial del modelo entropi\u00e9s m\u00e1ximo, no, no, no, no esperen que entren detalle, tampoco conozco mucho la detalle, el matem\u00e1tico del del conditional random fields, pero como herramienta digamos para el clasificaci\u00f3n de secuencias funciona muy bien. Yo dir\u00eda que si uno va a, a ver si me queda algo m\u00e1s, no, ac\u00e1 tienen un poco de show jugar, s\u00ed, de estas cosas. Hueca, sirve para jugar, pero es juguete en general. Salkill Learnes es una herramienta bastante, una librer\u00eda bastante polenta de, en Python y que est\u00e1 bastante de moda. Y ac\u00e1 me faltan, me faltan todas las nuevas bolas de bibliotecas de Dib Learning, \u00bfno?, de que son de Red Lunar y que son Torch, este, Teano, Keras, TensorFlow. Pero bueno, Salkill Learnes es una biblioteca de, de gen\u00e9rica, de Machine Learning en Python, Orange tambi\u00e9n. En Iletek\u00e1 es m\u00e1s de procedimiento en lenguaje natural, pero tiene por ejemplo un plazificador exceciano. CRF m\u00e1s m\u00e1s es un Toolkit para Condition Random Fields. PyBrain, creo que no, no, no corre m\u00e1s o no s\u00e9, que es Red Neuronal y Homebiteon. SMelite era la herramienta de Support Vector Machines, cuando estaba en moda. SMelite es el 99 para que se dieron una idea y estaba bastante estable porque no hay mucho para, es muy sencillo el modelo de la Support Vector Machines, por lo grande como me lo aplico. Yo dir\u00eda que, que si, si, si vamos a lo que, a lo que es el procedimiento en lenguaje natural a nivel de, a nivel de, como decir, de mercado o de herramienta o de, no me sabe la palabra, de industria, digamos, a nivel industrial, no es la palabra correcta pero est\u00e1. Yo dir\u00eda que el procedimiento en lenguaje natural est\u00e1 en lo que hemos aprendido hasta ahora. No? Es decir, todas estas cosas que hemos aprendido en las clases hasta ahora ya se encuentra a nivel industrial. A nivel industrial estoy hablando de las compa\u00f1\u00edas de Intermed, no? Reconocimiento de, en, reconocimiento de caracteres mal escrito, clasificaci\u00f3n, clasificaci\u00f3n de, sentimenta an\u00e1lisis, de todo lo que hemos hablado hasta ahora, no? En el grama y todas esas cosas. Tambi\u00e9n, a ver, no es lo \u00fanico, no? Machine Translation es un ejemplo de cosas que andan muy bien. Este, pero utilizan m\u00e9todos m\u00e1s o menos hasta ac\u00e1. Lo que quiero decir es que, y bueno, y ah\u00ed hay alg\u00fan componente sem\u00e1ntico tambi\u00e9n que lo van a ver despu\u00e9s con Luis, pero esas cosas m\u00e1s avanzadas, digamos, reci\u00e9n, reci\u00e9n se est\u00e1 empezando a hablar, pero en algunas cosas de, por ejemplo, reconocimiento de entidades, no? El otro d\u00eda lo ve\u00eda en un diario, digamos, unos dos a\u00f1os atr\u00e1s, que algo que dec\u00eda que era una aplicaci\u00f3n que reconoc\u00eda a partir del New York Times lugares, bueno, Google lo hace, no? Lugares y cuando arma las citas, cuando a partir de un correo te lo meten en el calendario, ah\u00ed lo que est\u00e1 haciendo es reconocimiento de entidades. Est\u00e1 reconociendo que dice el jueves 23, cena con tal y lo est\u00e1 viendo, est\u00e1 haciendo clasificaci\u00f3n secuencial, pero eso son cosas que en el academia est\u00e1n como hace como 10 a\u00f1os, digamos, los condillos hablando de FIT tienen como 10 a\u00f1os, reci\u00e9n est\u00e1n empezando como entrares, el tipo costo. Y hay cosas que todav\u00eda est\u00e1 por verse c\u00f3mo se van a incorporar, que son las que vamos a ver de ahora en adelante, que son el parsing, o sea, an\u00e1lisis m\u00e1s complejo, ni que hablar de an\u00e1lisis sem\u00e1ntico m\u00e1s all\u00e1 de la sem\u00e1ntica de palabras, son cosas que vamos a ir viendo despu\u00e9s, o sea, hay mucho todav\u00eda para mejorar y en el academia tambi\u00e9n, porque no est\u00e1 en todo, resuelto ni mucho menos. Por ejemplo, en el poder analizar sem\u00e1nticamente las cosas, estamos bastante lejos. Pero lo que quer\u00eda transmitir es que esto de la clasificaci\u00f3n es lo m\u00e1s que anda en la vuelta, digamos, \u00bfno? Y que con esto se va a hacer un mont\u00f3n de cosas. Bueno, clases que vienen arrancamos con parci, \u00bfs\u00ed? Gracias.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 25.26, "text": " La clase pasada estuvimos viendo una metodolog\u00eda de clasificaci\u00f3n en general, as\u00ed para cualquier", "tokens": [50364, 2369, 44578, 1736, 1538, 49777, 8372, 34506, 2002, 1131, 378, 29987, 368, 596, 296, 40802, 465, 2674, 11, 8582, 1690, 21004, 51627], "temperature": 0.0, "avg_logprob": -0.23375059127807618, "compression_ratio": 1.0879120879120878, "no_speech_prob": 0.13968464732170105}, {"id": 1, "seek": 2526, "start": 25.26, "end": 36.620000000000005, "text": " problema de clasificaci\u00f3n, especialmente c\u00f3mo separar el corpus, qu\u00e9 medidas utilizar. Una", "tokens": [50364, 12395, 368, 596, 296, 40802, 11, 41546, 12826, 3128, 289, 806, 1181, 31624, 11, 8057, 37295, 24060, 13, 15491, 50932], "temperature": 0.0, "avg_logprob": -0.23331860414485342, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.6084257364273071}, {"id": 2, "seek": 2526, "start": 36.620000000000005, "end": 43.22, "text": " cantidad de aspectos metodol\u00f3gicos que son muy importantes y que lo hicimos independiente del", "tokens": [50932, 33757, 368, 4171, 329, 1131, 378, 27629, 9940, 631, 1872, 5323, 27963, 288, 631, 450, 23697, 8372, 4819, 8413, 1103, 51262], "temperature": 0.0, "avg_logprob": -0.23331860414485342, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.6084257364273071}, {"id": 3, "seek": 2526, "start": 43.22, "end": 46.620000000000005, "text": " dominio en el que estamos, que es el depresamiento de lenguajas natural porque aplica para cualquier", "tokens": [51262, 8859, 1004, 465, 806, 631, 10382, 11, 631, 785, 806, 1367, 495, 16971, 368, 35044, 84, 1805, 296, 3303, 4021, 25522, 2262, 1690, 21004, 51432], "temperature": 0.0, "avg_logprob": -0.23331860414485342, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.6084257364273071}, {"id": 4, "seek": 2526, "start": 46.620000000000005, "end": 52.02, "text": " problema de clasificaci\u00f3n. Problemas de clasificaci\u00f3n y los m\u00e9todos a aplicar a utilizar se pueden", "tokens": [51432, 12395, 368, 596, 296, 40802, 13, 11676, 296, 368, 596, 296, 40802, 288, 1750, 20275, 378, 329, 257, 18221, 289, 257, 24060, 369, 14714, 51702], "temperature": 0.0, "avg_logprob": -0.23331860414485342, "compression_ratio": 1.6926406926406927, "no_speech_prob": 0.6084257364273071}, {"id": 5, "seek": 5202, "start": 52.02, "end": 56.82, "text": " definir en general. De hecho, en el curso de aprendizaje autom\u00e1tico, ustedes aprenden con m\u00e1s", "tokens": [50364, 1561, 347, 465, 2674, 13, 1346, 13064, 11, 465, 806, 31085, 368, 21003, 590, 11153, 3553, 28234, 11, 17110, 21003, 268, 416, 3573, 50604], "temperature": 0.0, "avg_logprob": -0.2032380505142925, "compression_ratio": 1.8625592417061612, "no_speech_prob": 0.11885479092597961}, {"id": 6, "seek": 5202, "start": 56.82, "end": 60.86, "text": " detalle lo que vimos en parte del curso de aprendizaje autom\u00e1tico, aprenden con m\u00e1s detalle lo que", "tokens": [50604, 1141, 11780, 450, 631, 49266, 465, 6975, 1103, 31085, 368, 21003, 590, 11153, 3553, 28234, 11, 21003, 268, 416, 3573, 1141, 11780, 450, 631, 50806], "temperature": 0.0, "avg_logprob": -0.2032380505142925, "compression_ratio": 1.8625592417061612, "no_speech_prob": 0.11885479092597961}, {"id": 7, "seek": 5202, "start": 60.86, "end": 70.18, "text": " ayer vimos en una clase sola. Porque se ven diferentes m\u00e9todos, excluimos cu\u00e1l era el m\u00e9todo en", "tokens": [50806, 257, 7224, 49266, 465, 2002, 44578, 34162, 13, 11287, 369, 6138, 17686, 20275, 378, 329, 11, 1624, 2781, 8372, 44318, 4249, 806, 20275, 17423, 465, 51272], "temperature": 0.0, "avg_logprob": -0.2032380505142925, "compression_ratio": 1.8625592417061612, "no_speech_prob": 0.11885479092597961}, {"id": 8, "seek": 5202, "start": 70.18, "end": 79.5, "text": " particular y hablamos en general un clasificador, un clasificador supervisado, dijimos aquel caso", "tokens": [51272, 1729, 288, 26280, 2151, 465, 2674, 517, 596, 296, 1089, 5409, 11, 517, 596, 296, 1089, 5409, 34409, 1573, 11, 47709, 8372, 2373, 338, 9666, 51738], "temperature": 0.0, "avg_logprob": -0.2032380505142925, "compression_ratio": 1.8625592417061612, "no_speech_prob": 0.11885479092597961}, {"id": 9, "seek": 7950, "start": 79.5, "end": 86.46, "text": " donde yo tengo un conjunto de instancias de cosas, un conjunto de clases discreto y tengo que", "tokens": [50364, 10488, 5290, 13989, 517, 37776, 368, 1058, 282, 12046, 368, 12218, 11, 517, 37776, 368, 596, 1957, 2983, 47330, 288, 13989, 631, 50712], "temperature": 0.0, "avg_logprob": -0.14462832094148825, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.12819652259349823}, {"id": 10, "seek": 7950, "start": 86.46, "end": 93.74, "text": " asignarle a cada instancia, la tarea de desasignarle a cada una de esas instancias uno del grupo de", "tokens": [50712, 382, 788, 36153, 257, 8411, 1058, 22862, 11, 635, 256, 35425, 368, 730, 296, 788, 36153, 257, 8411, 2002, 368, 23388, 1058, 282, 12046, 8526, 1103, 20190, 368, 51076], "temperature": 0.0, "avg_logprob": -0.14462832094148825, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.12819652259349823}, {"id": 11, "seek": 7950, "start": 93.74, "end": 98.06, "text": " clases. Si yo tengo un conjunto de documentos y quiero saber en qu\u00e9 idioma est\u00e1 lo que estoy", "tokens": [51076, 596, 1957, 13, 4909, 5290, 13989, 517, 37776, 368, 4166, 329, 288, 16811, 12489, 465, 8057, 18014, 6440, 3192, 450, 631, 15796, 51292], "temperature": 0.0, "avg_logprob": -0.14462832094148825, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.12819652259349823}, {"id": 12, "seek": 7950, "start": 98.06, "end": 103.18, "text": " haciendo es un problema de clasificaci\u00f3n. Tengo el conjunto de los documentos, tengo las clases", "tokens": [51292, 20509, 785, 517, 12395, 368, 596, 296, 40802, 13, 314, 30362, 806, 37776, 368, 1750, 4166, 329, 11, 13989, 2439, 596, 1957, 51548], "temperature": 0.0, "avg_logprob": -0.14462832094148825, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.12819652259349823}, {"id": 13, "seek": 7950, "start": 103.18, "end": 107.7, "text": " que son los idiomas posibles y yo tengo que a cada uno asociarle una clase. Podr\u00eda eventualmente", "tokens": [51548, 631, 1872, 1750, 18014, 7092, 1366, 14428, 288, 5290, 13989, 631, 257, 8411, 8526, 382, 78, 537, 36153, 2002, 44578, 13, 12646, 37183, 33160, 4082, 51774], "temperature": 0.0, "avg_logprob": -0.14462832094148825, "compression_ratio": 1.9397590361445782, "no_speech_prob": 0.12819652259349823}, {"id": 14, "seek": 10770, "start": 107.7, "end": 112.26, "text": " ser m\u00e1s de una clase, podemos tener un problema multiclase, es decir hay variantes \u00bfno? Yo podr\u00eda", "tokens": [50364, 816, 3573, 368, 2002, 44578, 11, 12234, 11640, 517, 12395, 30608, 75, 651, 11, 785, 10235, 4842, 3034, 9327, 3841, 1771, 30, 7616, 27246, 50592], "temperature": 0.0, "avg_logprob": -0.20879319449451483, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.07472072541713715}, {"id": 15, "seek": 10770, "start": 112.26, "end": 119.18, "text": " decir que a cada documento era signo m\u00e1s de una clase, por ejemplo si lo quiero clasificar el", "tokens": [50592, 10235, 631, 257, 8411, 4166, 78, 4249, 1465, 78, 3573, 368, 2002, 44578, 11, 1515, 13358, 1511, 450, 16811, 596, 296, 25625, 806, 50938], "temperature": 0.0, "avg_logprob": -0.20879319449451483, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.07472072541713715}, {"id": 16, "seek": 10770, "start": 119.18, "end": 125.74000000000001, "text": " t\u00f3pico de un documento, esto puede ser de espect\u00e1culos y de deportes o estamos hablando de", "tokens": [50938, 256, 47773, 2789, 368, 517, 4166, 78, 11, 7433, 8919, 816, 368, 38244, 842, 32397, 288, 368, 33485, 279, 277, 10382, 29369, 368, 51266], "temperature": 0.0, "avg_logprob": -0.20879319449451483, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.07472072541713715}, {"id": 17, "seek": 10770, "start": 125.74000000000001, "end": 136.74, "text": " guandanar a poner. En la clase de hoy lo que vamos a ver es vamos a hablar de los m\u00e9todos que hay", "tokens": [51266, 695, 39762, 289, 257, 19149, 13, 2193, 635, 44578, 368, 13775, 450, 631, 5295, 257, 1306, 785, 5295, 257, 21014, 368, 1750, 20275, 378, 329, 631, 4842, 51816], "temperature": 0.0, "avg_logprob": -0.20879319449451483, "compression_ratio": 1.646808510638298, "no_speech_prob": 0.07472072541713715}, {"id": 18, "seek": 13674, "start": 136.74, "end": 142.42000000000002, "text": " de clasificaci\u00f3n de algunos m\u00e9todos y de c\u00f3mo se aplican algunas tareas del procesamiento de", "tokens": [50364, 368, 596, 296, 40802, 368, 21078, 20275, 378, 329, 288, 368, 12826, 369, 18221, 282, 27316, 49423, 296, 1103, 17565, 16971, 368, 50648], "temperature": 0.0, "avg_logprob": -0.1866684370143439, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.007736380212008953}, {"id": 19, "seek": 13674, "start": 142.42000000000002, "end": 147.18, "text": " lenguaje natural. Vamos a hablar un poco de las caracter\u00edsticas del m\u00e9todo y de c\u00f3mo intanciarlo", "tokens": [50648, 35044, 84, 11153, 3303, 13, 10894, 257, 21014, 517, 10639, 368, 2439, 47990, 1103, 20275, 17423, 288, 368, 12826, 560, 38840, 19457, 50886], "temperature": 0.0, "avg_logprob": -0.1866684370143439, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.007736380212008953}, {"id": 20, "seek": 13674, "start": 147.18, "end": 155.10000000000002, "text": " en alg\u00fan caso de ejemplo. Como yo dec\u00eda en la clase pasada, los m\u00e9todos de clasificaci\u00f3n", "tokens": [50886, 465, 26300, 9666, 368, 13358, 13, 11913, 5290, 37599, 465, 635, 44578, 1736, 1538, 11, 1750, 20275, 378, 329, 368, 596, 296, 40802, 51282], "temperature": 0.0, "avg_logprob": -0.1866684370143439, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.007736380212008953}, {"id": 21, "seek": 13674, "start": 155.10000000000002, "end": 163.94, "text": " est\u00e1n muy difundidos en todos los diferentes an\u00e1lisis porque generalmente los elementos", "tokens": [51282, 10368, 5323, 679, 997, 7895, 465, 6321, 1750, 17686, 44113, 28436, 4021, 2674, 4082, 1750, 35797, 51724], "temperature": 0.0, "avg_logprob": -0.1866684370143439, "compression_ratio": 1.6434782608695653, "no_speech_prob": 0.007736380212008953}, {"id": 22, "seek": 16394, "start": 163.94, "end": 168.02, "text": " de dominio con lo que trabajamos son discretos, las palabras, las oraciones, los documentos,", "tokens": [50364, 368, 8859, 1004, 416, 450, 631, 9618, 2151, 1872, 2983, 1505, 329, 11, 2439, 35240, 11, 2439, 420, 9188, 11, 1750, 4166, 329, 11, 50568], "temperature": 0.0, "avg_logprob": -0.26804423014322915, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.014925514347851276}, {"id": 23, "seek": 16394, "start": 168.02, "end": 177.26, "text": " los tweets son todas cosas discretas. Entonces en general vamos a ver m\u00e9todos de clasificaci\u00f3n", "tokens": [50568, 1750, 25671, 1872, 10906, 12218, 2983, 1505, 296, 13, 15097, 465, 2674, 5295, 257, 1306, 20275, 378, 329, 368, 596, 296, 40802, 51030], "temperature": 0.0, "avg_logprob": -0.26804423014322915, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.014925514347851276}, {"id": 24, "seek": 16394, "start": 177.26, "end": 185.18, "text": " supervisadas. Si yo quisiera por ejemplo un ejemplo concreto, un proyecto que tuvimos el a\u00f1o", "tokens": [51030, 34409, 6872, 13, 4909, 5290, 37945, 10609, 1515, 13358, 517, 13358, 1588, 47330, 11, 517, 32285, 631, 38177, 8372, 806, 15984, 51426], "temperature": 0.0, "avg_logprob": -0.26804423014322915, "compression_ratio": 1.5053191489361701, "no_speech_prob": 0.014925514347851276}, {"id": 25, "seek": 18518, "start": 185.18, "end": 194.54000000000002, "text": " pasado que era que clasificaba un tweet si era un chiste o no, esa era una tarea de clasificaci\u00f3n,", "tokens": [50364, 24794, 631, 4249, 631, 596, 296, 1089, 5509, 517, 15258, 1511, 4249, 517, 417, 8375, 277, 572, 11, 11342, 4249, 2002, 256, 35425, 368, 596, 296, 40802, 11, 50832], "temperature": 0.0, "avg_logprob": -0.21258960050695083, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.26563432812690735}, {"id": 26, "seek": 18518, "start": 194.54000000000002, "end": 202.34, "text": " una tarea que tambi\u00e9n encaramos aunque no con demasiado \u00e9xito, era la de calificar el chiste", "tokens": [50832, 2002, 256, 35425, 631, 6407, 2058, 289, 2151, 21962, 572, 416, 39820, 1136, 87, 3528, 11, 4249, 635, 368, 2104, 25625, 806, 417, 8375, 51222], "temperature": 0.0, "avg_logprob": -0.21258960050695083, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.26563432812690735}, {"id": 27, "seek": 18518, "start": 202.34, "end": 210.14000000000001, "text": " en un rango, en un, por vacinar un valor de qu\u00e9 tan bueno estaba, digamos, si se pod\u00eda llegar", "tokens": [51222, 465, 517, 367, 17150, 11, 465, 517, 11, 1515, 2842, 6470, 517, 15367, 368, 8057, 7603, 11974, 17544, 11, 36430, 11, 1511, 369, 45588, 24892, 51612], "temperature": 0.0, "avg_logprob": -0.21258960050695083, "compression_ratio": 1.6022099447513811, "no_speech_prob": 0.26563432812690735}, {"id": 28, "seek": 21014, "start": 210.29999999999998, "end": 215.89999999999998, "text": " a capturar eso y ah\u00ed si yo como lo planteamos nosotros era que vos le pod\u00edas poner una,", "tokens": [50372, 257, 3770, 28586, 7287, 288, 12571, 1511, 5290, 2617, 450, 36829, 2151, 13863, 4249, 631, 13845, 476, 2497, 10025, 19149, 2002, 11, 50652], "temperature": 0.0, "avg_logprob": -0.2670988176689773, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.5594475865364075}, {"id": 29, "seek": 21014, "start": 215.89999999999998, "end": 220.29999999999998, "text": " dos, tres, cuatro, cinco estrellas, eso sigue siendo un problema de clasificaci\u00f3n supervisada,", "tokens": [50652, 4491, 11, 15890, 11, 28795, 11, 21350, 871, 19771, 296, 11, 7287, 34532, 31423, 517, 12395, 368, 596, 296, 40802, 34409, 1538, 11, 50872], "temperature": 0.0, "avg_logprob": -0.2670988176689773, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.5594475865364075}, {"id": 30, "seek": 21014, "start": 220.29999999999998, "end": 225.05999999999997, "text": " pero si esto yo lo considerara un continuo, ah\u00ed tendr\u00edamos un problema de regresi\u00f3n,", "tokens": [50872, 4768, 1511, 7433, 5290, 450, 1949, 2419, 517, 2993, 78, 11, 12571, 3928, 81, 16275, 517, 12395, 368, 47108, 2560, 11, 51110], "temperature": 0.0, "avg_logprob": -0.2670988176689773, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.5594475865364075}, {"id": 31, "seek": 21014, "start": 225.05999999999997, "end": 229.89999999999998, "text": " no son usuales, los problemas de regresi\u00f3n de pasamiento no van a que natural porque nuestros", "tokens": [51110, 572, 1872, 505, 901, 279, 11, 1750, 20720, 368, 47108, 2560, 368, 1736, 16971, 572, 3161, 257, 631, 3303, 4021, 24099, 51352], "temperature": 0.0, "avg_logprob": -0.2670988176689773, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.5594475865364075}, {"id": 32, "seek": 21014, "start": 229.89999999999998, "end": 236.42, "text": " ni\u00f1os generalmente son discretos. Bueno, pero vamos a m\u00e9todo de clasificaci\u00f3n supervisada y en", "tokens": [51352, 30712, 2674, 4082, 1872, 2983, 1505, 329, 13, 16046, 11, 4768, 5295, 257, 20275, 17423, 368, 596, 296, 40802, 34409, 1538, 288, 465, 51678], "temperature": 0.0, "avg_logprob": -0.2670988176689773, "compression_ratio": 1.7651515151515151, "no_speech_prob": 0.5594475865364075}, {"id": 33, "seek": 23642, "start": 236.42, "end": 244.06, "text": " particular vamos a hablar de m\u00e9todos probabilistas. Los m\u00e9todos probabilistas en general tenemos", "tokens": [50364, 1729, 5295, 257, 21014, 368, 20275, 378, 329, 31959, 14858, 13, 7632, 20275, 378, 329, 31959, 14858, 465, 2674, 9914, 50746], "temperature": 0.0, "avg_logprob": -0.17828452587127686, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.04656432941555977}, {"id": 34, "seek": 23642, "start": 244.06, "end": 248.57999999999998, "text": " la instancia, o sea yo no voy a volver sobre la terminolog\u00eda que vimos hace pasada, tenemos la", "tokens": [50746, 635, 1058, 22862, 11, 277, 4158, 5290, 572, 7552, 257, 33998, 5473, 635, 10761, 29987, 631, 49266, 10032, 1736, 1538, 11, 9914, 635, 50972], "temperature": 0.0, "avg_logprob": -0.17828452587127686, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.04656432941555977}, {"id": 35, "seek": 23642, "start": 248.57999999999998, "end": 254.82, "text": " instancia representada por atributos y queremos asignarlo a una clase, pero adem\u00e1s los m\u00e9todos", "tokens": [50972, 1058, 22862, 2906, 1538, 1515, 412, 2024, 34640, 288, 26813, 382, 788, 19457, 257, 2002, 44578, 11, 4768, 21251, 1750, 20275, 378, 329, 51284], "temperature": 0.0, "avg_logprob": -0.17828452587127686, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.04656432941555977}, {"id": 36, "seek": 23642, "start": 254.82, "end": 261.98, "text": " probabilistas lo que hacen es asignarle una probabilidad a cada clase posible. Entonces yo no", "tokens": [51284, 31959, 14858, 450, 631, 27434, 785, 382, 788, 36153, 2002, 31959, 4580, 257, 8411, 44578, 26644, 13, 15097, 5290, 572, 51642], "temperature": 0.0, "avg_logprob": -0.17828452587127686, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.04656432941555977}, {"id": 37, "seek": 26198, "start": 262.02000000000004, "end": 273.54, "text": " solo te digo esta instancia, esta instancia, este tweet es humor\u00edstico, sino que te digo este", "tokens": [50366, 6944, 535, 22990, 5283, 1058, 22862, 11, 5283, 1058, 22862, 11, 4065, 15258, 785, 14318, 19512, 2789, 11, 18108, 631, 535, 22990, 4065, 50942], "temperature": 0.0, "avg_logprob": -0.24680262247721355, "compression_ratio": 1.5919540229885059, "no_speech_prob": 0.013616307638585567}, {"id": 38, "seek": 26198, "start": 273.54, "end": 283.78000000000003, "text": " tweet tiene un 85% de chances en humor\u00edstico y no humor\u00edstico un 15%. Y esto por supuesto", "tokens": [50942, 15258, 7066, 517, 14695, 4, 368, 10486, 465, 14318, 19512, 2789, 288, 572, 14318, 19512, 2789, 517, 2119, 6856, 398, 7433, 1515, 34177, 51454], "temperature": 0.0, "avg_logprob": -0.24680262247721355, "compression_ratio": 1.5919540229885059, "no_speech_prob": 0.013616307638585567}, {"id": 39, "seek": 26198, "start": 283.78000000000003, "end": 289.94, "text": " tiene que ser una distribuci\u00f3n de probabilidad, sumar uno y tal, mayor que cero. Entonces", "tokens": [51454, 7066, 631, 816, 2002, 4400, 30813, 368, 31959, 4580, 11, 2408, 289, 8526, 288, 4023, 11, 10120, 631, 269, 2032, 13, 15097, 51762], "temperature": 0.0, "avg_logprob": -0.24680262247721355, "compression_ratio": 1.5919540229885059, "no_speech_prob": 0.013616307638585567}, {"id": 40, "seek": 29198, "start": 292.02000000000004, "end": 296.70000000000005, "text": " y adem\u00e1s los m\u00e9todos probabilistas intentan obtener una distribuci\u00f3n sobre las clases", "tokens": [50366, 288, 21251, 1750, 20275, 378, 329, 31959, 14858, 8446, 282, 28326, 260, 2002, 4400, 30813, 5473, 2439, 596, 1957, 50600], "temperature": 0.0, "avg_logprob": -0.20197448247595678, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.0012074466794729233}, {"id": 41, "seek": 29198, "start": 296.70000000000005, "end": 297.62, "text": " dado en los atributos.", "tokens": [50600, 29568, 465, 1750, 412, 2024, 34640, 13, 50646], "temperature": 0.0, "avg_logprob": -0.20197448247595678, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.0012074466794729233}, {"id": 42, "seek": 29198, "start": 303.98, "end": 309.18, "text": " Y por supuesto clasificar en general va a ser, uno va a elegir la clase con la probabilidad m\u00e1s alta.", "tokens": [50964, 398, 1515, 34177, 596, 296, 25625, 465, 2674, 2773, 257, 816, 11, 8526, 2773, 257, 14459, 347, 635, 44578, 416, 635, 31959, 4580, 3573, 26495, 13, 51224], "temperature": 0.0, "avg_logprob": -0.20197448247595678, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.0012074466794729233}, {"id": 43, "seek": 29198, "start": 310.62, "end": 315.38, "text": " As\u00ed es que no quiere simplemente dejar de volver esa distribuci\u00f3n para que otra etapa", "tokens": [51296, 17419, 785, 631, 572, 23877, 33190, 24391, 368, 33998, 11342, 4400, 30813, 1690, 631, 13623, 1030, 7961, 51534], "temperature": 0.0, "avg_logprob": -0.20197448247595678, "compression_ratio": 1.5175879396984924, "no_speech_prob": 0.0012074466794729233}, {"id": 44, "seek": 31538, "start": 315.38, "end": 328.62, "text": " del proceso lo utilice. Yo tengo la posibilidad de hacer eso. Los m\u00e9todos generativos, que son", "tokens": [50364, 1103, 29314, 450, 4976, 573, 13, 7616, 13989, 635, 1366, 33989, 368, 6720, 7287, 13, 7632, 20275, 378, 329, 1337, 36945, 11, 631, 1872, 51026], "temperature": 0.0, "avg_logprob": -0.19470143008541752, "compression_ratio": 1.510989010989011, "no_speech_prob": 0.057638201862573624}, {"id": 45, "seek": 31538, "start": 328.62, "end": 333.86, "text": " uno de los tipos de m\u00e9todos que hay, lo que intentan es, son los que hemos estado viendo", "tokens": [51026, 8526, 368, 1750, 37105, 368, 20275, 378, 329, 631, 4842, 11, 450, 631, 8446, 282, 785, 11, 1872, 1750, 631, 15396, 18372, 34506, 51288], "temperature": 0.0, "avg_logprob": -0.19470143008541752, "compression_ratio": 1.510989010989011, "no_speech_prob": 0.057638201862573624}, {"id": 46, "seek": 31538, "start": 333.86, "end": 337.9, "text": " hasta ahora en general y es lo que tratan de modelar la distribuci\u00f3n conjunta, es decir,", "tokens": [51288, 10764, 9923, 465, 2674, 288, 785, 450, 631, 21507, 282, 368, 2316, 289, 635, 4400, 30813, 18244, 1328, 11, 785, 10235, 11, 51490], "temperature": 0.0, "avg_logprob": -0.19470143008541752, "compression_ratio": 1.510989010989011, "no_speech_prob": 0.057638201862573624}, {"id": 47, "seek": 33790, "start": 337.9, "end": 350.17999999999995, "text": " la clase junto con los atributos, \u00bfs\u00ed? Y las etiquetas, \u00bfde acuerdo? \u00bfPor qu\u00e9? Porque es lo que", "tokens": [50364, 635, 44578, 24663, 416, 1750, 412, 2024, 34640, 11, 3841, 82, 870, 30, 398, 2439, 42177, 35120, 11, 3841, 1479, 28113, 30, 3841, 24907, 8057, 30, 11287, 785, 450, 631, 50978], "temperature": 0.0, "avg_logprob": -0.23329386991613052, "compression_ratio": 1.5, "no_speech_prob": 0.14316505193710327}, {"id": 48, "seek": 33790, "start": 350.17999999999995, "end": 356.58, "text": " necesitan para, a partir de la regla de Valle. Es decir, yo quiero la clase dada del conjunto de", "tokens": [50978, 11909, 9670, 1690, 11, 257, 13906, 368, 635, 1121, 875, 368, 691, 11780, 13, 2313, 10235, 11, 5290, 16811, 635, 44578, 274, 1538, 1103, 37776, 368, 51298], "temperature": 0.0, "avg_logprob": -0.23329386991613052, "compression_ratio": 1.5, "no_speech_prob": 0.14316505193710327}, {"id": 49, "seek": 33790, "start": 356.58, "end": 359.9, "text": " features, \u00bfse acuerdan que la feature era nuestra representaci\u00f3n del documento, \u00bfno?", "tokens": [51298, 4122, 11, 3841, 405, 696, 5486, 10312, 631, 635, 4111, 4249, 16825, 2906, 3482, 1103, 4166, 78, 11, 3841, 1771, 30, 51464], "temperature": 0.0, "avg_logprob": -0.23329386991613052, "compression_ratio": 1.5, "no_speech_prob": 0.14316505193710327}, {"id": 50, "seek": 35990, "start": 360.21999999999997, "end": 367.94, "text": " Caracter\u00edstica que, Valle a la redundancia caracterizaban al documento. Entonces, la probabilidad", "tokens": [50380, 2741, 14125, 19512, 2262, 631, 11, 691, 11780, 257, 635, 27830, 22862, 28760, 590, 18165, 419, 4166, 78, 13, 15097, 11, 635, 31959, 4580, 50766], "temperature": 0.0, "avg_logprob": -0.22777092122585973, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0631423220038414}, {"id": 51, "seek": 35990, "start": 367.94, "end": 373.58, "text": " de la clase dada de los atributos es igual, la probabilidad conjunta dividida de la probabilidad", "tokens": [50766, 368, 635, 44578, 274, 1538, 368, 1750, 412, 2024, 34640, 785, 10953, 11, 635, 31959, 4580, 18244, 1328, 4996, 2887, 368, 635, 31959, 4580, 51048], "temperature": 0.0, "avg_logprob": -0.22777092122585973, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0631423220038414}, {"id": 52, "seek": 35990, "start": 373.58, "end": 378.5, "text": " de los atributos, \u00bfs\u00ed? Por definici\u00f3n, por la definici\u00f3n de probabilidad condicional.", "tokens": [51048, 368, 1750, 412, 2024, 34640, 11, 3841, 82, 870, 30, 5269, 1561, 15534, 11, 1515, 635, 1561, 15534, 368, 31959, 4580, 2224, 33010, 13, 51294], "temperature": 0.0, "avg_logprob": -0.22777092122585973, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0631423220038414}, {"id": 53, "seek": 35990, "start": 383.17999999999995, "end": 387.94, "text": " \u00bfDe acuerdo? Entonces lo que tratan de modelar es esto. \u00bfPor qu\u00e9 lo hacen? \u00bfPor qu\u00e9 esta", "tokens": [51528, 3841, 11089, 28113, 30, 15097, 450, 631, 21507, 282, 368, 2316, 289, 785, 7433, 13, 3841, 24907, 8057, 450, 27434, 30, 3841, 24907, 8057, 5283, 51766], "temperature": 0.0, "avg_logprob": -0.22777092122585973, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0631423220038414}, {"id": 54, "seek": 38794, "start": 387.94, "end": 392.86, "text": " probabilidad generalmente son m\u00e1s f\u00e1ciles de estimar que las otras? \u00bfPor qu\u00e9 la puedo", "tokens": [50364, 31959, 4580, 2674, 4082, 1872, 3573, 17474, 279, 368, 8017, 289, 631, 2439, 20244, 30, 3841, 24907, 8057, 635, 21612, 50610], "temperature": 0.0, "avg_logprob": -0.2337682088216146, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.009773598983883858}, {"id": 55, "seek": 38794, "start": 392.86, "end": 399.46, "text": " estimar contando m\u00e1s f\u00e1cilmente? \u00bfPor qu\u00e9? Porque f\u00edjense que yo como condiciono en dada", "tokens": [50610, 8017, 289, 660, 1806, 3573, 17474, 4082, 30, 3841, 24907, 8057, 30, 11287, 283, 870, 73, 1288, 631, 5290, 2617, 2224, 18899, 78, 465, 274, 1538, 50940], "temperature": 0.0, "avg_logprob": -0.2337682088216146, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.009773598983883858}, {"id": 56, "seek": 38794, "start": 399.46, "end": 405.58, "text": " de la clase, digamos, yo, por ejemplo, puedo asumir independencia entre las variables aleatorias esta,", "tokens": [50940, 368, 635, 44578, 11, 36430, 11, 5290, 11, 1515, 13358, 11, 21612, 382, 449, 347, 4819, 10974, 3962, 2439, 9102, 6775, 1639, 4609, 5283, 11, 51246], "temperature": 0.0, "avg_logprob": -0.2337682088216146, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.009773598983883858}, {"id": 57, "seek": 38794, "start": 405.58, "end": 410.38, "text": " o sea, entre los atributos y puedo decir, si estas son independientes, p de x1 dado c", "tokens": [51246, 277, 4158, 11, 3962, 1750, 412, 2024, 34640, 288, 21612, 10235, 11, 1511, 13897, 1872, 4819, 20135, 11, 280, 368, 2031, 16, 29568, 269, 51486], "temperature": 0.0, "avg_logprob": -0.2337682088216146, "compression_ratio": 1.6244541484716157, "no_speech_prob": 0.009773598983883858}, {"id": 58, "seek": 41038, "start": 410.7, "end": 417.38, "text": " por p de x2 dado c, \u00bfno? \u00bfEsto no lo puedo hacer de este lado? Yo no puedo decir p de c", "tokens": [50380, 1515, 280, 368, 2031, 17, 29568, 269, 11, 3841, 1771, 30, 3841, 36, 20875, 572, 450, 21612, 6720, 368, 4065, 11631, 30, 7616, 572, 21612, 10235, 280, 368, 269, 50714], "temperature": 0.0, "avg_logprob": -0.22845561416060836, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.06654781848192215}, {"id": 59, "seek": 41038, "start": 417.38, "end": 422.94, "text": " dado x1, porque no funciona as\u00ed la probabilidad, digamos. La independencia la puedo dejar de ac\u00e1", "tokens": [50714, 29568, 2031, 16, 11, 4021, 572, 26210, 8582, 635, 31959, 4580, 11, 36430, 13, 2369, 4819, 10974, 635, 21612, 24391, 368, 23496, 50992], "temperature": 0.0, "avg_logprob": -0.22845561416060836, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.06654781848192215}, {"id": 60, "seek": 41038, "start": 422.94, "end": 432.9, "text": " al lado. Y cualquier propiedad de dependencia entre variables aleatorias se mira de este lado,", "tokens": [50992, 419, 11631, 13, 398, 21004, 2365, 1091, 345, 368, 5672, 10974, 3962, 9102, 6775, 1639, 4609, 369, 30286, 368, 4065, 11631, 11, 51490], "temperature": 0.0, "avg_logprob": -0.22845561416060836, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.06654781848192215}, {"id": 61, "seek": 41038, "start": 432.9, "end": 436.94, "text": " \u00bfno? Eso genera toda una teor\u00eda que se llama la de los modelos gr\u00e1ficos, que por supuesto no", "tokens": [51490, 3841, 1771, 30, 27795, 1337, 64, 11687, 2002, 40238, 2686, 631, 369, 23272, 635, 368, 1750, 2316, 329, 34613, 329, 11, 631, 1515, 34177, 572, 51692], "temperature": 0.0, "avg_logprob": -0.22845561416060836, "compression_ratio": 1.5791666666666666, "no_speech_prob": 0.06654781848192215}, {"id": 62, "seek": 43694, "start": 437.02, "end": 441.5, "text": " vamos a hablar ac\u00e1, pero que me dicen, bueno, \u00bfcu\u00e1l es la estructura que yo supongo en t\u00e9rminos", "tokens": [50368, 5295, 257, 21014, 23496, 11, 4768, 631, 385, 33816, 11, 11974, 11, 3841, 12032, 11447, 785, 635, 43935, 2991, 631, 5290, 9331, 25729, 465, 45198, 329, 50592], "temperature": 0.0, "avg_logprob": -0.2218042456585428, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.006358685437589884}, {"id": 63, "seek": 43694, "start": 441.5, "end": 446.7, "text": " de dependencia? Es decir, esta variable depende de esta, esta no, y as\u00ed. Y puedo modelarlo con", "tokens": [50592, 368, 5672, 10974, 30, 2313, 10235, 11, 5283, 7006, 47091, 368, 5283, 11, 5283, 572, 11, 288, 8582, 13, 398, 21612, 2316, 19457, 416, 50852], "temperature": 0.0, "avg_logprob": -0.2218042456585428, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.006358685437589884}, {"id": 64, "seek": 43694, "start": 446.7, "end": 455.42, "text": " un gr\u00e1fico, como va a tomar. Entonces, llegan a esto, \u00bfno? La probabilidad de la clase,", "tokens": [50852, 517, 34613, 78, 11, 2617, 2773, 257, 22048, 13, 15097, 11, 11234, 282, 257, 7433, 11, 3841, 1771, 30, 2369, 31959, 4580, 368, 635, 44578, 11, 51288], "temperature": 0.0, "avg_logprob": -0.2218042456585428, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.006358685437589884}, {"id": 65, "seek": 43694, "start": 455.42, "end": 459.18, "text": " dado los atributos, la probabilidad de la clase por la probabilidad de los atributos a la clase.", "tokens": [51288, 29568, 1750, 412, 2024, 34640, 11, 635, 31959, 4580, 368, 635, 44578, 1515, 635, 31959, 4580, 368, 1750, 412, 2024, 34640, 257, 635, 44578, 13, 51476], "temperature": 0.0, "avg_logprob": -0.2218042456585428, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.006358685437589884}, {"id": 66, "seek": 43694, "start": 459.18, "end": 463.58, "text": " Esto es valles, \u00bfno? Y ya lo hemos visto varias veces en el curso, no estamos inventando nada.", "tokens": [51476, 20880, 785, 371, 37927, 11, 3841, 1771, 30, 398, 2478, 450, 15396, 17558, 37496, 17054, 465, 806, 31085, 11, 572, 10382, 7962, 1806, 8096, 13, 51696], "temperature": 0.0, "avg_logprob": -0.2218042456585428, "compression_ratio": 1.7194244604316546, "no_speech_prob": 0.006358685437589884}, {"id": 67, "seek": 46358, "start": 464.21999999999997, "end": 470.18, "text": " Dividido la probabilidad de los atributos. Y bueno, y nada, lo que hemos hecho hasta ahora,", "tokens": [50396, 413, 1843, 2925, 635, 31959, 4580, 368, 1750, 412, 2024, 34640, 13, 398, 11974, 11, 288, 8096, 11, 450, 631, 15396, 13064, 10764, 9923, 11, 50694], "temperature": 0.0, "avg_logprob": -0.19032939672470092, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.000826750067062676}, {"id": 68, "seek": 46358, "start": 470.18, "end": 476.18, "text": " tanto la probabilidad priori, la PC como la probabilidad de verosimilitud, esta la puedo", "tokens": [50694, 10331, 635, 31959, 4580, 4059, 72, 11, 635, 6465, 2617, 635, 31959, 4580, 368, 1306, 329, 332, 388, 21875, 11, 5283, 635, 21612, 50994], "temperature": 0.0, "avg_logprob": -0.19032939672470092, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.000826750067062676}, {"id": 69, "seek": 46358, "start": 476.18, "end": 483.38, "text": " estimar a partir de los datos. Esto ya lo hemos hecho. Pero vamos a tener que simplificar el problema.", "tokens": [50994, 8017, 289, 257, 13906, 368, 1750, 27721, 13, 20880, 2478, 450, 15396, 13064, 13, 9377, 5295, 257, 11640, 631, 6883, 25625, 806, 12395, 13, 51354], "temperature": 0.0, "avg_logprob": -0.19032939672470092, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.000826750067062676}, {"id": 70, "seek": 48338, "start": 483.54, "end": 501.98, "text": " El m\u00e9todo nai valles lo que hace es asumir que los atributos son independientes entre s\u00ed,", "tokens": [50372, 2699, 20275, 17423, 297, 1301, 371, 37927, 450, 631, 10032, 785, 382, 449, 347, 631, 1750, 412, 2024, 34640, 1872, 4819, 20135, 3962, 8600, 11, 51294], "temperature": 0.0, "avg_logprob": -0.25466895553300967, "compression_ratio": 1.4087591240875912, "no_speech_prob": 0.004361976869404316}, {"id": 71, "seek": 48338, "start": 501.98, "end": 508.82, "text": " lo cual es una barbaridad conceptual, si por ejemplo estamos hablando de un texto y los atributos son", "tokens": [51294, 450, 10911, 785, 2002, 35822, 4580, 24106, 11, 1511, 1515, 13358, 10382, 29369, 368, 517, 35503, 288, 1750, 412, 2024, 34640, 1872, 51636], "temperature": 0.0, "avg_logprob": -0.25466895553300967, "compression_ratio": 1.4087591240875912, "no_speech_prob": 0.004361976869404316}, {"id": 72, "seek": 50882, "start": 508.82, "end": 514.9, "text": " las palabras que tiene. Realmente las palabras vienen acompa\u00f1adas, se hacen amigas entre ellas,", "tokens": [50364, 2439, 35240, 631, 7066, 13, 8467, 4082, 2439, 35240, 49298, 43142, 6872, 11, 369, 27434, 669, 328, 296, 3962, 38397, 11, 50668], "temperature": 0.0, "avg_logprob": -0.2953965491855267, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.02617531456053257}, {"id": 73, "seek": 50882, "start": 514.9, "end": 518.54, "text": " digamos, \u00bfno? Si hay muy palabras positivas, muy probable que haya otras palabras positivas.", "tokens": [50668, 36430, 11, 3841, 1771, 30, 4909, 4842, 5323, 35240, 11218, 24759, 11, 5323, 21759, 631, 24693, 20244, 35240, 11218, 24759, 13, 50850], "temperature": 0.0, "avg_logprob": -0.2953965491855267, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.02617531456053257}, {"id": 74, "seek": 50882, "start": 518.54, "end": 525.14, "text": " Bueno, valles dice, bueno, no s\u00e9, no s\u00e9. La probabilidad de una palabra solo depende de la clase.", "tokens": [50850, 16046, 11, 371, 37927, 10313, 11, 11974, 11, 572, 7910, 11, 572, 7910, 13, 2369, 31959, 4580, 368, 2002, 31702, 6944, 47091, 368, 635, 44578, 13, 51180], "temperature": 0.0, "avg_logprob": -0.2953965491855267, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.02617531456053257}, {"id": 75, "seek": 50882, "start": 528.78, "end": 533.9399999999999, "text": " Y por lo tanto eso hace que pueda partir la probabilidad, porque como son independientes,", "tokens": [51362, 398, 1515, 450, 10331, 7287, 10032, 631, 31907, 13906, 635, 31959, 4580, 11, 4021, 2617, 1872, 4819, 20135, 11, 51620], "temperature": 0.0, "avg_logprob": -0.2953965491855267, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.02617531456053257}, {"id": 76, "seek": 53394, "start": 533.94, "end": 540.1, "text": " la probabilidad de x1 dado x1 por xn dado c, la probabilidad de x1 dado c por la probabilidad de", "tokens": [50364, 635, 31959, 4580, 368, 2031, 16, 29568, 2031, 16, 1515, 2031, 77, 29568, 269, 11, 635, 31959, 4580, 368, 2031, 16, 29568, 269, 1515, 635, 31959, 4580, 368, 50672], "temperature": 0.0, "avg_logprob": -0.2477095490795071, "compression_ratio": 1.8932038834951457, "no_speech_prob": 0.028388531878590584}, {"id": 77, "seek": 53394, "start": 540.1, "end": 549.5400000000001, "text": " x2 dado c, bla, bla. Y bueno, \u00bfy c\u00f3mo construye un clasificador a partir de esto? Y bueno,", "tokens": [50672, 2031, 17, 29568, 269, 11, 16379, 11, 16379, 13, 398, 11974, 11, 3841, 88, 12826, 12946, 1200, 517, 596, 296, 1089, 5409, 257, 13906, 368, 7433, 30, 398, 11974, 11, 51144], "temperature": 0.0, "avg_logprob": -0.2477095490795071, "compression_ratio": 1.8932038834951457, "no_speech_prob": 0.028388531878590584}, {"id": 78, "seek": 53394, "start": 549.5400000000001, "end": 555.1, "text": " maximizo lo de arriba, busco la clase que maximice lo de arriba. Lo de abajo es independiente de la", "tokens": [51144, 5138, 19055, 450, 368, 28469, 11, 1255, 1291, 635, 44578, 631, 5138, 573, 450, 368, 28469, 13, 6130, 368, 30613, 785, 4819, 8413, 368, 635, 51422], "temperature": 0.0, "avg_logprob": -0.2477095490795071, "compression_ratio": 1.8932038834951457, "no_speech_prob": 0.028388531878590584}, {"id": 79, "seek": 53394, "start": 555.1, "end": 562.1400000000001, "text": " clase. Entonces busco la clase que maximice lo de arriba y ah\u00ed tengo un clasificador. \u00bfDe acuerdo?", "tokens": [51422, 44578, 13, 15097, 1255, 1291, 635, 44578, 631, 5138, 573, 450, 368, 28469, 288, 12571, 13989, 517, 596, 296, 1089, 5409, 13, 3841, 11089, 28113, 30, 51774], "temperature": 0.0, "avg_logprob": -0.2477095490795071, "compression_ratio": 1.8932038834951457, "no_speech_prob": 0.028388531878590584}, {"id": 80, "seek": 56394, "start": 563.94, "end": 571.5400000000001, "text": " Es muy sencillo, tomo todos los atributos que se me ocurren, los considero independientes. Ahora", "tokens": [50364, 2313, 5323, 46749, 78, 11, 2916, 78, 6321, 1750, 412, 2024, 34640, 631, 369, 385, 26430, 1095, 11, 1750, 1949, 78, 4819, 20135, 13, 18840, 50744], "temperature": 0.0, "avg_logprob": -0.25856546608798475, "compression_ratio": 1.47, "no_speech_prob": 0.00011866354907397181}, {"id": 81, "seek": 56394, "start": 571.5400000000001, "end": 578.86, "text": " lo moveremos en alg\u00fan ejemplo y busco la clase que maximiza. El m\u00e9todo Ney Valle funciona muy bien", "tokens": [50744, 450, 2402, 19065, 465, 26300, 13358, 288, 1255, 1291, 635, 44578, 631, 5138, 13427, 13, 2699, 20275, 17423, 1734, 88, 691, 11780, 26210, 5323, 3610, 51110], "temperature": 0.0, "avg_logprob": -0.25856546608798475, "compression_ratio": 1.47, "no_speech_prob": 0.00011866354907397181}, {"id": 82, "seek": 56394, "start": 578.86, "end": 587.2600000000001, "text": " como base para un clasificador y por poca plata uno hace un clasificador como la gente que capaz", "tokens": [51110, 2617, 3096, 1690, 517, 596, 296, 1089, 5409, 288, 1515, 714, 496, 30780, 8526, 10032, 517, 596, 296, 1089, 5409, 2617, 635, 3788, 631, 35453, 51530], "temperature": 0.0, "avg_logprob": -0.25856546608798475, "compression_ratio": 1.47, "no_speech_prob": 0.00011866354907397181}, {"id": 83, "seek": 58726, "start": 587.26, "end": 596.22, "text": " que hasta le pueden llamar un AI en la prensa. Yo no s\u00e9 de cu\u00e1ndo es el m\u00e9todo de Ney Valle,", "tokens": [50364, 631, 10764, 476, 14714, 16848, 289, 517, 7318, 465, 635, 659, 3695, 64, 13, 7616, 572, 7910, 368, 2702, 18606, 78, 785, 806, 20275, 17423, 368, 1734, 88, 691, 11780, 11, 50812], "temperature": 0.0, "avg_logprob": -0.26338445875379773, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.14878728985786438}, {"id": 84, "seek": 58726, "start": 596.22, "end": 603.9399999999999, "text": " me suena como de los a\u00f1os 60, si bien se basa en el teor\u00eda de Valle que de 1700, pero funciona", "tokens": [50812, 385, 459, 4118, 2617, 368, 1750, 11424, 4060, 11, 1511, 3610, 369, 987, 64, 465, 806, 40238, 2686, 368, 691, 11780, 631, 368, 43373, 11, 4768, 26210, 51198], "temperature": 0.0, "avg_logprob": -0.26338445875379773, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.14878728985786438}, {"id": 85, "seek": 58726, "start": 603.9399999999999, "end": 609.26, "text": " muy bien. En general, como primera aproximaci\u00f3n r\u00e1pida o algo, uno puede usar Ney Valle sin mucho", "tokens": [51198, 5323, 3610, 13, 2193, 2674, 11, 2617, 17382, 31270, 3482, 18213, 2887, 277, 8655, 11, 8526, 8919, 14745, 1734, 88, 691, 11780, 3343, 9824, 51464], "temperature": 0.0, "avg_logprob": -0.26338445875379773, "compression_ratio": 1.3773584905660377, "no_speech_prob": 0.14878728985786438}, {"id": 86, "seek": 60926, "start": 609.26, "end": 617.66, "text": " cargo de conciencia y funciona en general muy bien. El m\u00e9todo de Ney Valle es aplicado a la", "tokens": [50364, 19449, 368, 416, 537, 10974, 288, 26210, 465, 2674, 5323, 3610, 13, 2699, 20275, 17423, 368, 1734, 88, 691, 11780, 785, 18221, 1573, 257, 635, 50784], "temperature": 0.0, "avg_logprob": -0.22992242551317402, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.06031234189867973}, {"id": 87, "seek": 60926, "start": 617.66, "end": 625.66, "text": " clasificaci\u00f3n de documentos. Utiliza una de las formas de darlo es utilizando lo que", "tokens": [50784, 596, 296, 40802, 368, 4166, 329, 13, 12555, 388, 13427, 2002, 368, 2439, 33463, 368, 4072, 752, 785, 19906, 1806, 450, 631, 51184], "temperature": 0.0, "avg_logprob": -0.22992242551317402, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.06031234189867973}, {"id": 88, "seek": 60926, "start": 625.66, "end": 631.34, "text": " se llama una aproximaci\u00f3n vago words. Es el ejemplo, es como el ejemplo can\u00f3nico de", "tokens": [51184, 369, 23272, 2002, 31270, 3482, 371, 6442, 2283, 13, 2313, 806, 13358, 11, 785, 2617, 806, 13358, 393, 1801, 2789, 368, 51468], "temperature": 0.0, "avg_logprob": -0.22992242551317402, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.06031234189867973}, {"id": 89, "seek": 60926, "start": 631.34, "end": 638.22, "text": " clasificaci\u00f3n, digamos, el vago word. Yo digo tengo todo esto, es un documento que tiene una", "tokens": [51468, 596, 296, 40802, 11, 36430, 11, 806, 371, 6442, 1349, 13, 7616, 22990, 13989, 5149, 7433, 11, 785, 517, 4166, 78, 631, 7066, 2002, 51812], "temperature": 0.0, "avg_logprob": -0.22992242551317402, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.06031234189867973}, {"id": 90, "seek": 63822, "start": 638.22, "end": 644.38, "text": " estructura, que tiene un orden entre las palabras, que tiene una sintaxis, que tiene", "tokens": [50364, 43935, 2991, 11, 631, 7066, 517, 28615, 3962, 2439, 35240, 11, 631, 7066, 2002, 41259, 24633, 11, 631, 7066, 50672], "temperature": 0.0, "avg_logprob": -0.23066657723732364, "compression_ratio": 1.8029556650246306, "no_speech_prob": 0.007637791335582733}, {"id": 91, "seek": 63822, "start": 644.38, "end": 651.86, "text": " relaciones bien formadas, con una sem\u00e1ntica, yo no le hago caso a nada de eso. Y lo que hago", "tokens": [50672, 1039, 9188, 3610, 1254, 6872, 11, 416, 2002, 4361, 27525, 2262, 11, 5290, 572, 476, 38721, 9666, 257, 8096, 368, 7287, 13, 398, 450, 631, 38721, 51046], "temperature": 0.0, "avg_logprob": -0.23066657723732364, "compression_ratio": 1.8029556650246306, "no_speech_prob": 0.007637791335582733}, {"id": 92, "seek": 63822, "start": 651.86, "end": 659.1800000000001, "text": " solamente es considero que esto es una bolsa de palabras. La bolsa de se acuerdan, bolsa es", "tokens": [51046, 27814, 785, 1949, 78, 631, 7433, 785, 2002, 8986, 5790, 368, 35240, 13, 2369, 8986, 5790, 368, 369, 696, 5486, 10312, 11, 8986, 5790, 785, 51412], "temperature": 0.0, "avg_logprob": -0.23066657723732364, "compression_ratio": 1.8029556650246306, "no_speech_prob": 0.007637791335582733}, {"id": 93, "seek": 63822, "start": 659.1800000000001, "end": 666.98, "text": " como un set, pero que puede tener elemento repetido. Una bolsa de palabras y tengo el conteo de", "tokens": [51412, 2617, 517, 992, 11, 4768, 631, 8919, 11640, 47961, 13645, 2925, 13, 15491, 8986, 5790, 368, 35240, 288, 13989, 806, 34444, 78, 368, 51802], "temperature": 0.0, "avg_logprob": -0.23066657723732364, "compression_ratio": 1.8029556650246306, "no_speech_prob": 0.007637791335582733}, {"id": 94, "seek": 66698, "start": 666.98, "end": 676.58, "text": " cantidad de veces que una palabra aparece en ese documento. Mi representaci\u00f3n del documento es esto.", "tokens": [50364, 33757, 368, 17054, 631, 2002, 31702, 37863, 465, 10167, 4166, 78, 13, 10204, 2906, 3482, 1103, 4166, 78, 785, 7433, 13, 50844], "temperature": 0.0, "avg_logprob": -0.3424872618455153, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.012482834979891777}, {"id": 95, "seek": 66698, "start": 677.94, "end": 687.14, "text": " Me features son estos. Entonces, c\u00f3mo hago clasificaci\u00f3n, esto fue lo que hubo en la laboratoria", "tokens": [50912, 1923, 4122, 1872, 12585, 13, 15097, 11, 12826, 38721, 596, 296, 40802, 11, 7433, 9248, 450, 631, 11838, 78, 465, 635, 5938, 1639, 654, 51372], "temperature": 0.0, "avg_logprob": -0.3424872618455153, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.012482834979891777}, {"id": 96, "seek": 66698, "start": 687.14, "end": 694.78, "text": " del a\u00f1o pasado. Entonces, c\u00f3mo se instancia Ney Valle para el problema de clasificaci\u00f3n de documento?", "tokens": [51372, 1103, 15984, 24794, 13, 15097, 11, 12826, 369, 1058, 22862, 1734, 88, 691, 11780, 1690, 806, 12395, 368, 596, 296, 40802, 368, 4166, 78, 30, 51754], "temperature": 0.0, "avg_logprob": -0.3424872618455153, "compression_ratio": 1.5968586387434556, "no_speech_prob": 0.012482834979891777}, {"id": 97, "seek": 69478, "start": 694.78, "end": 700.66, "text": " Bueno, las posiciones son todas las posiciones que tengo en el documento que quiero evaluar.", "tokens": [50364, 16046, 11, 2439, 1366, 29719, 1872, 10906, 2439, 1366, 29719, 631, 13989, 465, 806, 4166, 78, 631, 16811, 6133, 289, 13, 50658], "temperature": 0.0, "avg_logprob": -0.27509707373541753, "compression_ratio": 1.903448275862069, "no_speech_prob": 0.0020506693981587887}, {"id": 98, "seek": 69478, "start": 700.66, "end": 704.74, "text": " Yo quiero evaluar en la clase. Aguardo, quiero evaluar la clase en un documento,", "tokens": [50658, 7616, 16811, 6133, 289, 465, 635, 44578, 13, 2725, 84, 12850, 11, 16811, 6133, 289, 635, 44578, 465, 517, 4166, 78, 11, 50862], "temperature": 0.0, "avg_logprob": -0.27509707373541753, "compression_ratio": 1.903448275862069, "no_speech_prob": 0.0020506693981587887}, {"id": 99, "seek": 69478, "start": 704.74, "end": 709.5, "text": " entonces tengo las posiciones, que son todos los tokens que aparecen en cada palabra, en el documento.", "tokens": [50862, 13003, 13989, 2439, 1366, 29719, 11, 631, 1872, 6321, 1750, 22667, 631, 15004, 13037, 465, 8411, 31702, 11, 465, 806, 4166, 78, 13, 51100], "temperature": 0.0, "avg_logprob": -0.27509707373541753, "compression_ratio": 1.903448275862069, "no_speech_prob": 0.0020506693981587887}, {"id": 100, "seek": 70950, "start": 709.5, "end": 730.58, "text": " Y la clase, seg\u00fan Ney Valle, es la clase que maximiza, quer\u00eda comentar algo ac\u00e1.", "tokens": [50364, 398, 635, 44578, 11, 36570, 1734, 88, 691, 11780, 11, 785, 635, 44578, 631, 5138, 13427, 11, 37869, 14541, 289, 8655, 23496, 13, 51418], "temperature": 0.0, "avg_logprob": -0.35198307037353516, "compression_ratio": 1.2887323943661972, "no_speech_prob": 0.014418949373066425}, {"id": 101, "seek": 70950, "start": 732.1, "end": 739.14, "text": " Esto en realidad es un conteo, pero yo ac\u00e1 la voy a contar seis veces. Por eso es un bug of words.", "tokens": [51494, 20880, 465, 25635, 785, 517, 34444, 78, 11, 4768, 5290, 23496, 635, 7552, 257, 27045, 28233, 17054, 13, 5269, 7287, 785, 517, 7426, 295, 2283, 13, 51846], "temperature": 0.0, "avg_logprob": -0.35198307037353516, "compression_ratio": 1.2887323943661972, "no_speech_prob": 0.014418949373066425}, {"id": 102, "seek": 73950, "start": 739.5, "end": 749.62, "text": " En las posiciones considero todas las posiciones posibles, como dec\u00eda, y calculo la clase como la", "tokens": [50364, 2193, 2439, 1366, 29719, 1949, 78, 10906, 2439, 1366, 29719, 1366, 14428, 11, 2617, 37599, 11, 288, 4322, 78, 635, 44578, 2617, 635, 50870], "temperature": 0.0, "avg_logprob": -0.3119545555114746, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0006950654787942767}, {"id": 103, "seek": 73950, "start": 749.62, "end": 758.7, "text": " clase que maximiza la probabilidad de cada palabra que aparece en el documento dado a esa clase.", "tokens": [50870, 44578, 631, 5138, 13427, 635, 31959, 4580, 368, 8411, 31702, 631, 37863, 465, 806, 4166, 78, 29568, 257, 11342, 44578, 13, 51324], "temperature": 0.0, "avg_logprob": -0.3119545555114746, "compression_ratio": 1.4772727272727273, "no_speech_prob": 0.0006950654787942767}, {"id": 104, "seek": 75870, "start": 759.7, "end": 766.0200000000001, "text": " \u00bfSe entiende? Es la clase que hace m\u00e1s probable, considerando independencia,", "tokens": [50414, 3841, 10637, 948, 45816, 30, 2313, 635, 44578, 631, 10032, 3573, 21759, 11, 1949, 1806, 4819, 10974, 11, 50730], "temperature": 0.0, "avg_logprob": -0.29527898736902186, "compression_ratio": 1.4364640883977902, "no_speech_prob": 0.00737544521689415}, {"id": 105, "seek": 75870, "start": 772.3000000000001, "end": 777.1800000000001, "text": " que esa palabra es T en ese documento, digamos, \u00bfno? La probabilidad de W subida o C.", "tokens": [51044, 631, 11342, 31702, 785, 314, 465, 10167, 4166, 78, 11, 36430, 11, 3841, 1771, 30, 2369, 31959, 4580, 368, 343, 1422, 2887, 277, 383, 13, 51288], "temperature": 0.0, "avg_logprob": -0.29527898736902186, "compression_ratio": 1.4364640883977902, "no_speech_prob": 0.00737544521689415}, {"id": 106, "seek": 75870, "start": 778.86, "end": 783.9000000000001, "text": " \u00bfY c\u00f3mo hago para hacer eso? Y bueno, para calcular esos valores, para estimar esos valores,", "tokens": [51372, 3841, 56, 12826, 38721, 1690, 6720, 7287, 30, 398, 11974, 11, 1690, 2104, 17792, 22411, 38790, 11, 1690, 8017, 289, 22411, 38790, 11, 51624], "temperature": 0.0, "avg_logprob": -0.29527898736902186, "compression_ratio": 1.4364640883977902, "no_speech_prob": 0.00737544521689415}, {"id": 107, "seek": 78390, "start": 784.9, "end": 791.38, "text": " yo digo, bueno, nuestro mejor estimador, este corrito quiere decir nuestro estimador,", "tokens": [50414, 5290, 22990, 11, 11974, 11, 14726, 11479, 8017, 5409, 11, 4065, 1181, 17492, 23877, 10235, 14726, 8017, 5409, 11, 50738], "temperature": 0.0, "avg_logprob": -0.2100838881272536, "compression_ratio": 1.8702702702702703, "no_speech_prob": 0.013503768481314182}, {"id": 108, "seek": 78390, "start": 791.38, "end": 795.42, "text": " nuestro mejor estimador de la clase, de la probabilidad priori, de la probabilidad,", "tokens": [50738, 14726, 11479, 8017, 5409, 368, 635, 44578, 11, 368, 635, 31959, 4580, 4059, 72, 11, 368, 635, 31959, 4580, 11, 50940], "temperature": 0.0, "avg_logprob": -0.2100838881272536, "compression_ratio": 1.8702702702702703, "no_speech_prob": 0.013503768481314182}, {"id": 109, "seek": 78390, "start": 795.42, "end": 801.06, "text": " estamos hablando de la probabilidad de la clase, si no tuvieramos la palabra, es decir,", "tokens": [50940, 10382, 29369, 368, 635, 31959, 4580, 368, 635, 44578, 11, 1511, 572, 38177, 811, 2151, 635, 31702, 11, 785, 10235, 11, 51222], "temperature": 0.0, "avg_logprob": -0.2100838881272536, "compression_ratio": 1.8702702702702703, "no_speech_prob": 0.013503768481314182}, {"id": 110, "seek": 78390, "start": 801.06, "end": 808.8199999999999, "text": " yo puedo tener una distribuci\u00f3n, yo tengo documentos que son o de deporte o de m\u00fasica,", "tokens": [51222, 5290, 21612, 11640, 2002, 4400, 30813, 11, 5290, 13989, 4166, 329, 631, 1872, 277, 368, 1367, 12752, 277, 368, 20091, 11, 51610], "temperature": 0.0, "avg_logprob": -0.2100838881272536, "compression_ratio": 1.8702702702702703, "no_speech_prob": 0.013503768481314182}, {"id": 111, "seek": 80882, "start": 808.82, "end": 812.9000000000001, "text": " vamos a suponer que son exclusivos, \u00bft\u00e1? La probabilidad de la clase es el n\u00famero de", "tokens": [50364, 5295, 257, 9331, 32949, 631, 1872, 15085, 16501, 11, 3841, 83, 842, 30, 2369, 31959, 4580, 368, 635, 44578, 785, 806, 14959, 368, 50568], "temperature": 0.0, "avg_logprob": -0.2231118258307962, "compression_ratio": 1.8125, "no_speech_prob": 0.020059701055288315}, {"id": 112, "seek": 80882, "start": 812.9000000000001, "end": 818.74, "text": " documentos de deporte sobre el total, o sea, mi probabilidad priori, \u00bfse acuerdan de Valle,", "tokens": [50568, 4166, 329, 368, 1367, 12752, 5473, 806, 3217, 11, 277, 4158, 11, 2752, 31959, 4580, 4059, 72, 11, 3841, 405, 696, 5486, 10312, 368, 691, 11780, 11, 50860], "temperature": 0.0, "avg_logprob": -0.2231118258307962, "compression_ratio": 1.8125, "no_speech_prob": 0.020059701055288315}, {"id": 113, "seek": 80882, "start": 818.74, "end": 823.34, "text": " \u00bfno? Yo tengo una probabilidad priori que lo que pienso antes de empezar a ver el documento y", "tokens": [50860, 3841, 1771, 30, 7616, 13989, 2002, 31959, 4580, 4059, 72, 631, 450, 631, 26274, 539, 11014, 368, 31168, 257, 1306, 806, 4166, 78, 288, 51090], "temperature": 0.0, "avg_logprob": -0.2231118258307962, "compression_ratio": 1.8125, "no_speech_prob": 0.020059701055288315}, {"id": 114, "seek": 80882, "start": 823.34, "end": 827.82, "text": " antes de ver el documento yo puedo decir, bueno, el 90% de los documentos son de deporte,", "tokens": [51090, 11014, 368, 1306, 806, 4166, 78, 5290, 21612, 10235, 11, 11974, 11, 806, 4289, 4, 368, 1750, 4166, 329, 1872, 368, 1367, 12752, 11, 51314], "temperature": 0.0, "avg_logprob": -0.2231118258307962, "compression_ratio": 1.8125, "no_speech_prob": 0.020059701055288315}, {"id": 115, "seek": 80882, "start": 827.82, "end": 835.22, "text": " entonces mi probabilidad a priori es 0.9, \u00bfte acuerdo? Es mucho m\u00e1s probable a priori que sea un", "tokens": [51314, 13003, 2752, 31959, 4580, 257, 4059, 72, 785, 1958, 13, 24, 11, 3841, 975, 28113, 30, 2313, 9824, 3573, 21759, 257, 4059, 72, 631, 4158, 517, 51684], "temperature": 0.0, "avg_logprob": -0.2231118258307962, "compression_ratio": 1.8125, "no_speech_prob": 0.020059701055288315}, {"id": 116, "seek": 83522, "start": 835.22, "end": 839.98, "text": " documento de deporte, yo voy a ajustar esa probabilidad con la probabilidad de las palabras de", "tokens": [50364, 4166, 78, 368, 1367, 12752, 11, 5290, 7552, 257, 41023, 289, 11342, 31959, 4580, 416, 635, 31959, 4580, 368, 2439, 35240, 368, 50602], "temperature": 0.0, "avg_logprob": -0.2538268718313664, "compression_ratio": 1.8112244897959184, "no_speech_prob": 0.04356709122657776}, {"id": 117, "seek": 83522, "start": 839.98, "end": 846.14, "text": " cada una, \u00bfte acuerdo? Entonces, yo estimo esa probabilidad priori con el n\u00famero de", "tokens": [50602, 8411, 2002, 11, 3841, 975, 28113, 30, 15097, 11, 5290, 871, 6934, 11342, 31959, 4580, 4059, 72, 416, 806, 14959, 368, 50910], "temperature": 0.0, "avg_logprob": -0.2538268718313664, "compression_ratio": 1.8112244897959184, "no_speech_prob": 0.04356709122657776}, {"id": 118, "seek": 83522, "start": 846.14, "end": 854.02, "text": " clase de documentos, que tienen la clase dividido el total de documentos. Y, similarmente,", "tokens": [50910, 44578, 368, 4166, 329, 11, 631, 12536, 635, 44578, 4996, 2925, 806, 3217, 368, 4166, 329, 13, 398, 11, 2531, 4082, 11, 51304], "temperature": 0.0, "avg_logprob": -0.2538268718313664, "compression_ratio": 1.8112244897959184, "no_speech_prob": 0.04356709122657776}, {"id": 119, "seek": 83522, "start": 857.46, "end": 864.1800000000001, "text": " estimo por conteo la probabilidad de cada palabra de la clase contando del total de", "tokens": [51476, 871, 6934, 1515, 34444, 78, 635, 31959, 4580, 368, 8411, 31702, 368, 635, 44578, 660, 1806, 1103, 3217, 368, 51812], "temperature": 0.0, "avg_logprob": -0.2538268718313664, "compression_ratio": 1.8112244897959184, "no_speech_prob": 0.04356709122657776}, {"id": 120, "seek": 86418, "start": 864.18, "end": 871.06, "text": " veces que aparecen todas las palabras en los documentos de esa clase, o sea, de todas las", "tokens": [50364, 17054, 631, 15004, 13037, 10906, 2439, 35240, 465, 1750, 4166, 329, 368, 11342, 44578, 11, 277, 4158, 11, 368, 10906, 2439, 50708], "temperature": 0.0, "avg_logprob": -0.18920002731622435, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.0017042836407199502}, {"id": 121, "seek": 86418, "start": 871.06, "end": 876.5799999999999, "text": " palabras que aparecen en los documentos de deporte, \u00bfcu\u00e1ntas veces aparece esa palabra en la de", "tokens": [50708, 35240, 631, 15004, 13037, 465, 1750, 4166, 329, 368, 1367, 12752, 11, 3841, 12032, 27525, 296, 17054, 37863, 11342, 31702, 465, 635, 368, 50984], "temperature": 0.0, "avg_logprob": -0.18920002731622435, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.0017042836407199502}, {"id": 122, "seek": 86418, "start": 876.5799999999999, "end": 884.2199999999999, "text": " deporte? Tiene sentido, \u00bfno? Es una palabra com\u00fan en un dominio de deportes, esta es lo que se", "tokens": [50984, 1367, 12752, 30, 314, 10174, 19850, 11, 3841, 1771, 30, 2313, 2002, 31702, 45448, 465, 517, 8859, 1004, 368, 33485, 279, 11, 5283, 785, 450, 631, 369, 51366], "temperature": 0.0, "avg_logprob": -0.18920002731622435, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.0017042836407199502}, {"id": 123, "seek": 86418, "start": 884.2199999999999, "end": 892.06, "text": " pregunta, y multiplica a todas esas probabilidades, que seguramente operativamente tengamos que usar", "tokens": [51366, 24252, 11, 288, 12788, 2262, 257, 10906, 23388, 31959, 10284, 11, 631, 22179, 3439, 2208, 10662, 3439, 10370, 2151, 631, 14745, 51758], "temperature": 0.0, "avg_logprob": -0.18920002731622435, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.0017042836407199502}, {"id": 124, "seek": 89206, "start": 892.06, "end": 898.38, "text": " un logaritmo y sumar, porque si no nos va a dar todav\u00eda muy chiquita, pero conceptualmente lo", "tokens": [50364, 517, 41473, 270, 3280, 288, 2408, 289, 11, 4021, 1511, 572, 3269, 2773, 257, 4072, 28388, 5323, 417, 3221, 2786, 11, 4768, 3410, 901, 4082, 450, 50680], "temperature": 0.0, "avg_logprob": -0.3593979601590139, "compression_ratio": 1.2318840579710144, "no_speech_prob": 0.048119060695171356}, {"id": 125, "seek": 89206, "start": 898.38, "end": 908.14, "text": " mismo. \u00bfSe entiende? \u00bfPor qu\u00e9, en vez de usar esto, tengo que usar esto?", "tokens": [50680, 12461, 13, 3841, 10637, 948, 45816, 30, 3841, 24907, 8057, 11, 465, 5715, 368, 14745, 7433, 11, 13989, 631, 14745, 7433, 30, 51168], "temperature": 0.0, "avg_logprob": -0.3593979601590139, "compression_ratio": 1.2318840579710144, "no_speech_prob": 0.048119060695171356}, {"id": 126, "seek": 92206, "start": 922.3399999999999, "end": 923.6999999999999, "text": " \u00bfPor qu\u00e9 tengo que hacer eso?", "tokens": [50378, 3841, 24907, 8057, 13989, 631, 6720, 7287, 30, 50446], "temperature": 0.0, "avg_logprob": -0.332468068158185, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.02437366172671318}, {"id": 127, "seek": 92206, "start": 936.14, "end": 939.4599999999999, "text": " \u00bfPor qu\u00e9 tengo que hacer esto? \u00bfQu\u00e9 es esto?", "tokens": [51068, 3841, 24907, 8057, 13989, 631, 6720, 7433, 30, 3841, 15137, 785, 7433, 30, 51234], "temperature": 0.0, "avg_logprob": -0.332468068158185, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.02437366172671318}, {"id": 128, "seek": 92206, "start": 943.8199999999999, "end": 949.9, "text": " La plaza, le agrego uno, acaba contador para que no tenga el problema de que, porque si una de", "tokens": [51452, 2369, 499, 12257, 11, 476, 4554, 1571, 8526, 11, 23485, 660, 5409, 1690, 631, 572, 36031, 806, 12395, 368, 631, 11, 4021, 1511, 2002, 368, 51756], "temperature": 0.0, "avg_logprob": -0.332468068158185, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.02437366172671318}, {"id": 129, "seek": 94990, "start": 949.9, "end": 952.9, "text": " estas probabilidades, lo mismo que nos pas\u00f3 con los engramas, si le suena conocido, porque es lo", "tokens": [50364, 13897, 31959, 10284, 11, 450, 12461, 631, 3269, 41382, 416, 1750, 465, 1342, 296, 11, 1511, 476, 459, 4118, 15871, 2925, 11, 4021, 785, 450, 50514], "temperature": 0.0, "avg_logprob": -0.27326074513522064, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06831774860620499}, {"id": 130, "seek": 94990, "start": 952.9, "end": 960.06, "text": " mismo, si una de aquella probabilidad de da cero, se me cancela toda la clase, la probabilidad de", "tokens": [50514, 12461, 11, 1511, 2002, 368, 2373, 9885, 31959, 4580, 368, 1120, 269, 2032, 11, 369, 385, 393, 66, 4053, 11687, 635, 44578, 11, 635, 31959, 4580, 368, 50872], "temperature": 0.0, "avg_logprob": -0.27326074513522064, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06831774860620499}, {"id": 131, "seek": 94990, "start": 960.06, "end": 970.5, "text": " clase va a ser cero. Entonces para eso hacemos la plaza, hacemos smoothing, suavizado, agreg\u00e1ndole", "tokens": [50872, 44578, 2773, 257, 816, 269, 2032, 13, 15097, 1690, 7287, 33839, 635, 499, 12257, 11, 33839, 899, 6259, 571, 11, 459, 706, 27441, 11, 623, 3375, 18606, 4812, 51394], "temperature": 0.0, "avg_logprob": -0.27326074513522064, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.06831774860620499}, {"id": 132, "seek": 97050, "start": 970.5, "end": 981.82, "text": " uno a cada contador. Por ejemplo, bueno todo esto que yo estoy diciendo est\u00e1 en el cap\u00edtulo 7,", "tokens": [50364, 8526, 257, 8411, 660, 5409, 13, 5269, 13358, 11, 11974, 5149, 7433, 631, 5290, 15796, 42797, 3192, 465, 806, 1410, 30389, 1614, 11, 50930], "temperature": 0.0, "avg_logprob": -0.23708257135355248, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.31227585673332214}, {"id": 133, "seek": 97050, "start": 981.82, "end": 988.82, "text": " m\u00e1s o menos, que es general, del cap\u00edtulo 7 del libro de Martin Yurashki. El libro de Martin Yurashki", "tokens": [50930, 3573, 277, 8902, 11, 631, 785, 2674, 11, 1103, 1410, 30389, 1614, 1103, 29354, 368, 9184, 398, 374, 1299, 2984, 13, 2699, 29354, 368, 9184, 398, 374, 1299, 2984, 51280], "temperature": 0.0, "avg_logprob": -0.23708257135355248, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.31227585673332214}, {"id": 134, "seek": 97050, "start": 988.82, "end": 993.9, "text": " est\u00e1 online, los cap\u00edtulos nuevos, de hecho todos los cap\u00edtulos correspondientes a clases que", "tokens": [51280, 3192, 2950, 11, 1750, 1410, 6712, 28348, 42010, 11, 368, 13064, 6321, 1750, 1410, 6712, 28348, 6805, 20135, 257, 596, 1957, 631, 51534], "temperature": 0.0, "avg_logprob": -0.23708257135355248, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.31227585673332214}, {"id": 135, "seek": 97050, "start": 993.9, "end": 1000.18, "text": " hemos dado est\u00e1n online, yo realmente les recomiendo leerlos un libro que est\u00e1 muy claro, no va a", "tokens": [51534, 15396, 29568, 10368, 2950, 11, 5290, 14446, 1512, 23334, 7304, 34172, 9389, 517, 29354, 631, 3192, 5323, 16742, 11, 572, 2773, 257, 51848], "temperature": 0.0, "avg_logprob": -0.23708257135355248, "compression_ratio": 1.748898678414097, "no_speech_prob": 0.31227585673332214}, {"id": 136, "seek": 100018, "start": 1000.18, "end": 1006.14, "text": " tener mucha m\u00e1s dificultad que lo que vemos en la clase, por lo menos no se, uno pierde perspectiva,", "tokens": [50364, 11640, 25248, 3573, 29615, 723, 345, 631, 450, 631, 20909, 465, 635, 44578, 11, 1515, 450, 8902, 572, 369, 11, 8526, 9766, 1479, 4096, 5931, 11, 50662], "temperature": 0.0, "avg_logprob": -0.4285461750436336, "compression_ratio": 1.4597156398104265, "no_speech_prob": 0.022055363282561302}, {"id": 137, "seek": 100018, "start": 1006.14, "end": 1019.62, "text": " no? Est\u00e1 claro, pero, pero... \u00bfQu\u00e9 le pasa? Le agrajo de si, si me giro nada, si, si, y ah\u00ed pueden", "tokens": [50662, 572, 30, 27304, 16742, 11, 4768, 11, 4768, 485, 3841, 15137, 476, 20260, 30, 1456, 623, 424, 5134, 368, 1511, 11, 1511, 385, 1735, 340, 8096, 11, 1511, 11, 1511, 11, 288, 12571, 14714, 51336], "temperature": 0.0, "avg_logprob": -0.4285461750436336, "compression_ratio": 1.4597156398104265, "no_speech_prob": 0.022055363282561302}, {"id": 138, "seek": 100018, "start": 1019.62, "end": 1026.4199999999998, "text": " chequear y hay algunos detalles m\u00e1s que me parecen muy interesantes, si a ustedes les interesa. Bueno,", "tokens": [51336, 947, 1077, 289, 288, 4842, 21078, 1141, 37927, 3573, 631, 385, 7448, 13037, 5323, 20157, 9327, 11, 1511, 257, 17110, 1512, 728, 13708, 13, 16046, 11, 51676], "temperature": 0.0, "avg_logprob": -0.4285461750436336, "compression_ratio": 1.4597156398104265, "no_speech_prob": 0.022055363282561302}, {"id": 139, "seek": 102642, "start": 1027.38, "end": 1032.94, "text": " supongamos que nosotros tenemos el cuerpo de entrenamiento que tenemos arriba, las oraciones que", "tokens": [50412, 9331, 556, 2151, 631, 13863, 9914, 806, 20264, 368, 45069, 16971, 631, 9914, 28469, 11, 2439, 420, 9188, 631, 50690], "temperature": 0.0, "avg_logprob": -0.2183752575436154, "compression_ratio": 1.546875, "no_speech_prob": 0.044733840972185135}, {"id": 140, "seek": 102642, "start": 1032.94, "end": 1040.8200000000002, "text": " est\u00e1n arriba y con una categor\u00eda negativa o positiva, alg\u00fan tipo, en este caso estamos haciendo", "tokens": [50690, 10368, 28469, 288, 416, 2002, 19250, 2686, 2485, 18740, 277, 11218, 5931, 11, 26300, 9746, 11, 465, 4065, 9666, 10382, 20509, 51084], "temperature": 0.0, "avg_logprob": -0.2183752575436154, "compression_ratio": 1.546875, "no_speech_prob": 0.044733840972185135}, {"id": 141, "seek": 102642, "start": 1040.8200000000002, "end": 1047.26, "text": " sentimenta an\u00e1lisis, es decir, analizar si la percepci\u00f3n es positiva o negativa sobre un documento.", "tokens": [51084, 2279, 2328, 64, 44113, 28436, 11, 785, 10235, 11, 2624, 9736, 1511, 635, 9016, 39859, 785, 11218, 5931, 277, 2485, 18740, 5473, 517, 4166, 78, 13, 51406], "temperature": 0.0, "avg_logprob": -0.2183752575436154, "compression_ratio": 1.546875, "no_speech_prob": 0.044733840972185135}, {"id": 142, "seek": 104726, "start": 1047.86, "end": 1058.14, "text": " En el cuerpo de los tweets hac\u00edamos algo as\u00ed, algo parecido, es decir, yo necesito saber si la clase", "tokens": [50394, 2193, 806, 20264, 368, 1750, 25671, 46093, 16275, 8655, 8582, 11, 8655, 7448, 17994, 11, 785, 10235, 11, 5290, 11909, 3528, 12489, 1511, 635, 44578, 50908], "temperature": 0.0, "avg_logprob": -0.2253491127327697, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.04056650400161743}, {"id": 143, "seek": 104726, "start": 1058.14, "end": 1066.26, "text": " del cuerpo del tweet es de humor o no humor. Bueno, y ah\u00ed tenemos algunos ejemplos negativos y otros", "tokens": [50908, 1103, 20264, 1103, 15258, 785, 368, 14318, 277, 572, 14318, 13, 16046, 11, 288, 12571, 9914, 21078, 10012, 5895, 329, 2485, 36945, 288, 16422, 51314], "temperature": 0.0, "avg_logprob": -0.2253491127327697, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.04056650400161743}, {"id": 144, "seek": 104726, "start": 1066.26, "end": 1072.42, "text": " positivos y queremos saber qu\u00e9 pasa con predictable with no originality. Entonces,", "tokens": [51314, 11218, 16501, 288, 26813, 12489, 8057, 20260, 416, 27737, 365, 572, 4957, 1860, 13, 15097, 11, 51622], "temperature": 0.0, "avg_logprob": -0.2253491127327697, "compression_ratio": 1.5157894736842106, "no_speech_prob": 0.04056650400161743}, {"id": 145, "seek": 107242, "start": 1073.14, "end": 1082.18, "text": " la probabilidad priori de la clase cu\u00e1l es y es el total de documentos hay 1, 2, 3, 4, 5,", "tokens": [50400, 635, 31959, 4580, 4059, 72, 368, 635, 44578, 44318, 785, 288, 785, 806, 3217, 368, 4166, 329, 4842, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 50852], "temperature": 0.0, "avg_logprob": -0.21598200798034667, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0025706994347274303}, {"id": 146, "seek": 107242, "start": 1083.78, "end": 1088.5, "text": " de los cuales tres son negativas y dos son positivas, o sea, que estas son nuestra probabilidad", "tokens": [50932, 368, 1750, 46932, 15890, 1872, 2485, 35725, 288, 4491, 1872, 11218, 24759, 11, 277, 4158, 11, 631, 13897, 1872, 16825, 31959, 4580, 51168], "temperature": 0.0, "avg_logprob": -0.21598200798034667, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0025706994347274303}, {"id": 147, "seek": 107242, "start": 1088.5, "end": 1098.14, "text": " priori. Y luego entramos a buscar la probabilidad de cada palabra. La probabilidad de predictable,", "tokens": [51168, 4059, 72, 13, 398, 17222, 8041, 2151, 257, 26170, 635, 31959, 4580, 368, 8411, 31702, 13, 2369, 31959, 4580, 368, 27737, 11, 51650], "temperature": 0.0, "avg_logprob": -0.21598200798034667, "compression_ratio": 1.6101694915254237, "no_speech_prob": 0.0025706994347274303}, {"id": 148, "seek": 109814, "start": 1098.14, "end": 1104.5, "text": " dado que la clase es negativa, es 1 que es la ocurrencia de predictable,", "tokens": [50364, 29568, 631, 635, 44578, 785, 2485, 18740, 11, 785, 502, 631, 785, 635, 26430, 1095, 2755, 368, 27737, 11, 50682], "temperature": 0.0, "avg_logprob": -0.31283075066022975, "compression_ratio": 1.6381909547738693, "no_speech_prob": 0.023743055760860443}, {"id": 149, "seek": 109814, "start": 1104.5, "end": 1109.66, "text": " predictable solo aparece en la segunda oraci\u00f3n y en un contexto negativo.", "tokens": [50682, 6069, 712, 6944, 37863, 465, 635, 21978, 420, 3482, 288, 465, 517, 47685, 2485, 18586, 13, 50940], "temperature": 0.0, "avg_logprob": -0.31283075066022975, "compression_ratio": 1.6381909547738693, "no_speech_prob": 0.023743055760860443}, {"id": 150, "seek": 109814, "start": 1112.5800000000002, "end": 1119.0200000000002, "text": " Entonces, a cada 1 y a cada tenemos el m\u00e1s 20 es para normalizar, para la plaza,", "tokens": [51086, 15097, 11, 257, 8411, 502, 288, 257, 8411, 9914, 806, 3573, 945, 785, 1690, 2710, 9736, 11, 1690, 635, 499, 12257, 11, 51408], "temperature": 0.0, "avg_logprob": -0.31283075066022975, "compression_ratio": 1.6381909547738693, "no_speech_prob": 0.023743055760860443}, {"id": 151, "seek": 109814, "start": 1119.0200000000002, "end": 1123.8600000000001, "text": " o sea, 1 m\u00e1s 1 y 14, que es el total de palabras m\u00e1s 20, 14 es el total de palabras diferente.", "tokens": [51408, 277, 4158, 11, 502, 3573, 502, 288, 3499, 11, 631, 785, 806, 3217, 368, 35240, 3573, 945, 11, 3499, 785, 806, 3217, 368, 35240, 20973, 13, 51650], "temperature": 0.0, "avg_logprob": -0.31283075066022975, "compression_ratio": 1.6381909547738693, "no_speech_prob": 0.023743055760860443}, {"id": 152, "seek": 112386, "start": 1124.86, "end": 1125.58, "text": " \u00bfDe acuerdo?", "tokens": [50414, 3841, 11089, 28113, 30, 50450], "temperature": 0.0, "avg_logprob": -0.34734907244691754, "compression_ratio": 1.4744897959183674, "no_speech_prob": 0.056708335876464844}, {"id": 153, "seek": 112386, "start": 1131.1399999999999, "end": 1136.86, "text": " De las palabras diferentes. \u00bfLa palabras? \u00bfC\u00f3mo f\u00e1ciles verlo ac\u00e1?", "tokens": [50728, 1346, 2439, 35240, 17686, 13, 3841, 5478, 35240, 30, 3841, 28342, 17474, 279, 1306, 752, 23496, 30, 51014], "temperature": 0.0, "avg_logprob": -0.34734907244691754, "compression_ratio": 1.4744897959183674, "no_speech_prob": 0.056708335876464844}, {"id": 154, "seek": 112386, "start": 1136.86, "end": 1139.2199999999998, "text": " S\u00ed, es la clase, \u00bfno?", "tokens": [51014, 12375, 11, 785, 635, 44578, 11, 3841, 1771, 30, 51132], "temperature": 0.0, "avg_logprob": -0.34734907244691754, "compression_ratio": 1.4744897959183674, "no_speech_prob": 0.056708335876464844}, {"id": 155, "seek": 112386, "start": 1140.74, "end": 1145.6599999999999, "text": " De la clase. \u00bfLa cantidad de palabras que hay en la clase? No, no son diferentes, son todas.", "tokens": [51208, 1346, 635, 44578, 13, 3841, 5478, 33757, 368, 35240, 631, 4842, 465, 635, 44578, 30, 883, 11, 572, 1872, 17686, 11, 1872, 10906, 13, 51454], "temperature": 0.0, "avg_logprob": -0.34734907244691754, "compression_ratio": 1.4744897959183674, "no_speech_prob": 0.056708335876464844}, {"id": 156, "seek": 112386, "start": 1145.6599999999999, "end": 1153.06, "text": " \u00bfDel total de palabras que hay? \u00bfVoy a contar? Positivo, 1, 2, 3, 4, 5, 6, 7, 8, 9.", "tokens": [51454, 3841, 40848, 3217, 368, 35240, 631, 4842, 30, 3841, 53, 939, 257, 27045, 30, 430, 9598, 6340, 11, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 11, 1614, 11, 1649, 11, 1722, 13, 51824], "temperature": 0.0, "avg_logprob": -0.34734907244691754, "compression_ratio": 1.4744897959183674, "no_speech_prob": 0.056708335876464844}, {"id": 157, "seek": 115386, "start": 1154.58, "end": 1158.9799999999998, "text": " Son todas, porque ac\u00e1 yo estoy considerando todas las ocurrencias. Es una de las cosas que se", "tokens": [50400, 5185, 10906, 11, 4021, 23496, 5290, 15796, 1949, 1806, 10906, 2439, 26430, 1095, 12046, 13, 2313, 2002, 368, 2439, 12218, 631, 369, 50620], "temperature": 0.0, "avg_logprob": -0.25377239615230235, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.0018009654013440013}, {"id": 158, "seek": 115386, "start": 1158.9799999999998, "end": 1164.06, "text": " le critican, ah\u00ed vayan, es general, es eso, que si yo repito muchas veces algo, le sumo probabilidad.", "tokens": [50620, 476, 7850, 282, 11, 12571, 371, 20506, 11, 785, 2674, 11, 785, 7287, 11, 631, 1511, 5290, 1085, 3528, 16072, 17054, 8655, 11, 476, 2408, 78, 31959, 4580, 13, 50874], "temperature": 0.0, "avg_logprob": -0.25377239615230235, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.0018009654013440013}, {"id": 159, "seek": 115386, "start": 1166.9399999999998, "end": 1171.3799999999999, "text": " Que a veces no es lo que se quiere, digamos. Si hay atributos que reiteran cosas,", "tokens": [51018, 4493, 257, 17054, 572, 785, 450, 631, 369, 23877, 11, 36430, 13, 4909, 4842, 412, 2024, 34640, 631, 25211, 282, 12218, 11, 51240], "temperature": 0.0, "avg_logprob": -0.25377239615230235, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.0018009654013440013}, {"id": 160, "seek": 115386, "start": 1171.3799999999999, "end": 1175.06, "text": " es como que est\u00e1n muy relacionados y no est\u00e1n aportando informaci\u00f3n.", "tokens": [51240, 785, 2617, 631, 10368, 5323, 27189, 4181, 288, 572, 10368, 1882, 477, 1806, 21660, 13, 51424], "temperature": 0.0, "avg_logprob": -0.25377239615230235, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.0018009654013440013}, {"id": 161, "seek": 115386, "start": 1176.8999999999999, "end": 1181.4199999999998, "text": " Entonces, ac\u00e1 est\u00e1n todas las probabilidades de las diferentes palabras. F\u00edjense,", "tokens": [51516, 15097, 11, 23496, 10368, 10906, 2439, 31959, 10284, 368, 2439, 17686, 35240, 13, 479, 870, 73, 1288, 11, 51742], "temperature": 0.0, "avg_logprob": -0.25377239615230235, "compression_ratio": 1.6452830188679246, "no_speech_prob": 0.0018009654013440013}, {"id": 162, "seek": 118142, "start": 1182.14, "end": 1185.8600000000001, "text": " bueno, ahora nos fijamos en el ejemplo. Con esas probabilidades, esa es nuestra,", "tokens": [50400, 11974, 11, 9923, 3269, 42001, 2151, 465, 806, 13358, 13, 2656, 23388, 31959, 10284, 11, 11342, 785, 16825, 11, 50586], "temperature": 0.0, "avg_logprob": -0.2791273711157627, "compression_ratio": 1.7294117647058824, "no_speech_prob": 0.006045125890523195}, {"id": 163, "seek": 118142, "start": 1186.8600000000001, "end": 1192.8200000000002, "text": " es como entrenamos nuestro clasificador, esencialmente. \u00bfDe acuerdo? Es decir, a partir del", "tokens": [50636, 785, 2617, 45069, 2151, 14726, 596, 296, 1089, 5409, 11, 785, 26567, 4082, 13, 3841, 11089, 28113, 30, 2313, 10235, 11, 257, 13906, 1103, 50934], "temperature": 0.0, "avg_logprob": -0.2791273711157627, "compression_ratio": 1.7294117647058824, "no_speech_prob": 0.006045125890523195}, {"id": 164, "seek": 118142, "start": 1192.8200000000002, "end": 1195.7, "text": " cuerpo de entrenamiento, yo calculo esta probabilidad y lo que estoy haciendo es entrenar.", "tokens": [50934, 20264, 368, 45069, 16971, 11, 5290, 4322, 78, 5283, 31959, 4580, 288, 450, 631, 15796, 20509, 785, 45069, 289, 13, 51078], "temperature": 0.0, "avg_logprob": -0.2791273711157627, "compression_ratio": 1.7294117647058824, "no_speech_prob": 0.006045125890523195}, {"id": 165, "seek": 118142, "start": 1196.98, "end": 1203.14, "text": " Como ustedes ven, son cuentas muy sencillas de hacer. El clasificador, no hay vaya,", "tokens": [51142, 11913, 17110, 6138, 11, 1872, 46414, 296, 5323, 46749, 296, 368, 6720, 13, 2699, 596, 296, 1089, 5409, 11, 572, 4842, 47682, 11, 51450], "temperature": 0.0, "avg_logprob": -0.2791273711157627, "compression_ratio": 1.7294117647058824, "no_speech_prob": 0.006045125890523195}, {"id": 166, "seek": 118142, "start": 1203.14, "end": 1208.94, "text": " la ventaja que tiene, es que es muy r\u00e1pido, muy, muy r\u00e1pido. Tanto para entrenar como para", "tokens": [51450, 635, 6931, 12908, 631, 7066, 11, 785, 631, 785, 5323, 24893, 11, 5323, 11, 5323, 24893, 13, 314, 5857, 1690, 45069, 289, 2617, 1690, 51740], "temperature": 0.0, "avg_logprob": -0.2791273711157627, "compression_ratio": 1.7294117647058824, "no_speech_prob": 0.006045125890523195}, {"id": 167, "seek": 120894, "start": 1209.9, "end": 1215.02, "text": " evaluar. Entonces, cuando uno quiera acercarse a un problema y ver, \u00bfqu\u00e9 tan dif\u00edcil es", "tokens": [50412, 6133, 289, 13, 15097, 11, 7767, 8526, 421, 10609, 696, 2869, 11668, 257, 517, 12395, 288, 1306, 11, 3841, 16412, 7603, 17258, 785, 50668], "temperature": 0.0, "avg_logprob": -0.25717641689159254, "compression_ratio": 1.508695652173913, "no_speech_prob": 0.0130604924634099}, {"id": 168, "seek": 120894, "start": 1215.02, "end": 1221.8600000000001, "text": " clasificar un cuerpo de humor? Entonces, se le arrima con un m\u00e9todo de esto,", "tokens": [50668, 596, 296, 25625, 517, 20264, 368, 14318, 30, 15097, 11, 369, 476, 5539, 4775, 416, 517, 20275, 17423, 368, 7433, 11, 51010], "temperature": 0.0, "avg_logprob": -0.25717641689159254, "compression_ratio": 1.508695652173913, "no_speech_prob": 0.0130604924634099}, {"id": 169, "seek": 120894, "start": 1223.7, "end": 1229.46, "text": " que lo entrenan dos patadas, y m\u00e1s o menos tiene una idea. Dice, ah, mirad, que pude clasificar", "tokens": [51102, 631, 450, 45069, 282, 4491, 1947, 6872, 11, 288, 3573, 277, 8902, 7066, 2002, 1558, 13, 413, 573, 11, 3716, 11, 3149, 345, 11, 631, 280, 2303, 596, 296, 25625, 51390], "temperature": 0.0, "avg_logprob": -0.25717641689159254, "compression_ratio": 1.508695652173913, "no_speech_prob": 0.0130604924634099}, {"id": 170, "seek": 120894, "start": 1229.46, "end": 1235.18, "text": " el 75, 80% del olor. O sea, lo que es un problema que tiene para mejorar un poco,", "tokens": [51390, 806, 9562, 11, 4688, 4, 1103, 2545, 284, 13, 422, 4158, 11, 450, 631, 785, 517, 12395, 631, 7066, 1690, 48858, 517, 10639, 11, 51676], "temperature": 0.0, "avg_logprob": -0.25717641689159254, "compression_ratio": 1.508695652173913, "no_speech_prob": 0.0130604924634099}, {"id": 171, "seek": 123518, "start": 1235.7, "end": 1242.98, "text": " tampoco es que es horrible y dif\u00edcil. Y luego, s\u00ed, empieza a afinar, a ajustar par\u00e1metros,", "tokens": [50390, 36838, 785, 631, 785, 9263, 288, 17258, 13, 398, 17222, 11, 8600, 11, 44577, 257, 3238, 6470, 11, 257, 41023, 289, 971, 842, 29570, 11, 50754], "temperature": 0.0, "avg_logprob": -0.2639887905860132, "compression_ratio": 1.5469798657718121, "no_speech_prob": 0.018816370517015457}, {"id": 172, "seek": 123518, "start": 1242.98, "end": 1247.0600000000002, "text": " a cambiar el m\u00e9todo, capaz que le mete una red o agregarle datos, le mete una red neuronal", "tokens": [50754, 257, 37738, 806, 20275, 17423, 11, 35453, 631, 476, 21245, 2002, 2182, 277, 4554, 2976, 306, 27721, 11, 476, 21245, 2002, 2182, 12087, 21523, 50958], "temperature": 0.0, "avg_logprob": -0.2639887905860132, "compression_ratio": 1.5469798657718121, "no_speech_prob": 0.018816370517015457}, {"id": 173, "seek": 123518, "start": 1247.0600000000002, "end": 1253.14, "text": " que est\u00e1 una semana entrenando. Pero con esto tiene una primera aproximaci\u00f3n, por lo menos.", "tokens": [50958, 631, 3192, 2002, 20205, 45069, 1806, 13, 9377, 416, 7433, 7066, 2002, 17382, 31270, 3482, 11, 1515, 450, 8902, 13, 51262], "temperature": 0.0, "avg_logprob": -0.2639887905860132, "compression_ratio": 1.5469798657718121, "no_speech_prob": 0.018816370517015457}, {"id": 174, "seek": 123518, "start": 1253.14, "end": 1258.6200000000001, "text": " A m\u00ed se alcanza, pasa alguien en los medios. Porque depende la tarea que estamos haciendo.", "tokens": [51262, 316, 14692, 369, 419, 7035, 2394, 11, 20260, 25814, 465, 1750, 46017, 13, 11287, 47091, 635, 256, 35425, 631, 10382, 20509, 13, 51536], "temperature": 0.0, "avg_logprob": -0.2639887905860132, "compression_ratio": 1.5469798657718121, "no_speech_prob": 0.018816370517015457}, {"id": 175, "seek": 123518, "start": 1258.6200000000001, "end": 1263.94, "text": " Bueno, \u00bfpero qu\u00e9 pasa? Entonces, \u00bfc\u00f3mo clasifico? Y bueno, si la palabra es prevista,", "tokens": [51536, 16046, 11, 3841, 49565, 8057, 20260, 30, 15097, 11, 3841, 46614, 596, 296, 1089, 78, 30, 398, 11974, 11, 1511, 635, 31702, 785, 12642, 5236, 11, 51802], "temperature": 0.0, "avg_logprob": -0.2639887905860132, "compression_ratio": 1.5469798657718121, "no_speech_prob": 0.018816370517015457}, {"id": 176, "seek": 126394, "start": 1263.94, "end": 1270.78, "text": " volvi\u00f3 en no originality, yo tengo la probabilidad de la oraci\u00f3n dada la categor\u00eda negativa por", "tokens": [50364, 1996, 4917, 812, 465, 572, 4957, 1860, 11, 5290, 13989, 635, 31959, 4580, 368, 635, 420, 3482, 274, 1538, 635, 19250, 2686, 2485, 18740, 1515, 50706], "temperature": 0.0, "avg_logprob": -0.2269542091771176, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.012467916123569012}, {"id": 177, "seek": 126394, "start": 1270.78, "end": 1277.38, "text": " la probabilidad de la categor\u00eda negativa. O sea, que es 3 dividido 5 por las diferentes", "tokens": [50706, 635, 31959, 4580, 368, 635, 19250, 2686, 2485, 18740, 13, 422, 4158, 11, 631, 785, 805, 4996, 2925, 1025, 1515, 2439, 17686, 51036], "temperature": 0.0, "avg_logprob": -0.2269542091771176, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.012467916123569012}, {"id": 178, "seek": 126394, "start": 1277.38, "end": 1283.7, "text": " probabilidades de las palabras que aparecen en la categor\u00eda negativa. Si se fijan ac\u00e1 estos", "tokens": [51036, 31959, 10284, 368, 2439, 35240, 631, 15004, 13037, 465, 635, 19250, 2686, 2485, 18740, 13, 4909, 369, 42001, 282, 23496, 12585, 51352], "temperature": 0.0, "avg_logprob": -0.2269542091771176, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.012467916123569012}, {"id": 179, "seek": 128370, "start": 1283.78, "end": 1301.14, "text": " 1 es porque no aparec\u00edan. Y ac\u00e1, f\u00edjense que originality es una palabra montinando a positiva,", "tokens": [50368, 502, 785, 4021, 572, 15004, 66, 11084, 13, 398, 23496, 11, 283, 870, 73, 1288, 631, 3380, 507, 785, 2002, 31702, 8143, 259, 1806, 257, 11218, 5931, 11, 51236], "temperature": 0.0, "avg_logprob": -0.4638156592845917, "compression_ratio": 1.0543478260869565, "no_speech_prob": 0.0707908347249031}, {"id": 180, "seek": 130114, "start": 1301.14, "end": 1316.18, "text": " \u00bfno? Es 1 sobre 29 contra 1 sobre 34. O sea, que est\u00e1 mejor en la positiva que en la negativa.", "tokens": [50364, 3841, 1771, 30, 2313, 502, 5473, 9413, 10742, 502, 5473, 12790, 13, 422, 4158, 11, 631, 3192, 11479, 465, 635, 11218, 5931, 631, 465, 635, 2485, 18740, 13, 51116], "temperature": 0.0, "avg_logprob": -0.25188276502821183, "compression_ratio": 1.4060913705583757, "no_speech_prob": 0.05762319266796112}, {"id": 181, "seek": 130114, "start": 1316.18, "end": 1320.9, "text": " \u00bfS\u00ed? \u00bfPor qu\u00e9? Porque aparecen contextos positivos, realmente. Ac\u00e1 el problema que", "tokens": [51116, 3841, 30463, 30, 3841, 24907, 8057, 30, 11287, 15004, 13037, 4319, 329, 11218, 16501, 11, 14446, 13, 5097, 842, 806, 12395, 631, 51352], "temperature": 0.0, "avg_logprob": -0.25188276502821183, "compression_ratio": 1.4060913705583757, "no_speech_prob": 0.05762319266796112}, {"id": 182, "seek": 130114, "start": 1320.9, "end": 1327.3400000000001, "text": " tienen no, adelante. Que es uno de los problemas que ahora vamos a ver. Pero de todos modos,", "tokens": [51352, 12536, 572, 11, 40214, 13, 4493, 785, 8526, 368, 1750, 20720, 631, 9923, 5295, 257, 1306, 13, 9377, 368, 6321, 1072, 329, 11, 51674], "temperature": 0.0, "avg_logprob": -0.25188276502821183, "compression_ratio": 1.4060913705583757, "no_speech_prob": 0.05762319266796112}, {"id": 183, "seek": 132734, "start": 1327.9399999999998, "end": 1334.78, "text": " multiplicando las probabilidades de cada palabra, llega que es m\u00e1s probable que sea negativa.", "tokens": [50394, 17596, 1806, 2439, 31959, 10284, 368, 8411, 31702, 11, 40423, 631, 785, 3573, 21759, 631, 4158, 2485, 18740, 13, 50736], "temperature": 0.0, "avg_logprob": -0.3184312275477818, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0075116693042218685}, {"id": 184, "seek": 132734, "start": 1336.3, "end": 1341.58, "text": " \u00bfY por qu\u00e9? Porque dice pred\u00edctabel, seguramente. \u00bfPor qu\u00e9 dice no?", "tokens": [50812, 3841, 56, 1515, 8057, 30, 11287, 10313, 3852, 870, 349, 18657, 11, 22179, 3439, 13, 3841, 24907, 8057, 10313, 572, 30, 51076], "temperature": 0.0, "avg_logprob": -0.3184312275477818, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0075116693042218685}, {"id": 185, "seek": 132734, "start": 1349.06, "end": 1354.6999999999998, "text": " En realidad esto es number crunching, \u00bfno? Es porque hay un motivo, digamos, uno de las", "tokens": [51450, 2193, 25635, 7433, 785, 1230, 13386, 278, 11, 3841, 1771, 30, 2313, 4021, 4842, 517, 35804, 11, 36430, 11, 8526, 368, 2439, 51732], "temperature": 0.0, "avg_logprob": -0.3184312275477818, "compression_ratio": 1.391304347826087, "no_speech_prob": 0.0075116693042218685}, {"id": 186, "seek": 135470, "start": 1354.7, "end": 1358.3, "text": " aplicaciones son siempre aposteriores en estas cosas, \u00bfno? Es decir, bueno, pas\u00f3 esto, pero en", "tokens": [50364, 18221, 9188, 1872, 12758, 1882, 7096, 72, 2706, 465, 13897, 12218, 11, 3841, 1771, 30, 2313, 10235, 11, 11974, 11, 41382, 7433, 11, 4768, 465, 50544], "temperature": 0.0, "avg_logprob": -0.30883437087855387, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.003847400425001979}, {"id": 187, "seek": 135470, "start": 1358.3, "end": 1365.98, "text": " realidad esto es un motivo de sus cuentas, inicialmente. \u00bfSe entiende? \u00bfSe entiende ac\u00e1?", "tokens": [50544, 25635, 7433, 785, 517, 35804, 368, 3291, 46414, 296, 11, 44076, 4082, 13, 3841, 10637, 948, 45816, 30, 3841, 10637, 948, 45816, 23496, 30, 50928], "temperature": 0.0, "avg_logprob": -0.30883437087855387, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.003847400425001979}, {"id": 188, "seek": 135470, "start": 1369.98, "end": 1377.1000000000001, "text": " Si nosotros queremos hacer sentimentan\u00e1lisis, para el caso particular de clasificaci\u00f3n de", "tokens": [51128, 4909, 13863, 26813, 6720, 16149, 282, 11447, 28436, 11, 1690, 806, 9666, 1729, 368, 596, 296, 40802, 368, 51484], "temperature": 0.0, "avg_logprob": -0.30883437087855387, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.003847400425001979}, {"id": 189, "seek": 135470, "start": 1377.1000000000001, "end": 1382.18, "text": " documentos que se llama sentimentan\u00e1lisis, que es ver la impresi\u00f3n respecto a algo,", "tokens": [51484, 4166, 329, 631, 369, 23272, 16149, 282, 11447, 28436, 11, 631, 785, 1306, 635, 35672, 2560, 35694, 257, 8655, 11, 51738], "temperature": 0.0, "avg_logprob": -0.30883437087855387, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.003847400425001979}, {"id": 190, "seek": 138218, "start": 1382.5, "end": 1387.3400000000001, "text": " a un documento, hay algunas reglas que permiten mejorar la performance.", "tokens": [50380, 257, 517, 4166, 78, 11, 4842, 27316, 1121, 7743, 631, 13423, 268, 48858, 635, 3389, 13, 50622], "temperature": 0.0, "avg_logprob": -0.28261926097254597, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.009050961583852768}, {"id": 191, "seek": 138218, "start": 1389.8600000000001, "end": 1393.94, "text": " Es lo mismo, es exactamente lo mismo, las clases son las mismas, pero se puede hacer", "tokens": [50748, 2313, 450, 12461, 11, 785, 48686, 450, 12461, 11, 2439, 596, 1957, 1872, 2439, 23220, 296, 11, 4768, 369, 8919, 6720, 50952], "temperature": 0.0, "avg_logprob": -0.28261926097254597, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.009050961583852768}, {"id": 192, "seek": 138218, "start": 1393.94, "end": 1399.38, "text": " alguna modificaci\u00f3n. Por ejemplo, no contar m\u00faltiples ocurrencias en la palabra en el mismo", "tokens": [50952, 20651, 1072, 40802, 13, 5269, 13358, 11, 572, 27045, 275, 43447, 72, 2622, 26430, 1095, 12046, 465, 635, 31702, 465, 806, 12461, 51224], "temperature": 0.0, "avg_logprob": -0.28261926097254597, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.009050961583852768}, {"id": 193, "seek": 138218, "start": 1399.38, "end": 1404.02, "text": " documento. Esto que yo les dec\u00eda hoy, cuento una vez olas. Y se dice, es muy, muy linda,", "tokens": [51224, 4166, 78, 13, 20880, 631, 5290, 1512, 37599, 13775, 11, 2702, 15467, 2002, 5715, 2545, 296, 13, 398, 369, 10313, 11, 785, 5323, 11, 5323, 287, 6837, 11, 51456], "temperature": 0.0, "avg_logprob": -0.28261926097254597, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.009050961583852768}, {"id": 194, "seek": 138218, "start": 1404.02, "end": 1411.9, "text": " linda, linda, cuento una vez olas. Eso se llama binary navages. El manejo de la", "tokens": [51456, 287, 6837, 11, 287, 6837, 11, 2702, 15467, 2002, 5715, 2545, 296, 13, 27795, 369, 23272, 17434, 5947, 1660, 13, 2699, 12743, 5134, 368, 635, 51850], "temperature": 0.0, "avg_logprob": -0.28261926097254597, "compression_ratio": 1.6867469879518073, "no_speech_prob": 0.009050961583852768}, {"id": 195, "seek": 141190, "start": 1411.94, "end": 1418.5800000000002, "text": " innovaci\u00f3n es todo un tema, es todo un tema, el manejo de la innovaci\u00f3n. Y una aproximaci\u00f3n", "tokens": [50366, 5083, 3482, 785, 5149, 517, 15854, 11, 785, 5149, 517, 15854, 11, 806, 12743, 5134, 368, 635, 5083, 3482, 13, 398, 2002, 31270, 3482, 50698], "temperature": 0.0, "avg_logprob": -0.23956102795071071, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0016651791520416737}, {"id": 196, "seek": 141190, "start": 1418.5800000000002, "end": 1427.66, "text": " muy, muy sencilla, muy na\u00ed, pero que mejora las cosas, pues bueno, yo a todo lo que dice", "tokens": [50698, 5323, 11, 5323, 3151, 66, 5291, 11, 5323, 1667, 870, 11, 4768, 631, 11479, 64, 2439, 12218, 11, 11059, 11974, 11, 5290, 257, 5149, 450, 631, 10313, 51152], "temperature": 0.0, "avg_logprob": -0.23956102795071071, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0016651791520416737}, {"id": 197, "seek": 141190, "start": 1427.66, "end": 1431.8600000000001, "text": " despu\u00e9s de didn't, lo clasifico no como like, sino como not like, invento una palabra nueva.", "tokens": [51152, 15283, 368, 994, 380, 11, 450, 596, 296, 1089, 78, 572, 2617, 411, 11, 18108, 2617, 406, 411, 11, 7962, 78, 2002, 31702, 28963, 13, 51362], "temperature": 0.0, "avg_logprob": -0.23956102795071071, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0016651791520416737}, {"id": 198, "seek": 141190, "start": 1435.14, "end": 1439.9, "text": " Podr\u00eda llegar a hacer alguna cosa un poco m\u00e1s elaborada si tuviera un parser, porque si yo", "tokens": [51526, 12646, 37183, 24892, 257, 6720, 20651, 10163, 517, 10639, 3573, 16298, 1538, 1511, 38177, 10609, 517, 21156, 260, 11, 4021, 1511, 5290, 51764], "temperature": 0.0, "avg_logprob": -0.23956102795071071, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0016651791520416737}, {"id": 199, "seek": 143990, "start": 1439.94, "end": 1445.6200000000001, "text": " tengo un parser, tengo el \u00e1rbol y tengo una rama que dice no todo lo que hay abajo.", "tokens": [50366, 13989, 517, 21156, 260, 11, 13989, 806, 35349, 17460, 288, 13989, 2002, 367, 2404, 631, 10313, 572, 5149, 450, 631, 4842, 30613, 13, 50650], "temperature": 0.0, "avg_logprob": -0.25351532059486465, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.0102434316650033}, {"id": 200, "seek": 143990, "start": 1445.6200000000001, "end": 1448.8200000000002, "text": " Entonces yo s\u00e9 el alcance de no. Ah\u00ed igual el problema est\u00e1 en c\u00f3mo hacer el parsing,", "tokens": [50650, 15097, 5290, 7910, 806, 20005, 719, 368, 572, 13, 49924, 10953, 806, 12395, 3192, 465, 12826, 6720, 806, 21156, 278, 11, 50810], "temperature": 0.0, "avg_logprob": -0.25351532059486465, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.0102434316650033}, {"id": 201, "seek": 143990, "start": 1448.8200000000002, "end": 1457.98, "text": " pero si yo le agrego informaci\u00f3n de parsing, la cosa puede mejorar. De parsing vamos a", "tokens": [50810, 4768, 1511, 5290, 476, 623, 3375, 78, 21660, 368, 21156, 278, 11, 635, 10163, 8919, 48858, 13, 1346, 21156, 278, 5295, 257, 51268], "temperature": 0.0, "avg_logprob": -0.25351532059486465, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.0102434316650033}, {"id": 202, "seek": 143990, "start": 1457.98, "end": 1466.8200000000002, "text": " hablar la semana que viene, pero yo dir\u00eda que... H\u00e1ganme acordar que hable el final", "tokens": [51268, 21014, 635, 20205, 631, 19561, 11, 4768, 5290, 4746, 2686, 631, 485, 389, 842, 1275, 1398, 38077, 289, 631, 324, 638, 806, 2572, 51710], "temperature": 0.0, "avg_logprob": -0.25351532059486465, "compression_ratio": 1.5466666666666666, "no_speech_prob": 0.0102434316650033}, {"id": 203, "seek": 146682, "start": 1466.98, "end": 1472.9399999999998, "text": " de esto, del parsing. Esta, pero esta es una primera aproximaci\u00f3n, \u00bfentiendes? Creo unas", "tokens": [50372, 368, 7433, 11, 1103, 21156, 278, 13, 20547, 11, 4768, 5283, 785, 2002, 17382, 31270, 3482, 11, 3841, 317, 1174, 279, 30, 40640, 25405, 50670], "temperature": 0.0, "avg_logprob": -0.2768787887861144, "compression_ratio": 1.501930501930502, "no_speech_prob": 0.015542634762823582}, {"id": 204, "seek": 146682, "start": 1472.9399999999998, "end": 1479.7, "text": " palabras nuevas ah\u00ed y ahora el like se cuenta como not like. Es muy na\u00ed porque dice todo lo que", "tokens": [50670, 35240, 42817, 12571, 288, 9923, 806, 411, 369, 17868, 2617, 406, 411, 13, 2313, 5323, 1667, 870, 4021, 10313, 5149, 450, 631, 51008], "temperature": 0.0, "avg_logprob": -0.2768787887861144, "compression_ratio": 1.501930501930502, "no_speech_prob": 0.015542634762823582}, {"id": 205, "seek": 146682, "start": 1479.7, "end": 1485.78, "text": " est\u00e1 despu\u00e9s de didn't, pero podr\u00eda haber otras cosas en el medio. No, no es tan sencillo, digamos,", "tokens": [51008, 3192, 15283, 368, 994, 380, 11, 4768, 27246, 15811, 20244, 12218, 465, 806, 22123, 13, 883, 11, 572, 785, 7603, 46749, 78, 11, 36430, 11, 51312], "temperature": 0.0, "avg_logprob": -0.2768787887861144, "compression_ratio": 1.501930501930502, "no_speech_prob": 0.015542634762823582}, {"id": 206, "seek": 146682, "start": 1485.78, "end": 1492.3799999999999, "text": " pues las oraciones son m\u00e1s complicadas. No, creo que pienses que, y hay un que ah\u00ed con oraci\u00f3n", "tokens": [51312, 11059, 2439, 420, 9188, 1872, 3573, 16060, 6872, 13, 883, 11, 14336, 631, 3895, 9085, 631, 11, 288, 4842, 517, 631, 12571, 416, 420, 3482, 51642], "temperature": 0.0, "avg_logprob": -0.2768787887861144, "compression_ratio": 1.501930501930502, "no_speech_prob": 0.015542634762823582}, {"id": 207, "seek": 149238, "start": 1492.5400000000002, "end": 1501.18, "text": " subordinada, puede ser m\u00e1s complejo que esto, pero no da rimamos. Y otra aproximaci\u00f3n, por supuesto,", "tokens": [50372, 1422, 6241, 1538, 11, 8919, 816, 3573, 44424, 5134, 631, 7433, 11, 4768, 572, 1120, 15982, 2151, 13, 398, 13623, 31270, 3482, 11, 1515, 34177, 11, 50804], "temperature": 0.0, "avg_logprob": -0.30717792009052475, "compression_ratio": 1.4685714285714286, "no_speech_prob": 0.01770167052745819}, {"id": 208, "seek": 149238, "start": 1501.18, "end": 1508.6200000000001, "text": " es usar lo que se llama lexicones de sentimiento, que son listas de palabras positivas y listas", "tokens": [50804, 785, 14745, 450, 631, 369, 23272, 476, 47228, 2213, 368, 2279, 14007, 11, 631, 1872, 1329, 296, 368, 35240, 11218, 24759, 288, 1329, 296, 51176], "temperature": 0.0, "avg_logprob": -0.30717792009052475, "compression_ratio": 1.4685714285714286, "no_speech_prob": 0.01770167052745819}, {"id": 209, "seek": 149238, "start": 1508.6200000000001, "end": 1512.42, "text": " de palabras negativas. Tengo una lista recolectada, \u00bfs\u00ed?", "tokens": [51176, 368, 35240, 2485, 35725, 13, 314, 30362, 2002, 27764, 850, 4812, 349, 1538, 11, 3841, 82, 870, 30, 51366], "temperature": 0.0, "avg_logprob": -0.30717792009052475, "compression_ratio": 1.4685714285714286, "no_speech_prob": 0.01770167052745819}, {"id": 210, "seek": 151242, "start": 1513.14, "end": 1518.98, "text": " En el caso de que est\u00e1 mostrando como se llena la palabra, \u00bfno? S\u00ed. Como no estaba, no ten\u00eda", "tokens": [50400, 2193, 806, 9666, 368, 631, 3192, 881, 19845, 2617, 369, 4849, 4118, 635, 31702, 11, 3841, 1771, 30, 12375, 13, 11913, 572, 17544, 11, 572, 23718, 50692], "temperature": 0.0, "avg_logprob": -0.4332692733663597, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.1045941412448883}, {"id": 211, "seek": 151242, "start": 1518.98, "end": 1523.1000000000001, "text": " supexicon, cualquier cosa que pusiera, de originales, \u00bfno? Cualquier cosa que pusiera", "tokens": [50692, 459, 494, 87, 11911, 11, 21004, 10163, 631, 31252, 10609, 11, 368, 3380, 279, 11, 3841, 1771, 30, 383, 901, 16622, 10163, 631, 31252, 10609, 50898], "temperature": 0.0, "avg_logprob": -0.4332692733663597, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.1045941412448883}, {"id": 212, "seek": 151242, "start": 1523.1000000000001, "end": 1529.14, "text": " sento para originales y que no estuvieran entre niches, la primera, \u00bfno? Ah, s\u00ed, s\u00ed, claro, claro,", "tokens": [50898, 2279, 78, 1690, 3380, 279, 288, 631, 572, 49777, 38516, 3962, 25570, 279, 11, 635, 17382, 11, 3841, 1771, 30, 2438, 11, 8600, 11, 8600, 11, 16742, 11, 16742, 11, 51200], "temperature": 0.0, "avg_logprob": -0.4332692733663597, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.1045941412448883}, {"id": 213, "seek": 151242, "start": 1529.14, "end": 1533.6200000000001, "text": " claro, claro, claro. De todos modos se supone que vos, en todo este tipo de m\u00e9todos, justamente lo", "tokens": [51200, 16742, 11, 16742, 11, 16742, 13, 1346, 6321, 1072, 329, 369, 9331, 546, 631, 13845, 11, 465, 5149, 4065, 9746, 368, 20275, 378, 329, 11, 41056, 450, 51424], "temperature": 0.0, "avg_logprob": -0.4332692733663597, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.1045941412448883}, {"id": 214, "seek": 151242, "start": 1533.6200000000001, "end": 1538.9, "text": " que supone es que como vos ten\u00e9s grandes vol\u00famenes, si no, no funcionan. Claro, claro, claro. Es decir,", "tokens": [51424, 631, 9331, 546, 785, 631, 2617, 13845, 2064, 2191, 16640, 1996, 2481, 2558, 279, 11, 1511, 572, 11, 572, 14186, 282, 13, 33380, 11, 16742, 11, 16742, 13, 2313, 10235, 11, 51688], "temperature": 0.0, "avg_logprob": -0.4332692733663597, "compression_ratio": 1.7985347985347986, "no_speech_prob": 0.1045941412448883}, {"id": 215, "seek": 153890, "start": 1538.9, "end": 1546.9, "text": " que lo que hacen es capturar algo a partir de muchas ocurrencias. Pero s\u00ed, si no parece,", "tokens": [50364, 631, 450, 631, 27434, 785, 3770, 28586, 8655, 257, 13906, 368, 16072, 26430, 1095, 12046, 13, 9377, 8600, 11, 1511, 572, 14120, 11, 50764], "temperature": 0.0, "avg_logprob": -0.2148865264596291, "compression_ratio": 1.6790697674418604, "no_speech_prob": 0.006682645063847303}, {"id": 216, "seek": 153890, "start": 1546.9, "end": 1555.5800000000002, "text": " si ten\u00e9s cero, es la misma para todas. En el lexic\u00f3n, entonces vos lo que pod\u00e9s hacer", "tokens": [50764, 1511, 2064, 2191, 269, 2032, 11, 785, 635, 24946, 1690, 10906, 13, 2193, 806, 476, 47228, 1801, 11, 13003, 13845, 450, 631, 2497, 2191, 6720, 51198], "temperature": 0.0, "avg_logprob": -0.2148865264596291, "compression_ratio": 1.6790697674418604, "no_speech_prob": 0.006682645063847303}, {"id": 217, "seek": 153890, "start": 1555.5800000000002, "end": 1558.94, "text": " es agregar, en tu clasificador, simplemente una fitur que dice la cantidad de palabras en", "tokens": [51198, 785, 4554, 2976, 11, 465, 2604, 596, 296, 1089, 5409, 11, 33190, 2002, 3318, 374, 631, 10313, 635, 33757, 368, 35240, 465, 51366], "temperature": 0.0, "avg_logprob": -0.2148865264596291, "compression_ratio": 1.6790697674418604, "no_speech_prob": 0.006682645063847303}, {"id": 218, "seek": 153890, "start": 1558.94, "end": 1562.74, "text": " un lexic\u00f3n positivo y la cantidad de palabras en un lexic\u00f3n negativo. Es decir, tiene tres", "tokens": [51366, 517, 476, 47228, 1801, 44710, 288, 635, 33757, 368, 35240, 465, 517, 476, 47228, 1801, 2485, 18586, 13, 2313, 10235, 11, 7066, 15890, 51556], "temperature": 0.0, "avg_logprob": -0.2148865264596291, "compression_ratio": 1.6790697674418604, "no_speech_prob": 0.006682645063847303}, {"id": 219, "seek": 156274, "start": 1562.74, "end": 1570.66, "text": " palabras negativas, es un X, Xn m\u00e1s 1 y Xn m\u00e1s 2, son dos atributos nom\u00e1s, \u00bfs\u00ed? Y la", "tokens": [50364, 35240, 2485, 35725, 11, 785, 517, 1783, 11, 1783, 77, 3573, 502, 288, 1783, 77, 3573, 568, 11, 1872, 4491, 412, 2024, 34640, 5369, 2490, 11, 3841, 82, 870, 30, 398, 635, 50760], "temperature": 0.0, "avg_logprob": -0.20846475873674666, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.43634033203125}, {"id": 220, "seek": 156274, "start": 1570.66, "end": 1575.6200000000001, "text": " cantidad de palabras en un lexic\u00f3n negativo. Le agrego dos atributos que, si recordamos", "tokens": [50760, 33757, 368, 35240, 465, 517, 476, 47228, 1801, 2485, 18586, 13, 1456, 4554, 1571, 4491, 412, 2024, 34640, 631, 11, 1511, 2136, 2151, 51008], "temperature": 0.0, "avg_logprob": -0.20846475873674666, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.43634033203125}, {"id": 221, "seek": 156274, "start": 1575.6200000000001, "end": 1581.18, "text": " en la clase pasada, van a seguramente estar m\u00e1s correlacionados con la clase y nos van", "tokens": [51008, 465, 635, 44578, 1736, 1538, 11, 3161, 257, 22179, 3439, 8755, 3573, 13983, 18803, 4181, 416, 635, 44578, 288, 3269, 3161, 51286], "temperature": 0.0, "avg_logprob": -0.20846475873674666, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.43634033203125}, {"id": 222, "seek": 156274, "start": 1581.18, "end": 1586.58, "text": " a poder dar una pista de su comportamiento. Si llegara a hacer un m\u00e9todo de regla que", "tokens": [51286, 257, 8152, 4072, 2002, 49516, 368, 459, 25883, 16971, 13, 4909, 11234, 2419, 257, 6720, 517, 20275, 17423, 368, 1121, 875, 631, 51556], "temperature": 0.0, "avg_logprob": -0.20846475873674666, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.43634033203125}, {"id": 223, "seek": 156274, "start": 1586.58, "end": 1592.46, "text": " dice, bueno, el que tiene m\u00e1s palabra positiva gana, porque juegan todas, intervienen mucho", "tokens": [51556, 10313, 11, 11974, 11, 806, 631, 7066, 3573, 31702, 11218, 5931, 290, 2095, 11, 4021, 27833, 1275, 10906, 11, 728, 85, 22461, 9824, 51850], "temperature": 0.0, "avg_logprob": -0.20846475873674666, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.43634033203125}, {"id": 224, "seek": 159246, "start": 1592.46, "end": 1597.14, "text": " en la clasificaci\u00f3n, \u00bfs\u00ed? \u00bfDe acuerdo?", "tokens": [50364, 465, 635, 596, 296, 40802, 11, 3841, 82, 870, 30, 3841, 11089, 28113, 30, 50598], "temperature": 0.0, "avg_logprob": -0.26234185177346936, "compression_ratio": 1.4813084112149533, "no_speech_prob": 0.01821318082511425}, {"id": 225, "seek": 159246, "start": 1597.14, "end": 1606.6200000000001, "text": " Otro ejemplo, \u00bfc\u00f3mo puedo hacer para calcular un tag de part of pitch si tengo la palabra", "tokens": [50598, 12936, 340, 13358, 11, 3841, 46614, 21612, 6720, 1690, 2104, 17792, 517, 6162, 368, 644, 295, 7293, 1511, 13989, 635, 31702, 51072], "temperature": 0.0, "avg_logprob": -0.26234185177346936, "compression_ratio": 1.4813084112149533, "no_speech_prob": 0.01821318082511425}, {"id": 226, "seek": 159246, "start": 1606.6200000000001, "end": 1611.02, "text": " y los postage de las palabras anteriores y siguientes? Y bueno, de la misma forma, \u00bfno?", "tokens": [51072, 288, 1750, 2183, 609, 368, 2439, 35240, 364, 34345, 2706, 288, 21152, 20135, 30, 398, 11974, 11, 368, 635, 24946, 8366, 11, 3841, 1771, 30, 51292], "temperature": 0.0, "avg_logprob": -0.26234185177346936, "compression_ratio": 1.4813084112149533, "no_speech_prob": 0.01821318082511425}, {"id": 227, "seek": 159246, "start": 1611.02, "end": 1614.9, "text": " La probabilidad de que sea un adjetivo, dado que la anterior es un determinante, el siguiente", "tokens": [51292, 2369, 31959, 4580, 368, 631, 4158, 517, 614, 7108, 6340, 11, 29568, 631, 635, 22272, 785, 517, 15957, 2879, 11, 806, 25666, 51486], "temperature": 0.0, "avg_logprob": -0.26234185177346936, "compression_ratio": 1.4813084112149533, "no_speech_prob": 0.01821318082511425}, {"id": 228, "seek": 161490, "start": 1614.9, "end": 1622.7, "text": " es un nombre y la palabra es blanco, es la probabilidad de que la clase sea un adjetivo", "tokens": [50364, 785, 517, 13000, 288, 635, 31702, 785, 888, 34300, 11, 785, 635, 31959, 4580, 368, 631, 635, 44578, 4158, 517, 614, 7108, 6340, 50754], "temperature": 0.0, "avg_logprob": -0.1753215789794922, "compression_ratio": 1.7579617834394905, "no_speech_prob": 0.5087547302246094}, {"id": 229, "seek": 161490, "start": 1622.7, "end": 1629.38, "text": " a priori, esto lo hago por conteo, la probabilidad de que una palabra sea blanco como adjetivo,", "tokens": [50754, 257, 4059, 72, 11, 7433, 450, 38721, 1515, 34444, 78, 11, 635, 31959, 4580, 368, 631, 2002, 31702, 4158, 888, 34300, 2617, 614, 7108, 6340, 11, 51088], "temperature": 0.0, "avg_logprob": -0.1753215789794922, "compression_ratio": 1.7579617834394905, "no_speech_prob": 0.5087547302246094}, {"id": 230, "seek": 161490, "start": 1629.38, "end": 1634.22, "text": " es decir, de todas las veces que hubo blanco, cu\u00e1ntas veces, miento, de todos los adjetivos", "tokens": [51088, 785, 10235, 11, 368, 10906, 2439, 17054, 631, 11838, 78, 888, 34300, 11, 44256, 296, 17054, 11, 275, 7814, 11, 368, 6321, 1750, 614, 7108, 16501, 51330], "temperature": 0.0, "avg_logprob": -0.1753215789794922, "compression_ratio": 1.7579617834394905, "no_speech_prob": 0.5087547302246094}, {"id": 231, "seek": 163422, "start": 1634.38, "end": 1644.54, "text": " cual era blanco, cu\u00e1ntas veces pas\u00f3 que antes de un adjetivo hubieron determinantes", "tokens": [50372, 10911, 4249, 888, 34300, 11, 44256, 296, 17054, 41382, 631, 11014, 368, 517, 614, 7108, 6340, 11838, 14440, 15957, 9327, 50880], "temperature": 0.0, "avg_logprob": -0.20524432811331242, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.2947479486465454}, {"id": 232, "seek": 163422, "start": 1644.54, "end": 1651.5, "text": " por la probabilidad de que el siguiente sea un nombre si este es un adjetivo. \u00bfSe entiende?", "tokens": [50880, 1515, 635, 31959, 4580, 368, 631, 806, 25666, 4158, 517, 13000, 1511, 4065, 785, 517, 614, 7108, 6340, 13, 3841, 10637, 948, 45816, 30, 51228], "temperature": 0.0, "avg_logprob": -0.20524432811331242, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.2947479486465454}, {"id": 233, "seek": 163422, "start": 1651.5, "end": 1655.82, "text": " Simplemente hago conteo de todas las veces que aparecieron cosas antes y las considero", "tokens": [51228, 21532, 4082, 38721, 34444, 78, 368, 10906, 2439, 17054, 631, 15004, 537, 16308, 12218, 11014, 288, 2439, 1949, 78, 51444], "temperature": 0.0, "avg_logprob": -0.20524432811331242, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.2947479486465454}, {"id": 234, "seek": 163422, "start": 1655.82, "end": 1661.58, "text": " independiente entre ellas, lo cual sabemos que no es cierto, pero es lo que hay, es lo", "tokens": [51444, 4819, 8413, 3962, 38397, 11, 450, 10911, 27200, 631, 572, 785, 28558, 11, 4768, 785, 450, 631, 4842, 11, 785, 450, 51732], "temperature": 0.0, "avg_logprob": -0.20524432811331242, "compression_ratio": 1.592760180995475, "no_speech_prob": 0.2947479486465454}, {"id": 235, "seek": 166158, "start": 1661.6599999999999, "end": 1670.6599999999999, "text": " que puedo computar. Y bueno, y como yo estoy calculando la probabilidad conjunta de esto,", "tokens": [50368, 631, 21612, 2807, 289, 13, 398, 11974, 11, 288, 2617, 5290, 15796, 4322, 1806, 635, 31959, 4580, 18244, 1328, 368, 7433, 11, 50818], "temperature": 0.0, "avg_logprob": -0.2204384273952908, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.004205307457596064}, {"id": 236, "seek": 166158, "start": 1670.6599999999999, "end": 1677.98, "text": " podr\u00eda llegar a generar ejemplos con la distribuci\u00f3n calculada, eso me puede ser \u00fatil para hacer", "tokens": [50818, 27246, 24892, 257, 1337, 289, 10012, 5895, 329, 416, 635, 4400, 30813, 4322, 1538, 11, 7287, 385, 8919, 816, 49191, 1690, 6720, 51184], "temperature": 0.0, "avg_logprob": -0.2204384273952908, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.004205307457596064}, {"id": 237, "seek": 166158, "start": 1677.98, "end": 1681.6999999999998, "text": " generaci\u00f3n de texto, todos estos m\u00e9todos me permiten, los m\u00e9todos de, por ejemplo,", "tokens": [51184, 1337, 3482, 368, 35503, 11, 6321, 12585, 20275, 378, 329, 385, 13423, 268, 11, 1750, 20275, 378, 329, 368, 11, 1515, 13358, 11, 51370], "temperature": 0.0, "avg_logprob": -0.2204384273952908, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.004205307457596064}, {"id": 238, "seek": 166158, "start": 1681.6999999999998, "end": 1687.1, "text": " de engrama me permiten generar tambi\u00e9n texto, que es la forma que hacen los generadores,", "tokens": [51370, 368, 465, 1342, 64, 385, 13423, 268, 1337, 289, 6407, 35503, 11, 631, 785, 635, 8366, 631, 27434, 1750, 1337, 11856, 11, 51640], "temperature": 0.0, "avg_logprob": -0.2204384273952908, "compression_ratio": 1.705607476635514, "no_speech_prob": 0.004205307457596064}, {"id": 239, "seek": 168710, "start": 1687.1, "end": 1695.1399999999999, "text": " que escriben parecido a alguien, digamos. Bueno, bueno, atacar los m\u00e9todos generativos", "tokens": [50364, 631, 30598, 1799, 7448, 17994, 257, 25814, 11, 36430, 13, 16046, 11, 11974, 11, 41015, 289, 1750, 20275, 378, 329, 1337, 36945, 50766], "temperature": 0.0, "avg_logprob": -0.24572348343698602, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00304458593018353}, {"id": 240, "seek": 168710, "start": 1695.1399999999999, "end": 1700.4199999999998, "text": " que son estos, es, como nadie valles. Un m\u00e9todo generativo es ese que busca una distribuci\u00f3n", "tokens": [50766, 631, 1872, 12585, 11, 785, 11, 2617, 28060, 371, 37927, 13, 1156, 20275, 17423, 1337, 18586, 785, 10167, 631, 37492, 2002, 4400, 30813, 51030], "temperature": 0.0, "avg_logprob": -0.24572348343698602, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00304458593018353}, {"id": 241, "seek": 168710, "start": 1700.4199999999998, "end": 1705.3799999999999, "text": " de todas las clases, prueba todas las clases y computa la distribuci\u00f3n conjunta con los", "tokens": [51030, 368, 10906, 2439, 596, 1957, 11, 48241, 10906, 2439, 596, 1957, 288, 2807, 64, 635, 4400, 30813, 18244, 1328, 416, 1750, 51278], "temperature": 0.0, "avg_logprob": -0.24572348343698602, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00304458593018353}, {"id": 242, "seek": 168710, "start": 1705.3799999999999, "end": 1711.1, "text": " atributos. Los m\u00e9todos discriminativos son un poco diferentes porque en lugar de,", "tokens": [51278, 412, 2024, 34640, 13, 7632, 20275, 378, 329, 20828, 36945, 1872, 517, 10639, 17686, 4021, 465, 11467, 368, 11, 51564], "temperature": 0.0, "avg_logprob": -0.24572348343698602, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.00304458593018353}, {"id": 243, "seek": 171110, "start": 1712.1, "end": 1719.98, "text": " en lugar de calcular la probabilidad de la conjunta dicen, bueno, no, de todo ejemplo,", "tokens": [50414, 465, 11467, 368, 2104, 17792, 635, 31959, 4580, 368, 635, 18244, 1328, 33816, 11, 11974, 11, 572, 11, 368, 5149, 13358, 11, 50808], "temperature": 0.0, "avg_logprob": -0.19106761502548003, "compression_ratio": 1.6645962732919255, "no_speech_prob": 0.0004061069630552083}, {"id": 244, "seek": 171110, "start": 1719.98, "end": 1725.86, "text": " cu\u00e1l de los dos es mejor, cu\u00e1l clase es mejor para este ejemplo, sin tratar de modelar", "tokens": [50808, 44318, 368, 1750, 4491, 785, 11479, 11, 44318, 44578, 785, 11479, 1690, 4065, 13358, 11, 3343, 42549, 368, 2316, 289, 51102], "temperature": 0.0, "avg_logprob": -0.19106761502548003, "compression_ratio": 1.6645962732919255, "no_speech_prob": 0.0004061069630552083}, {"id": 245, "seek": 171110, "start": 1725.86, "end": 1734.62, "text": " todas las clases posibles. Es decir, modelamos directamente la probabilidad, intento modelar", "tokens": [51102, 10906, 2439, 596, 1957, 1366, 14428, 13, 2313, 10235, 11, 2316, 2151, 46230, 635, 31959, 4580, 11, 8446, 78, 2316, 289, 51540], "temperature": 0.0, "avg_logprob": -0.19106761502548003, "compression_ratio": 1.6645962732919255, "no_speech_prob": 0.0004061069630552083}, {"id": 246, "seek": 173462, "start": 1734.62, "end": 1739.26, "text": " directamente la probabilidad condicional, la probabilidad de la clase daba los atributos,", "tokens": [50364, 46230, 635, 31959, 4580, 2224, 33010, 11, 635, 31959, 4580, 368, 635, 44578, 274, 5509, 1750, 412, 2024, 34640, 11, 50596], "temperature": 0.0, "avg_logprob": -0.31744384765625, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0435449592769146}, {"id": 247, "seek": 173462, "start": 1739.26, "end": 1748.2199999999998, "text": " \u00bfs\u00ed? Voy derecho a eso, \u00bfqu\u00e9 es m\u00e1s probable dado de todos estos atributos? Nada m\u00e1s.", "tokens": [50596, 3841, 82, 870, 30, 25563, 39055, 257, 7287, 11, 3841, 16412, 785, 3573, 21759, 29568, 368, 6321, 12585, 412, 2024, 34640, 30, 40992, 3573, 13, 51044], "temperature": 0.0, "avg_logprob": -0.31744384765625, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0435449592769146}, {"id": 248, "seek": 173462, "start": 1751.86, "end": 1757.6999999999998, "text": " Y hay varias aproximaciones, algunas que son probabil\u00edsticas como entrop\u00eda m\u00e1xima y otras", "tokens": [51226, 398, 4842, 37496, 31270, 9188, 11, 27316, 631, 1872, 31959, 19512, 9150, 2617, 948, 1513, 2686, 31031, 64, 288, 20244, 51518], "temperature": 0.0, "avg_logprob": -0.31744384765625, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0435449592769146}, {"id": 249, "seek": 175770, "start": 1757.7, "end": 1764.3, "text": " no. A ver, el preceptor de su porvector machine, ahora vamos a ver, no, vamos a ver la definici\u00f3n", "tokens": [50364, 572, 13, 316, 1306, 11, 806, 659, 1336, 284, 368, 459, 1515, 303, 1672, 3479, 11, 9923, 5295, 257, 1306, 11, 572, 11, 5295, 257, 1306, 635, 1561, 15534, 50694], "temperature": 0.0, "avg_logprob": -0.3660889501157014, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.03614545240998268}, {"id": 250, "seek": 175770, "start": 1764.3, "end": 1772.82, "text": " de su porvector machine. Pero esencialmente lo que te dicen es, bueno, esto est\u00e1 de tal lado.", "tokens": [50694, 368, 459, 1515, 303, 1672, 3479, 13, 9377, 785, 26567, 4082, 450, 631, 535, 33816, 785, 11, 11974, 11, 7433, 3192, 368, 4023, 11631, 13, 51120], "temperature": 0.0, "avg_logprob": -0.3660889501157014, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.03614545240998268}, {"id": 251, "seek": 175770, "start": 1775.6200000000001, "end": 1777.5, "text": " Si yo tengo estos puntos as\u00ed,", "tokens": [51260, 4909, 5290, 13989, 12585, 34375, 8582, 11, 51354], "temperature": 0.0, "avg_logprob": -0.3660889501157014, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.03614545240998268}, {"id": 252, "seek": 177750, "start": 1777.94, "end": 1787.66, "text": " entreno y despu\u00e9s te digo, bueno, este est\u00e1 de este, si este punto est\u00e1 del lado de los", "tokens": [50386, 948, 48207, 288, 15283, 535, 22990, 11, 11974, 11, 4065, 3192, 368, 4065, 11, 1511, 4065, 14326, 3192, 1103, 11631, 368, 1750, 50872], "temperature": 0.0, "avg_logprob": -0.38963651021321616, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.012217300944030285}, {"id": 253, "seek": 177750, "start": 1787.66, "end": 1796.26, "text": " redonditos. No s\u00e9 qu\u00e9 tan del lado est\u00e1 de los, esto no es probabilista, por ejemplo.", "tokens": [50872, 2182, 684, 11343, 13, 883, 7910, 8057, 7603, 1103, 11631, 3192, 368, 1750, 11, 7433, 572, 785, 31959, 5236, 11, 1515, 13358, 13, 51302], "temperature": 0.0, "avg_logprob": -0.38963651021321616, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.012217300944030285}, {"id": 254, "seek": 177750, "start": 1799.62, "end": 1802.74, "text": " O puedo hacer lo probabilista, pero igual lo \u00fanico que respondo es ac\u00e1 y de qu\u00e9 lado est\u00e1.", "tokens": [51470, 422, 21612, 6720, 450, 31959, 5236, 11, 4768, 10953, 450, 26113, 631, 4196, 78, 785, 23496, 288, 368, 8057, 11631, 3192, 13, 51626], "temperature": 0.0, "avg_logprob": -0.38963651021321616, "compression_ratio": 1.621301775147929, "no_speech_prob": 0.012217300944030285}, {"id": 255, "seek": 180274, "start": 1803.22, "end": 1808.06, "text": " Bueno, entonces vamos a ver uno que es el modelo de entrop\u00eda m\u00e1xima,", "tokens": [50388, 16046, 11, 13003, 5295, 257, 1306, 8526, 631, 785, 806, 27825, 368, 948, 1513, 2686, 31031, 64, 11, 50630], "temperature": 0.0, "avg_logprob": -0.28150178890417116, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.0026043024845421314}, {"id": 256, "seek": 180274, "start": 1809.5, "end": 1817.06, "text": " que es como lo que vamos a ver, es como la versi\u00f3n discriminativa del m\u00e9todo de Ney Valle.", "tokens": [50702, 631, 785, 2617, 450, 631, 5295, 257, 1306, 11, 785, 2617, 635, 47248, 20828, 18740, 1103, 20275, 17423, 368, 1734, 88, 691, 11780, 13, 51080], "temperature": 0.0, "avg_logprob": -0.28150178890417116, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.0026043024845421314}, {"id": 257, "seek": 180274, "start": 1821.3, "end": 1825.82, "text": " O tambi\u00e9n conocido como regresi\u00f3n multinomial log\u00edstica, que vamos a ver por qu\u00e9 se llama as\u00ed,", "tokens": [51292, 422, 6407, 15871, 2925, 2617, 47108, 2560, 45872, 298, 831, 3565, 19512, 2262, 11, 631, 5295, 257, 1306, 1515, 8057, 369, 23272, 8582, 11, 51518], "temperature": 0.0, "avg_logprob": -0.28150178890417116, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.0026043024845421314}, {"id": 258, "seek": 180274, "start": 1825.82, "end": 1831.26, "text": " y son modelos lo lineales para clasificaci\u00f3n, es decir, yo quiero la clase, si tengo una serie de", "tokens": [51518, 288, 1872, 2316, 329, 450, 1622, 4229, 1690, 596, 296, 40802, 11, 785, 10235, 11, 5290, 16811, 635, 44578, 11, 1511, 13989, 2002, 23030, 368, 51790], "temperature": 0.0, "avg_logprob": -0.28150178890417116, "compression_ratio": 1.5947136563876652, "no_speech_prob": 0.0026043024845421314}, {"id": 259, "seek": 183126, "start": 1831.26, "end": 1836.3, "text": " tributos. Hago.", "tokens": [50364, 1376, 5955, 329, 13, 389, 6442, 13, 50616], "temperature": 0.0, "avg_logprob": -0.8087139129638672, "compression_ratio": 0.6521739130434783, "no_speech_prob": 0.00980358850210905}, {"id": 260, "seek": 183630, "start": 1836.3, "end": 1863.98, "text": " E, ahora vamos a ver qu\u00e9 es esto, \u00bfno? F su I, son las features, son como un indicador de algunas,", "tokens": [50364, 462, 11, 9923, 5295, 257, 1306, 8057, 785, 7433, 11, 3841, 1771, 30, 479, 459, 286, 11, 1872, 2439, 4122, 11, 1872, 2617, 517, 4694, 5409, 368, 27316, 11, 51748], "temperature": 0.0, "avg_logprob": -0.4570419427120324, "compression_ratio": 1.075268817204301, "no_speech_prob": 0.006239620968699455}, {"id": 261, "seek": 186398, "start": 1864.14, "end": 1869.58, "text": " son features a partir de los atributos, ahora vamos a ver c\u00f3mo lo abrimos eso,", "tokens": [50372, 1872, 4122, 257, 13906, 368, 1750, 412, 2024, 34640, 11, 9923, 5295, 257, 1306, 12826, 450, 410, 5565, 329, 7287, 11, 50644], "temperature": 0.0, "avg_logprob": -0.23097856839497885, "compression_ratio": 1.5947712418300655, "no_speech_prob": 0.04122885689139366}, {"id": 262, "seek": 186398, "start": 1869.58, "end": 1876.7, "text": " pero son derivadas de estos atributos. Los W son los pesos, una serie de pesos que yo", "tokens": [50644, 4768, 1872, 10151, 6872, 368, 12585, 412, 2024, 34640, 13, 7632, 343, 1872, 1750, 33204, 11, 2002, 23030, 368, 33204, 631, 5290, 51000], "temperature": 0.0, "avg_logprob": -0.23097856839497885, "compression_ratio": 1.5947712418300655, "no_speech_prob": 0.04122885689139366}, {"id": 263, "seek": 186398, "start": 1876.7, "end": 1883.9, "text": " voy a intentar calcular, son los pesos de mi modelo, lo que yo voy a entrenar,", "tokens": [51000, 7552, 257, 46596, 2104, 17792, 11, 1872, 1750, 33204, 368, 2752, 27825, 11, 450, 631, 5290, 7552, 257, 45069, 289, 11, 51360], "temperature": 0.0, "avg_logprob": -0.23097856839497885, "compression_ratio": 1.5947712418300655, "no_speech_prob": 0.04122885689139366}, {"id": 264, "seek": 188390, "start": 1884.5400000000002, "end": 1894.8200000000002, "text": " aprender son los pesos de mi modelo. Y este es el producto, el dot product de ambos, es decir,", "tokens": [50396, 24916, 1872, 1750, 33204, 368, 2752, 27825, 13, 398, 4065, 785, 806, 47583, 11, 806, 5893, 1674, 368, 41425, 11, 785, 10235, 11, 50910], "temperature": 0.0, "avg_logprob": -0.19094962165469215, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.04072261601686478}, {"id": 265, "seek": 188390, "start": 1894.8200000000002, "end": 1904.7800000000002, "text": " esto va a ser W1 por F1 m\u00e1s W2 por F2 m\u00e1s W3 por F3, etc\u00e9tera. \u00bfDe acuerdo? Entonces yo,", "tokens": [50910, 7433, 2773, 257, 816, 343, 16, 1515, 479, 16, 3573, 343, 17, 1515, 479, 17, 3573, 343, 18, 1515, 479, 18, 11, 5183, 526, 23833, 13, 3841, 11089, 28113, 30, 15097, 5290, 11, 51408], "temperature": 0.0, "avg_logprob": -0.19094962165469215, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.04072261601686478}, {"id": 266, "seek": 188390, "start": 1904.7800000000002, "end": 1910.7, "text": " la feature esta, que seg\u00fan mi ejemplo, cuando yo vaya a evaluar, seg\u00fan mi ejemplo,", "tokens": [51408, 635, 4111, 5283, 11, 631, 36570, 2752, 13358, 11, 7767, 5290, 47682, 257, 6133, 289, 11, 36570, 2752, 13358, 11, 51704], "temperature": 0.0, "avg_logprob": -0.19094962165469215, "compression_ratio": 1.4545454545454546, "no_speech_prob": 0.04072261601686478}, {"id": 267, "seek": 191070, "start": 1910.7, "end": 1923.54, "text": " esto va a valer algo, lo multiplico por un n\u00famero fijo que va a depender de lo que yo entren\u00e9,", "tokens": [50364, 7433, 2773, 257, 1323, 260, 8655, 11, 450, 12788, 2789, 1515, 517, 14959, 283, 24510, 631, 2773, 257, 1367, 3216, 368, 450, 631, 5290, 45069, 526, 11, 51006], "temperature": 0.0, "avg_logprob": -0.2232386271158854, "compression_ratio": 1.295774647887324, "no_speech_prob": 0.003960589878261089}, {"id": 268, "seek": 191070, "start": 1923.54, "end": 1933.46, "text": " es decir, lo que yo quiero aprender es W, \u00bfde acuerdo? Eso, elev\u00f3 E a la suma de eso,", "tokens": [51006, 785, 10235, 11, 450, 631, 5290, 16811, 24916, 785, 343, 11, 3841, 1479, 28113, 30, 27795, 11, 7701, 812, 462, 257, 635, 2408, 64, 368, 7287, 11, 51502], "temperature": 0.0, "avg_logprob": -0.2232386271158854, "compression_ratio": 1.295774647887324, "no_speech_prob": 0.003960589878261089}, {"id": 269, "seek": 193346, "start": 1933.46, "end": 1941.1000000000001, "text": " ahora vamos a ver por qu\u00e9 hago esto. Y esta Z es simplemente un factor de normalizaci\u00f3n,", "tokens": [50364, 9923, 5295, 257, 1306, 1515, 8057, 38721, 7433, 13, 398, 5283, 1176, 785, 33190, 517, 5952, 368, 2710, 27603, 11, 50746], "temperature": 0.0, "avg_logprob": -0.24284461975097657, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.03506098687648773}, {"id": 270, "seek": 193346, "start": 1943.1000000000001, "end": 1952.06, "text": " es decir, de todos los W sub\u00ed que tengo, o sea, esto no necesariamente genera una distribuci\u00f3n", "tokens": [50846, 785, 10235, 11, 368, 6321, 1750, 343, 1422, 870, 631, 13989, 11, 277, 4158, 11, 7433, 572, 11909, 45149, 1337, 64, 2002, 4400, 30813, 51294], "temperature": 0.0, "avg_logprob": -0.24284461975097657, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.03506098687648773}, {"id": 271, "seek": 193346, "start": 1952.06, "end": 1959.54, "text": " de probabilidad, entonces este Z es como la suma de todos los casos posibles para llevarlo a una", "tokens": [51294, 368, 31959, 4580, 11, 13003, 4065, 1176, 785, 2617, 635, 2408, 64, 368, 6321, 1750, 25135, 1366, 14428, 1690, 30374, 752, 257, 2002, 51668], "temperature": 0.0, "avg_logprob": -0.24284461975097657, "compression_ratio": 1.486910994764398, "no_speech_prob": 0.03506098687648773}, {"id": 272, "seek": 195954, "start": 1959.62, "end": 1963.1399999999999, "text": " probabilidad, a que la suma me d\u00e9 uno, aquellos que habl\u00e1bamos unas clases atr\u00e1s, bueno,", "tokens": [50368, 31959, 4580, 11, 257, 631, 635, 2408, 64, 385, 2795, 8526, 11, 49835, 631, 26280, 27879, 2151, 25405, 596, 1957, 22906, 11, 11974, 11, 50544], "temperature": 0.0, "avg_logprob": -0.24553899675886207, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.015981385484337807}, {"id": 273, "seek": 195954, "start": 1963.1399999999999, "end": 1969.7, "text": " generalizado ac\u00e1. Esa es la famosa Z, que parece una pavada, pero es lo m\u00e1s dif\u00edcil de computar,", "tokens": [50544, 2674, 27441, 23496, 13, 2313, 64, 785, 635, 1087, 6447, 1176, 11, 631, 14120, 2002, 280, 706, 1538, 11, 4768, 785, 450, 3573, 17258, 368, 2807, 289, 11, 50872], "temperature": 0.0, "avg_logprob": -0.24553899675886207, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.015981385484337807}, {"id": 274, "seek": 195954, "start": 1969.7, "end": 1973.42, "text": " porque yo tengo que calcular este valor para todos los atributos posibles para que me d\u00e9 una distribuci\u00f3n.", "tokens": [50872, 4021, 5290, 13989, 631, 2104, 17792, 4065, 15367, 1690, 6321, 1750, 412, 2024, 34640, 1366, 14428, 1690, 631, 385, 2795, 2002, 4400, 30813, 13, 51058], "temperature": 0.0, "avg_logprob": -0.24553899675886207, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.015981385484337807}, {"id": 275, "seek": 195954, "start": 1976.86, "end": 1981.82, "text": " Entonces, los modelos de entrop\u00eda al m\u00e1ximo calculan la probabilidad de la clase utilizando", "tokens": [51230, 15097, 11, 1750, 2316, 329, 368, 948, 1513, 2686, 419, 38876, 4322, 282, 635, 31959, 4580, 368, 635, 44578, 19906, 1806, 51478], "temperature": 0.0, "avg_logprob": -0.24553899675886207, "compression_ratio": 1.6081632653061224, "no_speech_prob": 0.015981385484337807}, {"id": 276, "seek": 198182, "start": 1981.82, "end": 1991.4199999999998, "text": " esta f\u00f3rmula. Ahora vamos a ver por qu\u00e9. Pero antes vamos a hablar de otra cosa para llegar a eso,", "tokens": [50364, 5283, 283, 15614, 76, 3780, 13, 18840, 5295, 257, 1306, 1515, 8057, 13, 9377, 11014, 5295, 257, 21014, 368, 13623, 10163, 1690, 24892, 257, 7287, 11, 50844], "temperature": 0.0, "avg_logprob": -0.21954236030578614, "compression_ratio": 1.680473372781065, "no_speech_prob": 0.060095418244600296}, {"id": 277, "seek": 198182, "start": 1991.4199999999998, "end": 1995.6599999999999, "text": " y es de regresi\u00f3n lineal. Un problema de regresi\u00f3n lineal, que era lo que yo le dec\u00eda hoy,", "tokens": [50844, 288, 785, 368, 47108, 2560, 1622, 304, 13, 1156, 12395, 368, 47108, 2560, 1622, 304, 11, 631, 4249, 450, 631, 5290, 476, 37599, 13775, 11, 51056], "temperature": 0.0, "avg_logprob": -0.21954236030578614, "compression_ratio": 1.680473372781065, "no_speech_prob": 0.060095418244600296}, {"id": 278, "seek": 198182, "start": 1995.6599999999999, "end": 1999.58, "text": " es cuando uno intenta, un problema de regresi\u00f3n es cuando uno intenta calcular un valor,", "tokens": [51056, 785, 7767, 8526, 8446, 64, 11, 517, 12395, 368, 47108, 2560, 785, 7767, 8526, 8446, 64, 2104, 17792, 517, 15367, 11, 51252], "temperature": 0.0, "avg_logprob": -0.21954236030578614, "compression_ratio": 1.680473372781065, "no_speech_prob": 0.060095418244600296}, {"id": 279, "seek": 199958, "start": 1999.58, "end": 2011.3799999999999, "text": " de alg\u00fan valor real, un valor real, \u00bfs\u00ed? Entonces yo, si yo quiero saber, supongamos que yo", "tokens": [50364, 368, 26300, 15367, 957, 11, 517, 15367, 957, 11, 3841, 82, 870, 30, 15097, 5290, 11, 1511, 5290, 16811, 12489, 11, 9331, 556, 2151, 631, 5290, 50954], "temperature": 0.0, "avg_logprob": -0.3955460367976008, "compression_ratio": 1.1782178217821782, "no_speech_prob": 0.1821122169494629}, {"id": 280, "seek": 199958, "start": 2015.82, "end": 2017.34, "text": " tengo estos puntos ac\u00e1,", "tokens": [51176, 13989, 12585, 34375, 23496, 11, 51252], "temperature": 0.0, "avg_logprob": -0.3955460367976008, "compression_ratio": 1.1782178217821782, "no_speech_prob": 0.1821122169494629}, {"id": 281, "seek": 201734, "start": 2017.34, "end": 2036.3799999999999, "text": " \u00bfs\u00ed? Cuando yo hago regresi\u00f3n lineal, lo que hago es trazar, buscar una l\u00ednea que separe los", "tokens": [50364, 3841, 82, 870, 30, 21907, 5290, 38721, 47108, 2560, 1622, 304, 11, 450, 631, 38721, 785, 944, 26236, 11, 26170, 2002, 37452, 631, 369, 79, 543, 1750, 51316], "temperature": 0.0, "avg_logprob": -0.27870172069918725, "compression_ratio": 1.0434782608695652, "no_speech_prob": 0.025299586355686188}, {"id": 282, "seek": 203638, "start": 2036.38, "end": 2055.6600000000003, "text": " ejemplos. Eso esencialmente es, si esto es, va a ser una cosa como, si yo supongo que pasa por el", "tokens": [50364, 10012, 5895, 329, 13, 27795, 785, 26567, 4082, 785, 11, 1511, 7433, 785, 11, 2773, 257, 816, 2002, 10163, 2617, 11, 1511, 5290, 9331, 25729, 631, 20260, 1515, 806, 51328], "temperature": 0.0, "avg_logprob": -0.24569098154703775, "compression_ratio": 1.1829268292682926, "no_speech_prob": 0.13167785108089447}, {"id": 283, "seek": 205566, "start": 2055.66, "end": 2059.2999999999997, "text": " origen, esta recta, vamos a suponer que pasa por el origen, y si no, vemos c\u00f3mo se corrige.", "tokens": [50364, 2349, 268, 11, 5283, 11048, 64, 11, 5295, 257, 9331, 32949, 631, 20260, 1515, 806, 2349, 268, 11, 288, 1511, 572, 11, 20909, 12826, 369, 38576, 3969, 13, 50546], "temperature": 0.0, "avg_logprob": -0.4107956452803178, "compression_ratio": 1.2061855670103092, "no_speech_prob": 0.15789538621902466}, {"id": 284, "seek": 205566, "start": 2072.74, "end": 2074.3399999999997, "text": " Vamos a llamarle X1, X2.", "tokens": [51218, 10894, 257, 16848, 36153, 1783, 16, 11, 1783, 17, 13, 51298], "temperature": 0.0, "avg_logprob": -0.4107956452803178, "compression_ratio": 1.2061855670103092, "no_speech_prob": 0.15789538621902466}, {"id": 285, "seek": 207434, "start": 2074.34, "end": 2091.26, "text": " Esto es la recta que representa esto, \u00bfno? Es decir, el W1 y W2 me van a determinar la", "tokens": [50364, 20880, 785, 635, 11048, 64, 631, 49823, 7433, 11, 3841, 1771, 30, 2313, 10235, 11, 806, 343, 16, 288, 343, 17, 385, 3161, 257, 3618, 6470, 635, 51210], "temperature": 0.0, "avg_logprob": -0.2580675804752043, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.012800891883671284}, {"id": 286, "seek": 207434, "start": 2091.26, "end": 2095.34, "text": " legislaci\u00f3n de la recta. Ac\u00e1 est\u00e1 pasando por el origen porque no tiene elemento independiente,", "tokens": [51210, 6593, 3482, 368, 635, 11048, 64, 13, 5097, 842, 3192, 45412, 1515, 806, 2349, 268, 4021, 572, 7066, 47961, 4819, 8413, 11, 51414], "temperature": 0.0, "avg_logprob": -0.2580675804752043, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.012800891883671284}, {"id": 287, "seek": 207434, "start": 2095.34, "end": 2102.02, "text": " yo puedo inventar un W0 con un X0 que vale siempre 1, para agregarle, vamos a moverla a la recta.", "tokens": [51414, 5290, 21612, 7962, 289, 517, 343, 15, 416, 517, 1783, 15, 631, 15474, 12758, 502, 11, 1690, 4554, 2976, 306, 11, 5295, 257, 39945, 875, 257, 635, 11048, 64, 13, 51748], "temperature": 0.0, "avg_logprob": -0.2580675804752043, "compression_ratio": 1.4343434343434343, "no_speech_prob": 0.012800891883671284}, {"id": 288, "seek": 210202, "start": 2102.02, "end": 2109.54, "text": " \u00bfDe acuerdo? Entonces, lo que yo, cuando digo que hago regresi\u00f3n lineal, lo que digo es bueno,", "tokens": [50364, 3841, 11089, 28113, 30, 15097, 11, 450, 631, 5290, 11, 7767, 22990, 631, 38721, 47108, 2560, 1622, 304, 11, 450, 631, 22990, 785, 11974, 11, 50740], "temperature": 0.0, "avg_logprob": -0.254189962628244, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.017418840900063515}, {"id": 289, "seek": 210202, "start": 2109.54, "end": 2117.5, "text": " mis puntos yo asumo que son separables por una recta. Ah, perd\u00f3n, yo quiero estimar X2 dado", "tokens": [50740, 3346, 34375, 5290, 382, 40904, 631, 1872, 3128, 2965, 1515, 2002, 11048, 64, 13, 2438, 11, 12611, 1801, 11, 5290, 16811, 8017, 289, 1783, 17, 29568, 51138], "temperature": 0.0, "avg_logprob": -0.254189962628244, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.017418840900063515}, {"id": 290, "seek": 210202, "start": 2117.5, "end": 2126.46, "text": " X1, \u00bfde acuerdo? Entonces yo obtengo la recta para un nuevo X, vengo ac\u00e1 y calculo el I.", "tokens": [51138, 1783, 16, 11, 3841, 1479, 28113, 30, 15097, 5290, 28326, 1571, 635, 11048, 64, 1690, 517, 18591, 1783, 11, 371, 30362, 23496, 288, 4322, 78, 806, 286, 13, 51586], "temperature": 0.0, "avg_logprob": -0.254189962628244, "compression_ratio": 1.4583333333333333, "no_speech_prob": 0.017418840900063515}, {"id": 291, "seek": 212646, "start": 2126.46, "end": 2136.9, "text": " Entonces, I va a ser igual al WI por FI, que es esto, la sumatoria de los WI por FI es el dot", "tokens": [50364, 15097, 11, 286, 2773, 257, 816, 10953, 419, 343, 40, 1515, 479, 40, 11, 631, 785, 7433, 11, 635, 2408, 1639, 654, 368, 1750, 343, 40, 1515, 479, 40, 785, 806, 5893, 50886], "temperature": 0.0, "avg_logprob": -0.2706147323955189, "compression_ratio": 1.407035175879397, "no_speech_prob": 0.0023639488499611616}, {"id": 292, "seek": 212646, "start": 2136.9, "end": 2143.3, "text": " product de WI con F. \u00bfDe acuerdo? Simplemente estoy haciendo un estimador lineal de esto.", "tokens": [50886, 1674, 368, 343, 40, 416, 479, 13, 3841, 11089, 28113, 30, 21532, 4082, 15796, 20509, 517, 8017, 5409, 1622, 304, 368, 7433, 13, 51206], "temperature": 0.0, "avg_logprob": -0.2706147323955189, "compression_ratio": 1.407035175879397, "no_speech_prob": 0.0023639488499611616}, {"id": 293, "seek": 212646, "start": 2146.54, "end": 2155.7400000000002, "text": " \u00bfY c\u00f3mo hago para, como encuentro esta recta? Bueno, una de las formas m\u00e1s usuales es la que", "tokens": [51368, 3841, 56, 12826, 38721, 1690, 11, 2617, 23708, 340, 5283, 11048, 64, 30, 16046, 11, 2002, 368, 2439, 33463, 3573, 7713, 279, 785, 635, 631, 51828], "temperature": 0.0, "avg_logprob": -0.2706147323955189, "compression_ratio": 1.407035175879397, "no_speech_prob": 0.0023639488499611616}, {"id": 294, "seek": 215574, "start": 2155.74, "end": 2160.14, "text": " minimiza la suma de los cuadrados de la diferencia entre valores y predicciones, o sea,", "tokens": [50364, 4464, 13427, 635, 2408, 64, 368, 1750, 34434, 40491, 368, 635, 38844, 3962, 38790, 288, 47336, 23469, 11, 277, 4158, 11, 50584], "temperature": 0.0, "avg_logprob": -0.23227781619665758, "compression_ratio": 1.4108527131782946, "no_speech_prob": 0.0013564093969762325}, {"id": 295, "seek": 215574, "start": 2163.7799999999997, "end": 2176.5, "text": " esta recta minimiza esta distancia, \u00bfde acuerdo? La distancia, yo busco la recta que tenga la", "tokens": [50766, 5283, 11048, 64, 4464, 13427, 5283, 1483, 22862, 11, 3841, 1479, 28113, 30, 2369, 1483, 22862, 11, 5290, 1255, 1291, 635, 11048, 64, 631, 36031, 635, 51402], "temperature": 0.0, "avg_logprob": -0.23227781619665758, "compression_ratio": 1.4108527131782946, "no_speech_prob": 0.0013564093969762325}, {"id": 296, "seek": 217650, "start": 2176.5, "end": 2188.78, "text": " distancia m\u00ednima de esto al cuadrado y esto al cuadrado, \u00bfs\u00ed? Lo hago al cuadrado para que la", "tokens": [50364, 1483, 22862, 33656, 4775, 368, 7433, 419, 34434, 14974, 288, 7433, 419, 34434, 14974, 11, 3841, 82, 870, 30, 6130, 38721, 419, 34434, 14974, 1690, 631, 635, 50978], "temperature": 0.0, "avg_logprob": -0.1618802183765476, "compression_ratio": 1.453125, "no_speech_prob": 0.008551576174795628}, {"id": 297, "seek": 217650, "start": 2188.78, "end": 2195.02, "text": " suma sea positiva, para que no me afecte si estoy de un lado o del otro. La vieja regresa", "tokens": [50978, 2408, 64, 4158, 11218, 5931, 11, 1690, 631, 572, 385, 30626, 68, 1511, 15796, 368, 517, 11631, 277, 1103, 11921, 13, 2369, 4941, 2938, 47108, 64, 51290], "temperature": 0.0, "avg_logprob": -0.1618802183765476, "compression_ratio": 1.453125, "no_speech_prob": 0.008551576174795628}, {"id": 298, "seek": 219502, "start": 2195.02, "end": 2204.5, "text": " en un lineal, \u00bfs\u00ed? Entonces, no voy a entrar en detalles, pero yo calculo la f\u00f3rmula de los", "tokens": [50364, 465, 517, 1622, 304, 11, 3841, 82, 870, 30, 15097, 11, 572, 7552, 257, 20913, 465, 1141, 37927, 11, 4768, 5290, 4322, 78, 635, 283, 15614, 76, 3780, 368, 1750, 50838], "temperature": 0.0, "avg_logprob": -0.2562816882955617, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.19811683893203735}, {"id": 299, "seek": 219502, "start": 2204.5, "end": 2214.9, "text": " m\u00ednimos cuadrados que son, si lo piensan son todo multiplicaciones de cosas al cuadrado, m\u00e1s", "tokens": [50838, 33656, 8372, 34434, 40491, 631, 1872, 11, 1511, 450, 3895, 694, 282, 1872, 5149, 17596, 9188, 368, 12218, 419, 34434, 14974, 11, 3573, 51358], "temperature": 0.0, "avg_logprob": -0.2562816882955617, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.19811683893203735}, {"id": 300, "seek": 219502, "start": 2214.9, "end": 2220.58, "text": " cosas al cuadrado. O sea, que esto es positivo, es una funci\u00f3n positiva y convexa y entonces yo lo que", "tokens": [51358, 12218, 419, 34434, 14974, 13, 422, 4158, 11, 631, 7433, 785, 44710, 11, 785, 2002, 43735, 11218, 5931, 288, 42432, 64, 288, 13003, 5290, 450, 631, 51642], "temperature": 0.0, "avg_logprob": -0.2562816882955617, "compression_ratio": 1.5181347150259068, "no_speech_prob": 0.19811683893203735}, {"id": 301, "seek": 222058, "start": 2220.62, "end": 2225.22, "text": " trato de buscar es el m\u00ednimo de esa funci\u00f3n. El a\u00f1o que viene lo voy a escribir a eso,", "tokens": [50366, 504, 2513, 368, 26170, 785, 806, 47393, 368, 11342, 43735, 13, 2699, 15984, 631, 19561, 450, 7552, 257, 30598, 10119, 257, 7287, 11, 50596], "temperature": 0.0, "avg_logprob": -0.24205594796400803, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.12030299752950668}, {"id": 302, "seek": 222058, "start": 2225.22, "end": 2233.14, "text": " porque no s\u00e9 si queda claro. Este, a ustedes no les importa. Pero la cuesti\u00f3n es que yo termino", "tokens": [50596, 4021, 572, 7910, 1511, 23314, 16742, 13, 16105, 11, 257, 17110, 572, 1512, 33218, 13, 9377, 635, 50216, 785, 631, 5290, 1433, 2982, 50992], "temperature": 0.0, "avg_logprob": -0.24205594796400803, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.12030299752950668}, {"id": 303, "seek": 222058, "start": 2233.14, "end": 2238.94, "text": " minimizando una funci\u00f3n convexa, una funci\u00f3n convexa y una funci\u00f3n que es as\u00ed. As\u00ed es una", "tokens": [50992, 4464, 590, 1806, 2002, 43735, 42432, 64, 11, 2002, 43735, 42432, 64, 288, 2002, 43735, 631, 785, 8582, 13, 17419, 785, 2002, 51282], "temperature": 0.0, "avg_logprob": -0.24205594796400803, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.12030299752950668}, {"id": 304, "seek": 222058, "start": 2238.94, "end": 2245.74, "text": " funci\u00f3n convexa, \u00bfno? Que cualquier, cualquier par de puntos que yo una pasan todo por adentro del,", "tokens": [51282, 43735, 42432, 64, 11, 3841, 1771, 30, 4493, 21004, 11, 21004, 971, 368, 34375, 631, 5290, 2002, 1736, 282, 5149, 1515, 614, 317, 340, 1103, 11, 51622], "temperature": 0.0, "avg_logprob": -0.24205594796400803, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.12030299752950668}, {"id": 305, "seek": 224574, "start": 2246.22, "end": 2259.06, "text": " no? Vamos, ac\u00e1, esto es una funci\u00f3n convexa. S\u00ed, yo puedo unir ac\u00e1, \u00bfde acuerdo? \u00bfC\u00f3mo es", "tokens": [50388, 572, 30, 10894, 11, 23496, 11, 7433, 785, 2002, 43735, 42432, 64, 13, 12375, 11, 5290, 21612, 517, 347, 23496, 11, 3841, 1479, 28113, 30, 3841, 28342, 785, 51030], "temperature": 0.0, "avg_logprob": -0.30626245645376354, "compression_ratio": 1.7625, "no_speech_prob": 0.0019483991200104356}, {"id": 306, "seek": 224574, "start": 2259.06, "end": 2263.54, "text": " una funci\u00f3n convexa? \u00bfQu\u00e9 caracter\u00edstica tiene las funciones convexas? \u00bfCu\u00e1l es", "tokens": [51030, 2002, 43735, 42432, 64, 30, 3841, 15137, 34297, 2262, 7066, 2439, 1019, 23469, 42432, 296, 30, 3841, 35222, 11447, 785, 51254], "temperature": 0.0, "avg_logprob": -0.30626245645376354, "compression_ratio": 1.7625, "no_speech_prob": 0.0019483991200104356}, {"id": 307, "seek": 224574, "start": 2263.54, "end": 2269.7799999999997, "text": " qu\u00e9 caracter\u00edstica tiene la funci\u00f3n convexa? Ah, no se acuerdo. Las funciones convexas tienen el", "tokens": [51254, 8057, 34297, 2262, 7066, 635, 43735, 42432, 64, 30, 2438, 11, 572, 369, 28113, 13, 10663, 1019, 23469, 42432, 296, 12536, 806, 51566], "temperature": 0.0, "avg_logprob": -0.30626245645376354, "compression_ratio": 1.7625, "no_speech_prob": 0.0019483991200104356}, {"id": 308, "seek": 226978, "start": 2269.78, "end": 2277.46, "text": " tema de que cuando yo encuentro un m\u00ednimo local es un m\u00ednimo global. Si una funci\u00f3n es as\u00ed,", "tokens": [50364, 15854, 368, 631, 7767, 5290, 23708, 340, 517, 47393, 2654, 785, 517, 47393, 4338, 13, 4909, 2002, 43735, 785, 8582, 11, 50748], "temperature": 0.0, "avg_logprob": -0.23082269562615287, "compression_ratio": 1.6787878787878787, "no_speech_prob": 0.05052966997027397}, {"id": 309, "seek": 226978, "start": 2278.9, "end": 2284.0600000000004, "text": " yo puedo quedarme, buscar el m\u00ednimo ac\u00e1 y encontrarme con este m\u00ednimo local y buscar ac\u00e1.", "tokens": [50820, 5290, 21612, 13617, 35890, 11, 26170, 806, 47393, 23496, 288, 17525, 1398, 416, 4065, 47393, 2654, 288, 26170, 23496, 13, 51078], "temperature": 0.0, "avg_logprob": -0.23082269562615287, "compression_ratio": 1.6787878787878787, "no_speech_prob": 0.05052966997027397}, {"id": 310, "seek": 226978, "start": 2286.5400000000004, "end": 2292.02, "text": " S\u00ed, s\u00ed, claro, puedo llegar a quedar atascado ac\u00e1. Si yo tengo una funci\u00f3n convexa,", "tokens": [51202, 12375, 11, 8600, 11, 16742, 11, 21612, 24892, 257, 39244, 412, 4806, 1573, 23496, 13, 4909, 5290, 13989, 2002, 43735, 42432, 64, 11, 51476], "temperature": 0.0, "avg_logprob": -0.23082269562615287, "compression_ratio": 1.6787878787878787, "no_speech_prob": 0.05052966997027397}, {"id": 311, "seek": 229202, "start": 2293.02, "end": 2299.82, "text": " esto es informativo, si quieren hacer curso de prensa autom\u00e1tico, esto lo ven en detalle.", "tokens": [50414, 7433, 785, 1356, 18586, 11, 1511, 36706, 6720, 31085, 368, 659, 3695, 64, 3553, 28234, 11, 7433, 450, 6138, 465, 1141, 11780, 13, 50754], "temperature": 0.0, "avg_logprob": -0.1949574851989746, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.03279273584485054}, {"id": 312, "seek": 229202, "start": 2299.82, "end": 2306.42, "text": " Este, si yo tengo una funci\u00f3n convexa, yo puedo buscar un punto cualquiera y empezar a", "tokens": [50754, 16105, 11, 1511, 5290, 13989, 2002, 43735, 42432, 64, 11, 5290, 21612, 26170, 517, 14326, 10911, 35134, 288, 31168, 257, 51084], "temperature": 0.0, "avg_logprob": -0.1949574851989746, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.03279273584485054}, {"id": 313, "seek": 229202, "start": 2306.42, "end": 2312.42, "text": " calcular en la derivada y avanzar en la, en la direcci\u00f3n de la derivada y al final, al final del", "tokens": [51084, 2104, 17792, 465, 635, 10151, 1538, 288, 42444, 289, 465, 635, 11, 465, 635, 1264, 14735, 368, 635, 10151, 1538, 288, 419, 2572, 11, 419, 2572, 1103, 51384], "temperature": 0.0, "avg_logprob": -0.1949574851989746, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.03279273584485054}, {"id": 314, "seek": 229202, "start": 2312.42, "end": 2316.78, "text": " d\u00eda voy a encontrar si hago las cosas bien el m\u00ednimo de funci\u00f3n. Eso llama descenso por", "tokens": [51384, 12271, 7552, 257, 17525, 1511, 38721, 2439, 12218, 3610, 806, 47393, 368, 43735, 13, 27795, 23272, 7471, 268, 539, 1515, 51602], "temperature": 0.0, "avg_logprob": -0.1949574851989746, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.03279273584485054}, {"id": 315, "seek": 231678, "start": 2316.78, "end": 2322.46, "text": " gradiente, \u00bfs\u00ed? Y deber\u00eda ense\u00f1arse en primer a\u00f1o.", "tokens": [50364, 2771, 8413, 11, 3841, 82, 870, 30, 398, 29671, 2686, 31275, 11668, 465, 12595, 15984, 13, 50648], "temperature": 0.0, "avg_logprob": -0.21049476552892615, "compression_ratio": 1.647887323943662, "no_speech_prob": 0.052748508751392365}, {"id": 316, "seek": 231678, "start": 2324.94, "end": 2331.0600000000004, "text": " Hay otro m\u00e9todo de minimizaci\u00f3n. Son m\u00e9todos de minimizaci\u00f3n num\u00e9rica, \u00bfno? Son calculos num\u00e9ricos.", "tokens": [50772, 8721, 11921, 20275, 17423, 368, 4464, 27603, 13, 5185, 20275, 378, 329, 368, 4464, 27603, 1031, 32716, 11, 3841, 1771, 30, 5185, 4322, 329, 1031, 27578, 329, 13, 51078], "temperature": 0.0, "avg_logprob": -0.21049476552892615, "compression_ratio": 1.647887323943662, "no_speech_prob": 0.052748508751392365}, {"id": 317, "seek": 231678, "start": 2332.78, "end": 2337.86, "text": " Quiero decir, no hay una f\u00f3rmula cerrada para eso. Para el m\u00e9todo de m\u00ednimo cuadrado s\u00ed hay", "tokens": [51164, 2326, 12030, 10235, 11, 572, 4842, 2002, 283, 15614, 76, 3780, 10146, 19120, 1690, 7287, 13, 11107, 806, 20275, 17423, 368, 47393, 34434, 14974, 8600, 4842, 51418], "temperature": 0.0, "avg_logprob": -0.21049476552892615, "compression_ratio": 1.647887323943662, "no_speech_prob": 0.052748508751392365}, {"id": 318, "seek": 231678, "start": 2337.86, "end": 2340.86, "text": " una f\u00f3rmula cerrada, es decir, una f\u00f3rmula calcular, pero es m\u00e1s f\u00e1cil de hacer descenso", "tokens": [51418, 2002, 283, 15614, 76, 3780, 10146, 19120, 11, 785, 10235, 11, 2002, 283, 15614, 76, 3780, 2104, 17792, 11, 4768, 785, 3573, 17474, 368, 6720, 7471, 268, 539, 51568], "temperature": 0.0, "avg_logprob": -0.21049476552892615, "compression_ratio": 1.647887323943662, "no_speech_prob": 0.052748508751392365}, {"id": 319, "seek": 234086, "start": 2340.86, "end": 2346.1, "text": " por gradiente, pues m\u00e1s r\u00e1pido. Bueno, cuesti\u00f3n, que nosotros podemos saber c\u00f3mo hacer esto,", "tokens": [50364, 1515, 2771, 8413, 11, 11059, 3573, 24893, 13, 16046, 11, 50216, 11, 631, 13863, 12234, 12489, 12826, 6720, 7433, 11, 50626], "temperature": 0.0, "avg_logprob": -0.28216756558885764, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.05136648565530777}, {"id": 320, "seek": 234086, "start": 2346.1, "end": 2352.98, "text": " es decir, que yo puedo aprender los WB, o sea, los WB que minimizan, esos son los WB que queremos,", "tokens": [50626, 785, 10235, 11, 631, 5290, 21612, 24916, 1750, 343, 33, 11, 277, 4158, 11, 1750, 343, 33, 631, 4464, 590, 282, 11, 22411, 1872, 1750, 343, 33, 631, 26813, 11, 50970], "temperature": 0.0, "avg_logprob": -0.28216756558885764, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.05136648565530777}, {"id": 321, "seek": 234086, "start": 2352.98, "end": 2357.94, "text": " est\u00e1 claro, \u00bfno? Es decir, yo calculo a partir del cuerpo de entrenamiento,", "tokens": [50970, 3192, 16742, 11, 3841, 1771, 30, 2313, 10235, 11, 5290, 4322, 78, 257, 13906, 1103, 20264, 368, 45069, 16971, 11, 51218], "temperature": 0.0, "avg_logprob": -0.28216756558885764, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.05136648565530777}, {"id": 322, "seek": 234086, "start": 2357.94, "end": 2367.1800000000003, "text": " esos WB, y lo uso luego. Eso se trata de aprender. Entonces, este es un problema de", "tokens": [51218, 22411, 343, 33, 11, 288, 450, 22728, 17222, 13, 27795, 369, 31920, 368, 24916, 13, 15097, 11, 4065, 785, 517, 12395, 368, 51680], "temperature": 0.0, "avg_logprob": -0.28216756558885764, "compression_ratio": 1.5726872246696035, "no_speech_prob": 0.05136648565530777}, {"id": 323, "seek": 236718, "start": 2367.18, "end": 2373.1, "text": " regresi\u00f3n, donde yo quiero calcular un n\u00famero, pero ac\u00e1 estoy en un problema de clasificaci\u00f3n,", "tokens": [50364, 47108, 2560, 11, 10488, 5290, 16811, 2104, 17792, 517, 14959, 11, 4768, 23496, 15796, 465, 517, 12395, 368, 596, 296, 40802, 11, 50660], "temperature": 0.0, "avg_logprob": -0.2600810063349736, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.00961344875395298}, {"id": 324, "seek": 236718, "start": 2373.1, "end": 2377.58, "text": " o sea, que lo que yo quiero aprender es una categor\u00eda, una probabilidad. Entonces, mi primera", "tokens": [50660, 277, 4158, 11, 631, 450, 631, 5290, 16811, 24916, 785, 2002, 19250, 2686, 11, 2002, 31959, 4580, 13, 15097, 11, 2752, 17382, 50884], "temperature": 0.0, "avg_logprob": -0.2600810063349736, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.00961344875395298}, {"id": 325, "seek": 236718, "start": 2377.58, "end": 2388.14, "text": " aproximaci\u00f3n es, perd\u00f3n, es bueno, yo digo esto, la probabilidad, el n\u00famero que yo quiero estimar", "tokens": [50884, 31270, 3482, 785, 11, 12611, 1801, 11, 785, 11974, 11, 5290, 22990, 7433, 11, 635, 31959, 4580, 11, 806, 14959, 631, 5290, 16811, 8017, 289, 51412], "temperature": 0.0, "avg_logprob": -0.2600810063349736, "compression_ratio": 1.6065573770491803, "no_speech_prob": 0.00961344875395298}, {"id": 326, "seek": 238814, "start": 2389.14, "end": 2398.46, "text": " es la probabilidad de que sea clase, ac\u00e1 tenemos un caso positivo o negativo, \u00bfno? La probabilidad", "tokens": [50414, 785, 635, 31959, 4580, 368, 631, 4158, 44578, 11, 23496, 9914, 517, 9666, 44710, 277, 2485, 18586, 11, 3841, 1771, 30, 2369, 31959, 4580, 50880], "temperature": 0.0, "avg_logprob": -0.34996996087543036, "compression_ratio": 1.3732394366197183, "no_speech_prob": 0.015057070180773735}, {"id": 327, "seek": 238814, "start": 2398.46, "end": 2409.46, "text": " de que I va a ir a true, o sea, de la clase dado mi X. Entonces, yo lo que digo es bueno, hago", "tokens": [50880, 368, 631, 286, 2773, 257, 3418, 257, 2074, 11, 277, 4158, 11, 368, 635, 44578, 29568, 2752, 1783, 13, 15097, 11, 5290, 450, 631, 22990, 785, 11974, 11, 38721, 51430], "temperature": 0.0, "avg_logprob": -0.34996996087543036, "compression_ratio": 1.3732394366197183, "no_speech_prob": 0.015057070180773735}, {"id": 328, "seek": 240946, "start": 2409.46, "end": 2419.34, "text": " regresi\u00f3n, hago regresi\u00f3n, pero en lugar de calcular un n\u00famero, o sea, sigo calculando", "tokens": [50364, 47108, 2560, 11, 38721, 47108, 2560, 11, 4768, 465, 11467, 368, 2104, 17792, 517, 14959, 11, 277, 4158, 11, 4556, 78, 4322, 1806, 50858], "temperature": 0.0, "avg_logprob": -0.14098869787680135, "compression_ratio": 1.7375, "no_speech_prob": 0.003965652082115412}, {"id": 329, "seek": 240946, "start": 2419.34, "end": 2427.7400000000002, "text": " un n\u00famero que es el valor de la probabilidad. Ahora, \u00bfqu\u00e9 problema tiene esto? El problema", "tokens": [50858, 517, 14959, 631, 785, 806, 15367, 368, 635, 31959, 4580, 13, 18840, 11, 3841, 16412, 12395, 7066, 7433, 30, 2699, 12395, 51278], "temperature": 0.0, "avg_logprob": -0.14098869787680135, "compression_ratio": 1.7375, "no_speech_prob": 0.003965652082115412}, {"id": 330, "seek": 240946, "start": 2427.7400000000002, "end": 2432.54, "text": " que tiene esto es que no es una distribuci\u00f3n de probabilidad, no es un valor de probabilidad,", "tokens": [51278, 631, 7066, 7433, 785, 631, 572, 785, 2002, 4400, 30813, 368, 31959, 4580, 11, 572, 785, 517, 15367, 368, 31959, 4580, 11, 51518], "temperature": 0.0, "avg_logprob": -0.14098869787680135, "compression_ratio": 1.7375, "no_speech_prob": 0.003965652082115412}, {"id": 331, "seek": 243254, "start": 2432.66, "end": 2441.62, "text": " porque la probabilidad tiene que estar entre 0 y 1, \u00bfde acuerdo? Entonces, esto no me sirve", "tokens": [50370, 4021, 635, 31959, 4580, 7066, 631, 8755, 3962, 1958, 288, 502, 11, 3841, 1479, 28113, 30, 15097, 11, 7433, 572, 385, 4735, 303, 50818], "temperature": 0.0, "avg_logprob": -0.23973070432062019, "compression_ratio": 1.5, "no_speech_prob": 0.11359855532646179}, {"id": 332, "seek": 243254, "start": 2441.62, "end": 2444.42, "text": " a aplicarlo directamente, porque me puede dar cualquier cosa, yo quiero una probabilidad.", "tokens": [50818, 257, 18221, 19457, 46230, 11, 4021, 385, 8919, 4072, 21004, 10163, 11, 5290, 16811, 2002, 31959, 4580, 13, 50958], "temperature": 0.0, "avg_logprob": -0.23973070432062019, "compression_ratio": 1.5, "no_speech_prob": 0.11359855532646179}, {"id": 333, "seek": 243254, "start": 2444.42, "end": 2451.58, "text": " Entonces, lo que digo es bueno, pruebo con los odds, los odds que no s\u00e9 c\u00f3mo se traduce,", "tokens": [50958, 15097, 11, 450, 631, 22990, 785, 11974, 11, 32820, 1763, 416, 1750, 17439, 11, 1750, 17439, 631, 572, 7910, 12826, 369, 2479, 4176, 11, 51316], "temperature": 0.0, "avg_logprob": -0.23973070432062019, "compression_ratio": 1.5, "no_speech_prob": 0.11359855532646179}, {"id": 334, "seek": 245158, "start": 2452.58, "end": 2461.5, "text": " los odds son como las chances, como las apuestas, \u00bfno? Ten\u00e9s 2 a 1, 1 a 2,", "tokens": [50414, 1750, 17439, 1872, 2617, 2439, 10486, 11, 2617, 2439, 1882, 47794, 11, 3841, 1771, 30, 9380, 2191, 568, 257, 502, 11, 502, 257, 568, 11, 50860], "temperature": 0.0, "avg_logprob": -0.2617104178980777, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.04851723834872246}, {"id": 335, "seek": 245158, "start": 2461.5, "end": 2467.58, "text": " que es, esencialmente, la probabilidad de que sea verdadero comparado con la probabilidad de", "tokens": [50860, 631, 785, 11, 785, 26567, 4082, 11, 635, 31959, 4580, 368, 631, 4158, 13692, 2032, 6311, 1573, 416, 635, 31959, 4580, 368, 51164], "temperature": 0.0, "avg_logprob": -0.2617104178980777, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.04851723834872246}, {"id": 336, "seek": 245158, "start": 2467.58, "end": 2475.2599999999998, "text": " que no lo sea. Esto est\u00e1 un poco mejor, porque este resultado est\u00e1 entre 0 e infinito,", "tokens": [51164, 631, 572, 450, 4158, 13, 20880, 3192, 517, 10639, 11479, 11, 4021, 4065, 28047, 3192, 3962, 1958, 308, 7193, 3528, 11, 51548], "temperature": 0.0, "avg_logprob": -0.2617104178980777, "compression_ratio": 1.4827586206896552, "no_speech_prob": 0.04851723834872246}, {"id": 337, "seek": 247526, "start": 2475.26, "end": 2487.1400000000003, "text": " pero si es sin estar entre 0 y 1, entre, perd\u00f3n, yo quiero llevarlo a algo que est\u00e9", "tokens": [50364, 4768, 1511, 785, 3343, 8755, 3962, 1958, 288, 502, 11, 3962, 11, 12611, 1801, 11, 5290, 16811, 30374, 752, 257, 8655, 631, 34584, 50958], "temperature": 0.0, "avg_logprob": -0.22474868209273727, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.008229165337979794}, {"id": 338, "seek": 247526, "start": 2487.1400000000003, "end": 2493.86, "text": " entre menos infinito y m\u00e1s infinito que es esto, \u00bfno? Est\u00e1 claro, est\u00e1 claro. Esto est\u00e1 entre", "tokens": [50958, 3962, 8902, 7193, 3528, 288, 3573, 7193, 3528, 631, 785, 7433, 11, 3841, 1771, 30, 27304, 16742, 11, 3192, 16742, 13, 20880, 3192, 3962, 51294], "temperature": 0.0, "avg_logprob": -0.22474868209273727, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.008229165337979794}, {"id": 339, "seek": 247526, "start": 2493.86, "end": 2499.34, "text": " menos infinito y m\u00e1s infinito, el W por F, cualquier cosa. Ac\u00e1 yo lo reduzco a una cosa que", "tokens": [51294, 8902, 7193, 3528, 288, 3573, 7193, 3528, 11, 806, 343, 1515, 479, 11, 21004, 10163, 13, 5097, 842, 5290, 450, 40674, 1291, 257, 2002, 10163, 631, 51568], "temperature": 0.0, "avg_logprob": -0.22474868209273727, "compression_ratio": 1.6162790697674418, "no_speech_prob": 0.008229165337979794}, {"id": 340, "seek": 249934, "start": 2499.34, "end": 2521.94, "text": " est\u00e1 entre 0 y infinito, mejor. Bueno, pero para que esto quede entre, entre 0 y 1, lo que hago", "tokens": [50364, 3192, 3962, 1958, 288, 7193, 3528, 11, 11479, 13, 16046, 11, 4768, 1690, 631, 7433, 421, 4858, 3962, 11, 3962, 1958, 288, 502, 11, 450, 631, 38721, 51494], "temperature": 0.0, "avg_logprob": -0.33732226587110953, "compression_ratio": 1.1294117647058823, "no_speech_prob": 0.0167246013879776}, {"id": 341, "seek": 252194, "start": 2521.94, "end": 2528.54, "text": " es, le aplico el logaritmo, para que quede, perd\u00f3n, dije al rev\u00e9s, para que quede entre", "tokens": [50364, 785, 11, 476, 25522, 2789, 806, 41473, 270, 3280, 11, 1690, 631, 421, 4858, 11, 12611, 1801, 11, 39414, 419, 3698, 2191, 11, 1690, 631, 421, 4858, 3962, 50694], "temperature": 0.0, "avg_logprob": -0.26432490154979676, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.5294376015663147}, {"id": 342, "seek": 252194, "start": 2528.54, "end": 2533.42, "text": " 1 y infinito y m\u00e1s infinito, le aplico el logaritmo. Implico el logaritmo y entonces digo, bueno,", "tokens": [50694, 502, 288, 7193, 3528, 288, 3573, 7193, 3528, 11, 476, 25522, 2789, 806, 41473, 270, 3280, 13, 4331, 564, 2789, 806, 41473, 270, 3280, 288, 13003, 22990, 11, 11974, 11, 50938], "temperature": 0.0, "avg_logprob": -0.26432490154979676, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.5294376015663147}, {"id": 343, "seek": 252194, "start": 2535.42, "end": 2540.42, "text": " esto es lo que busc\u00e1bamos, yo quiero estimar el logaritmo de la probabilidad de las odds,", "tokens": [51038, 7433, 785, 450, 631, 1255, 66, 27879, 2151, 11, 5290, 16811, 8017, 289, 806, 41473, 270, 3280, 368, 635, 31959, 4580, 368, 2439, 17439, 11, 51288], "temperature": 0.0, "avg_logprob": -0.26432490154979676, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.5294376015663147}, {"id": 344, "seek": 252194, "start": 2541.78, "end": 2547.62, "text": " y por eso lleg\u00f3 a esa f\u00f3rmula tan rara con E, porque cuando yo despejo, y esto se lo dejo de", "tokens": [51356, 288, 1515, 7287, 46182, 257, 11342, 283, 15614, 76, 3780, 7603, 367, 2419, 416, 462, 11, 4021, 7767, 5290, 730, 494, 5134, 11, 288, 7433, 369, 450, 368, 5134, 368, 51648], "temperature": 0.0, "avg_logprob": -0.26432490154979676, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.5294376015663147}, {"id": 345, "seek": 254762, "start": 2547.62, "end": 2554.62, "text": " ver, cuando yo despejo P igual true, es f\u00e1cil, \u00bfno? Digamos, este logaritmo se transforma en un E", "tokens": [50364, 1306, 11, 7767, 5290, 730, 494, 5134, 430, 10953, 2074, 11, 785, 17474, 11, 3841, 1771, 30, 10976, 2151, 11, 4065, 41473, 270, 3280, 369, 4088, 64, 465, 517, 462, 50714], "temperature": 0.0, "avg_logprob": -0.2791014862060547, "compression_ratio": 1.2822085889570551, "no_speech_prob": 0.02087584137916565}, {"id": 346, "seek": 254762, "start": 2554.62, "end": 2564.38, "text": " a la W por F, \u00bfs\u00ed? Bueno, se lo dejo de ver. Cuesti\u00f3n, \u00bfqu\u00e9 queda de s\u00ed? E a la W por F dividido 1 m\u00e1s", "tokens": [50714, 257, 635, 343, 1515, 479, 11, 3841, 82, 870, 30, 16046, 11, 369, 450, 368, 5134, 368, 1306, 13, 383, 11493, 2560, 11, 3841, 16412, 23314, 368, 8600, 30, 462, 257, 635, 343, 1515, 479, 4996, 2925, 502, 3573, 51202], "temperature": 0.0, "avg_logprob": -0.2791014862060547, "compression_ratio": 1.2822085889570551, "no_speech_prob": 0.02087584137916565}, {"id": 347, "seek": 256438, "start": 2564.38, "end": 2579.94, "text": " E a la W por F. Las odds, se transforma en que es esta funci\u00f3n, \u00bfs\u00ed? Entonces, llegu\u00e9 a una funci\u00f3n", "tokens": [50364, 462, 257, 635, 343, 1515, 479, 13, 10663, 17439, 11, 369, 4088, 64, 465, 631, 785, 5283, 43735, 11, 3841, 82, 870, 30, 15097, 11, 11234, 42423, 257, 2002, 43735, 51142], "temperature": 0.0, "avg_logprob": -0.2717546366028867, "compression_ratio": 1.3161290322580645, "no_speech_prob": 0.1271842122077942}, {"id": 348, "seek": 256438, "start": 2579.94, "end": 2587.46, "text": " que me dice la probabilidad de que sea verdadero da la clase, a partir de haciendo unas cosas raras", "tokens": [51142, 631, 385, 10313, 635, 31959, 4580, 368, 631, 4158, 13692, 2032, 1120, 635, 44578, 11, 257, 13906, 368, 20509, 25405, 12218, 367, 35867, 51518], "temperature": 0.0, "avg_logprob": -0.2717546366028867, "compression_ratio": 1.3161290322580645, "no_speech_prob": 0.1271842122077942}, {"id": 349, "seek": 258746, "start": 2587.46, "end": 2594.9, "text": " con las features, nada menos. O sea, algo parecido al lineal, pero que la corrijo con esta funci\u00f3n.", "tokens": [50364, 416, 2439, 4122, 11, 8096, 8902, 13, 422, 4158, 11, 8655, 7448, 17994, 419, 1622, 304, 11, 4768, 631, 635, 1181, 11105, 78, 416, 5283, 43735, 13, 50736], "temperature": 0.0, "avg_logprob": -0.25073783699123336, "compression_ratio": 1.5427135678391959, "no_speech_prob": 0.11188614368438721}, {"id": 350, "seek": 258746, "start": 2594.9, "end": 2606.66, "text": " Esto es lo mismo que hace la red neuronal. No mismo. Esa funci\u00f3n se llama funci\u00f3n log\u00edstica y tiene", "tokens": [50736, 20880, 785, 450, 12461, 631, 10032, 635, 2182, 12087, 21523, 13, 883, 12461, 13, 2313, 64, 43735, 369, 23272, 43735, 3565, 19512, 2262, 288, 7066, 51324], "temperature": 0.0, "avg_logprob": -0.25073783699123336, "compression_ratio": 1.5427135678391959, "no_speech_prob": 0.11188614368438721}, {"id": 351, "seek": 258746, "start": 2606.66, "end": 2612.78, "text": " este aspecto. \u00bfCu\u00e1l es la caracter\u00edstica de la funci\u00f3n log\u00edstica? Y bueno, que parece un escal\u00f3n,", "tokens": [51324, 4065, 4171, 78, 13, 3841, 35222, 11447, 785, 635, 34297, 2262, 368, 635, 43735, 3565, 19512, 2262, 30, 398, 11974, 11, 631, 14120, 517, 17871, 1801, 11, 51630], "temperature": 0.0, "avg_logprob": -0.25073783699123336, "compression_ratio": 1.5427135678391959, "no_speech_prob": 0.11188614368438721}, {"id": 352, "seek": 261278, "start": 2612.98, "end": 2618.9, "text": " es parecida una cosa que vale cero, si es negativo y uno si es positivo, pero que es continua.", "tokens": [50374, 785, 7448, 37200, 2002, 10163, 631, 15474, 269, 2032, 11, 1511, 785, 2485, 18586, 288, 8526, 1511, 785, 44710, 11, 4768, 631, 785, 40861, 13, 50670], "temperature": 0.0, "avg_logprob": -0.2614697217941284, "compression_ratio": 1.6534090909090908, "no_speech_prob": 0.013697381131350994}, {"id": 353, "seek": 261278, "start": 2626.1000000000004, "end": 2632.9, "text": " Es una linda funci\u00f3n, es una funci\u00f3n smooth. Pero sigue pareciendo un escal\u00f3n. Si yo logro,", "tokens": [51030, 2313, 2002, 287, 6837, 43735, 11, 785, 2002, 43735, 5508, 13, 9377, 34532, 7448, 16830, 517, 17871, 1801, 13, 4909, 5290, 3565, 340, 11, 51370], "temperature": 0.0, "avg_logprob": -0.2614697217941284, "compression_ratio": 1.6534090909090908, "no_speech_prob": 0.013697381131350994}, {"id": 354, "seek": 261278, "start": 2632.9, "end": 2637.78, "text": " si estoy de este lado, m\u00e1s seguramente sea negativo y si estoy de este lado sea positivo. Pero puedo", "tokens": [51370, 1511, 15796, 368, 4065, 11631, 11, 3573, 22179, 3439, 4158, 2485, 18586, 288, 1511, 15796, 368, 4065, 11631, 4158, 44710, 13, 9377, 21612, 51614], "temperature": 0.0, "avg_logprob": -0.2614697217941284, "compression_ratio": 1.6534090909090908, "no_speech_prob": 0.013697381131350994}, {"id": 355, "seek": 263778, "start": 2637.78, "end": 2647.0600000000004, "text": " derivar a las esas cosas. Las red neuronal usan mucho eso. Y hacemos el chiste de no se puede entrar.", "tokens": [50364, 10151, 289, 257, 2439, 23388, 12218, 13, 10663, 2182, 12087, 21523, 505, 282, 9824, 7287, 13, 398, 33839, 806, 417, 8375, 368, 572, 369, 8919, 20913, 13, 50828], "temperature": 0.0, "avg_logprob": -0.35820497048867717, "compression_ratio": 1.3579545454545454, "no_speech_prob": 0.0836201086640358}, {"id": 356, "seek": 263778, "start": 2650.5, "end": 2653.82, "text": " No, es para que se sientan m\u00e1s. Bueno.", "tokens": [51000, 883, 11, 785, 1690, 631, 369, 262, 1196, 282, 3573, 13, 16046, 13, 51166], "temperature": 0.0, "avg_logprob": -0.35820497048867717, "compression_ratio": 1.3579545454545454, "no_speech_prob": 0.0836201086640358}, {"id": 357, "seek": 263778, "start": 2660.42, "end": 2665.34, "text": " Y bueno, \u00bfy c\u00f3mo clasificamos? Muy es f\u00e1cil. Si la probabilidad de que sea verdadero mayor que", "tokens": [51496, 398, 11974, 11, 3841, 88, 12826, 596, 296, 1089, 2151, 30, 39586, 785, 17474, 13, 4909, 635, 31959, 4580, 368, 631, 4158, 13692, 2032, 10120, 631, 51742], "temperature": 0.0, "avg_logprob": -0.35820497048867717, "compression_ratio": 1.3579545454545454, "no_speech_prob": 0.0836201086640358}, {"id": 358, "seek": 266534, "start": 2665.38, "end": 2673.7400000000002, "text": " la probabilidad que sea falso, dada el atributo, es lo mismo que decir que e a la w por f es mayor que", "tokens": [50366, 635, 31959, 4580, 631, 4158, 3704, 539, 11, 274, 1538, 806, 412, 2024, 8262, 11, 785, 450, 12461, 631, 10235, 631, 308, 257, 635, 261, 1515, 283, 785, 10120, 631, 50784], "temperature": 0.0, "avg_logprob": -0.2499225616455078, "compression_ratio": 1.446808510638298, "no_speech_prob": 0.0016956429462879896}, {"id": 359, "seek": 266534, "start": 2673.7400000000002, "end": 2689.94, "text": " 1. Por esto. \u00bfS\u00ed? Que es lo mismo que decir que w por f sea mayor que 0. Entonces clasificar es muy", "tokens": [50784, 502, 13, 5269, 7433, 13, 3841, 30463, 30, 4493, 785, 450, 12461, 631, 10235, 631, 261, 1515, 283, 4158, 10120, 631, 1958, 13, 15097, 596, 296, 25625, 785, 5323, 51594], "temperature": 0.0, "avg_logprob": -0.2499225616455078, "compression_ratio": 1.446808510638298, "no_speech_prob": 0.0016956429462879896}, {"id": 360, "seek": 268994, "start": 2689.94, "end": 2696.46, "text": " f\u00e1cil con este m\u00e9todo, porque lo \u00fanico que toca hacer es multiplicar w por f con los pesos que", "tokens": [50364, 17474, 416, 4065, 20275, 17423, 11, 4021, 450, 26113, 631, 43514, 6720, 785, 17596, 289, 261, 1515, 283, 416, 1750, 33204, 631, 50690], "temperature": 0.0, "avg_logprob": -0.22152337573823475, "compression_ratio": 1.5, "no_speech_prob": 0.011998250149190426}, {"id": 361, "seek": 268994, "start": 2696.46, "end": 2701.62, "text": " calcul\u00e9 por la feature y si me da mayor que 0 quiere decir que positivo y sino negativo. Eso es la", "tokens": [50690, 4322, 526, 1515, 635, 4111, 288, 1511, 385, 1120, 10120, 631, 1958, 23877, 10235, 631, 44710, 288, 18108, 2485, 18586, 13, 27795, 785, 635, 50948], "temperature": 0.0, "avg_logprob": -0.22152337573823475, "compression_ratio": 1.5, "no_speech_prob": 0.011998250149190426}, {"id": 362, "seek": 268994, "start": 2701.62, "end": 2707.46, "text": " regresi\u00f3n log\u00edstica. Se llama regresi\u00f3n, aunque se llama regresi\u00f3n es un m\u00e9todo de clasificaci\u00f3n.", "tokens": [50948, 47108, 2560, 3565, 19512, 2262, 13, 1100, 23272, 47108, 2560, 11, 21962, 369, 23272, 47108, 2560, 785, 517, 20275, 17423, 368, 596, 296, 40802, 13, 51240], "temperature": 0.0, "avg_logprob": -0.22152337573823475, "compression_ratio": 1.5, "no_speech_prob": 0.011998250149190426}, {"id": 363, "seek": 268994, "start": 2708.98, "end": 2709.7400000000002, "text": " \u00bfDe acuerdo?", "tokens": [51316, 3841, 11089, 28113, 30, 51354], "temperature": 0.0, "avg_logprob": -0.22152337573823475, "compression_ratio": 1.5, "no_speech_prob": 0.011998250149190426}, {"id": 364, "seek": 271994, "start": 2720.02, "end": 2726.82, "text": " Y la pregunta es bueno, pero ac\u00e1 yo todav\u00eda no respond\u00ed. \u00bfC\u00f3mo estimaba los pesos que", "tokens": [50368, 398, 635, 24252, 785, 11974, 11, 4768, 23496, 5290, 28388, 572, 4196, 870, 13, 3841, 28342, 8017, 5509, 1750, 33204, 631, 50708], "temperature": 0.0, "avg_logprob": -0.2568526738126513, "compression_ratio": 1.3743589743589744, "no_speech_prob": 0.013516228646039963}, {"id": 365, "seek": 271994, "start": 2726.82, "end": 2733.06, "text": " me iba all\u00ed? Se era contando. Ac\u00e1 tengo que hacer algunas cosas un poco m\u00e1s raras.", "tokens": [50708, 385, 33423, 34294, 30, 1100, 4249, 660, 1806, 13, 5097, 842, 13989, 631, 6720, 27316, 12218, 517, 10639, 3573, 367, 35867, 13, 51020], "temperature": 0.0, "avg_logprob": -0.2568526738126513, "compression_ratio": 1.3743589743589744, "no_speech_prob": 0.013516228646039963}, {"id": 366, "seek": 271994, "start": 2739.86, "end": 2745.1, "text": " Digo que mi w estimado es el que maximiza este producto de probabilidades de las diferentes", "tokens": [51360, 413, 7483, 631, 2752, 261, 8017, 1573, 785, 806, 631, 5138, 13427, 4065, 47583, 368, 31959, 10284, 368, 2439, 17686, 51622], "temperature": 0.0, "avg_logprob": -0.2568526738126513, "compression_ratio": 1.3743589743589744, "no_speech_prob": 0.013516228646039963}, {"id": 367, "seek": 274510, "start": 2745.42, "end": 2755.98, "text": " clases. Y me queda esta funci\u00f3n s\u00faper rara, s\u00faper fea, s\u00faper complicada, pero que adivinen que es", "tokens": [50380, 596, 1957, 13, 398, 385, 23314, 5283, 43735, 43282, 367, 2419, 11, 43282, 579, 64, 11, 43282, 16060, 1538, 11, 4768, 631, 614, 592, 5636, 631, 785, 50908], "temperature": 0.0, "avg_logprob": -0.22412118567041603, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.009398434311151505}, {"id": 368, "seek": 274510, "start": 2755.98, "end": 2762.3399999999997, "text": " convexa. Y como es convexa, bueno, yo quiero buscar el m\u00e1ximo de una funci\u00f3n convexa,", "tokens": [50908, 42432, 64, 13, 398, 2617, 785, 42432, 64, 11, 11974, 11, 5290, 16811, 26170, 806, 38876, 368, 2002, 43735, 42432, 64, 11, 51226], "temperature": 0.0, "avg_logprob": -0.22412118567041603, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.009398434311151505}, {"id": 369, "seek": 274510, "start": 2762.3399999999997, "end": 2769.58, "text": " lo mismo que le dec\u00eda hoy. Aplico desde eso por la diente o alg\u00fan otro m\u00e9todo de num\u00e9rico.", "tokens": [51226, 450, 12461, 631, 476, 37599, 13775, 13, 316, 564, 2789, 10188, 7287, 1515, 635, 1026, 1576, 277, 26300, 11921, 20275, 17423, 368, 1031, 526, 23776, 13, 51588], "temperature": 0.0, "avg_logprob": -0.22412118567041603, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.009398434311151505}, {"id": 370, "seek": 276958, "start": 2769.58, "end": 2776.86, "text": " Entonces tengo una forma de estimar esos w, el asunto que tengo es la forma de estimar.", "tokens": [50364, 15097, 13989, 2002, 8366, 368, 8017, 289, 22411, 261, 11, 806, 382, 24052, 631, 13989, 785, 635, 8366, 368, 8017, 289, 13, 50728], "temperature": 0.0, "avg_logprob": -0.2482708888267403, "compression_ratio": 1.4807692307692308, "no_speech_prob": 0.0006224142271094024}, {"id": 371, "seek": 276958, "start": 2783.66, "end": 2788.7799999999997, "text": " Y \u00bfqu\u00e9 pasa si tengo m\u00e1s de dos clases? Y bueno, tengo que hacer una cosa as\u00ed,", "tokens": [51068, 398, 3841, 16412, 20260, 1511, 13989, 3573, 368, 4491, 596, 1957, 30, 398, 11974, 11, 13989, 631, 6720, 2002, 10163, 8582, 11, 51324], "temperature": 0.0, "avg_logprob": -0.2482708888267403, "compression_ratio": 1.4807692307692308, "no_speech_prob": 0.0006224142271094024}, {"id": 372, "seek": 276958, "start": 2788.7799999999997, "end": 2792.5, "text": " calcular la feature a partir de cada clase con cada tributo.", "tokens": [51324, 2104, 17792, 635, 4111, 257, 13906, 368, 8411, 44578, 416, 8411, 1376, 5955, 78, 13, 51510], "temperature": 0.0, "avg_logprob": -0.2482708888267403, "compression_ratio": 1.4807692307692308, "no_speech_prob": 0.0006224142271094024}, {"id": 373, "seek": 279250, "start": 2792.66, "end": 2804.58, "text": " Metarlo dentro de la f\u00f3rmula y volver a normalizar. Y por eso nuestro m\u00e9todo se llama", "tokens": [50372, 6377, 19457, 10856, 368, 635, 283, 15614, 76, 3780, 288, 33998, 257, 2710, 9736, 13, 398, 1515, 7287, 14726, 20275, 17423, 369, 23272, 50968], "temperature": 0.0, "avg_logprob": -0.22014434521014875, "compression_ratio": 1.46875, "no_speech_prob": 0.010431254282593727}, {"id": 374, "seek": 279250, "start": 2804.58, "end": 2811.46, "text": " multinomial logistic regression, porque es una extensi\u00f3n de la regresi\u00f3n log\u00edstica a un caso de", "tokens": [50968, 45872, 298, 831, 3565, 3142, 24590, 11, 4021, 785, 2002, 1279, 694, 2560, 368, 635, 47108, 2560, 3565, 19512, 2262, 257, 517, 9666, 368, 51312], "temperature": 0.0, "avg_logprob": -0.22014434521014875, "compression_ratio": 1.46875, "no_speech_prob": 0.010431254282593727}, {"id": 375, "seek": 279250, "start": 2811.46, "end": 2820.34, "text": " m\u00faltiple clases. Y por supuesto no vamos a quedar con la clase que maximiza la probabilidad de", "tokens": [51312, 275, 43447, 72, 781, 596, 1957, 13, 398, 1515, 34177, 572, 5295, 257, 39244, 416, 635, 44578, 631, 5138, 13427, 635, 31959, 4580, 368, 51756], "temperature": 0.0, "avg_logprob": -0.22014434521014875, "compression_ratio": 1.46875, "no_speech_prob": 0.010431254282593727}, {"id": 376, "seek": 282250, "start": 2822.5, "end": 2823.5, "text": " este tributo. \u00bfDe acuerdo?", "tokens": [50364, 4065, 1376, 5955, 78, 13, 3841, 11089, 28113, 30, 50414], "temperature": 0.0, "avg_logprob": -0.4527438481648763, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0035223676823079586}, {"id": 377, "seek": 282250, "start": 2829.62, "end": 2837.9, "text": " Esta clase es un poquito m\u00e1s, entra m\u00e1s en detalles matem\u00e1ticos que el resto. Me parece", "tokens": [50720, 20547, 44578, 785, 517, 28229, 3573, 11, 22284, 3573, 465, 1141, 37927, 3803, 443, 7656, 9940, 631, 806, 28247, 13, 1923, 14120, 51134], "temperature": 0.0, "avg_logprob": -0.4527438481648763, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0035223676823079586}, {"id": 378, "seek": 282250, "start": 2837.9, "end": 2843.26, "text": " importante entender por qu\u00e9 esos atributos aparecen y por qu\u00e9 aparecen todas esas cosas con", "tokens": [51134, 9416, 20054, 1515, 8057, 22411, 412, 2024, 34640, 15004, 13037, 288, 1515, 8057, 15004, 13037, 10906, 23388, 12218, 416, 51402], "temperature": 0.0, "avg_logprob": -0.4527438481648763, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0035223676823079586}, {"id": 379, "seek": 282250, "start": 2843.26, "end": 2847.5, "text": " e. Y las cosas con e generalmente son para cambiar la curva. \u00bfQu\u00e9 pasa si es paiguiente?", "tokens": [51402, 308, 13, 398, 2439, 12218, 416, 308, 2674, 4082, 1872, 1690, 37738, 635, 1262, 2757, 13, 3841, 15137, 20260, 1511, 785, 2502, 16397, 8413, 30, 51614], "temperature": 0.0, "avg_logprob": -0.4527438481648763, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.0035223676823079586}, {"id": 380, "seek": 285250, "start": 2853.5, "end": 2859.14, "text": " Y por \u00faltimo, como comentario, \u00bfpor qu\u00e9 se llaman modelos de entrop\u00eda m\u00e1xima? La entrop\u00eda,", "tokens": [50414, 398, 1515, 21013, 11, 2617, 14541, 4912, 11, 3841, 2816, 8057, 369, 4849, 6147, 2316, 329, 368, 948, 1513, 2686, 31031, 64, 30, 2369, 948, 1513, 2686, 11, 50696], "temperature": 0.0, "avg_logprob": -0.20589141386101045, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.003658097004517913}, {"id": 381, "seek": 285250, "start": 2861.78, "end": 2867.14, "text": " no s\u00e9 si hablamos algo de entrop\u00eda en alguna clase. La entrop\u00eda es una medida que trata de ver", "tokens": [50828, 572, 7910, 1511, 26280, 2151, 8655, 368, 948, 1513, 2686, 465, 20651, 44578, 13, 2369, 948, 1513, 2686, 785, 2002, 32984, 631, 31920, 368, 1306, 51096], "temperature": 0.0, "avg_logprob": -0.20589141386101045, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.003658097004517913}, {"id": 382, "seek": 285250, "start": 2867.14, "end": 2874.02, "text": " qu\u00e9 tan parecido son los elementos de algo. Entonces, el principio de entrop\u00eda m\u00e1xima dice,", "tokens": [51096, 8057, 7603, 7448, 17994, 1872, 1750, 35797, 368, 8655, 13, 15097, 11, 806, 34308, 368, 948, 1513, 2686, 31031, 64, 10313, 11, 51440], "temperature": 0.0, "avg_logprob": -0.20589141386101045, "compression_ratio": 1.5966850828729282, "no_speech_prob": 0.003658097004517913}, {"id": 383, "seek": 287402, "start": 2874.02, "end": 2884.5, "text": " bueno, yo si tengo muchas distribuciones posibles, candidatas, algo, el hijo,", "tokens": [50364, 11974, 11, 5290, 1511, 13989, 16072, 4400, 46649, 1366, 14428, 11, 6268, 37892, 11, 8655, 11, 806, 38390, 11, 50888], "temperature": 0.0, "avg_logprob": -0.24545923868815103, "compression_ratio": 1.4059405940594059, "no_speech_prob": 0.07468272745609283}, {"id": 384, "seek": 287402, "start": 2886.34, "end": 2892.42, "text": " la que tiene entrop\u00eda m\u00e1xima, es decir, la que da dos mil datos, la que solo asume lo que los datos", "tokens": [50980, 635, 631, 7066, 948, 1513, 2686, 31031, 64, 11, 785, 10235, 11, 635, 631, 1120, 4491, 1962, 27721, 11, 635, 631, 6944, 382, 2540, 450, 631, 1750, 27721, 51284], "temperature": 0.0, "avg_logprob": -0.24545923868815103, "compression_ratio": 1.4059405940594059, "no_speech_prob": 0.07468272745609283}, {"id": 385, "seek": 287402, "start": 2892.42, "end": 2899.1, "text": " te dicen. \u00bfQu\u00e9 quiero decir eso? Si yo no conozco nada sobre un documento en el caso del 90-20, 90-10,", "tokens": [51284, 535, 33816, 13, 3841, 15137, 16811, 10235, 7287, 30, 4909, 5290, 572, 416, 15151, 1291, 8096, 5473, 517, 4166, 78, 465, 806, 9666, 1103, 4289, 12, 2009, 11, 4289, 12, 3279, 11, 51618], "temperature": 0.0, "avg_logprob": -0.24545923868815103, "compression_ratio": 1.4059405940594059, "no_speech_prob": 0.07468272745609283}, {"id": 386, "seek": 289910, "start": 2900.1, "end": 2907.86, "text": " asumo 90-10 porque puedo asumir a partir de los datos. Yo podr\u00eda asumir 0802 por", "tokens": [50414, 382, 40904, 4289, 12, 3279, 4021, 21612, 382, 449, 347, 257, 13906, 368, 1750, 27721, 13, 7616, 27246, 382, 449, 347, 1958, 4702, 17, 1515, 50802], "temperature": 0.0, "avg_logprob": -0.1883806876086314, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.011324333027005196}, {"id": 387, "seek": 289910, "start": 2907.86, "end": 2914.9, "text": " alg\u00fan motivo, pero si yo no s\u00e9 m\u00e1s que eso, estoy utilizando, o si no s\u00e9 nada,", "tokens": [50802, 26300, 35804, 11, 4768, 1511, 5290, 572, 7910, 3573, 631, 7287, 11, 15796, 19906, 1806, 11, 277, 1511, 572, 7910, 8096, 11, 51154], "temperature": 0.0, "avg_logprob": -0.1883806876086314, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.011324333027005196}, {"id": 388, "seek": 289910, "start": 2914.9, "end": 2919.7, "text": " si yo no s\u00e9 nada sobre un documento, no s\u00e9 nada, no tengo ninguna informaci\u00f3n a priori.", "tokens": [51154, 1511, 5290, 572, 7910, 8096, 5473, 517, 4166, 78, 11, 572, 7910, 8096, 11, 572, 13989, 36073, 21660, 257, 4059, 72, 13, 51394], "temperature": 0.0, "avg_logprob": -0.1883806876086314, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.011324333027005196}, {"id": 389, "seek": 289910, "start": 2919.7, "end": 2927.1, "text": " Y te doy un documento y te digo de qu\u00e9 clase es, es de deporte o es de espect\u00e1culo. \u00bfQu\u00e9 har\u00edan", "tokens": [51394, 398, 535, 360, 88, 517, 4166, 78, 288, 535, 22990, 368, 8057, 44578, 785, 11, 785, 368, 1367, 12752, 277, 785, 368, 38244, 842, 25436, 13, 3841, 15137, 2233, 11084, 51764], "temperature": 0.0, "avg_logprob": -0.1883806876086314, "compression_ratio": 1.5614035087719298, "no_speech_prob": 0.011324333027005196}, {"id": 390, "seek": 292710, "start": 2927.1, "end": 2938.7799999999997, "text": " ustedes? Si yo te digo 50-50, eso es aplicar el principio de entrop\u00eda m\u00e1xima, es decir,", "tokens": [50364, 17110, 30, 4909, 5290, 535, 22990, 2625, 12, 2803, 11, 7287, 785, 18221, 289, 806, 34308, 368, 948, 1513, 2686, 31031, 64, 11, 785, 10235, 11, 50948], "temperature": 0.0, "avg_logprob": -0.19393048967633927, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.02328970842063427}, {"id": 391, "seek": 292710, "start": 2938.7799999999997, "end": 2943.54, "text": " bueno, yo no tengo informaci\u00f3n, es todo equiprobable. \u00bfSe acuerdan que la entrop\u00eda es m\u00e1xima cuando", "tokens": [50948, 11974, 11, 5290, 572, 13989, 21660, 11, 785, 5149, 5037, 16614, 712, 13, 3841, 10637, 696, 5486, 10312, 631, 635, 948, 1513, 2686, 785, 31031, 64, 7767, 51186], "temperature": 0.0, "avg_logprob": -0.19393048967633927, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.02328970842063427}, {"id": 392, "seek": 292710, "start": 2943.54, "end": 2950.7, "text": " son todo equiprobable? Si yo agrego un poco de informaci\u00f3n y yo tengo un dado, pero yo te", "tokens": [51186, 1872, 5149, 5037, 16614, 712, 30, 4909, 5290, 4554, 1571, 517, 10639, 368, 21660, 288, 5290, 13989, 517, 29568, 11, 4768, 5290, 535, 51544], "temperature": 0.0, "avg_logprob": -0.19393048967633927, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.02328970842063427}, {"id": 393, "seek": 295070, "start": 2950.7, "end": 2960.3399999999997, "text": " aseguro que el 6 no sale nunca. \u00bfCu\u00e1l es la probabilidad de sacar un 1? Un quinto. O sea,", "tokens": [50364, 38174, 7052, 631, 806, 1386, 572, 8680, 13768, 13, 3841, 35222, 11447, 785, 635, 31959, 4580, 368, 43823, 517, 502, 30, 1156, 421, 17246, 13, 422, 4158, 11, 50846], "temperature": 0.0, "avg_logprob": -0.2189904178481504, "compression_ratio": 1.3762376237623761, "no_speech_prob": 0.07220808416604996}, {"id": 394, "seek": 295070, "start": 2960.3399999999997, "end": 2966.7, "text": " paso de ser un sexto, un quinto, porque tengo m\u00e1s informaci\u00f3n, pero siempre no debo un cuarto,", "tokens": [50846, 29212, 368, 816, 517, 42826, 78, 11, 517, 421, 17246, 11, 4021, 13989, 3573, 21660, 11, 4768, 12758, 572, 368, 1763, 517, 48368, 11, 51164], "temperature": 0.0, "avg_logprob": -0.2189904178481504, "compression_ratio": 1.3762376237623761, "no_speech_prob": 0.07220808416604996}, {"id": 395, "seek": 295070, "start": 2966.7, "end": 2971.8599999999997, "text": " porque no puedo sacarlo de ning\u00fan dato. Eso es el principio de entrop\u00eda m\u00e1xima. Si yo,", "tokens": [51164, 4021, 572, 21612, 4899, 19457, 368, 30394, 46971, 13, 27795, 785, 806, 34308, 368, 948, 1513, 2686, 31031, 64, 13, 4909, 5290, 11, 51422], "temperature": 0.0, "avg_logprob": -0.2189904178481504, "compression_ratio": 1.3762376237623761, "no_speech_prob": 0.07220808416604996}, {"id": 396, "seek": 297186, "start": 2971.98, "end": 2981.46, "text": " cuando elijo estas distribuciones posibles, aplico solo lo que los atributos me dicen,", "tokens": [50370, 7767, 806, 24510, 13897, 4400, 46649, 1366, 14428, 11, 25522, 2789, 6944, 450, 631, 1750, 412, 2024, 34640, 385, 33816, 11, 50844], "temperature": 0.0, "avg_logprob": -0.2265051289608604, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.0031349253840744495}, {"id": 397, "seek": 297186, "start": 2985.02, "end": 2992.6200000000003, "text": " aplicando el principio, el que tenga m\u00e1xima entrop\u00eda, a lo que llego es exactamente al", "tokens": [51022, 18221, 1806, 806, 34308, 11, 806, 631, 36031, 31031, 64, 948, 1513, 2686, 11, 257, 450, 631, 4849, 6308, 785, 48686, 419, 51402], "temperature": 0.0, "avg_logprob": -0.2265051289608604, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.0031349253840744495}, {"id": 398, "seek": 297186, "start": 2992.6200000000003, "end": 2998.42, "text": " mismo modelo que present\u00e9 antes. Por eso tambi\u00e9n los modelos se llaman modelos de entrop\u00eda m\u00e1xima,", "tokens": [51402, 12461, 27825, 631, 1974, 526, 11014, 13, 5269, 7287, 6407, 1750, 2316, 329, 369, 4849, 6147, 2316, 329, 368, 948, 1513, 2686, 31031, 64, 11, 51692], "temperature": 0.0, "avg_logprob": -0.2265051289608604, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.0031349253840744495}, {"id": 399, "seek": 299842, "start": 2999.02, "end": 3003.3, "text": " es porque son dos formas diferentes de llegar a los mismos. Si usted quiere en el detalle,", "tokens": [50394, 785, 4021, 1872, 4491, 33463, 17686, 368, 24892, 257, 1750, 47458, 13, 4909, 10467, 23877, 465, 806, 1141, 11780, 11, 50608], "temperature": 0.0, "avg_logprob": -0.30388560439601087, "compression_ratio": 1.5168067226890756, "no_speech_prob": 0.0035281020682305098}, {"id": 400, "seek": 299842, "start": 3003.3, "end": 3009.2200000000003, "text": " en la literatura est\u00e1 eso. No s\u00e9 si les interesa, pero al que les voy a interesar est\u00e1 muy bien.", "tokens": [50608, 465, 635, 2733, 19660, 3192, 7287, 13, 883, 7910, 1511, 1512, 728, 13708, 11, 4768, 419, 631, 1512, 7552, 257, 728, 18876, 3192, 5323, 3610, 13, 50904], "temperature": 0.0, "avg_logprob": -0.30388560439601087, "compression_ratio": 1.5168067226890756, "no_speech_prob": 0.0035281020682305098}, {"id": 401, "seek": 299842, "start": 3010.78, "end": 3015.06, "text": " Coinciden con eso. Coinciden con una distribuci\u00f3n de probabilidad para un modelo log\u00edstico", "tokens": [50982, 3066, 4647, 4380, 416, 7287, 13, 3066, 4647, 4380, 416, 2002, 4400, 30813, 368, 31959, 4580, 1690, 517, 27825, 3565, 19512, 2789, 51196], "temperature": 0.0, "avg_logprob": -0.30388560439601087, "compression_ratio": 1.5168067226890756, "no_speech_prob": 0.0035281020682305098}, {"id": 402, "seek": 299842, "start": 3015.06, "end": 3018.7000000000003, "text": " lupinional cuyo peso maximiza la verosimilitud en los datos de entrenamiento.", "tokens": [51196, 287, 1010, 259, 1966, 2702, 8308, 28149, 5138, 13427, 635, 1306, 329, 332, 388, 21875, 465, 1750, 27721, 368, 45069, 16971, 13, 51378], "temperature": 0.0, "avg_logprob": -0.30388560439601087, "compression_ratio": 1.5168067226890756, "no_speech_prob": 0.0035281020682305098}, {"id": 403, "seek": 302842, "start": 3029.1800000000003, "end": 3035.02, "text": " Por ejemplo, si yo quiero aplicar un modelo de entrop\u00eda m\u00e1xima al ejemplo del post time,", "tokens": [50402, 5269, 13358, 11, 1511, 5290, 16811, 18221, 289, 517, 27825, 368, 948, 1513, 2686, 31031, 64, 419, 13358, 1103, 2183, 565, 11, 50694], "temperature": 0.0, "avg_logprob": -0.2531346240675593, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.007452171295881271}, {"id": 404, "seek": 302842, "start": 3035.02, "end": 3043.78, "text": " la feature van a lucir as\u00ed. Tengo una feature 1 que dice vale f1, vale 1 si la palabra es", "tokens": [50694, 635, 4111, 3161, 257, 21296, 347, 8582, 13, 314, 30362, 2002, 4111, 502, 631, 10313, 15474, 283, 16, 11, 15474, 502, 1511, 635, 31702, 785, 51132], "temperature": 0.0, "avg_logprob": -0.2531346240675593, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.007452171295881271}, {"id": 405, "seek": 302842, "start": 3043.78, "end": 3055.66, "text": " reis y la clase es nombre y si no vale 0. Otra feature va a ser 1 si la anterior es t\u00fa y la clase", "tokens": [51132, 319, 271, 288, 635, 44578, 785, 13000, 288, 1511, 572, 15474, 1958, 13, 12936, 424, 4111, 2773, 257, 816, 502, 1511, 635, 22272, 785, 15056, 288, 635, 44578, 51726], "temperature": 0.0, "avg_logprob": -0.2531346240675593, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.007452171295881271}, {"id": 406, "seek": 305566, "start": 3055.66, "end": 3063.66, "text": " es verbo y si no es 0. Otra feature y como se imaginar\u00e1n la feature son, estamos hablando de", "tokens": [50364, 785, 1306, 1763, 288, 1511, 572, 785, 1958, 13, 12936, 424, 4111, 288, 2617, 369, 49048, 7200, 635, 4111, 1872, 11, 10382, 29369, 368, 50764], "temperature": 0.0, "avg_logprob": -0.25718483743788323, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.004719173535704613}, {"id": 407, "seek": 305566, "start": 3063.66, "end": 3073.2599999999998, "text": " miles o de millones de features, pues son todas las posibles, las relevantes. As\u00ed lucen las features", "tokens": [50764, 6193, 277, 368, 22416, 368, 4122, 11, 11059, 1872, 10906, 2439, 1366, 14428, 11, 2439, 7340, 279, 13, 17419, 21296, 268, 2439, 4122, 51244], "temperature": 0.0, "avg_logprob": -0.25718483743788323, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.004719173535704613}, {"id": 408, "seek": 305566, "start": 3073.2599999999998, "end": 3083.7799999999997, "text": " un modelo de entrop\u00eda m\u00e1xima, de un mont\u00f3n de features y yo lo que voy a hacer y adem\u00e1s son", "tokens": [51244, 517, 27825, 368, 948, 1513, 2686, 31031, 64, 11, 368, 517, 45259, 368, 4122, 288, 5290, 450, 631, 7552, 257, 6720, 288, 21251, 1872, 51770], "temperature": 0.0, "avg_logprob": -0.25718483743788323, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.004719173535704613}, {"id": 409, "seek": 308378, "start": 3083.78, "end": 3092.6600000000003, "text": " indicadores, eso generalmente vale 1 o 0, usualmente. Y lo que yo voy a hacer es, calculando a trav\u00e9s de", "tokens": [50364, 4694, 11856, 11, 7287, 2674, 4082, 15474, 502, 277, 1958, 11, 7713, 4082, 13, 398, 450, 631, 5290, 7552, 257, 6720, 785, 11, 4322, 1806, 257, 24463, 368, 50808], "temperature": 0.0, "avg_logprob": -0.2997471202503551, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.005945514887571335}, {"id": 410, "seek": 308378, "start": 3092.6600000000003, "end": 3103.2200000000003, "text": " contando, s\u00ed, voy a calcular los W, con aquello que hablamos hoy de la f\u00f3rmula de minimizarla,", "tokens": [50808, 660, 1806, 11, 8600, 11, 7552, 257, 2104, 17792, 1750, 343, 11, 416, 2373, 11216, 631, 26280, 2151, 13775, 368, 635, 283, 15614, 76, 3780, 368, 4464, 9736, 875, 11, 51336], "temperature": 0.0, "avg_logprob": -0.2997471202503551, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.005945514887571335}, {"id": 411, "seek": 308378, "start": 3105.6200000000003, "end": 3110.94, "text": " o sea que cada feature va a tener un peso indicando qu\u00e9 tanto afecta la feature corresponde para el", "tokens": [51456, 277, 4158, 631, 8411, 4111, 2773, 257, 11640, 517, 28149, 4694, 1806, 8057, 10331, 30626, 64, 635, 4111, 6805, 68, 1690, 806, 51722], "temperature": 0.0, "avg_logprob": -0.2997471202503551, "compression_ratio": 1.5226130653266332, "no_speech_prob": 0.005945514887571335}, {"id": 412, "seek": 311094, "start": 3110.94, "end": 3118.2200000000003, "text": " tag ese. Por ejemplo, si yo tengo aquello, se acuerdan que quer\u00edamos saber qu\u00e9 era reis en el post", "tokens": [50364, 6162, 10167, 13, 5269, 13358, 11, 1511, 5290, 13989, 2373, 11216, 11, 369, 696, 5486, 10312, 631, 7083, 16275, 12489, 8057, 4249, 319, 271, 465, 806, 2183, 50728], "temperature": 0.0, "avg_logprob": -0.269796187417549, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.007662511896342039}, {"id": 413, "seek": 311094, "start": 3118.2200000000003, "end": 3124.7000000000003, "text": " tag, \u00bfno? Que era la \u00fanica palabra que no sab\u00edamos lo que era. Entonces, estos son los pesos que yo", "tokens": [50728, 6162, 11, 3841, 1771, 30, 4493, 4249, 635, 30104, 31702, 631, 572, 5560, 16275, 450, 631, 4249, 13, 15097, 11, 12585, 1872, 1750, 33204, 631, 5290, 51052], "temperature": 0.0, "avg_logprob": -0.269796187417549, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.007662511896342039}, {"id": 414, "seek": 311094, "start": 3124.7000000000003, "end": 3130.98, "text": " entren\u00e9, lo que me dicen es que, f\u00edjense que los pesos que tenemos ac\u00e1, lo que me dicen es", "tokens": [51052, 45069, 526, 11, 450, 631, 385, 33816, 785, 631, 11, 283, 870, 73, 1288, 631, 1750, 33204, 631, 9914, 23496, 11, 450, 631, 385, 33816, 785, 51366], "temperature": 0.0, "avg_logprob": -0.269796187417549, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.007662511896342039}, {"id": 415, "seek": 311094, "start": 3133.1, "end": 3139.58, "text": " que la feature m\u00e1s importante es la size, en el caso, para que sea, si es un nombre, esta es muy", "tokens": [51472, 631, 635, 4111, 3573, 9416, 785, 635, 2744, 11, 465, 806, 9666, 11, 1690, 631, 4158, 11, 1511, 785, 517, 13000, 11, 5283, 785, 5323, 51796], "temperature": 0.0, "avg_logprob": -0.269796187417549, "compression_ratio": 1.6527196652719665, "no_speech_prob": 0.007662511896342039}, {"id": 416, "seek": 313958, "start": 3139.58, "end": 3150.7799999999997, "text": " negativa, o sea que resta valor y esta es muy positiva, f2 para un verbo, no me pregunten si son", "tokens": [50364, 2485, 18740, 11, 277, 4158, 631, 1472, 64, 15367, 288, 5283, 785, 5323, 11218, 5931, 11, 283, 17, 1690, 517, 1306, 1763, 11, 572, 385, 659, 7414, 1147, 1511, 1872, 50924], "temperature": 0.0, "avg_logprob": -0.2895210929538893, "compression_ratio": 1.4696969696969697, "no_speech_prob": 0.005123570095747709}, {"id": 417, "seek": 313958, "start": 3150.7799999999997, "end": 3160.66, "text": " n\u00fameros reales o no m\u00e1s guardas, f2 es, ah, si la previa es t\u00fa, si, la previa es t\u00fa, pesa muy", "tokens": [50924, 36545, 957, 279, 277, 572, 3573, 6290, 296, 11, 283, 17, 785, 11, 3716, 11, 1511, 635, 659, 11617, 785, 15056, 11, 1511, 11, 635, 659, 11617, 785, 15056, 11, 9262, 64, 5323, 51418], "temperature": 0.0, "avg_logprob": -0.2895210929538893, "compression_ratio": 1.4696969696969697, "no_speech_prob": 0.005123570095747709}, {"id": 418, "seek": 316066, "start": 3160.8199999999997, "end": 3174.3799999999997, "text": " positivamente para que eso sea un verbo, t\u00fa reis, \u00bfno? Y la f6, \u00bfqu\u00e9 es? Y pesa muy negativamente para", "tokens": [50372, 11218, 23957, 1690, 631, 7287, 4158, 517, 1306, 1763, 11, 15056, 319, 271, 11, 3841, 1771, 30, 398, 635, 283, 21, 11, 3841, 16412, 785, 30, 398, 9262, 64, 5323, 2485, 10662, 3439, 1690, 51050], "temperature": 0.0, "avg_logprob": -0.26003705538236177, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.2952049970626831}, {"id": 419, "seek": 316066, "start": 3174.3799999999997, "end": 3181.3399999999997, "text": " un nombre, f\u00edjense que hay features diferentes seg\u00fan la clase, porque son m\u00e1s de 1, m\u00e1s de 2,", "tokens": [51050, 517, 13000, 11, 283, 870, 73, 1288, 631, 4842, 4122, 17686, 36570, 635, 44578, 11, 4021, 1872, 3573, 368, 502, 11, 3573, 368, 568, 11, 51398], "temperature": 0.0, "avg_logprob": -0.26003705538236177, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.2952049970626831}, {"id": 420, "seek": 318134, "start": 3182.1800000000003, "end": 3189.78, "text": " \u00bfde acuerdo? Entonces, yo hago las cuentas, la probabilidad de que sea un nombre dado las", "tokens": [50406, 3841, 1479, 28113, 30, 15097, 11, 5290, 38721, 2439, 46414, 296, 11, 635, 31959, 4580, 368, 631, 4158, 517, 13000, 29568, 2439, 50786], "temperature": 0.0, "avg_logprob": -0.29860312938690187, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.08900006115436554}, {"id": 421, "seek": 318134, "start": 3189.78, "end": 3197.6600000000003, "text": " features es, es, es exactamente aplicar las features relevantes, ac\u00e1 es 0,8 porque", "tokens": [50786, 4122, 785, 11, 785, 11, 785, 48686, 18221, 289, 2439, 4122, 7340, 279, 11, 23496, 785, 1958, 11, 23, 4021, 51180], "temperature": 0.0, "avg_logprob": -0.29860312938690187, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.08900006115436554}, {"id": 422, "seek": 318134, "start": 3200.78, "end": 3206.86, "text": " multiplica la prim, la segunda y la sexta, porque son las que aplican a nn, \u00bfno? Si no valen 0.", "tokens": [51336, 12788, 2262, 635, 2886, 11, 635, 21978, 288, 635, 42826, 64, 11, 4021, 1872, 2439, 631, 18221, 282, 257, 297, 77, 11, 3841, 1771, 30, 4909, 572, 1323, 268, 1958, 13, 51640], "temperature": 0.0, "avg_logprob": -0.29860312938690187, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.08900006115436554}, {"id": 423, "seek": 321134, "start": 3211.5, "end": 3230.06, "text": " La 2, dijimos, y la 6, son las de t\u00fa, 0,8. No, es la 1 y la 6, la 1 y la 6, correcto. O sea,", "tokens": [50372, 2369, 568, 11, 47709, 8372, 11, 288, 635, 1386, 11, 1872, 2439, 368, 15056, 11, 1958, 11, 23, 13, 883, 11, 785, 635, 502, 288, 635, 1386, 11, 635, 502, 288, 635, 1386, 11, 3006, 78, 13, 422, 4158, 11, 51300], "temperature": 0.0, "avg_logprob": -0.29191095535069295, "compression_ratio": 1.330935251798561, "no_speech_prob": 0.00047882660874165595}, {"id": 424, "seek": 321134, "start": 3230.06, "end": 3238.6600000000003, "text": " si reis la palabra, porque la otra no aplica, f\u00edjense que como valen 0 no pasa nada con la", "tokens": [51300, 1511, 319, 271, 635, 31702, 11, 4021, 635, 13623, 572, 25522, 2262, 11, 283, 870, 73, 1288, 631, 2617, 1323, 268, 1958, 572, 20260, 8096, 416, 635, 51730], "temperature": 0.0, "avg_logprob": -0.29191095535069295, "compression_ratio": 1.330935251798561, "no_speech_prob": 0.00047882660874165595}, {"id": 425, "seek": 323866, "start": 3238.8199999999997, "end": 3245.62, "text": " multiplicaci\u00f3n, porque yo estoy diciendo e al a eso, \u00bfno? No me molesta el 0 en este caso. Y este", "tokens": [50372, 17596, 3482, 11, 4021, 5290, 15796, 42797, 308, 419, 257, 7287, 11, 3841, 1771, 30, 883, 385, 8015, 7841, 806, 1958, 465, 4065, 9666, 13, 398, 4065, 50712], "temperature": 0.0, "avg_logprob": -0.28857633569738367, "compression_ratio": 1.470873786407767, "no_speech_prob": 0.015871426090598106}, {"id": 426, "seek": 323866, "start": 3245.62, "end": 3252.66, "text": " es el factor de normalizaci\u00f3n, es simplemente para que esto de 0,20 y 0,8. Sumo esto m\u00e1s esto,", "tokens": [50712, 785, 806, 5952, 368, 2710, 27603, 11, 785, 33190, 1690, 631, 7433, 368, 1958, 11, 2009, 288, 1958, 11, 23, 13, 8626, 78, 7433, 3573, 7433, 11, 51064], "temperature": 0.0, "avg_logprob": -0.28857633569738367, "compression_ratio": 1.470873786407767, "no_speech_prob": 0.015871426090598106}, {"id": 427, "seek": 323866, "start": 3252.66, "end": 3261.5, "text": " sumo todas y bueno, entonces yo busco la clase que m\u00e1s se inicia que en este caso es verbo, \u00bfde acuerdo?", "tokens": [51064, 2408, 78, 10906, 288, 11974, 11, 13003, 5290, 1255, 1291, 635, 44578, 631, 3573, 369, 294, 15341, 631, 465, 4065, 9666, 785, 1306, 1763, 11, 3841, 1479, 28113, 30, 51506], "temperature": 0.0, "avg_logprob": -0.28857633569738367, "compression_ratio": 1.470873786407767, "no_speech_prob": 0.015871426090598106}, {"id": 428, "seek": 326150, "start": 3262.34, "end": 3272.42, "text": " Bueno, esos son los modelos de entropia m\u00e1xima. Hay otros modelos discriminativos que lo voy a", "tokens": [50406, 16046, 11, 22411, 1872, 1750, 2316, 329, 368, 948, 1513, 654, 31031, 64, 13, 8721, 16422, 2316, 329, 20828, 36945, 631, 450, 7552, 257, 50910], "temperature": 0.0, "avg_logprob": -0.2243330784333058, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.025798793882131577}, {"id": 429, "seek": 326150, "start": 3272.42, "end": 3279.26, "text": " mencionar r\u00e1pidamente, porque en la forma de aplicarlos es la misma, lo \u00fanico que hay ac\u00e1 de", "tokens": [50910, 37030, 289, 18213, 49663, 11, 4021, 465, 635, 8366, 368, 18221, 39734, 785, 635, 24946, 11, 450, 26113, 631, 4842, 23496, 368, 51252], "temperature": 0.0, "avg_logprob": -0.2243330784333058, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.025798793882131577}, {"id": 430, "seek": 326150, "start": 3279.26, "end": 3284.98, "text": " diferente es que es diferente la forma de elegir clasificador, es decir, si ac\u00e1 lo hac\u00edamos por", "tokens": [51252, 20973, 785, 631, 785, 20973, 635, 8366, 368, 14459, 347, 596, 296, 1089, 5409, 11, 785, 10235, 11, 1511, 23496, 450, 46093, 16275, 1515, 51538], "temperature": 0.0, "avg_logprob": -0.2243330784333058, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.025798793882131577}, {"id": 431, "seek": 328498, "start": 3284.98, "end": 3290.82, "text": " regresi\u00f3n log\u00edstica, el support vector machines, que es un m\u00e9todo que se puso muy de moda en", "tokens": [50364, 47108, 2560, 3565, 19512, 2262, 11, 806, 1406, 8062, 8379, 11, 631, 785, 517, 20275, 17423, 631, 369, 280, 24431, 5323, 368, 1072, 64, 465, 50656], "temperature": 0.0, "avg_logprob": -0.18782951213695384, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.11648684740066528}, {"id": 432, "seek": 328498, "start": 3291.82, "end": 3301.94, "text": " principio de este siglo, en la d\u00e9cada pasada digamos, es un m\u00e9todo que lo que hace es buscar", "tokens": [50706, 34308, 368, 4065, 48578, 11, 465, 635, 9198, 1538, 1736, 1538, 36430, 11, 785, 517, 20275, 17423, 631, 450, 631, 10032, 785, 26170, 51212], "temperature": 0.0, "avg_logprob": -0.18782951213695384, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.11648684740066528}, {"id": 433, "seek": 328498, "start": 3301.94, "end": 3307.1, "text": " separar linealmente, pero en vez de hacerlo por m\u00ednimos cuadrados, lo que dice es buscar la recta", "tokens": [51212, 3128, 289, 1622, 304, 4082, 11, 4768, 465, 5715, 368, 32039, 1515, 33656, 8372, 34434, 40491, 11, 450, 631, 10313, 785, 26170, 635, 11048, 64, 51470], "temperature": 0.0, "avg_logprob": -0.18782951213695384, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.11648684740066528}, {"id": 434, "seek": 330710, "start": 3307.42, "end": 3316.22, "text": " que separa m\u00e1s, que queda m\u00e1s en el medio digamos, la intuici\u00f3n atr\u00e1s de support vector", "tokens": [50380, 631, 3128, 64, 3573, 11, 631, 23314, 3573, 465, 806, 22123, 36430, 11, 635, 560, 84, 15534, 22906, 368, 1406, 8062, 50820], "temperature": 0.0, "avg_logprob": -0.22146154585338773, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.08588137477636337}, {"id": 435, "seek": 330710, "start": 3316.22, "end": 3325.7799999999997, "text": " machines que yo busco, si yo tengo los ejemplos as\u00ed, tengo muchas rectas que pasan, \u00bfs\u00ed? Yo trato", "tokens": [50820, 8379, 631, 5290, 1255, 1291, 11, 1511, 5290, 13989, 1750, 10012, 5895, 329, 8582, 11, 13989, 16072, 11048, 296, 631, 1736, 282, 11, 3841, 82, 870, 30, 7616, 504, 2513, 51298], "temperature": 0.0, "avg_logprob": -0.22146154585338773, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.08588137477636337}, {"id": 436, "seek": 330710, "start": 3325.7799999999997, "end": 3333.3399999999997, "text": " de encontrar la que maximiza el margen de los que est\u00e1n m\u00e1s cerca y queda en el medio, \u00bfs\u00ed,", "tokens": [51298, 368, 17525, 635, 631, 5138, 13427, 806, 1849, 1766, 368, 1750, 631, 10368, 3573, 26770, 288, 23314, 465, 806, 22123, 11, 3841, 82, 870, 11, 51676], "temperature": 0.0, "avg_logprob": -0.22146154585338773, "compression_ratio": 1.4922279792746114, "no_speech_prob": 0.08588137477636337}, {"id": 437, "seek": 333334, "start": 3334.1000000000004, "end": 3337.58, "text": " por eso se llama, los support vectors son estos, son los que est\u00e1n m\u00e1s cerca,", "tokens": [50402, 1515, 7287, 369, 23272, 11, 1750, 1406, 18875, 1872, 12585, 11, 1872, 1750, 631, 10368, 3573, 26770, 11, 50576], "temperature": 0.0, "avg_logprob": -0.21950679355197483, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.00674009695649147}, {"id": 438, "seek": 333334, "start": 3338.58, "end": 3344.82, "text": " los dem\u00e1s, si se fijan, no importan para el clasificador. \u00bfCu\u00e1l es la hip\u00f3tesis del support", "tokens": [50626, 1750, 34682, 11, 1511, 369, 42001, 282, 11, 572, 974, 282, 1690, 806, 596, 296, 1089, 5409, 13, 3841, 35222, 11447, 785, 635, 8103, 812, 7269, 271, 1103, 1406, 50938], "temperature": 0.0, "avg_logprob": -0.21950679355197483, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.00674009695649147}, {"id": 439, "seek": 333334, "start": 3344.82, "end": 3351.58, "text": " vector machines y por qu\u00e9 son tan robustos y por qu\u00e9, como est\u00e1n justo en el medio? Quiero decir,", "tokens": [50938, 8062, 8379, 288, 1515, 8057, 1872, 7603, 13956, 329, 288, 1515, 8057, 11, 2617, 10368, 40534, 465, 806, 22123, 30, 2326, 12030, 10235, 11, 51276], "temperature": 0.0, "avg_logprob": -0.21950679355197483, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.00674009695649147}, {"id": 440, "seek": 333334, "start": 3351.58, "end": 3361.6200000000003, "text": " si yo meto uno que est\u00e1 ac\u00e1, si un ejemplo a clasificar queda muy cerquita del borde, me puedo", "tokens": [51276, 1511, 5290, 1131, 78, 8526, 631, 3192, 23496, 11, 1511, 517, 13358, 257, 596, 296, 25625, 23314, 5323, 10146, 358, 2786, 1103, 272, 15127, 11, 385, 21612, 51778], "temperature": 0.0, "avg_logprob": -0.21950679355197483, "compression_ratio": 1.6217391304347826, "no_speech_prob": 0.00674009695649147}, {"id": 441, "seek": 336162, "start": 3361.62, "end": 3367.94, "text": " equivocar, \u00bfse entiende? Es m\u00e1s probable que me est\u00e9 equivocando, en cambio yo le pongo en el", "tokens": [50364, 48726, 47993, 11, 3841, 405, 948, 45816, 30, 2313, 3573, 21759, 631, 385, 34584, 48726, 905, 1806, 11, 465, 28731, 5290, 476, 280, 25729, 465, 806, 50680], "temperature": 0.0, "avg_logprob": -0.2982373496358709, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.008491987362504005}, {"id": 442, "seek": 336162, "start": 3367.94, "end": 3373.68, "text": " medio y bueno, quedan bastante lejos digamos, y de hecho funcionan muy bien clasificando. Fueron", "tokens": [50680, 22123, 288, 11974, 11, 13617, 282, 14651, 476, 19136, 36430, 11, 288, 368, 13064, 14186, 282, 5323, 3610, 596, 296, 1089, 1806, 13, 479, 5486, 266, 50967], "temperature": 0.0, "avg_logprob": -0.2982373496358709, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.008491987362504005}, {"id": 443, "seek": 336162, "start": 3373.68, "end": 3377.7, "text": " toda una revoluci\u00f3n en la support vector machines, ahora como ahora est\u00e1n de moda la red neuronal", "tokens": [50967, 11687, 2002, 16908, 30813, 465, 635, 1406, 8062, 8379, 11, 9923, 2617, 9923, 10368, 368, 1072, 64, 635, 2182, 12087, 21523, 51168], "temperature": 0.0, "avg_logprob": -0.2982373496358709, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.008491987362504005}, {"id": 444, "seek": 336162, "start": 3377.7, "end": 3381.42, "text": " en la support vector machines, hicieron lo mismo a principios, agarraron, fueron los primeros", "tokens": [51168, 465, 635, 1406, 8062, 8379, 11, 23697, 14440, 450, 12461, 257, 6959, 2717, 11, 623, 2284, 6372, 11, 28739, 1750, 12595, 329, 51354], "temperature": 0.0, "avg_logprob": -0.2982373496358709, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.008491987362504005}, {"id": 445, "seek": 336162, "start": 3381.42, "end": 3386.3399999999997, "text": " m\u00e9todos de discriminativo clasificaci\u00f3n que empezaron a batir todos los r\u00e9cords digamos de", "tokens": [51354, 20275, 378, 329, 368, 20828, 18586, 596, 296, 40802, 631, 18730, 6372, 257, 7362, 347, 6321, 1750, 3960, 66, 5703, 36430, 368, 51600], "temperature": 0.0, "avg_logprob": -0.2982373496358709, "compression_ratio": 1.693661971830986, "no_speech_prob": 0.008491987362504005}, {"id": 446, "seek": 338634, "start": 3386.34, "end": 3394.3, "text": " diferentes tareas, hasta que pasaron de moda con el tema de las, si bien se usan mucho pasaron", "tokens": [50364, 17686, 49423, 296, 11, 10764, 631, 1736, 6372, 368, 1072, 64, 416, 806, 15854, 368, 2439, 11, 1511, 3610, 369, 505, 282, 9824, 1736, 6372, 50762], "temperature": 0.0, "avg_logprob": -0.1882715668789176, "compression_ratio": 1.676300578034682, "no_speech_prob": 0.034773219376802444}, {"id": 447, "seek": 338634, "start": 3394.3, "end": 3399.94, "text": " de moda con el tema de las red neuronales que volvieron a batirle los r\u00e9cords, pero esencialmente", "tokens": [50762, 368, 1072, 64, 416, 806, 15854, 368, 2439, 2182, 12087, 266, 4229, 631, 1996, 85, 14440, 257, 7362, 347, 306, 1750, 3960, 66, 5703, 11, 4768, 785, 26567, 4082, 51044], "temperature": 0.0, "avg_logprob": -0.1882715668789176, "compression_ratio": 1.676300578034682, "no_speech_prob": 0.034773219376802444}, {"id": 448, "seek": 338634, "start": 3399.94, "end": 3405.54, "text": " el m\u00e9todo c\u00f3mo se aplica es el mismo, as\u00ed la diferencia es como te\u00f3ricamente como se calcula", "tokens": [51044, 806, 20275, 17423, 12826, 369, 25522, 2262, 785, 806, 12461, 11, 8582, 635, 38844, 785, 2617, 535, 812, 1341, 3439, 2617, 369, 4322, 64, 51324], "temperature": 0.0, "avg_logprob": -0.1882715668789176, "compression_ratio": 1.676300578034682, "no_speech_prob": 0.034773219376802444}, {"id": 449, "seek": 340554, "start": 3405.54, "end": 3418.06, "text": " que est\u00e1 ah\u00ed, hay otros m\u00e9todos de clasificaci\u00f3n, hacen el aprendizaje autom\u00e1tico, los aprenden,", "tokens": [50364, 631, 3192, 12571, 11, 4842, 16422, 20275, 378, 329, 368, 596, 296, 40802, 11, 27434, 806, 21003, 590, 11153, 3553, 28234, 11, 1750, 21003, 268, 11, 50990], "temperature": 0.0, "avg_logprob": -0.3438801933737362, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.4408515393733978}, {"id": 450, "seek": 340554, "start": 3418.06, "end": 3425.7, "text": " vecinos m\u00e1s cercanos, los can\u00edres, que es, clasifico un documento buscando los que est\u00e1n m\u00e1s cerca", "tokens": [50990, 42021, 15220, 3573, 36099, 31035, 11, 1750, 393, 870, 495, 11, 631, 785, 11, 596, 296, 1089, 78, 517, 4166, 78, 46804, 1750, 631, 10368, 3573, 26770, 51372], "temperature": 0.0, "avg_logprob": -0.3438801933737362, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.4408515393733978}, {"id": 451, "seek": 340554, "start": 3425.7, "end": 3429.58, "text": " del punto de vista tribu, calculo una distancia entre documentos y me quedo con los que est\u00e1n m\u00e1s", "tokens": [51372, 1103, 14326, 368, 22553, 1376, 6021, 11, 4322, 78, 2002, 1483, 22862, 3962, 4166, 329, 288, 385, 13617, 78, 416, 1750, 631, 10368, 3573, 51566], "temperature": 0.0, "avg_logprob": -0.3438801933737362, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.4408515393733978}, {"id": 452, "seek": 342958, "start": 3429.74, "end": 3437.7, "text": " cercanos, \u00bfno? Es como la idea de, bueno si este est\u00e1 ac\u00e1, \u00bfqui\u00e9nes son los vecinos m\u00e1s cercanos?", "tokens": [50372, 36099, 31035, 11, 3841, 1771, 30, 2313, 2617, 635, 1558, 368, 11, 11974, 1511, 4065, 3192, 23496, 11, 3841, 358, 5770, 279, 1872, 1750, 42021, 15220, 3573, 36099, 31035, 30, 50770], "temperature": 0.0, "avg_logprob": -0.2875265121459961, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.18871548771858215}, {"id": 453, "seek": 342958, "start": 3437.7, "end": 3442.5, "text": " Y bueno, supongamos que los tres vecinos m\u00e1s cercanos son estos, en este caso los tres son", "tokens": [50770, 398, 11974, 11, 9331, 556, 2151, 631, 1750, 15890, 42021, 15220, 3573, 36099, 31035, 1872, 12585, 11, 465, 4065, 9666, 1750, 15890, 1872, 51010], "temperature": 0.0, "avg_logprob": -0.2875265121459961, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.18871548771858215}, {"id": 454, "seek": 342958, "start": 3442.5, "end": 3447.02, "text": " circulitos, o sea que eso seguramente sea un circulito, podemos tener problemas cuando estamos", "tokens": [51010, 12515, 11343, 11, 277, 4158, 631, 7287, 22179, 3439, 4158, 517, 12515, 3528, 11, 12234, 11640, 20720, 7767, 10382, 51236], "temperature": 0.0, "avg_logprob": -0.2875265121459961, "compression_ratio": 1.6384180790960452, "no_speech_prob": 0.18871548771858215}, {"id": 455, "seek": 344702, "start": 3448.02, "end": 3455.14, "text": " los m\u00e9todos de vecinos m\u00e1s cercanos tienen la ventaja, obviamente, de que pueden reconocer,", "tokens": [50414, 1750, 20275, 378, 329, 368, 42021, 15220, 3573, 36099, 31035, 12536, 635, 6931, 12908, 11, 36325, 11, 368, 631, 14714, 43838, 260, 11, 50770], "temperature": 0.0, "avg_logprob": -0.3810263547030362, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.04853381961584091}, {"id": 456, "seek": 344702, "start": 3460.9, "end": 3462.34, "text": " pueden reconocer cl\u00e1steres,", "tokens": [51058, 14714, 43838, 260, 596, 842, 3120, 279, 11, 51130], "temperature": 0.0, "avg_logprob": -0.3810263547030362, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.04853381961584091}, {"id": 457, "seek": 344702, "start": 3468.2599999999998, "end": 3471.38, "text": " los m\u00e9todos de vecinos m\u00e1s cercanos definen una cosa as\u00ed,", "tokens": [51426, 1750, 20275, 378, 329, 368, 42021, 15220, 3573, 36099, 31035, 1561, 268, 2002, 10163, 8582, 11, 51582], "temperature": 0.0, "avg_logprob": -0.3810263547030362, "compression_ratio": 1.6636363636363636, "no_speech_prob": 0.04853381961584091}, {"id": 458, "seek": 347138, "start": 3471.38, "end": 3477.1, "text": " no, pero,", "tokens": [50364, 572, 11, 4768, 11, 50650], "temperature": 0.0, "avg_logprob": -0.4161968571799142, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.009671038947999477}, {"id": 459, "seek": 347138, "start": 3486.3, "end": 3492.82, "text": " la cosa as\u00ed, pueden reconocer cosas que no son lineales, tienen el problema de que a veces", "tokens": [51110, 635, 10163, 8582, 11, 14714, 43838, 260, 12218, 631, 572, 1872, 1622, 4229, 11, 12536, 806, 12395, 368, 631, 257, 17054, 51436], "temperature": 0.0, "avg_logprob": -0.4161968571799142, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.009671038947999477}, {"id": 460, "seek": 347138, "start": 3492.82, "end": 3500.7400000000002, "text": " sobreajustan demasiado, \u00e1rboles de decisi\u00f3n que no son muy realizados en el procesamiento de", "tokens": [51436, 5473, 1805, 381, 282, 39820, 11, 35349, 65, 7456, 368, 18206, 2560, 631, 572, 1872, 5323, 22828, 4181, 465, 806, 17565, 16971, 368, 51832], "temperature": 0.0, "avg_logprob": -0.4161968571799142, "compression_ratio": 1.4202898550724639, "no_speech_prob": 0.009671038947999477}, {"id": 461, "seek": 350074, "start": 3500.74, "end": 3505.58, "text": " la imaginaci\u00f3n, rando fores que son como una, hay muchos, muchos m\u00e9todos de clasificaci\u00f3n,", "tokens": [50364, 635, 23427, 3482, 11, 367, 1806, 726, 495, 631, 1872, 2617, 2002, 11, 4842, 17061, 11, 17061, 20275, 378, 329, 368, 596, 296, 40802, 11, 50606], "temperature": 0.0, "avg_logprob": -0.41864216180495273, "compression_ratio": 1.68, "no_speech_prob": 0.0036410533357411623}, {"id": 462, "seek": 350074, "start": 3506.58, "end": 3515.4599999999996, "text": " pero en todos lo que tienen en com\u00fan es que las medidas para realizar, para el m\u00e9todolog\u00eda", "tokens": [50656, 4768, 465, 6321, 450, 631, 12536, 465, 45448, 785, 631, 2439, 37295, 1690, 36461, 11, 1690, 806, 20275, 378, 29987, 51100], "temperature": 0.0, "avg_logprob": -0.41864216180495273, "compression_ratio": 1.68, "no_speech_prob": 0.0036410533357411623}, {"id": 463, "seek": 350074, "start": 3515.4599999999996, "end": 3527.74, "text": " es la que hay en la clasificaci\u00f3n pasada. Y eso desde el punto de vista de los m\u00e9todos de clasificaci\u00f3n", "tokens": [51100, 785, 635, 631, 4842, 465, 635, 596, 296, 40802, 1736, 1538, 13, 398, 7287, 10188, 806, 14326, 368, 22553, 368, 1750, 20275, 378, 329, 368, 596, 296, 40802, 51714], "temperature": 0.0, "avg_logprob": -0.41864216180495273, "compression_ratio": 1.68, "no_speech_prob": 0.0036410533357411623}, {"id": 464, "seek": 352774, "start": 3527.74, "end": 3534.14, "text": " puros, pero tambi\u00e9n se acuerdan que hab\u00edamos visto los m\u00e9todos de clasificaci\u00f3n secuencial,", "tokens": [50364, 1864, 329, 11, 4768, 6407, 369, 696, 5486, 10312, 631, 3025, 16275, 17558, 1750, 20275, 378, 329, 368, 596, 296, 40802, 907, 7801, 1013, 11, 50684], "temperature": 0.0, "avg_logprob": -0.19238573260011926, "compression_ratio": 1.8502415458937198, "no_speech_prob": 0.02860596962273121}, {"id": 465, "seek": 352774, "start": 3534.14, "end": 3541.7799999999997, "text": " cuando yo quiero asignar una secuencia de tangs, de clases, asumo que mi atributo tiene una secuencia,", "tokens": [50684, 7767, 5290, 16811, 382, 788, 289, 2002, 907, 47377, 368, 10266, 82, 11, 368, 596, 1957, 11, 382, 40904, 631, 2752, 412, 2024, 8262, 7066, 2002, 907, 47377, 11, 51066], "temperature": 0.0, "avg_logprob": -0.19238573260011926, "compression_ratio": 1.8502415458937198, "no_speech_prob": 0.02860596962273121}, {"id": 466, "seek": 352774, "start": 3541.7799999999997, "end": 3548.2999999999997, "text": " mi instancia es una secuencia, por ejemplo, una oraci\u00f3n, que es una secuencia de palabra,", "tokens": [51066, 2752, 1058, 22862, 785, 2002, 907, 47377, 11, 1515, 13358, 11, 2002, 420, 3482, 11, 631, 785, 2002, 907, 47377, 368, 31702, 11, 51392], "temperature": 0.0, "avg_logprob": -0.19238573260011926, "compression_ratio": 1.8502415458937198, "no_speech_prob": 0.02860596962273121}, {"id": 467, "seek": 352774, "start": 3548.2999999999997, "end": 3555.2999999999997, "text": " y quiero asignar una secuencia de tangs, bueno, hay versiones generativas, en el caso de los,", "tokens": [51392, 288, 16811, 382, 788, 289, 2002, 907, 47377, 368, 10266, 82, 11, 11974, 11, 4842, 3037, 279, 1337, 35725, 11, 465, 806, 9666, 368, 1750, 11, 51742], "temperature": 0.0, "avg_logprob": -0.19238573260011926, "compression_ratio": 1.8502415458937198, "no_speech_prob": 0.02860596962273121}, {"id": 468, "seek": 355530, "start": 3555.86, "end": 3561.6200000000003, "text": " la versi\u00f3n generativa de Naive Bayes son los hidden Marco Models, que lo vimos bastante en detalle", "tokens": [50392, 635, 47248, 1337, 18740, 368, 6056, 488, 7840, 279, 1872, 1750, 7633, 26535, 6583, 1625, 11, 631, 450, 49266, 14651, 465, 1141, 11780, 50680], "temperature": 0.0, "avg_logprob": -0.3613062540690104, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.0035552429035305977}, {"id": 469, "seek": 355530, "start": 3563.3, "end": 3574.42, "text": " en alguna clase anterior, y hay una versi\u00f3n tambi\u00e9n de clasificadores secuenciales,", "tokens": [50764, 465, 20651, 44578, 22272, 11, 288, 4842, 2002, 47248, 6407, 368, 596, 296, 1089, 11856, 907, 7801, 1013, 279, 11, 51320], "temperature": 0.0, "avg_logprob": -0.3613062540690104, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.0035552429035305977}, {"id": 470, "seek": 355530, "start": 3574.42, "end": 3580.9, "text": " que estos son por lejos los que bandan mejor, que son los temas secuenciales, que son los", "tokens": [51320, 631, 12585, 1872, 1515, 476, 19136, 1750, 631, 4116, 282, 11479, 11, 631, 1872, 1750, 40284, 907, 7801, 1013, 279, 11, 631, 1872, 1750, 51644], "temperature": 0.0, "avg_logprob": -0.3613062540690104, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.0035552429035305977}, {"id": 471, "seek": 358090, "start": 3581.1, "end": 3588.7000000000003, "text": " conditions al random fields, los conditions al random field tambi\u00e9n fueron una novedad en los temas", "tokens": [50374, 4487, 419, 4974, 7909, 11, 1750, 4487, 419, 4974, 2519, 6407, 28739, 2002, 572, 937, 345, 465, 1750, 40284, 50754], "temperature": 0.8, "avg_logprob": -0.5071884978051279, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.028847558423876762}, {"id": 472, "seek": 358090, "start": 3588.7000000000003, "end": 3592.6600000000003, "text": " de clasificaci\u00f3n secuencial, porque andan mucho mejor el general que los hidden Marco Models,", "tokens": [50754, 368, 596, 296, 40802, 907, 7801, 1013, 11, 4021, 293, 282, 9824, 11479, 806, 2674, 631, 1750, 7633, 26535, 6583, 1625, 11, 50952], "temperature": 0.8, "avg_logprob": -0.5071884978051279, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.028847558423876762}, {"id": 473, "seek": 358090, "start": 3592.6600000000003, "end": 3599.42, "text": " y tienen una, son como una versi\u00f3n, una versi\u00f3n secuencial del modelo entropi\u00e9s m\u00e1ximo,", "tokens": [50952, 288, 12536, 2002, 11, 1872, 2617, 2002, 47248, 11, 2002, 47248, 907, 7801, 1013, 1103, 27825, 948, 1513, 72, 2191, 38876, 11, 51290], "temperature": 0.8, "avg_logprob": -0.5071884978051279, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.028847558423876762}, {"id": 474, "seek": 358090, "start": 3601.86, "end": 3607.06, "text": " no, no, no, no esperen que entren detalle, tampoco conozco mucho la detalle, el matem\u00e1tico del", "tokens": [51412, 572, 11, 572, 11, 572, 11, 572, 10045, 268, 631, 948, 1095, 1141, 11780, 11, 36838, 416, 15151, 1291, 9824, 635, 1141, 11780, 11, 806, 3803, 443, 28234, 1103, 51672], "temperature": 0.8, "avg_logprob": -0.5071884978051279, "compression_ratio": 1.6946902654867257, "no_speech_prob": 0.028847558423876762}, {"id": 475, "seek": 360706, "start": 3607.06, "end": 3613.7, "text": " del conditional random fields, pero como herramienta digamos para el clasificaci\u00f3n de secuencias", "tokens": [50364, 1103, 27708, 4974, 7909, 11, 4768, 2617, 38271, 64, 36430, 1690, 806, 596, 296, 40802, 368, 907, 7801, 12046, 50696], "temperature": 0.0, "avg_logprob": -0.42290287017822265, "compression_ratio": 1.3055555555555556, "no_speech_prob": 0.38494041562080383}, {"id": 476, "seek": 360706, "start": 3613.7, "end": 3625.36, "text": " funciona muy bien. Yo dir\u00eda que si uno va a, a ver si me queda algo m\u00e1s, no, ac\u00e1 tienen", "tokens": [50696, 26210, 5323, 3610, 13, 7616, 4746, 2686, 631, 1511, 8526, 2773, 257, 11, 257, 1306, 1511, 385, 23314, 8655, 3573, 11, 572, 11, 23496, 12536, 51279], "temperature": 0.0, "avg_logprob": -0.42290287017822265, "compression_ratio": 1.3055555555555556, "no_speech_prob": 0.38494041562080383}, {"id": 477, "seek": 362536, "start": 3625.36, "end": 3636.32, "text": " un poco de show jugar, s\u00ed, de estas cosas. Hueca, sirve para jugar, pero es juguete en general.", "tokens": [50364, 517, 10639, 368, 855, 37692, 11, 8600, 11, 368, 13897, 12218, 13, 40015, 496, 11, 4735, 303, 1690, 37692, 11, 4768, 785, 9568, 84, 3498, 465, 2674, 13, 50912], "temperature": 0.0, "avg_logprob": -0.3804448744829963, "compression_ratio": 1.484375, "no_speech_prob": 0.5293262600898743}, {"id": 478, "seek": 362536, "start": 3636.32, "end": 3642.2000000000003, "text": " Salkill Learnes es una herramienta bastante, una librer\u00eda bastante polenta de, en Python y", "tokens": [50912, 318, 667, 373, 17216, 279, 785, 2002, 38271, 64, 14651, 11, 2002, 4939, 260, 2686, 14651, 1180, 8938, 368, 11, 465, 15329, 288, 51206], "temperature": 0.0, "avg_logprob": -0.3804448744829963, "compression_ratio": 1.484375, "no_speech_prob": 0.5293262600898743}, {"id": 479, "seek": 362536, "start": 3642.2000000000003, "end": 3652.2400000000002, "text": " que est\u00e1 bastante de moda. Y ac\u00e1 me faltan, me faltan todas las nuevas bolas de bibliotecas de", "tokens": [51206, 631, 3192, 14651, 368, 1072, 64, 13, 398, 23496, 385, 37108, 282, 11, 385, 37108, 282, 10906, 2439, 42817, 8986, 296, 368, 34344, 1370, 16369, 368, 51708], "temperature": 0.0, "avg_logprob": -0.3804448744829963, "compression_ratio": 1.484375, "no_speech_prob": 0.5293262600898743}, {"id": 480, "seek": 365224, "start": 3653.2, "end": 3658.9599999999996, "text": " Dib Learning, \u00bfno?, de que son de Red Lunar y que son Torch, este, Teano, Keras,", "tokens": [50412, 413, 897, 15205, 11, 3841, 1771, 22753, 368, 631, 1872, 368, 4477, 32077, 289, 288, 631, 1872, 7160, 339, 11, 4065, 11, 1989, 3730, 11, 591, 6985, 11, 50700], "temperature": 0.0, "avg_logprob": -0.47975880759102957, "compression_ratio": 1.3564356435643565, "no_speech_prob": 0.02953983284533024}, {"id": 481, "seek": 365224, "start": 3662.56, "end": 3670.6, "text": " TensorFlow. Pero bueno, Salkill Learnes es una biblioteca de, de gen\u00e9rica, de Machine Learning", "tokens": [50880, 37624, 13, 9377, 11974, 11, 318, 667, 373, 17216, 279, 785, 2002, 34344, 1370, 496, 368, 11, 368, 1049, 32716, 11, 368, 22155, 15205, 51282], "temperature": 0.0, "avg_logprob": -0.47975880759102957, "compression_ratio": 1.3564356435643565, "no_speech_prob": 0.02953983284533024}, {"id": 482, "seek": 365224, "start": 3670.6, "end": 3679.3599999999997, "text": " en Python, Orange tambi\u00e9n. En Iletek\u00e1 es m\u00e1s de procedimiento en lenguaje natural, pero tiene", "tokens": [51282, 465, 15329, 11, 17106, 6407, 13, 2193, 286, 2631, 916, 842, 785, 3573, 368, 6682, 14007, 465, 35044, 84, 11153, 3303, 11, 4768, 7066, 51720], "temperature": 0.0, "avg_logprob": -0.47975880759102957, "compression_ratio": 1.3564356435643565, "no_speech_prob": 0.02953983284533024}, {"id": 483, "seek": 367936, "start": 3679.36, "end": 3686.32, "text": " por ejemplo un plazificador exceciano. CRF m\u00e1s m\u00e1s es un Toolkit para Condition Random Fields.", "tokens": [50364, 1515, 13358, 517, 499, 921, 1089, 5409, 454, 384, 537, 3730, 13, 14123, 37, 3573, 3573, 785, 517, 15934, 22681, 1690, 21793, 849, 37603, 48190, 13, 50712], "temperature": 0.0, "avg_logprob": -0.5639033998761859, "compression_ratio": 1.3367875647668395, "no_speech_prob": 0.0090631740167737}, {"id": 484, "seek": 367936, "start": 3688.32, "end": 3695.32, "text": " PyBrain, creo que no, no, no corre m\u00e1s o no s\u00e9, que es Red Neuronal y Homebiteon.", "tokens": [50812, 9953, 33, 7146, 11, 14336, 631, 572, 11, 572, 11, 572, 29731, 3573, 277, 572, 7910, 11, 631, 785, 4477, 1734, 374, 266, 304, 288, 8719, 65, 642, 266, 13, 51162], "temperature": 0.0, "avg_logprob": -0.5639033998761859, "compression_ratio": 1.3367875647668395, "no_speech_prob": 0.0090631740167737}, {"id": 485, "seek": 367936, "start": 3696.52, "end": 3701.96, "text": " SMelite era la herramienta de Support Vector Machines, cuando estaba en moda.", "tokens": [51222, 13115, 338, 642, 4249, 635, 38271, 64, 368, 18073, 691, 20814, 12089, 1652, 11, 7767, 17544, 465, 1072, 64, 13, 51494], "temperature": 0.0, "avg_logprob": -0.5639033998761859, "compression_ratio": 1.3367875647668395, "no_speech_prob": 0.0090631740167737}, {"id": 486, "seek": 370196, "start": 3702.96, "end": 3710.08, "text": " SMelite es el 99 para que se dieron una idea y estaba bastante estable porque no hay mucho para,", "tokens": [50414, 13115, 338, 642, 785, 806, 11803, 1690, 631, 369, 274, 14440, 2002, 1558, 288, 17544, 14651, 37444, 4021, 572, 4842, 9824, 1690, 11, 50770], "temperature": 0.0, "avg_logprob": -0.3646659396943592, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.010560566559433937}, {"id": 487, "seek": 370196, "start": 3710.08, "end": 3714.76, "text": " es muy sencillo el modelo de la Support Vector Machines, por lo grande como me lo aplico.", "tokens": [50770, 785, 5323, 46749, 78, 806, 27825, 368, 635, 18073, 691, 20814, 12089, 1652, 11, 1515, 450, 8883, 2617, 385, 450, 25522, 2789, 13, 51004], "temperature": 0.0, "avg_logprob": -0.3646659396943592, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.010560566559433937}, {"id": 488, "seek": 370196, "start": 3718.96, "end": 3728.84, "text": " Yo dir\u00eda que, que si, si, si vamos a lo que, a lo que es el procedimiento en lenguaje natural a nivel", "tokens": [51214, 7616, 4746, 2686, 631, 11, 631, 1511, 11, 1511, 11, 1511, 5295, 257, 450, 631, 11, 257, 450, 631, 785, 806, 6682, 14007, 465, 35044, 84, 11153, 3303, 257, 24423, 51708], "temperature": 0.0, "avg_logprob": -0.3646659396943592, "compression_ratio": 1.4820512820512821, "no_speech_prob": 0.010560566559433937}, {"id": 489, "seek": 372884, "start": 3728.84, "end": 3739.6000000000004, "text": " de, a nivel de, como decir, de mercado o de herramienta o de, no me sabe la palabra, de industria,", "tokens": [50364, 368, 11, 257, 24423, 368, 11, 2617, 10235, 11, 368, 24775, 277, 368, 38271, 64, 277, 368, 11, 572, 385, 12275, 635, 31702, 11, 368, 2735, 4668, 11, 50902], "temperature": 0.0, "avg_logprob": -0.22984702412675068, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002373912837356329}, {"id": 490, "seek": 372884, "start": 3739.6000000000004, "end": 3749.04, "text": " digamos, a nivel industrial, no es la palabra correcta pero est\u00e1. Yo dir\u00eda que el procedimiento en", "tokens": [50902, 36430, 11, 257, 24423, 9987, 11, 572, 785, 635, 31702, 3006, 64, 4768, 3192, 13, 7616, 4746, 2686, 631, 806, 6682, 14007, 465, 51374], "temperature": 0.0, "avg_logprob": -0.22984702412675068, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002373912837356329}, {"id": 491, "seek": 372884, "start": 3749.04, "end": 3755.8, "text": " lenguaje natural est\u00e1 en lo que hemos aprendido hasta ahora. No? Es decir, todas estas cosas que", "tokens": [51374, 35044, 84, 11153, 3303, 3192, 465, 450, 631, 15396, 21003, 2925, 10764, 9923, 13, 883, 30, 2313, 10235, 11, 10906, 13897, 12218, 631, 51712], "temperature": 0.0, "avg_logprob": -0.22984702412675068, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002373912837356329}, {"id": 492, "seek": 375580, "start": 3755.8, "end": 3760.96, "text": " hemos aprendido en las clases hasta ahora ya se encuentra a nivel industrial. A nivel industrial", "tokens": [50364, 15396, 21003, 2925, 465, 2439, 596, 1957, 10764, 9923, 2478, 369, 43274, 257, 24423, 9987, 13, 316, 24423, 9987, 50622], "temperature": 0.0, "avg_logprob": -0.27611351013183594, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.016766473650932312}, {"id": 493, "seek": 375580, "start": 3760.96, "end": 3770.2400000000002, "text": " estoy hablando de las compa\u00f1\u00edas de Intermed, no? Reconocimiento de, en, reconocimiento de", "tokens": [50622, 15796, 29369, 368, 2439, 29953, 10025, 368, 5751, 1912, 11, 572, 30, 1300, 1671, 905, 14007, 368, 11, 465, 11, 43838, 14007, 368, 51086], "temperature": 0.0, "avg_logprob": -0.27611351013183594, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.016766473650932312}, {"id": 494, "seek": 375580, "start": 3770.2400000000002, "end": 3777.92, "text": " caracteres mal escrito, clasificaci\u00f3n, clasificaci\u00f3n de, sentimenta an\u00e1lisis, de todo lo que hemos", "tokens": [51086, 28760, 279, 2806, 49451, 11, 596, 296, 40802, 11, 596, 296, 40802, 368, 11, 2279, 2328, 64, 44113, 28436, 11, 368, 5149, 450, 631, 15396, 51470], "temperature": 0.0, "avg_logprob": -0.27611351013183594, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.016766473650932312}, {"id": 495, "seek": 375580, "start": 3777.92, "end": 3783.6400000000003, "text": " hablado hasta ahora, no? En el grama y todas esas cosas. Tambi\u00e9n, a ver, no es lo \u00fanico, no?", "tokens": [51470, 26280, 1573, 10764, 9923, 11, 572, 30, 2193, 806, 677, 2404, 288, 10906, 23388, 12218, 13, 25682, 11, 257, 1306, 11, 572, 785, 450, 26113, 11, 572, 30, 51756], "temperature": 0.0, "avg_logprob": -0.27611351013183594, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.016766473650932312}, {"id": 496, "seek": 378364, "start": 3783.64, "end": 3793.48, "text": " Machine Translation es un ejemplo de cosas que andan muy bien. Este, pero utilizan m\u00e9todos m\u00e1s", "tokens": [50364, 22155, 6531, 24278, 785, 517, 13358, 368, 12218, 631, 293, 282, 5323, 3610, 13, 16105, 11, 4768, 19906, 282, 20275, 378, 329, 3573, 50856], "temperature": 0.0, "avg_logprob": -0.20411836946165407, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.009479292668402195}, {"id": 497, "seek": 378364, "start": 3793.48, "end": 3798.52, "text": " o menos hasta ac\u00e1. Lo que quiero decir es que, y bueno, y ah\u00ed hay alg\u00fan componente sem\u00e1ntico", "tokens": [50856, 277, 8902, 10764, 23496, 13, 6130, 631, 16811, 10235, 785, 631, 11, 288, 11974, 11, 288, 12571, 4842, 26300, 4026, 1576, 4361, 27525, 2789, 51108], "temperature": 0.0, "avg_logprob": -0.20411836946165407, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.009479292668402195}, {"id": 498, "seek": 378364, "start": 3798.52, "end": 3805.04, "text": " tambi\u00e9n que lo van a ver despu\u00e9s con Luis, pero esas cosas m\u00e1s avanzadas, digamos, reci\u00e9n,", "tokens": [51108, 6407, 631, 450, 3161, 257, 1306, 15283, 416, 25133, 11, 4768, 23388, 12218, 3573, 42444, 6872, 11, 36430, 11, 4214, 3516, 11, 51434], "temperature": 0.0, "avg_logprob": -0.20411836946165407, "compression_ratio": 1.411764705882353, "no_speech_prob": 0.009479292668402195}, {"id": 499, "seek": 380504, "start": 3805.04, "end": 3810.32, "text": " reci\u00e9n se est\u00e1 empezando a hablar, pero en algunas cosas de, por ejemplo,", "tokens": [50364, 4214, 3516, 369, 3192, 18730, 1806, 257, 21014, 11, 4768, 465, 27316, 12218, 368, 11, 1515, 13358, 11, 50628], "temperature": 0.0, "avg_logprob": -0.2576821770998511, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.486572802066803}, {"id": 500, "seek": 380504, "start": 3813.2, "end": 3819.64, "text": " reconocimiento de entidades, no? El otro d\u00eda lo ve\u00eda en un diario, digamos, unos dos a\u00f1os atr\u00e1s,", "tokens": [50772, 43838, 14007, 368, 948, 10284, 11, 572, 30, 2699, 11921, 12271, 450, 1241, 2686, 465, 517, 1026, 4912, 11, 36430, 11, 17780, 4491, 11424, 22906, 11, 51094], "temperature": 0.0, "avg_logprob": -0.2576821770998511, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.486572802066803}, {"id": 501, "seek": 380504, "start": 3819.64, "end": 3825.8, "text": " que algo que dec\u00eda que era una aplicaci\u00f3n que reconoc\u00eda a partir del New York Times lugares,", "tokens": [51094, 631, 8655, 631, 37599, 631, 4249, 2002, 18221, 3482, 631, 43838, 2686, 257, 13906, 1103, 1873, 3609, 11366, 33105, 11, 51402], "temperature": 0.0, "avg_logprob": -0.2576821770998511, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.486572802066803}, {"id": 502, "seek": 380504, "start": 3825.8, "end": 3832.68, "text": " bueno, Google lo hace, no? Lugares y cuando arma las citas, cuando a partir de un correo te lo", "tokens": [51402, 11974, 11, 3329, 450, 10032, 11, 572, 30, 441, 697, 8643, 288, 7767, 46422, 2439, 4814, 296, 11, 7767, 257, 13906, 368, 517, 29731, 78, 535, 450, 51746], "temperature": 0.0, "avg_logprob": -0.2576821770998511, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.486572802066803}, {"id": 503, "seek": 383268, "start": 3832.68, "end": 3837.52, "text": " meten en el calendario, ah\u00ed lo que est\u00e1 haciendo es reconocimiento de entidades. Est\u00e1", "tokens": [50364, 1131, 268, 465, 806, 37022, 4912, 11, 12571, 450, 631, 3192, 20509, 785, 43838, 14007, 368, 948, 10284, 13, 27304, 50606], "temperature": 0.0, "avg_logprob": -0.2868557288998463, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.026458632200956345}, {"id": 504, "seek": 383268, "start": 3837.52, "end": 3842.12, "text": " reconociendo que dice el jueves 23, cena con tal y lo est\u00e1 viendo, est\u00e1 haciendo clasificaci\u00f3n", "tokens": [50606, 850, 8957, 16830, 631, 10313, 806, 27833, 977, 6673, 11, 41777, 416, 4023, 288, 450, 3192, 34506, 11, 3192, 20509, 596, 296, 40802, 50836], "temperature": 0.0, "avg_logprob": -0.2868557288998463, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.026458632200956345}, {"id": 505, "seek": 383268, "start": 3842.12, "end": 3847.64, "text": " secuencial, pero eso son cosas que en el academia est\u00e1n como hace como 10 a\u00f1os, digamos, los", "tokens": [50836, 907, 7801, 1013, 11, 4768, 7287, 1872, 12218, 631, 465, 806, 28937, 10368, 2617, 10032, 2617, 1266, 11424, 11, 36430, 11, 1750, 51112], "temperature": 0.0, "avg_logprob": -0.2868557288998463, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.026458632200956345}, {"id": 506, "seek": 383268, "start": 3847.64, "end": 3851.3999999999996, "text": " condillos hablando de FIT tienen como 10 a\u00f1os, reci\u00e9n est\u00e1n empezando como entrares, el tipo", "tokens": [51112, 2224, 47476, 29369, 368, 479, 3927, 12536, 2617, 1266, 11424, 11, 4214, 3516, 10368, 18730, 1806, 2617, 22284, 495, 11, 806, 9746, 51300], "temperature": 0.0, "avg_logprob": -0.2868557288998463, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.026458632200956345}, {"id": 507, "seek": 383268, "start": 3851.3999999999996, "end": 3857.04, "text": " costo. Y hay cosas que todav\u00eda est\u00e1 por verse c\u00f3mo se van a incorporar, que son las que vamos", "tokens": [51300, 2063, 78, 13, 398, 4842, 12218, 631, 28388, 3192, 1515, 7996, 12826, 369, 3161, 257, 8788, 289, 11, 631, 1872, 2439, 631, 5295, 51582], "temperature": 0.0, "avg_logprob": -0.2868557288998463, "compression_ratio": 1.7173913043478262, "no_speech_prob": 0.026458632200956345}, {"id": 508, "seek": 385704, "start": 3857.12, "end": 3864.4, "text": " a ver de ahora en adelante, que son el parsing, o sea, an\u00e1lisis m\u00e1s complejo, ni que hablar de", "tokens": [50368, 257, 1306, 368, 9923, 465, 40214, 11, 631, 1872, 806, 21156, 278, 11, 277, 4158, 11, 44113, 28436, 3573, 44424, 5134, 11, 3867, 631, 21014, 368, 50732], "temperature": 0.0, "avg_logprob": -0.2375932768279431, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.009509934112429619}, {"id": 509, "seek": 385704, "start": 3864.4, "end": 3868.84, "text": " an\u00e1lisis sem\u00e1ntico m\u00e1s all\u00e1 de la sem\u00e1ntica de palabras, son cosas que vamos a ir viendo despu\u00e9s,", "tokens": [50732, 44113, 28436, 4361, 27525, 2789, 3573, 30642, 368, 635, 4361, 27525, 2262, 368, 35240, 11, 1872, 12218, 631, 5295, 257, 3418, 34506, 15283, 11, 50954], "temperature": 0.0, "avg_logprob": -0.2375932768279431, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.009509934112429619}, {"id": 510, "seek": 385704, "start": 3870.84, "end": 3878.48, "text": " o sea, hay mucho todav\u00eda para mejorar y en el academia tambi\u00e9n, porque no est\u00e1 en todo,", "tokens": [51054, 277, 4158, 11, 4842, 9824, 28388, 1690, 48858, 288, 465, 806, 28937, 6407, 11, 4021, 572, 3192, 465, 5149, 11, 51436], "temperature": 0.0, "avg_logprob": -0.2375932768279431, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.009509934112429619}, {"id": 511, "seek": 385704, "start": 3878.48, "end": 3885.24, "text": " resuelto ni mucho menos. Por ejemplo, en el poder analizar sem\u00e1nticamente las cosas,", "tokens": [51436, 725, 3483, 1353, 3867, 9824, 8902, 13, 5269, 13358, 11, 465, 806, 8152, 2624, 9736, 4361, 842, 580, 23653, 2439, 12218, 11, 51774], "temperature": 0.0, "avg_logprob": -0.2375932768279431, "compression_ratio": 1.6462882096069869, "no_speech_prob": 0.009509934112429619}, {"id": 512, "seek": 388524, "start": 3885.3199999999997, "end": 3893.3999999999996, "text": " estamos bastante lejos. Pero lo que quer\u00eda transmitir es que esto de la clasificaci\u00f3n es", "tokens": [50368, 10382, 14651, 476, 19136, 13, 9377, 450, 631, 37869, 17831, 347, 785, 631, 7433, 368, 635, 596, 296, 40802, 785, 50772], "temperature": 0.0, "avg_logprob": -0.267776542239719, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0017595585668459535}, {"id": 513, "seek": 388524, "start": 3893.3999999999996, "end": 3898.6, "text": " lo m\u00e1s que anda en la vuelta, digamos, \u00bfno? Y que con esto se va a hacer un mont\u00f3n de cosas.", "tokens": [50772, 450, 3573, 631, 21851, 465, 635, 41542, 11, 36430, 11, 3841, 1771, 30, 398, 631, 416, 7433, 369, 2773, 257, 6720, 517, 45259, 368, 12218, 13, 51032], "temperature": 0.0, "avg_logprob": -0.267776542239719, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0017595585668459535}, {"id": 514, "seek": 388524, "start": 3899.72, "end": 3906.2, "text": " Bueno, clases que vienen arrancamos con parci, \u00bfs\u00ed? Gracias.", "tokens": [51088, 16046, 11, 596, 1957, 631, 49298, 50235, 66, 2151, 416, 971, 537, 11, 3841, 82, 870, 30, 26909, 13, 51412], "temperature": 0.0, "avg_logprob": -0.267776542239719, "compression_ratio": 1.4067796610169492, "no_speech_prob": 0.0017595585668459535}], "language": "es"}