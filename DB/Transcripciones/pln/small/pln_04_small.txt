Bueno, primero vamos a hablar de lenguajes regulares. Ustedes hicieron el curso de teoría
de lenguajes y este año generalmente damos una introducción más teórica a lenguajes
regulares. Este año decidí cortar esa parte porque en realidad ustedes ya sufrieron bastante
a Prada y además y compañías en todos esos temas de lenguajes regulares y sus propiedades
teóricas y propiedades de clausura y sus diferentes modelos. Hoy vamos a dar solo un repaso acá para
nivelar un poco porque son del tipo de métodos que forman la base de otra cantidad y que para
algunas tareas aplican directamente. Y arranco con esta frase porque como yo decía creo en la primera
en la primera clase Chonky por allá por 1957 mostró que el inglés no era pues no era
susceptible de ser representado con automatas finitos y eso hizo que la investigación en este
tipo de automatas en el procedimiento de lenguajes natural se tuviera como oportunitas años digamos
y después volvieron. Ahora vamos a hablar un poquito más de eso pero ¿qué son? ¿qué son los
lenguajes regulares? Me queda el elegísimo el teclado de acá. ¿Por qué nos interesa los lenguajes
regulares? Porque esencialmente la visto como herramienta para el procedimiento de lenguajes
natural son una herramienta para especificar texto mediante patrones. Es decir yo quiero
especificar un conjunto de textos con una expresión sola y generalmente puede utilizar
expresiones regulares. Si yo quiero expresar todas las palabras que empiezan con C tengo que
expresar de alguna forma algo como le voy a poner en P.C punto asterisco. Es una C seguida de cualquier
cosa cualquier cantidad de veces. Ahora vamos a ver un poco más de eso pero esencialmente es una
forma de expresar muchas y quiero encontrar en un texto todas las palabras que aparecen que empiezan
con mayúscula y terminan con A sí voy a poner una cosa como H con H mayúscula que dice cualquier
cosa terminan con A. ¿De acuerdo? Ya me aparecen algunas cosas como bueno pero acá tiene que haber
no haber espacios en el medio por ejemplo ¿no? Pero y toda una serie de discusiones pero lo importante
es que yo puedo expresar conjuntos un patrón a través de una expresión. Un conjunto de texto
a partir de un patrón. Tienen expresividad limitada ¿se acuerdan de la jerarquía Chonki? No, es decir
nosotros tenemos los los lenguajes regulares, los lenguajes libres de contexto y los lenguajes
recursivamente enumerables por acá afuera. Los lenguajes regulares son solo un subconjunto
de los lenguajes pero antes ¿qué son los lenguajes? ¿qué son los lenguajes formales? ¿se acuerdan
lo que eran los lenguajes formales? ¿a quién se acuerda? Yo tengo un alfabeto finito de símbolos
¿sí? Defino tiras de símbolos de acuerdo y los lenguajes son un subconjunto de las tiras
que yo puedo formar sobre ese alfabeto. Solamente eso, es un conjunto de tiras, conjunto de palabras.
Los lenguajes regulares solo hay uno de los lenguajes son regulares en tanto pueden ser
expresados de la forma que lo vamos a ver ahora con este tipo de expresiones regulares. O sea,
no puedo expresar cualquier cosa con lenguajes regulares. Por ejemplo, el ejemplo de manual es
que yo no puedo expresar cosas de la forma igual cantidad de as que debes. ¿sí? Esto no lo puedo
expresar con lenguajes regulares. Miran con cara, esto lo debían saberlo. No puedo expresar cosas
como ésta. Una tía y su reverso. Eso no lo puedo expresar con expresiones regulares. Tiene
presión limitada, pero a cambio de eso son muy, muy, muy, muy eficientes. Es decir,
típicamente cuando yo quiero hacer algo, hacer una búsqueda eficiente de algo en un texto,
si lo logro expresar esa búsqueda con una expresión regular sé que lo voy a encontrar
de una forma muy eficiente. Si lo vuelvo determinístico, incluso de orden,
en el largo de la palabra que te buscan. ¿te acuerdo? Y son fácilmente adaptables para
buscar un patrón en un corpus. Si yo tengo que buscar un texto como en Unix, con Grep,
o en un editor de texto cuando controlo de buscar o lo que sea. Si yo lo puedo
especificar con un patrón, generalmente nosotros ponemos una palabra que de última es un patrón
sencillo que solamente expresa una suntira. Pero yo podría, casi todos los editores
profesionales permiten buscar expresiones regulares. Especificar esos patrones para
buscar. Y casi todos los lenguajes, de hecho creo que todos los lenguajes de programación
hoy en día tienen forma de especificar expresiones regulares. Por ejemplo, si yo tengo un problema
de programación y quiero decir, bueno, si lo que encuentro es una url y quiero expresar
una url de alguna forma, lo expreso con una expresión regular. No voy a escribir toda
la url del mundo o que me pueden aparecer. ¿te entiendes? Es decir, yo estoy especificando
conjuntos. Y esas expresiones regulares son fácilmente adaptables para buscar un corpus.
Ahora vamos a ver lo que es un corpus devolviendo todas las ocurrencias del patrón. Un corpus
para el pensamiento del lenguaje natural es una cosa muy utilizada y es nuestra base
de trabajo, es un conjunto de textos. Nosotros llamamos corpus o el plural córpora cuando
hablamos de un conjunto de textos. Si yo tengo un conjunto de textos, eso es un corpus. Ahora
vamos a hablar un poquito más de corpus. Vamos a volver a hablar de corpus, pero por
ahora lo que estamos hablando es que tengo textos. Bien, ¿qué pasa? Los lenguajes regulares
están completamente estudiados y yo diría que podrían considerarse un problema resuelto.
Se vienen al curso de teoría de lenguajes y saben lo que un ingeniero tiene que saber
sobre lenguajes regulares y eso está en un libro de 1979. Y de hecho toda la teoría
es anterior al año 1970, si no me equivoco, ¿de acuerdo? O sea que están muy bien estudiados
y además están desde UNIX incorporados los lenguajes de programación. UNIX ya venía
con expresiones regulares, con CED, con grep, con AWK, todos los herramientas que incluían
las expresiones regulares dentro. Era en épocas maravillosa porque los mismos que
definían la teoría de la depresión regular era los que trabajaban en implementarla, eso
hoy ya no se ve tanto. Es decir, los que estaban creando los fundamentos eran los mismos Richie
de Kernigan que eran los que lo implementaban en UNIX. Y bueno, y lo que les decía, todos
los lenguajes hoy lo incluyen. Bueno, ¿y cómo son, cómo lucen las expresiones regulares?
Por ejemplo, una expresión regular puede tener, ¿esto es cómo aparecen las expresiones
regulares en los lenguajes de programación? Que no es lo mismo que las expresiones regulares
que vieron en el curso de teoría del lenguaje que era con mucho menos operadores, con los
mínimos operadores que permitían definirla. Pero la expresividad es la misma, acá lo
que hay es forma de abreviar cosas, para no tener que escribir expresiones regulares
monstruosas, digamos. Entonces, hay cosas como, la expresión regular cabeza encuentra
el patrón acá a cabeza, la palabra en esta escrita. Este signo exclamación, reconoce
el signo exclamación. Este B corta, B larga, B minúscula, B mayúscula, entre corchetes
es como un or, digamos, ¿sí? Que sería lo mismo que buscar vagón con B corta o vagón
con B larga. Esto busca A, B o C, ¿sí? Fíjese que acá lo que está devolviendo es la primera
ocurrencia. Eso depende como yo programé la función de búsqueda, ¿no? Números del
0 al 9, algo que no sea mayúscula, este corchete funciona como un not, y esto es que no sea
una S, ¿de acuerdo? Esto no, nos voy a entrar en muchos detalles, digamos, la idea es que
y acá así es como luce una búsqueda en Python, por ejemplo, con la biblioteca de presión
regular de Python. A un search del patrón, y esto sería nuestro cuerpo, digamos, ¿no?
Y ahí devuelve las ocurrencias. Más cosas, ¿no? Ahí sí. Fíjense que el símbolo de,
¿cómo es? De aceto circunflejo al final no tiene ningún significado especial, y en el
medio de a poco. Este signo de pregunta quiere decir que el carácter anterior es opcional,
el punto macheda cualquier carácter, etcétera. Bueno, estos operadores son syntactic sugar,
son formas de abreviar otras cosas, digamos, ¿no? Si yo pongo barra B, estoy pensando en
un número, el 0 al 9 es otra forma de escribir 0 al 9, son facilidades, no agrega expresividad,
vuelve la vida más fácil de que está programando. Y aquí hay forma de sustituir. Esto es utilizar
una expresión en una sustitución. En casi todos los liguajes de programación, además,
uno puede incluir cosas como decir, quiero que lo que dice acá sea exactamente lo mismo
que lo que dice acá. ¿Se entiende? Por ejemplo, acá dice hoy quiero estar aquí mañana quiero,
porque estamos apeando el quie, ¿sí? Con este. ¿De acuerdo? ¿Entiendes? Esto formalmente no son
expresiones regulares, sino que son extensiones. De hecho, esto es más expresivo que la expresión
regular. Esto es un poco de repaso, digamos, ¿no? Del tipo de cosas que puedo hacer, pero lo que nos
interesa para este curso es que yo puedo buscar rápidamente y especificar muchas cosas a la vez
buscar. Si yo, el asunto es, si yo puedo resolver algo en expresión regular, debería resolverlo
con expresión regular. ¿Por qué? Porque hacerlo más complejo, me voy a perder eficiencia, porque
no hay nada más eficiente que la expresión regular. ¿De acuerdo? Más o menos muchas veces no puedo,
o me queda muy complicado describirlo. Porque la expresión regular, fíjense que uno tiene
que cubrir toda la casuística que quiere identificar, lo cual es muy fácil cuando digo todas las
palabras que empiezan con C, pero cuando yo quiero empezar a hacer cosas más complicadas como
identificar, y ahora lo vamos a ver, separar palabras, por ejemplo, ahí digo, bueno, ¿pero qué separa una palabra,
un espacio? Sí, pero también puede ser un signo de puntuación, sí, y un dos punto también. Entonces,
yo podría decir buscar todos los signos de puntuación entre dos palabras, pero ¿y qué pasa si dieron
enter, etcétera? Es decir, representar todo por una expresión regular a veces es bastante complicado.
Ahora vamos a hablar un poco más de cómo se usan las expresiones regulares,
pero como les decía, las expresiones regulares definen un conjunto de tiras que se llaman
lenguajes regulares, ¿sí? Y que tienen interesantísimas propiedades de clausura,
como vieron en el curso de teoria del lenguaje, lo cual hace que yo pueda, por ejemplo, si quiero
buscar una cosa, algo que está representado por una expresión regular, y dos cosas que están
representadas por expresiones, el or de ambas es cerrado, o sea que tengo una forma, no lo puse acá,
tengo una forma inmediata de buscar ambos a la vez, es decir, es tan fácil buscar una cosa como
buscar una cosa u otra, siempre es determinístico porque son cerrados bajo todas estas propiedades,
o si yo quiero buscar, es tan fácil buscar algo que sea, que diga que si encuentro el string
perro como algo que no sea perro, ¿no explicó? Porque son cerrados, porque si yo tengo un lenguaje
regular que representa perro, inmediatamente tengo la negación, ¿sí? Pero lo más interesante de
eso es que la demostración de que son clausuras son constructivas, ¿por qué? Porque los lenguajes
regulares, además de poder ser expresados con una expresión regular, ¿sí? También pueden ser
representados por un automata finito, que ahora vamos a ver lo que es, pero seguro que se acuerdan
de ti del lenguaje, es decir, exactamente las mismas tiras que yo expreso con una expresión regular
hay una automata, una máquina de estado que la reconoce, ¿sí? Un lenguaje regular es el conjunto
de strings sobre un alfabeto sin más reconocidos por un automata finito, o sea, es una definición
alternativa de lo mismo. Entonces, ¿cómo sería un automata finito para dirección? ¿Se acuerdan de
cómo son los automatas finitos, ¿no? Este pizarrón. ¿Se acuerdan de cómo son los automatas finitos? Son
esencialmente un conjunto de estados, ¿sí? Un estado por donde se comienza, ¿sí? Transiciones que
tienen símbolos y que van de un estado al otro, ¿sí? Y algunos estados especiales que se llaman
estados finales, ¿sí? ¿Se acuerdan de esto, ¿no? Todos símbolos sobre el alfabeto. ¿Cómo seguir
un automata finito para direcciones de correo? Más o menos. Este automata. Vamos a suponer que yo
acá escribo todos los símbolos, ¿no? Tengo que hacer un arco por cada símbolo.
Supongamos que el alfabeto son miúsculas, mayúsculas de arroba y punto, ¿no?
¿Este automata reconoce dirección de correo?
Yo diría que sí, pero, pero, exactamente, reconoce eso y mucho más, o sea, que no es lo que queremos.
Nosotros veremos un automata que genere exactamente, o tan exactamente como podamos, las direcciones
de correo. Entonces, ¿cómo sería?
¿Qué hago?
En el primer estado, voy a instar de las normas, pero ¿cuándo se puede estar?
¿Qué sería? A, Z, A, Z.
Sí, ¿y después? Bueno, sacamos los puntos. Tenemos letras a punto y a arroba.
A ver, una arroba.
Sí.
Y, punto, lo mismo, ¿no? Vamos a suponer que el arco ya sabemos esto, ¿sí?
¿Con qué símbolo?
¿Oye era esto?
¿Qué le parece?
¿Qué le parece?
Es una basura.
¿Sí?
Es una basura.
¿Por?
Es un problema de su momento de arrobo.
¿Cómo? ¿Cómo, perdón?
No se puede poner arrobo a algo.
¿Un problema que tiene que te pueden poner arrobo a algo? Sí.
¿Qué otro, tiene algún otro problema más?
¿Qué otra cosa pasa?
Que solo me admite algo punto com, punto algo, no? Sólo punto, y podría tener más de uno.
Y acá, si no le ponemos el épsil, lo necesariamente tiene que tener una letra, lo cual necesita
mal. Especificar una expresión regular no siempre es este, del todo sencillo y determina
y es parte de nuestro problema de especificación, igual que estuvimos bastante bien. Aquí hay
una que no es mucho mejor que la que hicimos nosotros, no, porque ahí es más o menos. Ahí anda
con gmail.com, pero no anda con un tío bui, pero tal. Es decir, lo que tiene de bueno es que, a ver,
lo que tiene de bueno es reitero, es que si yo tengo eso rápidamente puedo encontrar esas,
las expresiones, la iré yo de correo, cómo la especificé, la puedo rápidamente encontrar en un
cuerpo. ¿Por qué las puedo, a ver, por qué las puedo? Porque yo siempre tengo la forma, y acá
viene la explicación de por qué es eficiente, porque yo siempre tengo la forma de esto. Esto es
un automata finito determinista. Quiere decir que yo simplemente leo la entrada, voy recorriendo
el automata y si cuando terminé llegué a un estado final quiere decir que reconoce esa entrada. Eso
lo hago en orden en tantos pasos como largo tenga la entrada. No importa y voy recorriendo el
texto, ¿no? ¿Cómo se hace la búsqueda es otra cosa, pero la idea es que voy recorriendo y intentando
detectarse aparece uno de esos. Entonces, en los asuntos así, los lenguajes regulares siempre
pueden ser representados por una expresión regular. Es mucho más fácil escribir una expresión
regular en general que dibujar el automata, que es una cosa que va a poder ir creciendo y quedar
enorme, ¿no? Porque yo puedo hacer una expresión regular, como le decía la demostración de la
clausura y es constructiva. Entonces yo hago, si yo quiero reconocer expresiones de correo o
nombre de países de África, hago una expresión regular para la dirección del correo, otra
expresión regular para los países de África, genero los automatas de ambos, ¿sí? Que hay
algoritmos para convertir expresiones. Por eso le decía que acá está lo que tiene bueno que
acá está todo resuelto. Genero los automatas de ambos y simplemente creo una automata nuevo que lo
que tiene es un estado inicial y que me manda a una automata o al otro. Y eso me da el oro. Ahora
vamos a lo que sería, pero está. ¿De acuerdo? Es decir, siempre puedo construir algo eficiente
para reconocer cualquier cosa expresada con expresión regular. ¿Alguna duda? Bueno, esa es la
definición de automata finito que coincide con lo que hablamos, ¿no? Tengo un conjunto finito de
estados, un alfabeto, un estado inicial y un conjunto de estados finales. ¿De acuerdo? Y cuando
reconozco y bueno, sí, la función que me dice ir desde un, hay una función delta extendido que va
del estado inicial, lee una tira y llega otro estado, si esa función me llega a un final. Bueno,
o sea, no importa si yo paso por un final en el camino, tengo que terminar mi tira en el estado final.
La pregunta que les dejo de ver es cuál sería la expresión regular para las direcciones de
correo. Y no solo eso, hay una tercera forma de representar lo que es a través de gramáticas
regulares, ¿sí? Ya vamos a hablar luego de gramáticas, pero hay una gramática que permite
expresar los lenguajes regulares, exactamente las mismas tiras. Pero siempre son visiones
alternativas de un conjunto de tiras. Siempre vamos a estar hablando de conjunto de tira,
en todos los lenguajes, un lenguaje es un conjunto de tiras, incluido los lenguajes naturales.
Los automotafinitos además pueden ser, nosotros hablamos de no deterministas,
de deterministas, perdón, pero pueden ser no deterministas. El lenguaje de las ovejas que
tengo acá es de una cierta cantidad de S y una última E. Esto es no determinista porque no
sé cuándo es la última. Acá puedo dar vuelta o no. ¿Te acuerdo? Los automotafinitos no deterministas
reconocen una tira cuando algún camino lleva un estado final. Parecen más expresivos que los
lenguajes deterministas, pero no lo son. Conocemos algoritmos para transformar de esto a un automotafinito
determinista. Y no solo eso, conocemos cómo va a hacer para que esto, este automata, que no
son los no deterministas, sino que tiene transiciones, es decir, que yo me puedo mover desde
este estado a este estado sin consumir la entrada. Esto parece más expresivo, pero no lo es. Todos son
los mismos lenguajes regulares. ¿De acuerdo? Entonces yo siempre puedo transformar un automata
finito no determinista que a veces me queda más fácil especificar. El lenguaje de las ovejas es
más fácil especificar con un no determinista. Por lenguaje cuya última letra es una A.
Yo puedo transformar esto a uno determinista y reconocer en tiempo lineal. ¿Por qué no siempre me
conviene, me convendría a hacer eso? ¿Por qué no siempre me convendría y hay algoritmo para
reconocer automatas no deterministas? O sea, una forma que yo tengo es, yo quiero reconocer
una expresión regular, la especifico con un automata no determinista, lo convierto a determinista y el
algoritmo es sencillo de buscar recorrer los arcos, como hicimos acá con el dedo, ¿no? Programar eso.
¿Por qué a mí me podía convenir directamente tratar un algoritmo de que recorra esto?
¿Por qué? En teoría del lenguaje se quedaron con la idea de que era siempre mejor volar
los deterministas porque son mucho mejores y más lindos, pero también son mucho más grandes,
porque yo para resolver el no determinismo si bien puedo lo resuelvo creando estados nuevos
y a mí me puede convenir en lugar de crear una nueva automata mucho más grande y que además tengo
el costo de conversión, ¿no? Es mantener en memoria en qué posición estoy, mantener el no
determinismo. Si ustedes se acuerdan, el algoritmo de Thompson, lo que permitía era pasar de este
automata a uno determinista y lo que hiciera, ¿en qué estado estoy? Estoy en Q0, bien una B estoy en
Q1. Si bien una E estoy en Q2. Si viene una E en Q2, yo puedo estar en Q2 o en Q3. Recuerdo la lista
de los estados en que estoy. Seguramente lo aprendieron para hacerlo en el segundo parcial y
pues se olvidaron. Entonces, en estos modelos el problema es elegir el camino adecuado para procesar
la tira. Yo no voy a entrar mucho en detalle en esto, pero ¿cómo puedo elegir el camino adecuado
para procesar la tira? ¿Cómo digo si un automata reconoce una tira? Y los caminos son varios,
esencialmente uno es backup, es como es el viejo querido backtracking, es decir, si yo tengo este
automata, ¿verdad? Cuando tengo una duda, tomo un camino, digo bueno, viene una E, me fui acá o me
quede acá. Bueno, yo supongo que me quede acá y si después viene un signo de exclamación,
quedo trancado, o sea que ese camino no me servía y tomo el camino alternativo, ¿de acuerdo? O sea,
hago backtracking, pero también puedo hacer look ahead y es mirar adelante las transiciones que
tengo, mirar los siguientes símbolos que vienen en la entrada para ver si son compatibles con lo
que tengo en el automata, o eso que les contaba recién del algoritmo de Thompson, hacer paralelismo,
es decir, contar todos los caminos que tengo, posibles, ¿de acuerdo?
Bueno, esto es lo que ya les dije, ¿no? En general se utilizan automatas finitos no
terminan con un algoritmo adecuado. Esto es solo una introducción, no pretende ser más que eso,
hay un curso para eso. Como yo les decía, para cada cosa que yo, cada tema que damos acá,
hay un curso. No es que siempre exista un curso, pero digamos, se podría dar un curso.
Y acá volvemos un poco a lo que nos interesa en este curso y esto que hablamos, los lenguajes
regulares son lenguajes formales, ¿sí? O sea, los lenguajes formales son conjunto de tiras
sobre un alfabeto finito o palabras y estos conjuntos tienen en general gramáticas que los
generan exactamente, sabemos exactamente cómo generarlos. Esos son los lenguajes formales,
que son los lenguajes que ustedes han conocido hasta el momento en la carrera,
todos los lenguajes de programación son lenguajes formales. Esencialmente,
son no ambigos para empezar. No puedo fácilmente dar dos interpretaciones, no, no puedo dar
dos interpretaciones y copilar a código máquina, digamos, no, no puedo hacer dos cosas a la vega.
Los lenguajes naturales son los que la gente habla. Es muy difícil modelar con un lenguaje
formal al lenguaje natural porque son ambigos, más, ¿se acuerdan de l'héritier? Porque son
vagos, porque hay cosas que uno dice que se puede interpretar de una forma, que no son claras,
que uno lo interpreta por el contexto. ¿Se acuerdan lo que vimos, no? Que según como yo diga,
venía a cenar, te espero, o vení que te espero, digamos, depende del contexto, pues son
diferentes. Bueno, y un poco de esto, ¿no? Depende del contexto. Las cosas que se dicen
dependen de dónde les tú dicen. Sin embargo, yo acá les dejo algún link. Uno podría llegar
a discutir si no puede representarlo con un lenguaje de obra. Entonces, eso no es fácil,
eso está clarísimo. Que no es sencillo, yo no puedo hacer expresiones regulares para todo
el lenguaje natural. Pero dado que la cantidad de palabras se finita, yo eventualmente podría
llegar a armar algo, que representar a todas las palabras que se pudieran decir. Sería muy difícil,
sobre todo por la creatividad del lenguaje, porque el lenguaje está movimentando palabras todo el
tiempo. Hay un artículo que es de 1962, 69, creo, que discute un poquito de eso.
¿Y qué pasa con las expresiones regulares y el lenguaje natural? Bueno, hay un fenómeno
en el lenguaje natural que se llama center embedding, que yo puedo decir un hombre llora,
puedo decir un hombre que una mujer ama llora, puedo decir un hombre que una mujer que un niño
adora ama llora. Y así puedo seguir a infinito. La estructura de estas oraciones es grupo nominal
verbo. Grupo nominal 1, verbo 1. Grupo nominal 1, grupo nominal 2, verbo 2, verbo 1. Grupo nominal
1 y supuestamente de alguna forma yo tengo que mostrar que este verbo está pegado a este grupo
nominal, este verbo está pegado a este grupo nominal y este verbo está pegado a este grupo
nominal. ¿Sí? ¿Tan relacionado desde el punto de vista que llora? ¿Quién es que llora? ¿Quién llora
acá? Un hombre. No es la mujer la que talla. Si ustedes ven esta estructura, 1, 2, 3, 3, 2, 1,
es muy parecido a esto que teníamos acá, bueno, que lo borré. W, W, reverso, que es el tipo de
cosas que sabemos que no se pueden representar con el precio regular. Esto fue lo que hizo,
si yo mal no recuerdo decir a Chonky que el lenguaje, el lenguaje natural no era expresable. Es decir,
yo no tengo forma de modelar con el precio regular, es esto, teóricamente no puedo.
¿Sí? Ahora, también yo puedo decir que yo lo des... A mí no saben lo que me contó Armarete
por ejemplo, porque yo con esto ya me mareo y probablemente una malla no seamos capaces de
entenderlo, no seamos capaces de procesarlo. Entonces, yo digo bueno, pero esto sí es
arbitrariamente largo, pero si yo supongo que lo más que puedo llegar es a 3, ahí sí puedo
expresarlo con una presión regular. ¿De acuerdo? ¿Se entiende? Es decir, nuestra capacidad teóricamente
podemos armar la llave, pero no la podemos volar, no la podemos compilar, digamos, en otra cabeza.
Entonces, un poco ponen discusión que vos no puedas... Esto como argumento de, ah bueno,
dejar estudiar el precio regular es porque para el lenguaje natural nunca te van a servir para nada.
¿Y esto qué pasó?
No podemos modelar el lenguaje natural con expresiones regulares.
Pero sí podemos modelar algunos fenómenos. Típicamente se modelan con expresiones regulares.
La fonología se ha representado durante mucho tiempo en el estudio de los sonidos, ¿no?
¿De cómo los sonidos forman las palabras? La morfología que lo vamos a ver, la que hace que viene.
Y la sintaxis de superficie, digamos, superficial, reconocer los grupos nominales y grupos verbales
se ha resuelto con expresiones regulares. Es decir, ciertos subgrupos de problemas se pueden
resolver. ¿De acuerdo? ¿Hasta acá? ¿Alguna pregunta? No, ninguna duda. Esas son nuestras,
nuestro primer modelo que es el de las expresiones regulares. Ahí tienen el capítulo 2 del libro
de Martin Yuravsky, es lo que usamos para la clase de hoy. Si lo pueden encontrar en líneas,
hasta porque sea tan en la tercera edición, están los drafts de algunos capítulos de tercera
edición, los pueden encontrar si no me aparecen por ahí. ¿Lo acuerdo? Bien. Bueno, eso fue un poco el
repaso de expresión regulares. Y ahora vamos a ir a la primera tarea que enfrentamos como en el
procesamiento de Lemos Genatural Trisionemente. En realidad hay una primera tarea que está
siempre subestimada y que lleva mucho tiempo en general que es la de preprocesamiento. Es decir,
yo puedo partir de un texto escrito en un formato electrónicamente amigable, un texto en ansio,
en un icode, pero generalmente para llegar del mundo real a ese texto yo tengo que hacer todo
el procesamiento porque los textos vienen en páginas web, con marcas de HTML o hay que extraerlo
de un PDF. Yo me acuerdo que en una época que hacíamos algunos trabajos con gente del
Pasteur, la gente decía, había un compañero que me decía, bueno, no sabes nada, me venía a decir
que venía ese procesamiento de Lemos Genatural y no sabía sacar el texto de un PDF. Porque su problema
ante eso era sacar de los PDF, de los Papers, a texto puro. Qué bastante difícil por entreparer
decir porque el PDF es una cosa de imprimir. Ese trabajo lleva mucho, es muy engorroso,
lleva mucho tiempo y está generalmente subestimado el tiempo que lleva y en este curso lo vamos a seguir
subestimando porque vamos a asumir que partimos un texto como la gente, digamos, sin ese tipo de cosas.
O sea que yo tengo un texto escrito. Vamos a suponer también, bueno, no tenemos por qué suponerlo,
pero si les queda cómodo, que es un texto razonable, que no tiene cosas muy raras como
como Twitter o como poesía incluso, no? Yo creo que analizar poesía había
tenido otros problemas, seguramente. De hecho se hace, de hecho tenemos un proyecto de grado
o varios que analizan cosas de Twitter, pero ahí cambian un poquito las reglas.
Ahora vamos a suponer un texto, un texto narrativo, una noticia, pues así, gente normática.
Y lo que nos va a interesar es ver el tema de la normalización de los textos. Es decir,
yo agarro ese texto y quiero de alguna forma analizarlo. Bueno, para empezar tenemos que ver
cuáles son las unidades del texto. Y ahí yo puse una definición que dice segmento del discurso
unificado, perdón, segmento del discurso, todos los años, me pongo la pausa, segmento del
discurso unificado habitualmente por el acento, el significado y pausas potenciales, inicial y
final. Eso es la definición de, ¿qué es eso? Acento, no, más chico, porque la pregunta es
cuáles son las más pequeñas, ¿no? Palabras, ¿no? Las palabras, ustedes vieron que en todas
estas definiciones siempre se ponen muy cuidadosos porque siempre apasionan, pero tal coste
una palabra y no. Unificado habitualmente por el acento, no sé por qué el acento. Ah,
porque por claro, porque tienen el brujo y las esas cosas, ¿no? El significado, tienen
un significado, la clase que viene va a hablar de morfología, donde hay parte más chica
en la palabra, pero que no tiene significado independiente. Y pausas potenciales, inicial
y final, porque nosotros nos creemos que las palabras tienen espacios, pausas, pero nosotros
no hablamos. Ah, sí, no, así no, así. ¿Lo acuerdo? Bueno, esa es la definición de
palabra, o sea, nuestra primera aproximación va a ser, bueno, vamos a identificar las palabras
de todo el texto, ¿sí? Y vamos a ver ahí, por ejemplo, un pedacito, un texto de un cuento
muy lindo de Borje que se llama la elef. ¿Qué palabras hay ahí? Díganme palabras.
¿Qué palabras vamos encontrando? Bueno, yo les digo, si no se animan. La, candente,
mañana, de, fácil, ¿no? Los espacios se paran palabras. Febrero, en, qué, ¿verdad?
Puedo seguir hasta el final. Vea tribitervos, son dos palabras y una sola. Son dos palabras,
¿no? Pero a mí me podría interesar, para, posteriores análisis, decir que esto se comporta
como un nombre propio solo, ¿no? Eso podría ser interesante. Si yo quiero saber todos los
textos que hablan de Beatriz y Terbo, este, me podría dar a interesar de identificarlo
como una sola cosa. Ella llama multibord expression, o tiares con nombre, que muchas veces me puede
interesar identificarlas. ¿Murió? ¿Coma? ¿Coma el palabra o no es palabra? ¿Pende para qué?
No tiene acento, o sea, de la definición de la Academia de Pañola no, no es una palabra
pero está. Pero a mí me puede, me destacó más algo hace ahí, ¿no? Sepadre a dos. Enunciado,
creo. Después de una imperiosa, blá, blá. Acá tenemos la Plaza Constitución, que es,
es un lugar. Puticoma. Y acá hay otra cosa también muy interesante, ¿qué es lo que termina acá?
La oración, ¿no? ¿Cómo saben que termina la oración?
Pueden un punto, ¿no? Punto, sin una pregunta o sin exclamación termina la oración.
Este punto podría ser en una abreviatura, ¿no?
Podemos cortar la oce al medio y tenemos problema.
¿El punto y cómo separa oraciones? Sí, yo qué sé. Cómo que sí, ¿no?
Yo les avise que en este curso no esperen todas las respuestas porque no siempre están.
Trabajamos con un lenguaje, a ver, trabajamos con un material que es ambiguo, que es, es
movedizo, digamos, no podemos pretender tener todo determinístico. Pero si no sería muy fácil.
Por eso un cuerpo en español no es igual que un cuerpo en inglés, porque las características
son diferentes y ni les digo un cuerpo en chino.
Acá hay más cosas. En realidad no sé qué aporta esto, pero este es lindo, te cuento.
Mentón son tres puntos subvencibles. Al final eso termina oración.
Bueno, toquenizar es un problema que en general es bastante fácil.
Pero para llegar a un nivel completo de análisis tiene sus problemitas.
Hay un típico muy clásico que se llama what is a word, what is a sentence.
Están las referencias que habla de los problemas que hay al toquenizar, que no son tan sencillos como...
¿Qué hace que no sea un problema tan sencillo? Un problema típico de problemático que en realidad con la...
Un problema típico que en realidad con el advenimiento de los formatos directamente digitales,
eso está menos que el corte de guiones en el borde.
Yo creo que metí un artificial por acá, porque esto no lo tenía.
La primera, comu nión.
Comu nión es una palabra sola, pero yo en el texto la tengo separado por un guión.
Y yo tengo que ver si ese guión se parte de una palabra al medio o es una palabra compuesta con un guión.
Echos se pasan en español, no hay, pero bueno, pero no siempre trabajamos en español.
Bueno, y entonces un poquito de...
Además de otra cosa, yo tengo que identificar para el análisis...
¿Cuáles son las palabras que aparecen?
Ah, hay otra pregunta, a ver.
Por ejemplo, este de... y este de, ¿son el mismo?
Va, justo agarre una, porque de nuevo es una... no es lo que hice preguntar.
Esto es una expresión, ¿no? Que generalmente se interpreta toda junta,
a mí me podría convenir... pero más allá de eso, a ver, déjame buscar otro.
No lo tengo, bueno, pero aparte de lo que es de nuevo, que es una expresión,
este deeta en mayúscula y deeta en minúscula.
¿Son la misma palabra o no?
Son, en general son.
¿Me interesa pasarlas a minúscula?
¿A la dos?
Para pensando en cómo analizar.
Cuando yo normalizo, trato de dejar el texto, lo más fácil de analizar para después.
Separo las palabras claramente y digo, esto es una palabra.
No me confío, solo en el espacio, porque si acá hay dos espacios,
sigue siendo una separación entre palabras.
Lo importante es la distinción entre lo que se llama word type.
En español le decimos más bien palabra, o palabra diferente,
no hay una traducción directa que es...
Si yo tengo un texto, las palabras son las palabras diferentes que hay.
Y a cada uno de estos, como lo llamamos, lo llamamos tokens.
Las apariciones de una palabra en un texto se llama tokens.
Y la tarea de partir esto en así, se llama tokenización.
No es lo mismo la palabra que el token.
Geniamente un corpus, que es un conjunto de textos,
tiene muchísimas más tokens que palabras, porque se repiten.
Vamos a ver algunas definiciones de cosas sobre las que vamos a hablar.
Bueno, como le decía, el corpus es una colección de textos,
seguramente lo vamos a usar en todo el curso.
La oración, yo cuando hice el curso español uno me dijeron,
la oración es una estructura anidada por un verbo,
lo cual me suena a definición más sintáctica que otra cosa.
Busqué la definición que hay en la Wikipedia,
me casi no la puedo copiar todo porque era demasiado,
pero más o menos es un constituyente sintático,
un constituyente sintático más pequeño y necesario
para expresar un predicado completo, lo que quiera que eso sea.
No, es decir, afirmamos algo,
la demás la miro a ella porque me da miedo meter la pata.
Nosotros ven oraciones, oraciones, no digamos, oraciones.
El perro comió el hueso, oraciones.
Tienen un verbo, un verbo tiene, llueve, es oración.
Tenemos la versión hablada, que son los enunciados,
o uterans, que es la versión para lenguaje hablado,
que por ahora queda como conocimiento general
porque acá no vamos a hablar de lenguaje hablado.
Y después tenemos también los lemas.
La forma de superficie es la palabra como la conocemos,
con todas sus flexiones,
no vamos a hablar de la clase que viene,
inflesiones, derivaciones, pero es...
Las flexiones son las que, por ejemplo, dan el género y el número,
y las derivaciones son las que construyen una palabra a partir de otra,
como ve los mentes.
Mente es una derivación, inflesión de derivativa,
una derivación.
¿La palabra cómo está?
Pero el lema es cuando nosotros representamos
una palabra a un conjunto de ellas que tienen el mismo significado,
que tienen la misma raíz, la misma categoría gramatical,
¿qué es la categoría gramatical?
¿Qué es la categoría gramatical?
Tiene que saber eso, tiene que saber eso.
Si es un verbo, si es un sustantivo.
Y el mismo significado, o sea, gato, gata, gato, gatas,
todos tienen un lema que es gato.
¿Para qué puede servir identificar el lema de una palabra?
Por ejemplo...
Sí, pero ¿por qué me interesa reconocer, distinguir gato de gata de gato
de gatos?
Típicamente en la recuperación de información.
Cuando yo quiero traer los documentos que hablan de gato,
yo pongo gato,
si hay un documento que dice gatos, seguramente me va a salir.
Por eso me interesa, y es típico de recuperación de información,
buscar por lema, no por la palabra, por la forma aflecionada,
por la surface phone.
Entonces, estos son conceptos.
El lema es como el representante canónico de las formas de superficie afleccionada, digamos.
Bueno, vamos a hablar más de eso en la clase que viene.
Bien, como le decía en un corpus, yo tengo los word types,
que son las palabras distintas en el corpus,
y los tokens que son el total de palabras en el corpus.
Total de apariciones de palabras en el corpus.
¿Qué pasa?
Por ejemplo...
Bueno, esto queda de ver porque es muy fácil.
¿Cuántas palabras hay ahí? ¿Cuántos tokens?
Y la discusión que tuvimos, ¿no?
Tengo que ver si cuento como tokens a las comas,
generalmente se consideran tokens a las comas,
porque a mí me interesa que aparezcan en el texto.
A veces las unifico como signos de puntuación en el análisis.
Muchas veces se hace eso.
Pero yo creo que eso se hace para facilitar el análisis,
no porque esté bien, porque a mí me interesaría separar una coma de un punto.
Muchas veces no se hace.
Es decir, para el etapa subsidente,
esto se identifica con una marca que es un signo de puntuación.
Pero por acá hay palabras que seguramente van a haber más tokens,
¿qué palabra?
Porque acá hay un la...
Acá hay dos A.
Bueno...
Y bueno, por ejemplo,
hay tradicionalmente uno en este tipo de análisis
trabaja sobre corpus grandes.
Los corpus que nos permiten analizar las diferentes ocurrencias de cosas
en el lenguaje son corpus grandes,
porque yo necesito ver la casuística.
Los lingüistas hace muchos...
hace décadas que trabajan sobre corpus,
donde identifican la...
¿Cómo se comporta el verbo ser en el lepaño?
Y bueno, tengo que ver todas las ocurrencias de ser que aparecen
y estudiar cómo se...
es bien empilco esto.
Si nosotros nos creemos que la real academia pañola sabe todo,
pero en realidad primero que de algún lado tuve que aprender
y luego que...
uno tiene que estudiar la casuística, ¿de acuerdo?
Entonces, hay un corpus bastante conocido que es el corpus crea,
el corpus de referencia del español actual,
que junta...
acá tienen un link para verlo,
los detalles, pero junta textos de diferentes lugares,
mayormente de España, pero también de América Latina,
de diferentes temas.
Cuando uno construye un corpus es todo un trabajo,
porque primero uno tiene que identificar
de qué quiere armar el corpus, porque como le decía,
un corpus de inglés es igual a un corpus español, obviamente,
pero un corpus de noticias no es lo mismo que un corpus de...
poemas,
porque las cosas que hay,
incluso las frecuencias de la palabra van a ser diferentes.
Seguramente la cantidad de palabras desconocidas
en un corpus de noticias sea muchísimo menor
que la cantidad de palabras desconocidas
en un corpus de poemas,
porque uno cuando cree poemas se le da por inventar cosas.
Por inventar palabras, cosas también.
Por ejemplo, el corpus crea tiene 125 millones de tokens,
es un corpus bastante grande
para los parámetros del año 2000,
para este, ahora no,
ahora vamos a ver un poquito más,
y tiene 737.799 palabras distintas.
Esto debería converger, digamos,
al tamaño del vocabulario que existe,
que no es lo mismo que el vocabulario,
que no es el número, me olvidé de notarlo,
que no me lo acuerdo,
de un diccionario, porque otra forma mira, un diccionario.
El diccionario me dice todas las palabras,
pero me lo dice, me trae los lemas.
Entonces va a ser más chico, digamos.
Esto es como un diccionario, palabras flexionadas.
Por supuesto, no son las palabras del lenguaje,
porque si yo no lo emití en este corpus,
no, no existe.
El corpus crea está separado en diferentes secciones,
es típico de los corpus eso también,
que vos tenés diferentes secciones,
estos son de Venezuela, estos son de...
Generalmente vos en los corpus tenés eso,
de dividir su corpus, digamos.
Por si vos querés especificar cosas,
por ejemplo, si querés hablar del lepaño y del río La Plata,
te remitía ese corpus.
Y todos los análisis que uno hace en esto,
de todo lo que hablan en el curso,
siempre tiene que decir sobre qué corpus los hizo.
Porque uno,
entra en lo que sea,
que significa entrenar,
porque uno aprende ya sea mano,
o automáticamente de un corpus,
pero además tiene que evaluar sobre un corpus.
A ver cómo le fue, eso lo vamos a hablar luego.
Siempre va a ser sobre...
uno tiene que decir sobre qué corpus es.
A mí esto me dio tal resultado en el corpus crea.
Lo cual no quiere decir que me va a averiguar el resultado
en un corpus de Twitter.
Hace poquito se aprobó,
hubo un proyecto grado del grupo nuestro,
se aprobó ahora, es un par de meses,
que construyeron a partir de fuentes de noticias,
de la wikipedia,
y otros foros,
y otros fuentes,
un corpus de 6 mil millones de tokens.
Esto es un muy buen corpus.
Incluso,
a nivel de lo que hay para el inglés,
que son de 8 mil millones más o menos,
y...
ahí aparecieron 1.460 millones de palabras de...
no, no puedes ver.
Perdón.
Un millón,
un millón 460...
1,5 millones de palabras de tinta.
Fíjense...
que para...
125 millones de tokens,
había 737 mil palabras,
para 6 mil millones,
había el doble.
Lo que decíamos, debería ir convergiendo,
pero si no apareciendo cosa rara.
Esto es un corpus del español.
Es un corpus...
no anotado.
Quiere decir que nadie lo miró a mano,
obviamente.
Solamente, y no es poco,
una gran cantidad de textos.
No puedo aprender yo de un corpus...
de ese tipo de cosas.
¿Cómo se agrupan las palabras?
La frecuencia de las palabras.
Cuanto más grande sea el corpus,
más clara va a ser mi noción
de la frecuencia de las palabras.
La palabra más común en español creo que es D.
Saber esas frecuencias,
saberlas contando,
supongo que es muy representativo
de mi lenguaje, porque es todo...
es una cantidad de cosas que ha dicho
una cantidad de gente.
Esto, este tipo de cosas,
son los que ha hecho que
radicalmente cambiara el pasamiento
de lenguaje natural en los últimos años.
Porque tengo muchos elementos nuevos.
Y hoy en día es prácticamente
impensable hacer análisis a mano.
Subestimando el poder
de todas estas cosas.
Lo cual no quiere decir que uno no haga estudios analíticos,
pero tiene otras herramientas totalmente nuevas.
Si yo quiero saber cómo es
el verbo ser en el español,
bueno, tengo corpus muy grande
para probar mis hipótesis.
Bueno, la toquenización es
identificar las palabras, dijimos que era bastante fácil,
pero parecían cosas como los que fuimos
encontrando.
Por ejemplo,
esto que está entre comillas,
tengo que ver si lo puedo considerar
solo o una entidad.
O varios toquen con una entidad.
Tengo que ver si las comillas las considero toquen aparte.
Las fechas,
los números,
las direcciones web.
Fenómenos que en el español no tenemos
que son estos...
¿Cómo que se llaman?
Son...
Contracciones, pero con...
Apóstrafo.
Nosotros tenemos contracciones,
como sabe cualquiera que hizo
Crucigramas a Idel,
pero...
esto no lo tenemos.
Esto también hay que ver cómo separarlo.
Y Idel, son todos problemas al Idel
en el mundo real de la análisis,
porque uno viene muy contento
separando por palabras y se encuentra con al,
que es una palabra, pues son dos.
Entonces, después,
uno armó un modelo
de toque.
Pus un ponense una lista de palabras,
me olvidé de la separación.
Sí, pero cuando lo quieren machear
contra el texto original,
dice, bueno, primera palabra, segunda palabra,
no, para... Después que hizo la análisis,
quiere volver al texto para mostrarlo.
Cuando vuelve, hubo dos palabras
que se le transformaron en una,
o mejor dicho, una palabra que se le transformó en dos,
cuando vuelve se equivoca, muestra
las cosas corridas, de hecho, sucede.
¿Todo por qué? Por Idel, que son
una palabra que se pagó.
También,
también, ese es otro problema
al de los críticos. Si a usted puede
interesar sacar el de sí,
porque para la análisis es muy importante.
Exactamente,
para la análisis son dos palabras.
Y yo tengo
que conservar de alguna forma,
y perdón, porque parece,
parece trivial, si yo no tengo,
dice, bueno, yo hago una lista y se paro con los espacios,
pero no, pues yo de alguna forma tengo que tener,
y es un lío de implementación, le digo,
muchas veces.
Uno queda muy contento con su lista,
trabaja con su lista de palabras,
después hace,
ahora, pues, en el clase que viene
lo vamos a ver, le hace análisis
gramatical,
análisis sintáctico, arma el arbolito,
bla, bla, cuando quiere volver
a mostrar la oración, no sabe dónde la tenía.
De un punto de vista de implementación,
estamos hablando, ¿no?
Sí.
Cuando vamos a aprovechar un texto,
¿vás tardado por lo que
vamos a hacer?
No.
Vás tardado por lo que vos quieras hacer.
Es decir,
depende más de la tarea que estés completando.
Por ejemplo, si vos vas a hacer
un conteo simple
de palabras,
no te calienta esto.
De hecho,
capaz que te interesa tenerlo junto,
porque tem
es algo que
es una colocación,
digamos, un dos palabras, se da mucho junto,
yo qué sé.
Ahora, si vos querías hacer análisis sintáctico,
cómo se organiza
el arbol de la oración, esto te va a interesar
separarlo sin duda.
¿De acuerdo?
Eso depende mucho de tu tarea.
Si de todos modos
es bastante estándar
estas cosas tenerla separada.
De hecho, hay un estándar muy sencillo.
Ah, bueno, porque además hay otro problema
y eso también lo he vivido.
Que es, vos toquenizas
con una herramienta.
Esto es un problema
bien de herramientas.
Uno toqueniza de alguna forma en su programa
y después utiliza una otra
herramienta para ponerle la categoría
gramatical a cada palabra.
Si esta herramienta,
al hacer el análisis gramatical,
a la vez toqueniza según su criterio,
lo toqué muchas veces.
Yo tenía un tágar
para hacer mi tesis
en textos biológicos,
de biología molecular,
que aparecen muchas palabras raras.
Y tenía un toque
en etiqueta gramatical propio.
Específico de lo entrenado
sobre un corpus de ese tipo.
Y no era una herramienta
cerrada, lo que hacía era.
Tomaba el texto, lo toquenizaba y lo notaba.
Para machear este texto
que me había toquenizado con mi texto original,
como no toquenizaba igual,
yo me lo sé, había cosas que
por ejemplo decía 25 guión
hidro, nunca entendí nada de lo que estaban haciendo.
25
hidroxil, no sé qué,
y mi
toquenizador, mi método de toquenización
de 7 guión, es una palabra.
El toquenizador
del otro
lo partí acá para devaluar el 25 por un lado
y yo después tenía que
volver a unificarlos para poder
seguir trabajando.
Esto era un nombre
y esto era un número, yo qué sé.
Bueno,
acá hay un estándar de toquenización
bastante conocido, es el del pen-triban.
El pen-triban es
un corpus
de podemos ver qué es, anotado.
Es decir, no sólo tomaron textos,
sino que analizaron cada oración.
Le pusieron la categoría gramatical
a cada palabra.
Y para eso tuvieron primero
que toquenizar. La toquenización
del pen-triban es muy sencilla.
Se separan los signos
de fundación de las palabras.
Se separan las contracciones
y las separan en it.
Y las comillas dobles se transforman
en comillas
de apertura de cierre para separarlas
y los paréntesis
y los corchetes y las llaves
se transforman en símbolos así.
Esto por un tema de facilitar el par sin nada más.
Es un estándar. Lo que tiene de bueno
es que es un estándar. Que si yo aplico el pen-triban
sé lo que me da.
En vez de estar inventando
yo mi propio
algoritmo de toquenización.
Pero bueno, después yo puedo querer
post-processar, digamos,
porque me aparece cierta realidad.
Identificar nombres o cosas así.
Bueno, el chino y el japonés
tienen algún problema y es que no marcan
los límites de las palabras.
Sino que
cada simbolito hansi
del chino
representa morfemas o sílabas.
Entonces, toquenizar acá
es un poco más difícil.
¿Cómo?
¿Cómo?
No, son normales.
Para ellos son normales.
Ah, romana.
No, no, no.
Acá hay una palabra de hecho.
De hecho, hay palabras.
¿Tenés un analizador que va sobre el chino?
No. Analizan derecho.
Sí.
Analizan derecho sobre los caracteres hansi.
Bueno, el problema es que no tenemos espacio
para separar, pero de hecho las palabras
existen en tanto unidades con significado.
Pero no la ve en el texto.
Es lo mismo que nos pasa cuando hablamos.
Si vos querés toquenizar
el texto hablado, vas a tener un problema.
Hay un algoritmo muy popular,
muy básico, pero muy popular.
Se llama MaxMatch.
¿Qué tiene? ¿Una lista de palabras?
De palabras, digamos.
No de morfemas como de nada.
Y es muy sencillo.
Comienza al principio de la entrada.
De la entrada que tiene del texto.
Elige siempre la palabra más larga
en la posición actual de la entrada.
Y si no encuentra ninguna
se queda con una letra sola.
Y avanza.
¿Verdad?
Por ejemplo,
si yo tengo la entrada, me saca
de la cancha sin motivo.
Acá ubica
mesa
K,
el símbolo del K es calcio,
D
y la cancha es larga.
Y acá ves la mesa
de la... no anda muy bien.
En español en inglés no funciona muy bien
el MaxMatch, porque las palabras son más largas,
pero en el chino funciona bastante bien.
A nadie usa esto
en español.
Es el método más básico
de cosas.
Y hay mejoras sobre esto,
empezando a la vez de izquierda a derecha,
por ejemplo, de la izquierda a derecha
o de derecha a izquierda,
al mismo tiempo de los dos lados,
como yo le decía, hay variantes.
Y esto
nos lleva
a una cosa bastante interesante que es
si yo tengo
qué tan bueno, hay todo
un tema en el procesamiento de lenguaje natural
que es la evaluación.
Yo cuando ejecuto una tarea
tengo que
evaluar mi resultado.
Sobre qué la tengo que
evaluar y esto vale siempre
sobre un cuerpo que no sé para aprender.
Si de alguna forma
yo aprendo a toquenizar viendo
cómo se separan las palabras con los textos de esto
que tuve mirando y no sé qué,
no puede utilizar, esto vale
como regla general, después lo vamos a ver más
en detalle con los métodos de aprendizaje automático, pero
yo no puedo utilizar
el mismo texto del que aprendí
para evaluar mi resultado, porque
me va a dar todo bien
o por lo menos
me va a dar mejor que
si yo siempre tengo que evaluar sobre texto no visto
o sea, yo siempre
que tengo un cuerpo sobre el cual trabajar
tengo que agarrar una porción
del texto, típicamente 15-20%
o podemos ver más en detalle.
Y lo guardo a un costado
hasta que llegue el momento de evaluar.
Y cómo valúo la toquenización
y bueno, si yo
tengo la nuestra entrada
y tengo lo que se llama un gold standard
un texto
correctamente segmentado
alguien, un ser humano
me dijo, bueno, la toquenización correcta
es, me saca
de la cancha sin motivo.
Y cómo se calcula
la performance
de un toquenizador
y bueno, con la word error rate
o sea, el
ratio de error de las palabras
que es
entre estos dos
entre estos dos
lista de palabras
que tengo que cambiar
para llegar de esta a esta
si?
que hay, como sería
que sería lo que tendría que hacer yo
y donde cambiar
quiere decir insertar, borrar o
sustituir
que tengo que hacer?
mesa
mesa por
me
y que más?
y k por
saca, no?
ah, porque me coqué
le guardo
entonces
eso es la tasa de error
acá tengo un error de 2, cuanto más baja mejor
si tengo 0 de porque son igual
luego eso se llama
distancia mínima de edición
y lo vamos a ver luego
distancia mínima de edición, pero
en palabras
¿de acuerdo?
bueno, además de
toquenizar
además de toquenizar
bueno
ya lo hemos hablado prácticamente
todo
la normalización implica
llevar las palabras a un formato
estándar para procesarlas
llevar los números
a un formato único
porque así no metemos ruido
para nuestro análisis posterior
acuérdense que esto es la base
de una cascada de tareas
el principio de una cascada de tareas
la url y otra forma
de identificarlas y marcarlas
detectar entidades con nombre
y este
que es folding o llevar toda minúscula
o mayúscula que a veces lo hacemos a veces no
según nuestra tarea
bueno
tradicionalmente la toquenización
y la normalización se han realizado
utilizando automata finito
porque son problemas bastante sencillos
y porque además son
como son el comienzo de la cascada
necesitamos que sean rápido
yo necesito toquenizar rápidamente
para poder después empezar el análisis
entonces
y porque además los automatas de hecho funcionan
esa
esa especificación
del
del algoritmo del PEN-Tribank
tiene su equivalente
en un pequeño programita en sed
la herramienta UNIX para toquenizar con eso
de acuerdo
y cualquier
biblioteca presionamiento del lenguaje natural decente
por ejemplo en el ETK
te permite especificar un toquenizador
en base a una expresión regular
como quiere separar las palabras
y
y lo hace
bueno
también lematizar
a veces nos puede interesar tener solo el lema
por ejemplo si mis documentos lo voy a usar para recuperar información
con tenerlo el lema me alcanza
hay una forma mucho más sencilla
porque
lematizar implica hacer un análisis morfológico
de la palabra
sacar es lo que va a ver la clase que viene
la carla raíz
y las derivaciones
o los afijos
o sea la pedacita
es un poco más costoso
hay un método muy viejo
que se llama STEMIN
que es mucho más simple
que simplemente cortar las palabras
yo sé que perros
perritos
perro
es el STEM
y yo lo puedo utilizar como una aproximación al lema
va a cometer errores
pero
es muchísimo más rápido
y no necesito ningún tipo
análisis morfológico
eso es el famoso
STEMIN
que mal que bien se sigue usando
y ya pasaron como 30 y pico de años
de que se le hizo
si
de eso vamos a hablar en la clase que viene
que es hemorfología
y no solo la irregularidad del lugar
sino la ortográfica también
lo imposible
y eso
si
se puede resolver la clase que viene
igual se resuelve
también
se puede resolver con algo mismo de estado finito
bueno
también hay otro tema
y con esto vamos a terminar
voy a hacer trabajar hoy
y con esto vamos al último tema
de la cosa que además nos interesa
segmentar en oraciones
y ustedes diosme cómo separamos
en oraciones
cómo separamos
¿dónde están las oraciones?
supongamos que los punticomas no son oraciones
más fácil es la goma
eh
empiezan con mayúscula
empiezan con mayúscula
y terminar
con un punto
problemas
ahí llegamos
como un 90 y pico de precisión
vamos a ver
pero algún problema tiene
las abreviaturas
que pasa con la abreviatura
tiene punto
hay punto que son internas la oración
nombre propio
se puede marear con el tema de la mayúscula
claro
si todo el mundo es más bueno encontrar problemas
encontrando problemas
que hay otro problema
si empiezan con un número
por ejemplo
empiezan con mayúscula
es más difícil
no se me ocurre pero puede haber
si buscan un córpul de 6 mil millones de palabras
les aseguro que encuentran todos los casos
este
y es eso
efectivamente es eso
reconocer oraciones
se puede hacer con expresión regulares
como dice el que
tratando de
buscar algunos casos especiales
pero
esta gente
los métodos más
el estado del arte en separación de oraciones
es
utiliza
una especie
una especie anal
lo que se llama análisis no supervisado
o clasificación no supervisado
cuando yo digo análisis
no supervisado
luego el curso movió mucho
pero no supervisado quiere decir que yo no tengo ningún
texto anotado
nadie me dijo cómo eran las oraciones
simplemente me dieron el texto en crudo
y yo hice mi análisis adentro de ese texto
cuando yo hablo de clasificación supervisada
alguien me dijo
acá empieza la oración y acá termina
¿de acuerdo?
alguna anotador humano
como cuando comparamos hoy con el toquenizador
tengo un gol de estándar
acá no
esta gente lo que hizo fue
crear un toquenizador entrenado
agarró un gran conjunto de textos
y tomó ciertas hipótesis
y dijo bueno
identifica candidatos
a abreviaturas
y dice bueno
en general
las palabras que terminan en punto
son abreviaturas
¿sí?
en general
las abreviaturas son cortas
o sea que si una palabra es corta es más probable
que sea abreviatura que no otra vez
y en general
tienen puntos internos
¿de acuerdo?
y con eso
trataron de ver
contar en un texto
las frecuencias
contar en un texto
la cantidad de veces
que esa palabra
la misma palabra aparecía con y sin punto
si yo por ejemplo
la palabra
etcétera
casi todas las veces aparece con punto
o de hecho todas las veces aparecen con punto
en el corpus
o sea que fuertemente candidata
hacer una abreviatura
pero si yo
y aparece Guillermo
como un punto al final
seguramente no sea una abreviatura
sino que sea el final de enhoración
¿de acuerdo?
entonces lo que hacían
en ese tipo de conteos en el texto
para ver cuáles eran más candidatas
digamos a
a priori hacer
abreviaturas
¿por qué se trataba esto de desambivar el punto?
que es nuestro problema
que mencionamos hoy ¿no?
con la palabra
con la palabra
lo mismo hacía cuando eran palabras cortas
palabras largas
con esa lista de candidato
después veían en qué contexto aparecían
metían una segunda fase
dice bueno, pero dice Guillermo punto
y la palabra que la sigue empieza en minúscula
entonces capaz que sí era una abreviatura
no se me ocurre porque Guillermo punto
es una abreviatura pero ta
de acuerdo
o
colocaciones
por ejemplo cuando yo digo
saben lo que son las colocaciones
son palabras que aparecen juntas
de nuevo
es una colocación
no sé si dice colocación
pero en inglés dice colocation
no debe ser colocación
por ejemplo si yo digo
y etcétera
y etcétera
o coma etcétera
y etcétera
parecen muchas veces juntos
entonces
y entonces
digamos
es muy raro que se aparece
con un punto en el medio
ese punto sea de separación
el ejemplo que dijí es horrible
el año que viene voy a elegir uno bueno
pero ese punto es raro
que sea de separación
porque siempre que aparecen juntas
digamos yo digo
de nuevo
este es horrible pero no es tan malo como en el anterior
de nuevo siempre aparece así
de acuerdo si en algún momento aparece
con un punto acá probablemente se hace
un fin de eración
de acuerdo
pensimo el ejemplo
y luego las palabras
iniciales frecuentes es decir
cuando aparece una mayúscula
digamos
esa palabra que apareció
que tan candidata es
hacer el comienzo de una oración
a hacer una mayúscula
en vez de ser un hombre ser un comienzo de oración
y vemos que hay palabras que mucho más frecuentemente
aparecen al comienzo de la oración
la con mayúscula
es una buena candidata a hacer
comienzo de oración
porque hay muchos la
al comienzo de las oraciones
entonces eso aumenta
mi probabilidad de que sea
entonces en base a un estudio de conteo
y de frecuencias
de lo que se llama likelyhood
o verosimilitud
desanvivo en el punto
dicen bueno esto es un punto de abreviatura
o es un punto de oración
y de esa forma se paran oraciones
si
eso en el entecac existe
y yo les voy a mostrar un poco el corpus
hay firefox hablando lo que veo
y
esto es una base
de jurisprudencia del Poder Judicial del Uruguay
¿que es eso?
esto esta publicado
donde dice
¿que dice?
estas son las sentencias
de
una base de sentencias
de ejemplo
del Poder Judicial Uruguayo
si se fijan
este es el texto
que interesante
no eran todas iguales
fíjense que si yo quiero
parar el texto
no tengo muchos problemas
pero acá parecen cosas raras
como este es punto que me interesaría
que quedara dentro de la oración
pero
me sorprendí un poco
acá hay un número que aparece con punto
pero además hay otras sentencias
que
terminan en punto y raya
punto y guión
punto y guión
hay el separador totalmente raro
para lo que es el estandar
y que uno tiene que ver como modificar la toquenización
este corpus es el que van a usar
ustedes para el laboratorio
así que vayan a seguir haciendo amigos de él
este
mira referencias
están por ahí
los invito a le darlas
alguna pregunta
acá
bueno tal vez ya ha sido demasiado
para ustedes
si no hay dudas
vamos a hablar
de distancia mínima de edición
para empezar
que es como ver
cuál es la distancia entre dos palabras
una noción de distancia entre dos palabras
que esencialmente captura la idea
de parecido
es un punto de vista
ortográfico
y después vamos a seguir hablando
de morfología
gracias
