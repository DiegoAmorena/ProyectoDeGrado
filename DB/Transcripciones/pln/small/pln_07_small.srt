WEBVTT

00:00.000 --> 00:26.680
En la clase de hoy, vamos a ver un tema nuevo que es el de los modelos de lenguaje.

00:27.680 --> 00:35.680
Si se acuerdan a la clase pasada, vimos dos temas que eran bastante diferentes.

00:35.680 --> 00:44.680
El de los traductores para resolver el tema de la morfología de estado finito, unos artefactos

00:44.680 --> 00:54.680
de estado finito que permiten resolver temas a través de un método de reglas.

00:54.680 --> 01:10.680
Y de esa forma resuelvo el tema de convertir de la palabra a su análisis y viceversa.

01:10.680 --> 01:14.680
En la segunda parte vimos un método que era bastante diferente de su concepción,

01:14.680 --> 01:21.680
que es su método estadístico, que lo que hace era aplicando el modelo del canal ruidoso,

01:21.680 --> 01:27.680
aproximarse al problema de corregir errores ortográficos.

01:27.680 --> 01:35.680
Cuando yo hablo de un modelo probabilista, lo que estoy diciendo es que además de, por ejemplo,

01:35.680 --> 01:43.680
clasificar o sugerir una solución, lo que hace es asignarle probabilidades a las posibles respuestas.

01:43.680 --> 01:50.680
Un método probabilista, típicamente no da una respuesta, sino que devuelve una distribución de probabilidad.

01:50.680 --> 02:00.680
Es decir, si yo tengo varios eventos posibles, una distribución de probabilidad es un número

02:00.680 --> 02:08.680
entre 0 y 1 que yo asigno a cada evento posible, de forma que la suma de todos los eventos dan 1,

02:08.680 --> 02:10.680
eso es lo que llamamos una distribución de probabilidad.

02:10.680 --> 02:14.680
Entre 0 y 1 son todos, son todos mayores o iguales que 0, menores y iguales que 1,

02:14.680 --> 02:17.680
y además su suma da 1, eso es una distribución de probabilidad.

02:17.680 --> 02:20.680
0, 5, 0, 25, 0, 25 es una distribución de probabilidad.

02:20.680 --> 02:26.680
Si el evento 1 tiene probabilidad 0, 5, el otro 0, 25, y el otro 0, 25, eso es una distribución de probabilidad.

02:26.680 --> 02:30.680
Si no suma a un 1, no son una distribución de probabilidad.

02:30.680 --> 02:37.680
Y si yo, por ejemplo, tengo un evento que ocurre 10 veces, si por ejemplo hago conteo de frecuencia,

02:37.680 --> 02:41.680
por ejemplo, no digo hay un evento 1 que ocurre 10 veces,

02:41.680 --> 02:48.680
hay un evento 2 que ocurre 5 y hay un evento 3 que ocurre 5.

02:48.680 --> 02:58.680
Eso no es una distribución de probabilidad, porque esto no está entre 0 y 1, porque no suman 1.

02:58.680 --> 03:05.680
¿Cómo hago yo para convertir esto en una distribución de probabilidad?

03:06.680 --> 03:11.680
Lo que hago es dividir por el total de ocurrencias, ¿verdad?

03:11.680 --> 03:22.680
Que en este caso es 20 y eso me da la proporción respecto a 1,

03:22.680 --> 03:24.680
y eso es siempre una distribución de probabilidad.

03:24.680 --> 03:29.680
Esto se llama normalizar para obtener una probabilidad.

03:29.680 --> 03:35.680
Esto ustedes lo van a ver que lo vamos a ver en varias veces.

03:35.680 --> 03:45.680
El método de este corrección utilizaba fuertemente la regla de Valles para modelar la situación.

03:45.680 --> 03:50.680
Hasta ahora hemos hablado en todas las cosas que hemos tratado de palabras aisladas.

03:50.680 --> 03:53.680
La morfología estudia, primero hablamos de cómo separar las palabras

03:53.680 --> 03:58.680
y después vimos cómo analizarla internamente, pero siempre hablábamos de palabras aisladas.

03:58.680 --> 04:07.680
Acá lo que vamos a empezar a mirar es qué pasa cuando las palabras aparecen juntas.

04:07.680 --> 04:26.680
Es decir, nosotros lo que vamos a hablar es de la probabilidad de una secuencia de palabras.

04:26.680 --> 04:35.680
¿Por qué esto importa?

04:35.680 --> 04:41.680
Porque, como ustedes bien sabrán, las palabras en el idioma pañón las aparecen solas.

04:41.680 --> 04:43.680
Y no cualquier palabra sigue a otra palabra.

04:43.680 --> 04:53.680
Nosotros tenemos una cantidad de reglas para expresar en el idioma que hace que el orden importe.

04:53.680 --> 05:03.680
Es decir, lo que se trata es de ver cómo tener en cuenta ese orden nos puede ayudar a otra estaria.

05:03.680 --> 05:06.680
Creo que con algún ejemplo lo vamos a ver más claro.

05:06.680 --> 05:15.680
Primero que nada, vamos a recordar a Chonky, que esto yo lo comentaba en la primera clase,

05:15.680 --> 05:25.680
aquello de que Chonky dijo la noción de probabilidad de una oración es completamente inútil bajo cualquier interpretación de este término.

05:25.680 --> 05:29.680
Y trancó por 20 años, la investigación hasta acá apareció.

05:29.680 --> 05:42.680
Chelline, que volvió a revivir el tema de los métodos probabilistas o basados en conteos para aproximarse a los problemas de procesamiento en el lenguaje natural.

05:43.680 --> 05:50.680
Chonky lo que decía esencialmente es cuando nosotros lo hacemos con teo y sacamos conclusión en base a cuenta, en base a números, en base a experiencia,

05:50.680 --> 05:53.680
que es típicamente lo que vamos a ver en este caso de los enegramas,

05:53.680 --> 05:58.680
estamos obteniendo soluciones a problemas, pero no estamos entendiendo qué es lo que está pasando.

05:58.680 --> 06:00.680
Y eso es una discusión catalida de hoy sí.

06:00.680 --> 06:06.680
Es decir, hay una famosa discusión por ahí en internet entre Chonky,

06:06.680 --> 06:12.680
esto te hablando hace dos o tres años, o cinco años, entre Chonky y Peter Norby,

06:15.680 --> 06:17.680
que discute un poco esto.

06:17.680 --> 06:26.680
Es decir, si esto que estamos haciendo ahora y que ha tenido tan buenos resultados desde el punto de vista de reconocimiento del habla y el procesamiento del lenguaje natural

06:26.680 --> 06:31.680
es en realidad inteligencia artificial o es solamente number crunching que no nos aporta mucho.

06:32.680 --> 06:36.680
Norby lo que le dice es bueno, de hecho la ciencia es siempre más o menos funcionada así.

06:39.680 --> 06:42.680
Bueno, entonces ¿cuál es el objetivo de lo que vamos a hablar acá?

06:42.680 --> 06:43.680
Son de modelos del lenguaje.

06:43.680 --> 06:48.680
El objetivo del modelo del lenguaje es calcular la probabilidad de una secuencia de palabra.

06:49.680 --> 06:55.680
Es decir, qué tan probable es en mil lenguajes que una secuencia se dé.

06:56.680 --> 06:57.680
¿De acuerdo?

06:58.680 --> 07:00.680
¿Para qué nos puede servir eso?

07:00.680 --> 07:07.680
Bueno, imagínense ustedes que, y acá vamos a recordar otra vez el modelo del canal ruidoso, de la otra vez,

07:07.680 --> 07:18.680
imagínense ustedes que tengo este texto escrito y por medio de un método que no sé cuál es.

07:19.680 --> 07:24.680
Tengo dos oraciones candidatas, ¿de acuerdo?

07:24.680 --> 07:25.680
Dos textos candidatos.

07:28.680 --> 07:34.680
Uno que es PRNEVA para el curso de PLN y PREVA para el curso de PLN.

07:35.680 --> 07:36.680
¿De acuerdo?

07:36.680 --> 07:44.680
Y además supongamos que el método que utilicé para reconocer la escritura

07:45.680 --> 07:47.680
me dice que este es más probable que este.

07:49.680 --> 07:50.680
¿Cuál vamos a elegir?

07:50.680 --> 07:51.680
¿Cuál vamos a elegir?

07:53.680 --> 07:54.680
Vamos a elegir el de abajo.

07:57.680 --> 07:58.680
¿Por qué?

07:58.680 --> 08:00.680
¿Por qué esto no es una palabra válida?

08:01.680 --> 08:07.680
Pero aún siendo una palabra válida, o aún suponiendo que fuera una palabra válida,

08:09.680 --> 08:12.680
podría darse un caso donde yo identifico una palabra válida, ¿se acuerdan lo corrección?

08:13.680 --> 08:16.680
Aún así, yo podría decir, bueno, pero en este lugar,

08:21.680 --> 08:24.680
esa palabra no calza, digan.

08:25.680 --> 08:26.680
Si de alguna forma yo sé.

08:27.680 --> 08:31.680
Es decir, si yo logro detectar que esta oración es más probable que esta,

08:31.680 --> 08:35.680
de alguna forma, eso me va a ayudar en la tarea de reconocimiento.

08:36.680 --> 08:38.680
Lo mismo pasa con el reconocimiento del habla,

08:38.680 --> 08:41.680
de lo que hablamos el otro día con el speed recognition,

08:41.680 --> 08:43.680
y cuando yo hablo y digo una palabra, ustedes me escuchan.

08:45.680 --> 08:49.680
Entonces, los modelos de lenguaje sirven para ayudar en este tipo de tarea.

08:49.680 --> 08:52.680
Típicamente los modelos de lenguaje ayudan en otra tarea.

08:54.680 --> 08:55.680
Nos agregan mucha información.

08:58.680 --> 09:02.680
Entonces, cuando nosotros hacemos reconocimiento de escritura,

09:03.680 --> 09:05.680
un poco lo que decimos es,

09:07.680 --> 09:12.680
¿Cuál es la probabilidad de la oración origen dada la observación que tengo?

09:12.680 --> 09:15.680
Yo tengo una observación, ¿sí?

09:16.680 --> 09:18.680
¿Cuál es la probabilidad de una oración origen?

09:19.680 --> 09:25.680
Es proporcionar a la probabilidad de la observación dada la oración

09:25.680 --> 09:27.680
por la probabilidad de la oración.

09:28.680 --> 09:29.680
¿Y esto qué es?

09:29.680 --> 09:32.680
Eso es valles, es la regla de valles.

09:33.680 --> 09:36.680
Entonces, nosotros por valles sabemos eso,

09:36.680 --> 09:40.680
y como ven, acá aparece la noción de probabilidad de la oración.

09:40.680 --> 09:43.680
Por eso es que nos interesa conocer la probabilidad de la oración.

09:46.680 --> 09:50.680
Ahora, ¿Cómo calculamos la probabilidad de la oración?

09:51.680 --> 09:53.680
Bueno, hay algún ejemplo más, ¿no?

09:54.680 --> 09:56.680
Por ejemplo, en la traducción autonática,

09:59.680 --> 10:03.680
en la traducción autonática, si tenemos estos tres candidatos,

10:03.680 --> 10:06.680
nuevamente a mí me va a ayudar conocer el orden

10:07.680 --> 10:10.680
o saber cuál es la más probable en mi lenguaje.

10:16.680 --> 10:19.680
En la corrección de errores, como vimos en la vez pasada,

10:20.680 --> 10:24.680
hordas de botero es una secuencia muy de poca probabilidad,

10:24.680 --> 10:26.680
y pensemos un poquito,

10:30.680 --> 10:32.680
¿Preguntemos no?

10:36.680 --> 10:43.680
¿Por qué esta oración no les parece que sea muy probable?

10:44.680 --> 10:49.680
¿Qué nos podría determinar que esta oración no es muy probable?

10:59.680 --> 11:03.680
¿O esta? ¿Implementación a la educación ley?

11:04.680 --> 11:07.680
¿Por qué podemos suponer que esa no es probable?

11:15.680 --> 11:17.680
Bueno, a mí se me ocurren dos razones principales,

11:17.680 --> 11:20.680
dos aproximaciones, una es por la sintaxis, ¿no?

11:21.680 --> 11:24.680
La sintaxis del idioma pañón no es así.

11:25.680 --> 11:29.680
¿Nos decimos educación ley, educación que...?

11:30.680 --> 11:32.680
En la segunda, porque no publicamos la procesión.

11:32.680 --> 11:33.680
¿Por qué no qué?

11:33.680 --> 11:36.680
En la procesión, porque si tenemos sus y de botero,

11:36.680 --> 11:38.680
estamos publicando, ¿verdad?

11:41.680 --> 11:44.680
Ah, bueno, pero ese podría ser un sus de un tercero, ¿no?

11:45.680 --> 11:49.680
Acá seguramente lo que hay es un error toráfico de sus hordas de botero.

11:50.680 --> 11:52.680
O sea, acá tenemos un tema de sintaxis.

11:52.680 --> 11:54.680
Acá no tenemos un tema de sintaxis.

11:57.680 --> 12:00.680
Deberíamos conocer un poco de semántica para asociar botero,

12:00.680 --> 12:02.680
que pintaba mujeres gordas y entonces...

12:03.680 --> 12:05.680
O una aproximación un poco más humilde,

12:05.680 --> 12:08.680
que es la segunda, es la una aproximación más detalística,

12:08.680 --> 12:10.680
porque si nosotros...

12:10.680 --> 12:13.680
y que juega con el hecho de que tenemos grandes volúmenes de texto

12:13.680 --> 12:16.680
y de ahí el cambio de los modelos probabilísticos,

12:16.680 --> 12:20.680
es que sus gordas de botero seguramente apareció

12:20.680 --> 12:22.680
antes en mis corpus de texto

12:23.680 --> 12:25.680
y hordas de botero, no.

12:27.680 --> 12:29.680
Es una aproximación mucho más detalística.

12:29.680 --> 12:32.680
Eso es lo que vamos a hacer en los modelos de diagrama, justamente.

12:32.680 --> 12:34.680
A partir de grandes volúmenes de texto,

12:34.680 --> 12:36.680
detectar e calcular las probabilidades.

12:37.680 --> 12:39.680
Es una aproximación puramente estadística,

12:39.680 --> 12:41.680
es bien salvaje, es.

12:41.680 --> 12:43.680
Yo no sé qué estructura tiene esto,

12:43.680 --> 12:45.680
pero sé que esto no se dio nunca

12:45.680 --> 12:47.680
de botero, sí, muchas veces.

12:47.680 --> 12:49.680
Entonces, es más probable que me haya equivocado.

12:57.680 --> 12:59.680
A ver, relacionado con esto,

12:59.680 --> 13:01.680
ahora vamos a ver por qué está relacionado.

13:01.680 --> 13:04.680
Está el tema de la predicción de la siguiente palabra.

13:06.680 --> 13:09.680
¿Cuáles se imaginan que es la siguiente palabra

13:09.680 --> 13:11.680
a la primera oración?

13:12.680 --> 13:14.680
¿Cuál puede ser la siguiente palabra?

13:17.680 --> 13:18.680
¿Para?

13:19.680 --> 13:20.680
¿Para?

13:20.680 --> 13:21.680
Para.

13:21.680 --> 13:22.680
¿Para?

13:22.680 --> 13:23.680
¿Para?

13:23.680 --> 13:24.680
¿Para?

13:24.680 --> 13:25.680
¿Para?

13:25.680 --> 13:27.680
¿Para es una preposición, no?

13:33.680 --> 13:34.680
¿Qué más?

13:34.680 --> 13:36.680
¿Qué otra cosa puede decir ahí?

13:36.680 --> 13:38.680
¿Cuál, por ejemplo?

13:38.680 --> 13:40.680
¿Un pronóstico alentador?

13:40.680 --> 13:42.680
¿Un pronóstico alentador?

13:43.680 --> 13:45.680
¿O puede decir un pronóstico terrible?

13:45.680 --> 13:47.680
¿Un pronóstico...

13:48.680 --> 13:49.680
¿O qué otra cosa más?

13:49.680 --> 13:51.680
Hay uno más común para mí.

13:51.680 --> 13:54.680
Elmitió un pronóstico meteorológico, no?

13:55.680 --> 13:58.680
¿A raíz de este fenómeno se sucederán tormentas?

13:58.680 --> 14:00.680
Fuertes,

14:00.680 --> 14:02.680
importantes,

14:02.680 --> 14:04.680
muy.

14:04.680 --> 14:06.680
No creo que hay diga tormentas

14:06.680 --> 14:08.680
gatito, ¿no?

14:08.680 --> 14:11.680
Esto no es muy probable que sea la palabra siguiente.

14:11.680 --> 14:13.680
Nuevamente, ¿por qué sabemos esto?

14:13.680 --> 14:16.680
Porque es muy raro que hay un día tormentas gatito, digamos, ¿no?

14:17.680 --> 14:19.680
Entonces,

14:19.680 --> 14:21.680
esto que tenemos acá

14:21.680 --> 14:23.680
es

14:23.680 --> 14:26.680
las posibilidades que hay de siguiente palabra.

14:26.680 --> 14:29.680
Dadas todas las anteriores.

14:29.680 --> 14:32.680
Es decir, yo tengo todo el contexto, lo que se llama contexto,

14:32.680 --> 14:35.680
dado el contexto de la palabra que sigue acá.

14:35.680 --> 14:37.680
¿Sí?

14:37.680 --> 14:39.680
Una de las, lo que nosotros vamos a querer hacer

14:39.680 --> 14:41.680
en un modelo de lenguaje,

14:41.680 --> 14:43.680
como camino para calcular la probabilidad de una oración,

14:43.680 --> 14:45.680
es dado el contexto

14:45.680 --> 14:47.680
calcular la palabra.

14:47.680 --> 14:49.680
Siguiente.

14:49.680 --> 14:51.680
¿Sí?

14:53.680 --> 14:55.680
Rachas de viento fuerte de componente.

14:57.680 --> 14:59.680
Veremos que.

15:01.680 --> 15:03.680
Bueno, resulta ser que de los ejemplos que yo tomé,

15:03.680 --> 15:06.680
ah bueno, puse viento fuerte de componente,

15:06.680 --> 15:08.680
el linómede emitió pronótico especial,

15:08.680 --> 15:10.680
o sea que le ramos,

15:10.680 --> 15:12.680
se sonan tormentas fuertes,

15:12.680 --> 15:14.680
viento fuerte de componente sudo este,

15:14.680 --> 15:16.680
ejemplo, predicción.

15:19.680 --> 15:21.680
Vamos a poner un poquito de notación

15:21.680 --> 15:23.680
antes de que,

15:23.680 --> 15:25.680
antes de seguir,

15:25.680 --> 15:27.680
porque vamos a ver cómo enfrentamos este problema,

15:27.680 --> 15:29.680
es decir, ¿cómo calculamos esa probabilidad?

15:29.680 --> 15:31.680
Un poco de notación para seguir,

15:31.680 --> 15:33.680
yo lo que estoy diciendo es

15:34.680 --> 15:36.680
la probabilidad de que una variable aleatoria

15:36.680 --> 15:38.680
ahí

15:38.680 --> 15:40.680
valga,

15:40.680 --> 15:42.680
tome el valor conocimiento,

15:42.680 --> 15:44.680
en este caso tendría una variable aleatoria

15:44.680 --> 15:46.680
por cada posición en el texto,

15:46.680 --> 15:48.680
¿verdad?

15:48.680 --> 15:50.680
Tengo una X1, que es la primera palabra, aquí dos,

15:50.680 --> 15:52.680
que es la segunda, aquí tres.

15:52.680 --> 15:54.680
Son variables aleatorias, que la variable aleatoria

15:54.680 --> 15:56.680
es un mapeo, es una función que mapea

15:56.680 --> 15:58.680
de un evento, un número entre 0 y 1.

15:58.680 --> 16:00.680
¿La probabilidad de una?

16:00.680 --> 16:02.680
La probabilidad de una.

16:04.680 --> 16:06.680
Perdón.

16:06.680 --> 16:08.680
Perdón.

16:08.680 --> 16:10.680
Bueno, no vamos a entrar en definiciones,

16:10.680 --> 16:12.680
mapea con un real y la probabilidad

16:12.680 --> 16:14.680
me devuelve un número entre 0 y 1,

16:14.680 --> 16:16.680
es decir, yo defino la probabilidad

16:16.680 --> 16:18.680
de una variable aleatoria,

16:18.680 --> 16:20.680
como

16:20.680 --> 16:22.680
la distribución

16:22.680 --> 16:24.680
de probabilidad de una variable aleatoria,

16:24.680 --> 16:26.680
dado los diferentes valores que puede tomar

16:27.680 --> 16:29.680
¿Cuál es el valor de cada uno de ellos?

16:29.680 --> 16:31.680
¿Sí?

16:31.680 --> 16:33.680
Y esto, ¿cuál es el rango?

16:33.680 --> 16:35.680
¿Qué valor es probable que tiene acá

16:35.680 --> 16:37.680
una variable aleatoria

16:37.680 --> 16:39.680
que refiera palabras?

16:42.680 --> 16:44.680
Todo el vocabulario, ¿no?

16:44.680 --> 16:46.680
Todas las palabras diferentes que yo puedo tener.

16:46.680 --> 16:48.680
¿De acuerdo?

16:48.680 --> 16:50.680
Entonces nosotros vamos a poner

16:50.680 --> 16:52.680
en notación probabilidad de conocimiento,

16:52.680 --> 16:54.680
de que la palabra sea conocimiento.

16:56.680 --> 16:58.680
Vamos a denotar W1n

16:58.680 --> 17:00.680
1n

17:00.680 --> 17:02.680
a la secuencia

17:02.680 --> 17:04.680
de palabras W1

17:04.680 --> 17:06.680
W2

17:06.680 --> 17:08.680
Wn, por ejemplo, en una eración

17:08.680 --> 17:10.680
y vamos a decir

17:10.680 --> 17:12.680
vamos a decir que vamos a hablar

17:12.680 --> 17:14.680
de la probabilidad de

17:14.680 --> 17:16.680
la secuencia de palabras queriendo decir, bueno,

17:16.680 --> 17:18.680
la probabilidad de la que la primera sea W1

17:18.680 --> 17:20.680
que la segunda sea W2, etc.

17:20.680 --> 17:22.680
¿De acuerdo?

17:22.680 --> 17:24.680
O sea que esta distribución de probabilidad

17:26.680 --> 17:28.680
tiene como rango

17:28.680 --> 17:30.680
todas las secuencias posibles de palabras.

17:30.680 --> 17:32.680
¿Sí?

17:32.680 --> 17:34.680
O sea que si mi vocabulario es B

17:34.680 --> 17:36.680
tengo

17:36.680 --> 17:38.680
N a la B

17:40.680 --> 17:42.680
V a la N

17:42.680 --> 17:44.680
V a la N

17:44.680 --> 17:46.680
Posita V a la N

17:46.680 --> 17:48.680
O sea que es enorme

17:48.680 --> 17:50.680
esencialmente, ¿no?

17:50.680 --> 17:52.680
Todas las posibles secuencias.

17:52.680 --> 17:54.680
Y

17:54.680 --> 17:56.680
vamos a recordar

17:56.680 --> 17:58.680
la chain rule

17:58.680 --> 18:00.680
la regla de multiplicación

18:00.680 --> 18:02.680
de las probabilidades

18:02.680 --> 18:04.680
que es, si yo tengo la probabilidad de una

18:04.680 --> 18:06.680
secuencia de palabras

18:06.680 --> 18:08.680
W1, Wn

18:08.680 --> 18:10.680
esto es

18:10.680 --> 18:12.680
la probabilidad de la primera

18:12.680 --> 18:14.680
palabra, de alguna forma

18:14.680 --> 18:16.680
la calculo

18:16.680 --> 18:18.680
por la probabilidad de la segunda dada

18:18.680 --> 18:20.680
la primera, dado que la primera

18:20.680 --> 18:22.680
fue W1

18:22.680 --> 18:24.680
observen acá que

18:24.680 --> 18:26.680
no son independientes

18:26.680 --> 18:28.680
es decir, las palabras por definición

18:28.680 --> 18:30.680
acá, no son eventos independientes

18:30.680 --> 18:32.680
es decir

18:32.680 --> 18:34.680
tengo una cierta probabilidad de que

18:34.680 --> 18:36.680
empiece con W1

18:36.680 --> 18:38.680
la multiplico por la probabilidad de que

18:38.680 --> 18:40.680
la segunda sea W2, dado que la primera

18:40.680 --> 18:42.680
fue W1

18:42.680 --> 18:44.680
por la probabilidad que

18:44.680 --> 18:46.680
la tercera sea W3, dado que las dos primeras

18:46.680 --> 18:48.680
fueron uno de hoy así

18:48.680 --> 18:50.680
de acuerdo

18:50.680 --> 18:52.680
de esa forma con esta regla yo

18:52.680 --> 18:54.680
y al final Wn

18:54.680 --> 18:56.680
la última dada todas las anteriores

18:56.680 --> 18:58.680
esto se llama

18:58.680 --> 19:00.680
regla de la cadena

19:00.680 --> 19:02.680
yo con la regla de la cadena

19:02.680 --> 19:04.680
puedo calcular la probabilidad

19:06.680 --> 19:08.680
de una secuencia o de una oración

19:08.680 --> 19:10.680
dada la secuencia

19:10.680 --> 19:12.680
si logro calcular estas

19:12.680 --> 19:14.680
probabilidades, o sea

19:14.680 --> 19:16.680
si logro calcular

19:16.680 --> 19:18.680
predecir las palabras

19:18.680 --> 19:20.680
correctamente

19:20.680 --> 19:22.680
voy a

19:22.680 --> 19:24.680
poder predecir la secuencia

19:24.680 --> 19:26.680
de esa forma paso de la predicción al cálculo

19:26.680 --> 19:28.680
de toda la probabilidad de la oración ¿se entienden?

19:32.680 --> 19:34.680
bien

19:34.680 --> 19:36.680
entonces vamos a quedarnos con esa notación

19:38.680 --> 19:40.680
entonces yo digo bueno

19:40.680 --> 19:42.680
un ejemplo no, si yo quiero saber

19:42.680 --> 19:44.680
la probabilidad de

19:44.680 --> 19:46.680
viento fuerte de componente sudoeste

19:46.680 --> 19:48.680
como el que está soplando

19:48.680 --> 19:50.680
no sé si de componente sudoeste pero es fuerte

19:50.680 --> 19:52.680
es la probabilidad de viento

19:52.680 --> 19:54.680
por la probabilidad de fuerte

19:54.680 --> 19:56.680
dado viento por la probabilidad de

19:56.680 --> 19:58.680
dado viento fuerte etc.

19:58.680 --> 20:00.680
nada menos que la regla de la cadena

20:06.680 --> 20:08.680
entonces

20:08.680 --> 20:10.680
yo quiero saber la última

20:10.680 --> 20:12.680
p de sudoeste dado viento fuerte

20:12.680 --> 20:14.680
de componente

20:14.680 --> 20:16.680
y vos con google por ejemplo y digo

20:16.680 --> 20:18.680
bueno viento fuerte de componente

20:18.680 --> 20:20.680
aparece 9.230 veces

20:22.680 --> 20:24.680
viento fuerte de componente sudoeste

20:24.680 --> 20:26.680
aparece 347 veces

20:26.680 --> 20:28.680
y yo entonces

20:28.680 --> 20:30.680
voy a estimar la probabilidad de esa

20:30.680 --> 20:32.680
por medio de conteos

20:32.680 --> 20:34.680
la cantidad de veces que apareció

20:34.680 --> 20:36.680
viento fuerte de componente sudoeste

20:36.680 --> 20:38.680
dividido la cantidad de veces que aparece

20:38.680 --> 20:40.680
fuerte de componente

20:40.680 --> 20:42.680
dividido 9.230

20:42.680 --> 20:44.680
¿Aguardo?

20:44.680 --> 20:46.680
y esta es la probabilidad

20:46.680 --> 20:48.680
de que la siguiente palabra

20:48.680 --> 20:50.680
sea sudoeste en mi estimación

20:50.680 --> 20:52.680
si ustedes se fijan

20:52.680 --> 20:54.680
esto es una probabilidad

20:54.680 --> 20:56.680
porque

20:56.680 --> 20:58.680
contando

20:58.680 --> 21:00.680
todas las palabras posibles que pueden seguir

21:00.680 --> 21:02.680
acá si yo logro determinar cuáles son

21:04.680 --> 21:06.680
yo sé que van a ver 9.230

21:06.680 --> 21:08.680
van a sumar 9.230

21:08.680 --> 21:10.680
¿no?

21:10.680 --> 21:12.680
es decir todos los casos posibles

21:12.680 --> 21:14.680
miro todos los casos junto a lo que es la siguiente palabra

21:16.680 --> 21:18.680
eso hace que como esto me va a dar

21:18.680 --> 21:20.680
9.230 a la suma de todas las cantidades

21:20.680 --> 21:22.680
esto va a dar uno

21:22.680 --> 21:24.680
el total

21:24.680 --> 21:26.680
entonces esto sí es una distribución de probabilidad

21:26.680 --> 21:28.680
entonces que estamos bien

21:28.680 --> 21:30.680
efectivamente aquello es una probabilidad

21:30.680 --> 21:32.680
¿de acuerdo? esto lo que me dice es

21:32.680 --> 21:34.680
bueno

21:34.680 --> 21:36.680
el 3,76% de las veces

21:36.680 --> 21:38.680
es sudoeste la siguiente palabra

21:48.680 --> 21:50.680
eso que acabamos de hacer

21:50.680 --> 21:52.680
es estimar la probabilidad

21:52.680 --> 21:54.680
a partir de la frecuencia

21:54.680 --> 21:56.680
de ocurrencia en un cuerpo grande

21:56.680 --> 21:58.680
eso Google es un cuerpo grande

21:58.680 --> 22:00.680
muy grande

22:00.680 --> 22:02.680
y eso se llama principio máximo

22:02.680 --> 22:04.680
pero similitud que lo vimos en la de pasada

22:04.680 --> 22:06.680
es trato de hacer

22:06.680 --> 22:08.680
calcular la probabilidad en base

22:08.680 --> 22:10.680
a lo mejor posible

22:10.680 --> 22:12.680
a los datos que tengo

22:12.680 --> 22:14.680
es decir considero

22:14.680 --> 22:16.680
yo estoy considerando que los datos que tengo

22:16.680 --> 22:18.680
es decir el corpo de Google

22:18.680 --> 22:20.680
es una buena aproximación

22:20.680 --> 22:22.680
del mundo de lenguaje

22:22.680 --> 22:24.680
en realidad

22:24.680 --> 22:26.680
yo no sé si en realidad

22:26.680 --> 22:28.680
efectivamente cuando los seres humanos hablamos

22:28.680 --> 22:30.680
hay un 3,76%

22:30.680 --> 22:32.680
de probabilidad

22:32.680 --> 22:34.680
de que después de decir

22:34.680 --> 22:36.680
viento fuerte componente

22:36.680 --> 22:38.680
viene sudoeste

22:38.680 --> 22:40.680
pero el corpo de Google

22:40.680 --> 22:42.680
que es lo mejor que tengo como aproximación

22:42.680 --> 22:44.680
me dice eso y eso es lo que yo utilizo

22:44.680 --> 22:46.680
como un estimador de máxima verosimilitud

22:46.680 --> 22:48.680
es lo mejor que puedo acercarme

22:48.680 --> 22:50.680
con el cuerpo que tengo

22:50.680 --> 22:52.680
eso es lo que vamos a hacer todo el tiempo acá

22:52.680 --> 22:54.680
calcular

22:54.680 --> 22:56.680
componentes de máxima verosimilitud

22:58.680 --> 23:00.680
pero tenemos algún problema

23:00.680 --> 23:02.680
y es

23:02.680 --> 23:04.680
en el otro casos

23:04.680 --> 23:06.680
dice, a raíz de estos fenómenos se producirán

23:06.680 --> 23:08.680
tormentas fuertes

23:08.680 --> 23:10.680
la probabilidad de fuertes

23:10.680 --> 23:12.680
y a raíz de estos fenómenos se producirán tormentas

23:14.680 --> 23:16.680
tienen un problema y es que

23:16.680 --> 23:18.680
nunca apareció en mi corpus

23:18.680 --> 23:20.680
a raíz de estos fenómenos

23:20.680 --> 23:22.680
se producirán tormentas

23:22.680 --> 23:24.680
y nunca apareció en mi corpus

23:24.680 --> 23:26.680
a raíz de estos fenómenos se producirán tormentas fuertes

23:26.680 --> 23:28.680
y

23:28.680 --> 23:36.440
Y eso nos da una horrible visión por cero, que queremos evitar, o sea que nuestra probabilidad

23:36.440 --> 23:41.720
da infinito, no sé, no está definida.

23:41.720 --> 23:46.560
Esto, una pregunta, ¿esto les parece que es un fenómeno común o no, que nos puede

23:46.560 --> 23:54.120
pasar cuando estemos estimando todo el tiempo, porque por más grande que sea el corpus, el

23:54.120 --> 23:58.960
lenguaje es muy creativo?

23:58.960 --> 24:05.080
Entonces tenemos que buscar forma y además porque estamos haciendo un conteo de palabras

24:05.080 --> 24:06.080
de oraciones muy largas.

24:06.080 --> 24:12.320
O sea que la regla de la cadena no resuelve mi problema, porque yo, una aproximación

24:12.320 --> 24:17.600
bien naif para calcular la probabilidad de calcular toda la secuencia posible, ¿sí?

24:17.600 --> 24:21.120
¿cuántas veces aparece la secuencia que quiero calcular, la oración del total de

24:21.120 --> 24:22.120
oraciones?

24:22.120 --> 24:25.560
Bueno, tengo un corpus evidentemente grande, pero esta aproximación tampoco nos ayuda

24:25.560 --> 24:31.960
mucho porque sigo teniendo contestos muy largos, porque si ustedes se fijan en la regla de

24:31.960 --> 24:37.560
la cadena, bueno, en lo que acabamos de hacer, la última probabilidad es casi la misma

24:37.560 --> 24:51.040
que la primera, menos una palabra, tengo que buscar una forma de achicar eso.

24:51.040 --> 25:00.680
Entonces, una de las ideas fuerzas para computar esta probabilidad es en lugar de tomar todas

25:00.680 --> 25:08.120
las palabras, tomar sobre las últimas, es decir, yo me quedo con las últimas n menos

25:08.120 --> 25:20.400
un palabras, ¿sí? n menos n, bueno. ¿sí? En esto es enigrante, ¿no? Y las otras no

25:20.400 --> 25:29.120
las considero, digo, bueno, mi humilde aproximación para que esto se pueda volver manejable es

25:29.120 --> 25:32.960
decir, bueno, yo en realidad solamente me importan las últimas palabras afectan a la

25:32.960 --> 25:42.320
que voy a predecir, son las últimas. Y de eso se tratan los modelos enigramas que

25:42.320 --> 25:45.840
utilizan lo que se llama, eso que acabo de decir, yo llamo hipótesis de marcovo, hipótesis

25:45.840 --> 25:56.880
marcoviana. Solamente las últimas palabras afectan a siguiente, hay un límite. Y fíjense

25:56.880 --> 26:04.080
que en la hipótesis de bigrama, yo digo, cada palabra es la próxima por la anterior,

26:04.080 --> 26:09.640
simplemente, estoy diciendo una cosa tan sencilla como la última, la última palabra es la

26:09.640 --> 26:15.520
única, cada palabra es la siguiente, pero las anteriores no. Es muy fuerte, ¿no? Y

26:15.520 --> 26:24.440
de trigramas son dos y con n, con n son n. ¿sí? Con la hipótesis de bigrama, mi probabilidad

26:24.440 --> 26:31.160
es mucho más sencilla que antes, porque es como cada palabra solo depende, vamos a mirar,

26:31.160 --> 26:40.080
uno bueno uno no está más, pero cada palabra depende de la anterior, simplemente me queda

26:40.080 --> 26:52.400
que la probabilidad de una secuencia es la probabilidad de la primera, por la probabilidad

26:52.400 --> 27:12.200
de la segunda a la primera, por la probabilidad de la tercera a la segunda, etcétera. ¿Le

27:12.200 --> 27:19.880
guardo? Acá nos falta este PW1 en esa fórmula, pero no nos preocupa demasiado porque eso

27:19.880 --> 27:24.760
lo resolvemos poniendo una marca al comienzo de la secuencia que siempre vale uno, su probabilidad,

27:24.760 --> 27:31.120
es decir, que toda la gración empieza con una marca. Y si no, multiplico acá, ¿no? Si

27:31.120 --> 27:36.920
no, si lo quiero hacer de otra forma, agregue un PW0 acá y lo mismo. Pero esencialmente

27:36.920 --> 27:41.200
lo importante acá es que esto se transforma en una simple multiplicación de probabilidades

27:41.200 --> 27:49.000
de una palabra dada en anterior. ¿Y cómo hago para calcular esto? ¿Cómo puedo calcular

27:49.000 --> 28:00.400
esto acá? ¿Cómo calcula la probabilidad de una palabra dada en anterior? Contando,

28:00.400 --> 28:04.120
pero solamente tienen cuenta dos, lo cual lo vuelve un problema mucho más manejable.

28:04.120 --> 28:13.440
Y eso es justo lo que vamos a hacer. Un modelo de lenguaje intenta predecir la próxima palabra

28:13.440 --> 28:18.680
de una oración a partir de las n menos una anteriores y, por supuesto, que importa el

28:18.680 --> 28:29.320
orden en ese cálculo, ¿no? También tenemos que plantearnos cuando hagamos los engramas,

28:30.320 --> 28:34.920
cuando calculemos la probabilidad de una palabra, bueno, cosas que ya hemos conversado. ¿Qué

28:34.920 --> 28:44.720
elemento vamos a contar? Por ejemplo, tengo un tema de toquenización, esta coma, ¿la tengo

28:44.720 --> 28:50.040
que considerar un engrama o no la tengo que considerar un engrama? ¿La tengo que considerar

28:50.040 --> 28:54.760
un token o no la tengo que considerar un token? ¿Me interesa? Bueno, eso seguramente va a

28:54.760 --> 28:57.880
depender un poco de la aplicación en la que lo estoy aplicando, a lo que lo estoy utilizando.

28:57.880 --> 29:07.640
O tengo un cuerpo oral donde tengo disfluencias, disfluencias, creo que se llama esto. ¿Qué tengo

29:07.640 --> 29:12.120
que hacer con las mayúsculas? ¿Qué hago con la forma flexionada? Todo los problemas de la

29:12.120 --> 29:16.920
toquenización me parecen en los engramas, es decir, estos son cascadas, digamos, ¿no? Yo acabo

29:16.920 --> 29:21.400
de tener la toquenización realizada. En realidad no hay respuesta universal, depende de la tarea

29:21.480 --> 29:27.240
que estamos haciendo. Por ejemplo, típicamente los corporeales están todos pasados a mayúsculas,

29:27.240 --> 29:39.800
porque como son más continuos, la identificación de oraciones no es tan importante. Si yo voy

29:39.800 --> 29:45.440
a hacer análisis, si estoy haciendo un análisis de cómo se usan los signos de puntuación

29:45.440 --> 29:48.960
en mi lenguaje, obviamente la coma la tengo que identificar, sino capaz que no me interese.

29:49.960 --> 29:56.440
O me puede interesar todo estos, mapearlos a una cosa sola que se llama signo de puntuación

29:58.040 --> 30:03.880
y juntar los puntos con las comas. Van a tener que hacer eso en el laboratorio.

30:03.880 --> 30:09.240
Ya se van a colar. Bueno, nada, se necesita un pretratamiento disponible al menos para

30:09.240 --> 30:16.480
las oraciones y el modelo no hay modelos generales. También va a depender un poco,

30:16.960 --> 30:23.680
nuestros números van a depender de la cantidad de palabras. El dictionary,

30:23.680 --> 30:31.800
el Oxford English dictionary tiene 290.000 entradas, el Tresor de la langue francés tiene 54.000

30:31.800 --> 30:38.880
y el dicionario de la radio 88.000. ¿Por qué les parece que hay tantas más acá que acá?

30:39.880 --> 30:47.600
Porque el dicionario no parece en la forma flexionada y el español está mucho más flexionado.

30:51.680 --> 30:57.360
O sea que el inglés la tiene que arreglar más solito. Bueno, y después tenemos corpus.

30:58.800 --> 31:03.800
Esto ya hablamos un poco y aquello distinguir entre el número de token que son la cantidad de

31:03.800 --> 31:07.560
ocurrencias que hay en el texto y el número de palabras distintas, el vocabular.

31:14.040 --> 31:17.360
Acá está la respuesta a la pregunta que hacíamos antes. ¿Cómo estimamos los

31:17.360 --> 31:21.640
bigramas utilizando otra vez lo que se llama un estimador de máxima verosimilituos,

31:21.640 --> 31:26.120
lo que se llama métodos de frecuencias relativas? Que es, cuento las cantidades de

31:26.280 --> 31:35.840
la cantidad de veces que apareció una palabra con, por ejemplo, la probabilidad de fuerte,

31:35.840 --> 31:44.720
dado viento, se aproxima como la cantidad de veces que aparece viento fuerte.

31:44.960 --> 31:57.320
Por la dividida de la cantidad de veces que apareció

32:06.400 --> 32:14.360
dividido todas las posibles continuaciones. ¿De acuerdo? Viento fuerte, viento calmo,

32:14.360 --> 32:22.760
viento, viento dile, viento, no sé, lo que quieras. Y sumo todas las posibles,

32:22.760 --> 32:25.920
lo que estoy haciendo es normalizando, como hablamos al principio de, como hablamos acá,

32:25.920 --> 32:32.080
¿no? Estoy normalizando. Ahora, esto aquí es equivalente. ¿Cómo puedo simplificar esto?

32:32.320 --> 32:42.000
Si yo tengo todas las veces que aparecen viento fuerte,

32:42.000 --> 32:48.000
viento calmo, no sé, ¿cuál es la suma de todo eso?

32:53.840 --> 32:57.720
Es la cantidad de veces que apareció viento. Esto es igual a la cantidad de

32:57.720 --> 33:01.120
secas que aparecen vientos en el cuerpo. ¿Cómo puedo recordar?

33:03.240 --> 33:05.280
Como son todas las posibles ocurrencias.

33:10.680 --> 33:15.720
Ahí tenemos la simplificación y, además, para tener en cuenta la primera y última

33:15.720 --> 33:20.040
palabra en oración, le vamos a agregar siempre los símbolos de comienzo y de fin. Eso para

33:20.040 --> 33:27.080
asegurarnos de que, para no tener que calcular separada la probabilidad de la primera palabra.

33:27.240 --> 33:34.760
Yo sé que la primera palabra siempre es ese y calculo la probabilidad de la primera en el texto,

33:34.760 --> 33:43.080
digamos, ponerle el dado que la anterior era ese. De acuerdo? Y así lo dejo en una sola forma.

33:44.840 --> 33:53.960
Por ejemplo, si supongamos que yo tengo ese cuerpo, ¿no? Juan abrió la puerta, el viento abrió

33:54.040 --> 34:00.520
la puerta, enero abrió limones en tus mejillas nuevas, Juan recoge limones. Y quiero saber

34:00.520 --> 34:06.600
la probabilidad de estas oraciones. Evidentemente no las tengo en el cuerpo, o sea, que no puedo

34:06.600 --> 34:17.320
contar directamente. Pero quiero utilizar un modelo de diagrama para calcular. Y con lo que

34:17.400 --> 34:26.440
sabemos es bastante sencillo. Primero que nada decimos, bueno, la probabilidad de Juan abrió

34:26.440 --> 34:35.560
limones es probabilidad de Juan dado el comienzo. Probabilidad de comienzo siempre es uno. Probabilidad

34:35.560 --> 34:44.040
de abrió dado Juan, probabilidad de limones dado abrió, etcétera, ¿no? Fíjense que la probabilidad

34:44.040 --> 34:49.240
Juan dado el comienzo de la cantidad de veces que apareció Juan en la marca de comienzo dividido

34:49.240 --> 34:59.960
de la cantidad de marcas de comienzo que es uno. Entonces esto me da 2 de 4. Ah, porque hay cuatro

34:59.960 --> 35:04.680
oraciones. Perdón. Claro, porque yo estoy haciendo contegos directamente. No, no estoy haciendo

35:05.640 --> 35:16.040
2 de 4 veces arrancó con Juan, ¿sí? Juan abrió es una de dos. Ya había aparecido Juan abrió

35:17.160 --> 35:23.880
en el corpus y Juan aparece dos veces. O sea, de dos veces le apareció Juan y la siguiente apareció

35:23.880 --> 35:31.640
una vez abrió. Y así sigo multiplicando y como me multiplico la fracción y me da, bueno, 0-0-42.

35:31.640 --> 35:43.000
Esa es la probabilidad de Juan abrió el limón. Enero abrió la puerta 0-17. Esto no tiene mucho

35:43.000 --> 35:48.200
sentido, ¿no? A ver, justamente el hecho de que sea un ejemplo de juguete le hace perder la gracia

35:48.200 --> 35:57.160
todo esto porque esto funciona porque tengo grandes volúmenes, sino es una pasada. ¿Y acá que nos

35:57.160 --> 36:21.640
pasó? ¿Qué puede haber pasado acá? La palabra come nunca está. ¿Y en la puerta?

36:21.640 --> 36:31.400
En la puerta está. La primera se explica porque come nunca está, ¿no?

36:31.400 --> 36:45.640
Creo que está así. Perdón. La así, la puerta.

36:45.640 --> 37:04.040
¿Por qué da 0? Porque lo que no está es en la. En la no aparece nunca. Si ustedes miran acá la

37:04.040 --> 37:09.160
probabilidad de, perdón, la cantidad de, la probabilidad de esto es la probabilidad de que

37:09.160 --> 37:16.680
empiece con él, ya tenemos un problema con el comienzo con él porque creo que no hay ninguna.

37:16.680 --> 37:24.200
Ninguna empieza con él y eso ya tiene un problema. Y además en la tampoco está. O sea que el

37:24.200 --> 37:32.920
conteo me va a dar 0. Si el bigrama no aparece en el cuerpo de entrenamiento, siempre mi

37:33.240 --> 37:41.000
problema da 0. Y más interesante aún, si cualquier bigrama de todos los que aparecen en la oración

37:41.000 --> 37:51.640
da 0, la probabilidad de la oración es 0. Eso es un gran problema. Resolver el problema de eso,

37:51.640 --> 37:55.640
de lo que se llama el suavizado de engramas que vamos a ver cómo. Tenemos que buscar alguna forma

37:55.640 --> 38:01.480
de resolver eso que nos va a pasar siempre. Es decir, como nuestro cuerpo nunca puede ser tan,

38:01.880 --> 38:06.520
aunque solo sean dos palabras, igual puede parecerme parezca de palabras que no aparecieron y yo no me

38:06.520 --> 38:20.200
puedo trascar con eso. ¿De acuerdo? Bueno, nos queda ese pendiente de 0 que lo vamos a ver

38:20.200 --> 38:23.560
después porque ya te quiero comentar alguna cosa. Pero vamos a acordarnos de eso, que tuvimos este

38:23.640 --> 38:35.160
problema pendiente. Bien, en general, ustedes dirán, bueno, pero ¿cuál es el mejor N? ¿Por qué? ¿Cuál es el tema?

38:35.160 --> 38:49.320
¿Es? ¿Cuanto? ¿Cuanto más largo sea el tigrama que yo utilizo? Más información tengo de contexto. Es decir,

38:49.560 --> 38:53.400
intuitivamente es mejor estimar con cinco palabras que con una.

38:57.400 --> 39:00.520
Estamos guardados con eso. ¿Cuál es el problema de los tigramas largos?

39:05.000 --> 39:06.680
¿Por qué no puedo usar 15?

39:12.280 --> 39:15.640
Porque tenemos el mismo problema por el que llegamos acá, que con 15 no tengo

39:15.720 --> 39:19.720
cuerpo suficientemente grande como para que aparezcan esa ocurrencia.

39:22.040 --> 39:28.040
Entonces, ese balance entre cantidad de ocurrencia, porque si yo no tengo una buena estimación de la

39:28.040 --> 39:32.360
cantidad de ocurrencia, no voy a poder estimar bien la probabilidad. Con lo que yo estoy estimando la

39:32.360 --> 39:37.080
probabilidad partido en conteos. Si yo tengo una, dos, tres ocurrencias, seguramente esa probabilidad

39:37.080 --> 39:42.440
sea artificial. Porque si hubo una ocurrencia en un cuerpo de miles de millones de palabras,

39:42.920 --> 39:50.040
no me está diciendo mucho. Generalmente en igualtré se obtienen buenos resultados.

39:54.120 --> 40:02.200
Por lo menos para aproximarse da muy bien. Google hace unos años atrás sacó un cuerpo de negra,

40:02.200 --> 40:09.400
un sí, una lista de negramas de hasta cinco. Me acuerdo en esa época bien en ese.

40:13.400 --> 40:16.920
O sea que determinar n va a depender un poco de la tarea y ese se me dio a ojo, digamos, pues una

40:16.920 --> 40:23.000
tarea un poco complica. Ahora vamos a ver un poco de evaluación. Ita y lo que decíamos, ¿no? ¿Se

40:23.000 --> 40:26.840
agregan? Como cuando son trigramas, tengo que agregar dos símbolos al comienzo de la oración.

40:28.600 --> 40:29.080
Tengo a poner.

40:32.600 --> 40:40.120
Enero, abrió. Porque yo necesito dos de contexto para calcular el trigramo en detalle.

40:40.840 --> 40:53.880
Y bueno, y la pregunta es cómo calculamos

40:57.320 --> 41:00.920
desde el punto de vista metodológico, cómo hacemos para calcular buenas probabilidades.

41:00.920 --> 41:06.680
Ya vimos cómo se hace el conteo. Ya ahora quiero ver cómo organiza el corpus y me parece

41:06.760 --> 41:11.240
que es interesante ver esto porque nos va a pasar en muchas cosas, en este tema de

41:11.240 --> 41:17.160
preservamiento del lenguaje natural y que muchas veces induce el mal uso metodológico de estas cosas

41:19.560 --> 41:24.840
lleva error. Entonces me parece que vale la pena comentarlo esto.

41:25.880 --> 41:32.840
Yo. Yo dije que iba a hacer conteo para calcular las probabilidades, ¿no? Entonces yo por acá tengo

41:32.920 --> 41:36.600
un corpus, un corpus de texto.

41:41.960 --> 41:48.600
Sí. Entonces esencialmente lo que tengo son muchos textos, ¿no? Obviamente, esencialmente no,

41:48.600 --> 41:51.000
tengo muchos textos. Esa es la definición de corpus.

41:51.320 --> 42:05.640
Y yo voy a establecer, voy a crear un modelo de una, de un, un modelo de un lenguaje. Es decir,

42:05.640 --> 42:11.160
yo lo que quiero construir con esto de las probabilidades de las olaciones es un modelo

42:11.160 --> 42:15.560
del idioma español. Yo tengo un corpus de texto en español y quiero hacer un modelo del idioma

42:16.040 --> 42:21.720
español. Supongo que yo entren un modelo, entrenar el modelo en este caso quiere decir

42:21.720 --> 42:27.800
calcular todas esas probabilidades. ¿Cómo hago para saber qué tan bueno es?

42:30.200 --> 42:31.400
¿Sí? ¿Cómo lo evalúo?

42:35.400 --> 42:38.280
Supongo que yo, ahora vamos a hablar de cuál es la medida, pero supongo que yo tengo una

42:38.360 --> 42:42.360
medida de performa que me dice, bueno, aplicarle tu modelo a este texto.

42:44.920 --> 42:49.400
Sí. Supongamos que la medida es el que le asigne, ahora vamos a ver por qué, pero el que le asigne

42:49.400 --> 42:57.720
mayor probabilidad a todo el texto, a las oraciones del texto, es el mejor. El mejor modelo es el que

42:57.720 --> 43:07.400
la asigna probabilidad mayor a las oraciones que tengo en el texto. Si yo aplico mi método,

43:08.040 --> 43:12.600
mi modelo, o sea, evalúo mi modelo. Sobre este mismo corpus, ¿qué problema tengo?

43:14.600 --> 43:20.520
Que me va a dar bárbaro porque lo calculé ahí. Es decir, yo nunca puedo, nunca, pero nunca,

43:20.520 --> 43:26.360
nunca, evaluar un modelo en el mismo corpus en el que entrené. Esto aplica siempre. Cabe que

43:26.360 --> 43:30.680
yo utilizo un método estadístico, aprendizaje automático. Lo más importante a saber en el

43:30.680 --> 43:37.080
aprendizaje automático es nunca evalúes tu modelo en un corpus, en el mismo corpus que entrenaste,

43:37.160 --> 43:39.720
porque por definiciones estás haciendo trampa, eso lo que se llama

43:41.320 --> 43:47.480
sobreajuste. Vos sobreajustas a tu corpus de entrenamiento. Entonces yo lo que voy a hacer es

43:48.520 --> 43:55.320
dividir mi corpus en dos y voy a decir, este es el corpus de entrenamiento,

43:57.800 --> 43:58.680
voy a poner en inglés,

44:01.320 --> 44:02.360
y el corpus de evaluación.

44:02.360 --> 44:18.280
Entonces lo que yo voy a hacer es entrenar y ¿cuánto se para acá? Bueno,

44:21.960 --> 44:24.600
la regla más o menos es 80-20.

44:32.360 --> 44:36.760
Pregunto, ¿por qué me interesaría que esto fuera lo más grande posible?

44:43.800 --> 44:50.760
Para que tener más información. ¿Y por qué no abuso 90-10 o 95-5 o 97-3?

44:54.520 --> 44:54.760
¿Cómo?

44:55.720 --> 44:57.560
¿Quieres evaluarlo con una cantidad de datos?

44:57.560 --> 45:02.040
Tengo que solucionar ese balance, entretener una cantidad razonable de datos para hablar,

45:02.040 --> 45:09.320
porque si yo le evaluo sobre una oración, la varianza es muy grande, es decir, la posibilidad

45:09.320 --> 45:14.920
de equivocarme es muy grande. Entonces una regla es más o menos 80-20.

45:15.880 --> 45:16.680
¿Sí?

45:23.880 --> 45:29.800
Y bueno, ahí habla de 90-10, yo tengo la regla de 80-20.

45:32.920 --> 45:38.200
Va a surgir un problema adicional acá, y es que ahora lo que voy a mover es,

45:38.520 --> 45:47.720
por ejemplo, si yo quiero saber cuántos elegí el n, ¿no?

45:49.720 --> 45:58.040
Yo quiero elegir el n, yo necesito, lo que puedo hacer es pruebo con un n acá,

45:58.360 --> 46:08.360
modelo 1, n igual 2, y hago modelo 2, n igual 3.

46:14.360 --> 46:21.320
Esto es un poco más útil de ver. Y lo evaluo acá y digo m1 y m2, y me quedo con el que me da mejor.

46:22.280 --> 46:27.080
Eso metodologicamente no está bien. ¿Por qué?

46:30.760 --> 46:37.240
Y esto es una de las cosas que es más difícil de entender a veces. Si yo pruebo los dos modelos

46:37.240 --> 46:42.360
acá, de alguna forma también estoy haciendo trampa, porque supónganse que yo tengo no dos

46:42.360 --> 46:47.240
parámetros, porque acá tengo un parámetro que tiene dos valores. Supongamos que yo quiero

46:47.320 --> 46:56.040
ajustar otro parámetro de mi método que puede tomar 500 valores posibles. Si yo hago 500

46:56.040 --> 47:05.000
entrenamientos y 500 pruebas, muy probablemente también esté ajustando acá, esté sobreajustando

47:05.000 --> 47:09.480
acá, porque estoy eligiendo de los 500, y a veces pueden ser miles o cientos de miles,

47:10.920 --> 47:15.400
el que mejor anda en este cuerpo de evaluación, o sea que estoy sobreajustando el cuerpo de evaluación.

47:15.720 --> 47:21.000
Entonces, para el ajuste de parámetros, yo usualmente lo que tengo que hacer es definir

47:22.280 --> 47:30.760
dividir este corpus, sacar un pedacito del cuerpo de entrenamiento,

47:34.440 --> 47:37.480
que lo llamo corpus held auto, corpus de desarrollo.

47:38.280 --> 47:47.800
Y lo que hago es entreno sobre esta parte y evaluo sobre el held auto, y me reservo este

47:49.320 --> 47:53.480
de evaluación, solamente para cuando tengo mi modelo definitivo y quiero saber su

47:53.480 --> 47:56.200
performance, con su medida de evaluación, ¿de acuerdo?

48:00.600 --> 48:04.840
Esto lo van a, algo como esto van a tener que presentar en el laboratorio,

48:05.800 --> 48:08.280
decir, cómo evaluarían el método, un método.

48:10.520 --> 48:15.080
Hay otras posibilidades que no implican un corpus held auto, por ejemplo, hacer lo que se

48:15.080 --> 48:21.000
llama cross validation, que es separo este pedacito, entreno sobre esto y evaluo sobre este,

48:23.000 --> 48:29.160
si, después separo otra franjita, entreno sobre el resto y evaluo sobre la franjita y así con

48:29.480 --> 48:36.440
en cada franca, franjas y saco el promedio, eso me sirve para no desperdiciar, digamos,

48:36.440 --> 48:42.600
esta parte del corpus, para poder utilizar todo el cuerpo de entrenamiento, se llama cross

48:42.600 --> 48:50.680
validation. Vamos a volver a hablar un poquito de cross validation, cuando hablemos de

48:50.680 --> 48:55.560
clasificación, pero lo que me interesa es que le quede claro la diferencia entre estos corpus

48:56.200 --> 49:04.200
y cuando, como decía, cuando tengo el modelo final, uso esto solamente para evaluar las

49:04.200 --> 49:11.160
performas, en una medida que determinaría ese unitaria. ¿Cómo evaluamos un modelo bueno?

49:11.160 --> 49:16.320
La manera correcta de evaluar un modelo debería, sería empíricamente, es decir, si yo quiero

49:16.320 --> 49:21.880
valorar un modelo de lenguaje y lo estoy usando para el reconocimiento del habla, debería ser una

49:21.880 --> 49:27.880
evaluación de qué también reconozco el habla o qué también reconozco la escritura, pero eso puede

49:27.880 --> 49:31.800
ser muy costoso a veces, o yo puedo estar haciendo un modelo en lenguaje y no sé para qué se va a

49:31.800 --> 49:39.280
usar, entonces me interesa mucho o me puede interesar tener una media intrínseca de la

49:39.280 --> 49:50.280
performa de mi modelo. Entonces, vamos a ver una forma de evaluar. A mí esta parte de este

49:50.280 --> 49:59.560
parte en el libro está puesta como un tema avanzado, pero a mí me parece interesante mostrarlo porque

49:59.560 --> 50:06.240
porque la entropía es un concepto que aparece muchas veces en el profesoramiento del lenguaje

50:06.240 --> 50:11.920
natural y en otras cosas me parece que le vale la pena por lo menos aproximarse. Supongan que

50:11.920 --> 50:17.040
yo tengo una variada de la aleatoria y todo esto voy a llegar a una forma de evaluar un modelo,

50:17.040 --> 50:23.240
¿no? No hay que empezar a hablar de esto porque sí. Supongan que yo tengo una variada de la

50:23.240 --> 50:28.640
aleatoria que tiene varios eventos posibles, en nuestro caso dijimos que eran las palabras

50:28.640 --> 50:39.360
posibles. La entropía, la entropía es una variada de la aleatoria que es un concepto que viene de la

50:39.360 --> 50:53.320
teoría de la información, de Claude Shannon. La teoría de información lo que hablaba era, bueno,

50:53.320 --> 50:58.760
alguno capaz que hicieron, lo vieron en un curso, pero la teoría de información lo que trataba

50:58.760 --> 51:02.080
era de medir cuánto me cuesta a mí transmitir un mensaje. ¿Cómo puedo transmitir un mensaje

51:02.080 --> 51:11.080
de forma óptima? Digamos, es un poco la idea, o qué hay atrás de una comunicación. La noción

51:11.080 --> 51:18.120
de entropía, esta función es, tengo el evento, quiero decir, la probabilidad del evento por el

51:18.120 --> 51:24.080
valorismo de esa probabilidad, ¿sí? La entropía tiene como característica fundamental que es una

51:24.080 --> 51:32.520
medida que, si hay un evento que tiene toda la masa de probabilidad, la entropía es mínima. Es decir,

51:32.520 --> 51:38.280
si yo tengo un dado que está tan cargado y una forma, algo que, equivalentemente se puede decir

51:38.280 --> 51:45.000
que la entropía mide migrado disertidumbre sobre un evento. Si yo tengo un dado que está tan cargado,

51:45.000 --> 51:51.160
que cabe que lo tiro, sé que siempre va a salir seis, no tengo disertidumbre. Mi entropía es cero.

51:53.160 --> 52:04.080
En cambio, si el dado está perfectamente calibrado, equilibrado, ¿sí? Mi entropía es máxima.

52:05.080 --> 52:10.160
Es decir, ¿por cómo está definida la entropía? No puedo tener

52:14.960 --> 52:20.920
entropía más alta que cuando los eventos están equipobables. Entonces, justamente la entropía,

52:20.920 --> 52:26.360
generalmente lo que uno mide con la entropía es eso. ¿Qué tan parecido son los resultados? ¿Qué

52:26.360 --> 52:32.520
tan balanceados están de alguna forma? Cuanto más incertidumbre tengo, ¿por qué tan más balanceados?

52:32.520 --> 52:38.280
Si yo no tengo ni la menor idea de la palabra que sigue, mi entropía es máxima.

52:46.200 --> 52:53.040
Y además tiene otra característica que es que si el logaritmo es en base 2, este número,

52:55.120 --> 52:58.440
la entropía me mide la cantidad de bits que yo necesito,

52:59.440 --> 53:09.240
mínimo para transmitir los eventos. Esto es lo mejor forma de verlo con un ejemplo.

53:11.040 --> 53:16.440
Supongamos, y es el ejemplo que aparece en el libro, supongamos que yo tengo ocho caballos.

53:18.040 --> 53:22.160
Sí, tengo ocho caballos y quiero transmitir las apuestas que se están haciendo por un cable.

53:22.160 --> 53:28.520
Entonces digo, bueno, una forma cantada de transmitirlo o directa de transmitir llamar al

53:28.520 --> 53:47.720
primer caballo 001, 010, 011, 100, 101, 110, 111. ¿De acuerdo? Acá yo uso ocho bits.

53:48.040 --> 53:57.080
Cada vez que se apuesta por el caballo 01, yo pongo 001, blablabla. Entonces en total yo

53:57.080 --> 54:03.720
utilizo tres bits para transmitirlo por un cable. Tres bits por cada apuesta, ¿no? Ahora,

54:03.720 --> 54:11.880
cuando nosotros vemos las apuestas descubrimos que la mitad de las veces se apuesta por el caballo 1.

54:12.880 --> 54:20.240
Un cuarto del caballo 02, un tercio, blablabla. Un octavo del caballo 03, un 16ado del caballo 04,

54:20.240 --> 54:27.680
y todos estos se apuestan mucho menos. Teniendo en cuenta eso, yo lo que trato de hacer ahora es decir,

54:27.680 --> 54:37.120
bueno, quiero proponer una codificación mejor que hace que yo, los caballos que se apuestan más,

54:37.120 --> 54:44.400
o sea que tengo que transmitir más seguido, los codifico con menos bits. ¿De acuerdo? La

54:44.400 --> 54:50.720
mitad de los bits, el primer bit, lo utilizo solo para el caballo 01. Es decir, que si es un 0,

54:52.000 --> 55:03.840
es que transmitir el caballo 01 necesita un solo bit. Si es un 01, si es un 01 y un 0 después,

55:03.840 --> 55:12.120
es el caballo 02. Si son 01 y un 0, después es el caballo 03. Si son 01 y un 0, fíjense que yo

55:12.120 --> 55:22.040
para transmitir estos caballos utilizo 1, 2, 3, 4, 5, 6 bits. Utilizo más bits. Pero como son

55:22.040 --> 55:30.240
mucho menos probables, mi entropía me da 2 bits. O sea, el promedio de bits que yo utilizo según

55:30.240 --> 55:41.040
la distribución es 2 bits, que es más baja que los 3 bit originales. ¿Se entiende? Incorporando

55:41.040 --> 55:48.160
la información de la distribución bajo. Podemos mejorar eso. No, no podemos mejorar eso. Nunca

55:48.160 --> 55:52.160
vamos a, la entropía lo que nos dice es eso. Nunca vas a encontrar una, porque justamente la

55:52.160 --> 55:57.480
entropía es 2. Como la entropía es 2, la entropía me da una cota inferior sobre cuánto puedo llegar.

55:57.960 --> 56:02.400
Con menos de 2 bits no puedo. ¿De acuerdo?

56:04.960 --> 56:06.920
Entonces se preguntarán para qué sirve esto.

56:11.200 --> 56:15.560
De hecho no, la entropía es una cota de lo que decía, una cota mínima para el número de bits

56:15.560 --> 56:22.120
necesarios. A partir de la entropía yo puedo calcular la entropía de una secuencia.

56:22.120 --> 56:33.320
La entropía de una secuencia es de todas las combinaciones posibles,

56:35.360 --> 56:39.720
de una secuencia la probabilidad de esa combinación es lo mismo para aplicado a secuencia.

56:39.720 --> 56:44.280
Este si lo ven es un número muy complicado porque es la sumatoria de una cantidad impresionante del

56:44.280 --> 56:50.560
número, porque son todas las combinaciones posibles de secuencia. Eso es lo que me

56:50.560 --> 56:58.560
dice es la entropía de la secuencia. ¿Qué tanta incertidumbre hay en una secuencia?

57:07.920 --> 57:18.520
Y la tasa entropía sería eso dividido de n, es decir el promedio, porque si no la secuencia

57:18.520 --> 57:24.320
más larga o no la entropíamos antes. El promedio por palabra de la entropía.

57:29.840 --> 57:42.920
Entonces, la entropía de un lenguaje que sería como la medida de qué tanta incertidumbre hay en un

57:42.920 --> 57:52.840
lenguaje. ¿Qué tanto puedo yo llegar a predecir lo que va a seguir diciendo el lenguaje?

57:52.840 --> 57:58.200
Ese al límite, pero como valoró, no en un contexto en general en el lenguaje,

57:58.200 --> 58:05.360
es una medida para el lenguaje. Ese al límite cuando la secuencia tiene infinito de la tasa

58:05.360 --> 58:08.120
entropía.

58:18.000 --> 58:22.400
Y que sé que acá es la suma, como decíamos, es la suma de todas las secuencias posibles.

58:22.400 --> 58:28.360
Es decir, que es una cosa imposible, calcular. Pero hay un teorema que es el de Llano Muamí

58:28.360 --> 58:34.640
Lambrayman que dice que el lenguaje es estacionario y ergódico. Estacionario y ergódico quiere

58:34.640 --> 58:41.880
decir que no importa dónde yo esté parado en una secuencia, todas las posiciones, las probabilidades

58:41.880 --> 58:48.240
son las mismas de continuidad. Lo cual no es así en el lenguaje, porque lo que yo digo ahora

58:48.240 --> 58:53.680
incide dentro de lo que estoy diciendo entre un minuto más. No, no es aleatorio, digamos. Pero

58:53.680 --> 59:00.520
suponiendo eso es una simplificación, lo que me permite es simplemente para calcular la entropía,

59:01.520 --> 59:08.080
la tasa de entropía del lenguaje es simplemente uno sobre n dividido en logaritmo. Fíjense que

59:08.080 --> 59:12.960
perdí las probabilidades de cada una de las de la secuencia. Es como que si yo tomo una secuencia

59:12.960 --> 59:21.040
suficientemente larga del lenguaje, voy a incluir a todas las subsecuencias. O sea que si yo una

59:21.040 --> 59:26.160
secuencia suficientemente larga, puede ser el cuerpo de evaluación. Yo puedo calcular la entropía

59:26.160 --> 59:43.320
sobre el cuerpo de evaluación. Entonces esto es un número, hasta ahora lo que dije acá es un

59:43.320 --> 59:50.200
número, no sabemos por qué tengo esto. Pero fíjense que si yo puedo calcular lo que se llama la

59:50.200 --> 59:57.720
entropía cruzada, porque yo que tengo, yo tengo un lenguaje que genera las palabras con una cierta

59:57.720 --> 01:00:03.720
distribución de probabilidad, que es lo que queremos averiguar, que es tan lo que es lo que es

01:00:03.720 --> 01:00:09.080
nuestro problema original, cómo da las palabras anteriores y genera la siguiente. Eso es algo

01:00:09.080 --> 01:00:13.720
que he desconocido, no sabemos cómo es, porque es el del lenguaje español el que yo quiero

01:00:14.040 --> 01:00:21.880
pero yo tengo un modelo M, que es el modelo de negramas. La entropía cruzada lo que dice es bueno

01:00:21.880 --> 01:00:32.960
calculamos esta H utilizando la probabilidad original por el logaritmo de la probabilidad

01:00:32.960 --> 01:00:38.800
asignada por el modelo. La probabilidad de la secuencia es la que tenía el lenguaje general,

01:00:38.800 --> 01:00:46.960
que no la conozco, y el logaritmo sí, o sea esa distancia, esa largo envícese del modelo.

01:00:48.560 --> 01:00:53.240
Según el teorema otra vez, ya no manmilan, yo puedo sacar esta probabilidad simplificándola,

01:00:53.240 --> 01:01:02.200
suponiendo que es ergodico, y digo bueno, la entropía cruzada depende solo del logaritmo

01:01:02.200 --> 01:01:13.880
de la probabilidad asignada por el modelo. Y esto es lo interesante, cualquier entropía cruzada

01:01:13.880 --> 01:01:20.480
que yo obtenga, que yo calcule con un modelo, va a ser mayor necesariamente que la entropía

01:01:20.640 --> 01:01:33.120
dé lenguaje. Cualquier modelo va a asignarme una entropía mayor a la de lenguaje, esto es la cota inferior.

01:01:33.120 --> 01:01:58.080
Entonces fíjense que como son todas mayores, cuanto más parecido sea mi modelo, al modelo

01:01:58.080 --> 01:02:03.240
del lenguaje, cuanto más aparecido, asigne probabilidad más parecida de las de acá,

01:02:03.240 --> 01:02:14.440
por como está definido, va a ser mejor. Entonces, cuanto menor sea la entropía cruzada de mi modelo,

01:02:14.440 --> 01:02:19.040
evaluado sobre una secuencia suficientemente larga, es decir, sobre el corpo de evaluación,

01:02:19.040 --> 01:02:26.560
mejor va a ser mi aproximación. Y justamente, la medida de esa intrínseca que estábamos buscando

01:02:26.560 --> 01:02:41.440
era esto, que es dos, ¿por qué es dos? No lo sé, porque es lo mismo, es para sacarlos

01:02:41.440 --> 01:02:49.000
logaritmos nada más, es dos a la entropía cruzada, a este valor, y esto se llama perplejidad. La

01:02:49.000 --> 01:03:06.880
perplejidad es lo que mide qué tan bueno es intrínseamente mi modelo sobre mi cuerpo de

01:03:06.880 --> 01:03:12.000
entrenamiento, sobre mi cuerpo de evaluación. Es decir, si yo tengo dos modelos, el que asigne

01:03:12.000 --> 01:03:19.080
mayor probabilidad, menor perplejidad, mayor probabilidad al cuerpo de evaluación, es mejor

01:03:19.080 --> 01:03:23.600
desde ese punto de vista, lo consideramos mejor. ¿Por qué? Porque tiene menos dudas de cómo se

01:03:23.600 --> 01:03:33.200
comporta, porque la perplejidad es como la incertidumbre que yo tengo ante... Dada una palabra,

01:03:33.200 --> 01:03:36.920
cuando yo me paro una palabra, ¿cuál es mi incertidumbre? Mi branching factor,

01:03:37.320 --> 01:03:42.240
en cuanto se puede abrir la siguiente palabra en promedio? Un poco eso es lo que captura la

01:03:42.240 --> 01:03:49.160
perplejidad. Mi lenguaje va a tener un branching factor, es decir, no es que es cero, pero mi modelo

01:03:49.160 --> 01:03:53.680
siempre va a calcular algo mayor o igual a ese branching factor. Cuanto más bajo sea,

01:03:53.680 --> 01:03:58.440
es que quiere decir que yo no estoy acercando más a la perplejidad posta, por eso la perplejidad

01:03:58.440 --> 01:04:10.600
es la medida de que también hace las cosas. ¿De acuerdo? Bueno, no, eso es cuentas.

01:04:13.240 --> 01:04:20.240
Por ejemplo, si nosotros entrenamos unigramas, bigramas y triramas en un corpo de artículo

01:04:20.240 --> 01:04:28.560
de Wall Street Journal de 38 millones de palabras, probaron el cuerpo sobre un modelo de un

01:04:28.560 --> 01:04:35.320
cuerpo de prueba de 1,5 millones de palabras y calcularon la perplejidad. Y fíjense que la

01:04:35.320 --> 01:04:44.640
perplejidad con los unigramas es de 962. ¿No sabemos cuál es el mínimo de esto? No sabemos cuánto

01:04:44.640 --> 01:04:50.560
puede bajar, pero sabemos que con bigrama llega a 170 y con triramas a 109. Es decir, si yo tengo

01:04:50.560 --> 01:04:55.360
dos palabras antes, puedo predecir con mejor, porque acá es con unigrama, es la probabilidad

01:04:55.360 --> 01:05:01.680
que a palabra no dice mucho. Si yo tengo el anterior, lo rápidamente baja. Y si se fija,

01:05:01.680 --> 01:05:14.240
cuando agrega un tercero baja, pero no tanto, ni cerca tanto. Bueno, lo último que nos queda

01:05:14.240 --> 01:05:23.800
hablar es muy bien. ¿Qué pasó con las probabilidades nuladas? ¿Se acuerdan que nos quedaban las

01:05:23.800 --> 01:05:28.880
probabilidades nuladas cuando no había conteo? Bueno, uno de los problemas es la palabra que

01:05:28.880 --> 01:05:34.920
no existen. La palabra que no existen lo único que podemos hacer o lo que típicamente se hace es

01:05:34.920 --> 01:05:42.960
crear un vocabulario fijo y sustituyo las palabras de conocida por una especial. Esto es típicamente

01:05:43.120 --> 01:05:47.640
lo que se hace. Es decir, todas las palabras de conocida las considero una sola palabra que nos

01:05:47.640 --> 01:05:55.840
equivalece. Y cuando aparecen enigramas que no ocurren, este es el caso de come que no aparecía,

01:05:57.040 --> 01:06:01.120
pero puede ser que el enigrama no ocurra, lo que voy a hacer son técnicas de suavizado.

01:06:01.120 --> 01:06:17.920
Yo tengo, ¿se acuerdan? Tengo el contador de, por ejemplo, acá es un enigrama, ¿no?

01:06:19.600 --> 01:06:25.120
Contador de la palabra, el cantidad de veces de la palabra, dividido el total de token que hay.

01:06:25.440 --> 01:06:37.440
Y así calculo las probabilidades. La técnica de la plaz, lo que dice es, bueno, le agrego uno

01:06:37.440 --> 01:06:41.000
a cada contador, o sea que nunca me va a dar cero, lo hago a lo bestia, digamos, ¿no? Compare

01:06:41.000 --> 01:06:46.040
que no me dé cero, le sumo uno. Y le sumo B, ¿se acuerdan? Que lo he vivido en la clase pasada. Le sumo

01:06:46.040 --> 01:07:01.120
B para que esto me siga dando una distribución de probabilidad. Esto simplemente lo que hace

01:07:01.120 --> 01:07:09.240
es calcular un contador ajustado, multiplica por T y divide por T más B, es decir, multiplica

01:07:09.240 --> 01:07:25.080
por el junial y divide por esto, ¿no? Por el PWBI. Por ejemplo, si yo digo, si este es mi corpo

01:07:25.080 --> 01:07:30.680
entrenamiento, esta es la historia de un hombre de la ciudad que creo, fíjense que me conté

01:07:30.680 --> 01:07:44.560
o da uno, y quiso me da cero. Perdón, este es el conteo. Ahí va, el conteo de esta es uno,

01:07:44.560 --> 01:07:51.120
de la es dos y de quiso es cero. La probabilidad de esta es uno dividido trece. En total de palabras,

01:07:51.120 --> 01:07:57.000
una es esta y es cero, cero, ocho. La es dos dividido trece y quiso me da cero en la probabilidad

01:07:57.000 --> 01:08:05.240
de que no queremos que no de cero. Si nosotros aplicamos la plaza, lo que me da es sumo veinticinco,

01:08:05.240 --> 01:08:13.520
¿no? Son doce palabras en el vocabulario porque la única que está repetida es la. O sea que tengo

01:08:13.520 --> 01:08:22.520
doce en el vocabulario, no trece, trece es T y doce es B. Entonces, se hago dos dividido veinticinco y

01:08:22.520 --> 01:08:29.600
así me da las nuevas probabilidades. Y acá quiso deja de ser cero. El contador ajustado

01:08:29.600 --> 01:08:33.720
de lo que nos permite es comparar lo que teníamos antes con lo que teníamos ahora. Por ejemplo,

01:08:33.720 --> 01:08:44.960
esta valía uno y baja a cero noventa y seis. ¿De acuerdo? La valía dos y baja a uno cuarenta y cuatro.

01:08:44.960 --> 01:08:56.640
Y quiso va de cero a cero cuarenta y ocho. Si se fijan acá, lo que se llama descuento,

01:08:56.640 --> 01:09:05.480
que es el cociente entre los dos valores, me permite ver que le estoy sacando más masa de

01:09:05.480 --> 01:09:14.480
probabilidad a la que a esta, que queda casi igual. Es decir, le tenía a la plaza el problema,

01:09:14.480 --> 01:09:21.000
porque ¿qué es lo que está pasando acá? Esto es lo que me muestra, es que yo le tengo que sacar

01:09:21.000 --> 01:09:27.400
masa de probabilidad a los que aparecen, porque todo me tiene que sumar uno, todas las probabilidades

01:09:27.400 --> 01:09:32.760
me tienen que sumar uno. Si yo iba a agregar diagramas que antes estaban en cero, tengo que

01:09:32.760 --> 01:09:38.000
sacarle probabilidad a los que están, pues no me suma más que uno. Entonces, esto es lo que tiene

01:09:38.000 --> 01:09:45.960
que castiga mucho a los más frecuentes. Les sacan mucho probabilidad a los más frecuentes y como

01:09:45.960 --> 01:09:54.680
que premia demasiado a los que no aparecen. Hay otras técnicas, no vamos a entrar en eso que tratan

01:09:54.680 --> 01:10:03.560
de ajustarlo un poco mejor, pues ahora vamos a mover algunas, perdón. Mueve demasiada probabilidad.

01:10:03.560 --> 01:10:14.640
Otra posibilidad es usar un delta en lugar de uno. Ese delta tengo que calcularlo, se acuerdan lo

01:10:14.720 --> 01:10:19.320
calamos del cuerpo de, siempre que yo tengo esos parámetros para calcular, los calculo sobre el

01:10:19.320 --> 01:10:34.040
cuerpo de desarrollo. Finalmente, hay otra, esa es una aproximación, es decir, con técnicas sobre

01:10:34.040 --> 01:10:41.080
el contenci. Hay otra posibilidad que son un poco más avanzadas, digamos que es cuando yo quiero

01:10:41.080 --> 01:10:50.240
estimar, por ejemplo, en técnicas de trigrama, una palabra a partir de las dos anteriores y

01:10:52.480 --> 01:10:59.920
no existen casos de las dos anteriores en el texto, de las dos anteriores seguidas a W,

01:11:00.920 --> 01:11:12.240
acá es WN, perdón. Sí, lo que hago es hacer lo que se llama BACOV, calcularlo a través de la

01:11:12.240 --> 01:11:15.840
probabilidad de la anterior. Bueno, si no tengo la anterior, pruebo con la anterior, se entiende,

01:11:15.840 --> 01:11:24.880
eso se llama hacer BACOV. El BACOV, tenés que resolver también que ahora otra vez se

01:11:24.880 --> 01:11:29.560
está introduciendo nuevas, luego caso que no tenías antes. Estas probabilidades tengo que

01:11:29.560 --> 01:11:32.920
calcularlo y darle más a la probabilidad. O sea, otra vez tengo que mover probabilidad.

01:11:37.040 --> 01:11:44.400
Cuando los corpus son muy, muy, muy grandes, una forma alternativa y es un método muy nuevo,

01:11:44.400 --> 01:11:49.840
se llama STUPID BACOV, que es, como mi cuerpo es muy grande, típicamente el cuerpo de Google,

01:11:50.000 --> 01:11:58.320
no normalizo nada las probabilidades, conteo nomás como me fué y ya está. Si una no me da

01:11:58.320 --> 01:12:07.080
pruebo con la anterior, si es igual tengo un montón de edad. O también se puede hacer

01:12:07.080 --> 01:12:12.120
interpolación, es decir, la probabilidad de una palabra dada a las dos anteriores

01:12:12.120 --> 01:12:22.840
es la probabilidad de la palabra, la probabilidad nueva, es la probabilidad original de la palabra

01:12:22.840 --> 01:12:28.360
dada a las dos anteriores por un cierto lambda, más un cierto lambda 2 por la probabilidad de la

01:12:28.360 --> 01:12:35.680
palabra dada solo en el bigrama, más la probabilidad del unigrama. Y combino las tres a la vez,

01:12:35.840 --> 01:12:43.440
es como combino las tres técnicas a la vez, ¿de acuerdo? Es decir, le doy un cierto peso a las

01:12:43.440 --> 01:12:49.200
probabilidades que yo quiero. De esta forma, porque acá podría ser que existiera el bigrama anterior,

01:12:49.200 --> 01:12:54.720
pero existiera una vez sola, entonces yo no le tengo mucha confianza a esa. Puedes usarme y no

01:12:54.720 --> 01:12:58.800
le tenga mucha confianza, entonces le doy un cierto peso a este también, capaz que le doy un

01:12:58.800 --> 01:13:04.160
poquito más alto a este. O sea, el 7 existe, está todo bien, pero este siempre me ayuda. Y de esa

01:13:04.160 --> 01:13:09.440
forma va balanceo. ¿Cómo calculo estos lambda y con el cuerpo de... tengo que

01:13:12.080 --> 01:13:17.200
de alguna forma calcularlo sobre el cuerpo de desarrollo o el cuerpo gel dado?

01:13:22.320 --> 01:13:23.760
También hay interpolación

01:13:25.760 --> 01:13:31.360
condicionada por el contexto, o sea, hay un lambda, acá ya lo que pasa es un poco más raro,

01:13:31.440 --> 01:13:36.560
y un poco más moderno. Digamos que es que más de estas épocas, digamos, donde a mí ya no me

01:13:36.560 --> 01:13:41.920
preocupa tanto tener muchos parámetros. Acá estoy definiendo un parámetro para cada combinación de palabras.

01:13:50.560 --> 01:13:57.040
Y hasta aquí llegamos hoy. Esto es el... es este capítulo que tengo acá, capítulo 4 del

01:13:57.120 --> 01:14:00.800
libro de Juraski. Tiene algunas cositas más, pero esencialmente es eso.

01:14:03.920 --> 01:14:10.720
Y es lo que vamos a hablar de en este curso de Enigrama. La clase que viene presentamos laboratorio.

