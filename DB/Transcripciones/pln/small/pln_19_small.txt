Una vez que elegí con el paso 1, elegí cuántas palabras en español le voy a usar, en el paso
2 lo que voy a elegir es una alineación, una función de alineación que me dice cada
palabra con cuál se va a corresponder, cada palabra del lado del español con qué palabra
en inglés se va a corresponder. Este modelo asume de manera muy naiv que todas las alineaciones
que yo puedo tener son equiprobables. O sea, asume que yo voy a tener un conjunto de alineaciones
posibles y todas van a tener la misma probabilidad. Bien, entonces, la probabilidad de elegir
una alineación en particular, si yo tengo un montón de alineaciones, digamos, la probabilidad
de elegir una alineación en particular, va a ser uno sobre la cantidad de alineaciones
que tengo, porque en realidad todas van a ser equiprobables. Bien, entonces, ¿cuántas
alineaciones puedo tener entre dos oraciones, una oración en inglés que tiene largo I y
una oración en español que tiene largo J? ¿Cómo puedo calcular cuántas alineaciones
existen? Más o menos, sí, casi la J. Recuerden que del lado de inglés yo tenía ciertas
palabras, en inglés tenía la palabra E1, E2 hasta, subí, y en español tenía las palabras
F1, F2 hasta F subj. Entonces, yo podía trazar líneas para alinear, pero además en inglés
yo siempre he considerado que tenía un token nul, entonces todas las palabras que no estaban
alineadas del lado del español iban a parar ahí. Así que en inglés en realidad no tengo
i posibilidades, tengo una más, tengo i más uno. Entonces, ¿cuántas formas tengo yo
de mapear estas J posibilidades en español con las I de inglés? Exacto, i más uno a
la J, porque yo tengo i más un opciones para la primera y más una opciones para la segunda,
etcétera, hasta que llevo al final. Así que son i más uno a las J alineaciones posibles.
Ojo, el nul es como una pisadita que hago yo para alinear cosas que no tienen un correspondiente.
O sea, yo tenía una palabra en español que… Varias de las Fes pueden estar alineadas
de nul, no importa en qué orden están. Eso. Bien, entonces eran i más uno a las J posibles
alineaciones, por lo tanto, la probabilidad de elegir una alineación A dada la oración
en inglés, la probabilidad de elegir una alineación cualquiera dada la oración en
inglés va a ser el producto de la probabilidad de haber sorteado un valor J primero, que era
epsilon por la probabilidad de elegir una alineación cualquiera para ese J, que es
uno sobre i más uno a la J. Bien, entonces esto lo resumimos como epsilon sobre i más
uno a la J. Epsilon sobre i más uno a la J es la probabilidad de, dada una oración
en inglés, elegir cierta alineación que yo voy a utilizar. Bien, ese fue el segundo
paso. El tercer paso es una vez que ya tengo la alineación, voy mirando cada palabra del
lado en inglés y le voy poniendo una palabra correspondiente del lado español. Para acá
voy a asumir que yo tengo una tabla de traducción, una tabla de traducción que me dice que tiene
de un lado todas las palabras en español y del otro lado todas las palabras en inglés,
entonces mi tabla va a tener una forma como, por ejemplo, hacer una tabla así que de un
lado va a decir las palabras en español como banco, perro, gato y más cosas y del otro
lado va a tener las correspondientes en inglés como bank, bench, cut, tree y más cosas.
Y entonces esta tabla va a decir la probabilidad de traducir una cosa en la gota. Entonces banco
probablemente tenga cierta probabilidad para bank y cierta probabilidad para bench, 0.4
y 0.6, 0.06 puse. Y para cut no va a tener ninguna probabilidad y para tree tampoco
y después perro no va a tener nada de esto, pero sí después y cut va a ser, no sé 0.8
en este caso, etcétera. Voy a tener una tabla bastante grande que tiene todas las posibilidades
de traducir una palabra como otra. Entonces si yo tengo esa tabla, lo que puedo decir
es que la forma de calcular la probabilidad de esa oración final que yo traduje va a
depender de cuáles son las palabras que yo elija, va a depender de cuáles son las palabras
que yo haya puesto dentro de mi oración para traducir. Entonces esa tabla que está
ahí definida, le llamamos acá en la slide, aparece como t de f sub x sub y dice que la
probabilidad de traducir la palabra es sub y como f sub x. Entonces saca de una cosa importante.
Si tenemos la oración en inglés, la oración en inglés recuerdan que tenía las palabras
f sub 1, f sub 2 hasta f sub n, la oración en español tenía las palabras f sub 1, f
sub 1, f sub 2 hasta f sub j. Y yo tenía en el medio una función de alineación que
me decía qué palabra se correspondía con cuál. Entonces no era f sub n ni f sub j.
Era f sub i y f sub j grande. Esto era f sub i y esto era f sub j grande. Entonces si yo
tengo una palabra cualquiera dentro de la oración en español, tengo un f sub j chica
dentro de la oración en español, esto se va a corresponder con algún f sub i chica
en la oración en inglés, digamos. Yo sé que esto se cumple por la función de alineación
porque agarra y mapea todas las palabras que están en español con algo que está del
lado del inglés, potencialmente con el token vacío nul.
Bien, entonces tengo una palabra del lado del español que es f sub j y una palabra del
lado del inglés que es f sub i. ¿Cuál es la relación entre f sub j y f sub i? ¿Cómo
es la relación entre sí, digamos? Yo puedo decir que el i es igual a algo de j. ¿De alguna
manera? La función de alineación, ahí está. O sea, el i es igual a la función de alineación
aplicada j. Como la i, el índice de este de acá es igual a la función de alineación
aplicada j. Entonces, yo puedo decir que la palabra f sub i es igual a la palabra e sub
a sub j. Así que puedo decir que, en realidad, los que están alineados son la palabra f
sub j está alineada con la palabra e sub a sub j. Y ahí me saqué el i de encima, digamos.
Simplemente, iterando sobre las palabras, iterando sobre la j puedo establecer la correspondencia
entre las dos palabras. Y eso es un poco lo que dice acá para terminar de armar lo que
es el modelo de traducción. Para terminar de armar el modelo de traducción dicen que
en el tercer paso yo voy a elegir cuáles son las palabras. Entonces, lo que voy a hacer
es iterar sobre todas las palabras y haciendo el producto de todas las probabilidades. O
sea, el producto de dado que yo tenía la palabra f sub j, perdón, dado que yo tenía la palabra
e sub a sub j en inglés, entonces elegir la palabra f sub j en español. Eso hago una
productoria con todos los valores de las distintas palabras. Bien, entonces ahí llegué
a el último de los valores que quería calcular, que es la probabilidad de f dado que conozco
ahí es igual a la productoria con j igual a 1 hasta j grande de el valor de la tabla
de traducción, que es t sub f sub j, t de f sub j e sub a sub j. Bueno, ahí tengo cómo
en cada paso fui calculando cosas, este se correspondía al paso uno del modelo, paso
uno, este se corresponde con el paso dos del modelo, en realidad este ya tiene el paso
uno y el paso dos juntos porque ya tengo el éxilón acá y este se corresponde con el
paso tres del modelo. El paso tres de la historia de generación. Mi objetivo con todos estos
valores que están acá es calcular p de f dado e. ¿Qué parámetros introduce? ¿Qué
parámetros fueron surgiendo a medida que yo iba iterando sobre estos pasos? Bueno, en
primer lugar el éxilón aquel que estábamos viendo, este es un valor que yo tendría que
estimar a partir de mirar en los corpus como son los largos de las oraciones relativos
y el otro parámetro importante es aquella tabla de allá, aquella tabla de traducción
es que me dice banco, con qué probabilidad lo puedo traducir como bank, con qué probabilidad
lo puedo traducir como bench, etcétera, etcétera. Esa tabla en realidad es un parámetro del
modelo, es un parámetro del sistema que si yo lo tuviera me alcanzaría con eso para
poder construirme este modelo y calcular la probabilidad de cualquier par de oraciones.
Bien, y entonces antes de continuar vamos a terminar de armar cuál es la imagen de esto,
que es decir yo en realidad lo que quería calcular era p de f dado e, que eso va a ser
mi modelo de traducción y de hecho va a ser el encargado de medir la adecuación de una
frase. P de f dado e lo puedo calcular con esta descomposición de pasos que hice acá
en realidad porque lo hago de la siguiente manera. Yo quiero calcular p de f dado e y
entonces voy a mirar lo que dice acá, p de f dado e es igual a la sumatoriana de p de f
dado e. ¿Qué significa eso? Que para traducir entre una oración en español y una oración
en inglés, o más bien para traducir entre una oración en inglés y una oración en
español hay muchas formas de alinear las palabras entre el inglés y en español y una
vez que yo elegí una forma de alinear hay muchas formas de elegir las palabras que vienen
después digamos yo miro la tarjeta de traducción y capaz que hay varias maneras de elegir distintas
palabras. Entonces lo que eso significa es que no existe una sola manera de traducir una
oración en inglés a una oración en español. Yo puedo encontrar varias formas de alinear
las palabras y varias formas de elegir las palabras de manera que muchas alineaciones son
posibles. Entonces para saber cuál es la probabilidad de traducir p de f dado e, entonces
yo voy a tener que sumar sobre todas las alineaciones posibles sobre todas las formas
de alinear las dos oraciones f y e, voy a tener que iterar sobre eso y para cada una
voy a tener que calcular la probabilidad parcial. Entonces digamos yo tengo cinco formas de
alinear las dos oraciones, cinco es un número un poco raro pero digamos tengo n formas de
alinear las dos oraciones, voy a tener que mirar bueno para la primera alineación cuál
es la probabilidad de encontrar la oración f para la segunda alineación cuál es la
probabilidad de encontrar la oración f para la tercera oración y así hasta llegar al
final y agarro y sumo todo eso. Eso lo puedo hacer porque las alineaciones son una descomposición
del espacio de probabilidades. En realidad yo puedo descomponer el espacio de probabilidades
en pedacitos disjuntos y cada alineación va a ser uno de ellos. Así que digamos que
para calcular el modelo de traducción p de f dado e necesito sumar sobre todas las alineaciones
posibles. Ahora lo que me falta es saber cómo calculo este valor de acá. Así que lo que
estoy diciendo es que la probabilidad de f dado e es la suma sobre las alineaciones
de la probabilidad de f y esa alineación dado e. Eso es simplemente lo que dice ahí
en la slide. Lo que me falta calcular entonces es esta parte de acá y esa parte de acá la
calculo de esta manera. Yo digo que la probabilidad de f dado e es igual, ahí está más o menos
el resultado final pero podemos sacar qué es lo que tendría que poner de este lado.
Ahora sí me acuerdo bien. Ah, ahí está. Por definición de probabilidad condicional.
Eso. p de f dado e, le voy a dar varias maneras a hacerlo pero esto se puede definir como
p de f a e sobre p de e. No? Por definición de probabilidad condicional. Pero además
esto si quiero podría llegar a decir esto es lo mismo que p de f a e sobre p de e por
la cualidad que me faltaba. No. A e. Por p de a e sobre p de a e. Era esto lo quería.
O sea, yo puedo agarrar esta probabilidad que está acá y multiplicarla y dividirla por
el mismo número, que sé que son mayores que cero, así que en definitiva esa división
me va a dar uno. Y ahí yo puedo tomar y asigno este con este y este con este. En definitiva
lo que me queda es si asocio estos dos me va a quedar p de f dado a e y si asocio estos
dos de acá me va a quedar p de a dado e. ¿Qué es lo que dice allá? La probabilidad
de p de f a dado e, bueno, sí, de los dos, de f y a dado e es igual a la probabilidad
de f dado a e por la probabilidad de a dado e. Bien, y estos dos valores que están acá
no los elegí por casualidad sino que son los valores que tenía antes en el modelo.
O sea, yo tenía que el p de a dado e era igual a epsilon sobre y más uno a la j y el
otro era la productoria desde j igual a 1 hasta j grande de las valores de traducción,
el f sub j y el e sub a sub j. Entonces, en definitiva puedo calcular p de f a dado e
y además puedo calcular haciendo una suma sobre todas las alineaciones posibles, puedo
calcular el p de f dado e. Bien, con eso y con todo ese montón de cocciones llegamos
a construir lo que es un modelo de traducción, o sea, solamente teniendo una tabla de traducciones
que me diga cuál es la probabilidad de traducir una palabra. Como otra palabra, yo puedo llegar
a definirme cuál es la probabilidad de traducir una oración dada otra oración. Bien, y hay
una cosa más, bueno, esto ya lo estuvimos viendo que aplicamos en cada paso, y hay una
cosa más que es si yo tuviera las dos oraciones, digamos, la oración en inglés y la oración
en español y además tuviera la tabla esta con todas las probabilidades, yo podría hacer
un algoritmo de programación dinámica, un algoritmo estilo Viterbi que vaya recorriendo
alineaciones y me diga cuál es la alineación más probable. No vamos a ver los detalles
del algoritmo, pero hay una forma de decir, bueno, voy recorriendo las dos oraciones y
me voy quedando con las subsecciones más probables y al final me termina devolviendo
cuál es la alineación más probable dada esas oraciones. O sea, que si yo tuviera ya
esa tabla de traducciones, esa tabla de probabilidad de traducción, podría construirme las alineaciones
del corpus. Así que bueno, hasta el momento decíamos, bueno, suponemos que tenemos esta
tabla de traducción que me dice para bank si se traduce, perdón, para banco si se traduce
como bank o como bench, etc. Estaba diciendo que tenía esa tabla, pero en realidad la
realidad es que no tengo esa tabla y me gustaría poder construirla. Entonces, nos gustaría
poder estimar esas probabilidades para poder construirme esa tabla. Si yo tuviera un corpus
paralelo, simplemente podría ir recorriendo el corpus y contando cuántas veces aparece
banco alineado con bench y cuántas veces aparece alineado con bank y ahí sacaría
una probabilidad, pero no tengo las alineaciones. Y por lo que vimos, digamos, recién, si yo
tuviera la tabla, entonces yo además podría ir recorriendo el corpus y construirme las
alineaciones. Así que si yo tuviera las alineaciones podría contar y sacar la tabla, si yo tuviera
la tabla podría pasarle un algoritmo y construir las alineaciones. Pero la verdad que no tengo
ninguna de las dos cosas. Entonces se vuelve un problema de huevo y la gallina. O sea,
si yo tuviera las alineaciones construiría el modelo, construiría la tabla de probabilidades,
si yo tuviera la tabla de probabilidades podría construir las alineaciones. Para este tipo
de problemas, en los cuales yo tengo como dos variables interdependientes y no conozco
exactamente el valor de ninguna de las dos, se utiliza lo que se conoce como el algoritmo
expectation maximization o maximización de la esperanza. Y bueno, es un algoritmo que
sirve exactamente para este tipo de problemas. En realidad lo que va a hacer el algoritmo
iterar es un algoritmo iterativo que va tratando de converger una solución y lo que hace es
decir, bueno, yo no tengo ninguno de los dos valores. O sea, si yo tuviera mi tabla de
probabilidad de traducción me podría calcular las alineaciones y tuviera mis alineaciones
me podría calcular la probabilidad de traducción. Entonces lo que hace es decir, bueno, asumo
que mi tabla de traducción va a ser uniforme, digamos. Cualquier palabra se puede traducir
como cualquier otra palabra con la misma probabilidad. A partir de eso calculo alineaciones y a partir
de esas nuevas alineaciones calculo otra vez la tabla. Y de vuelta, con esa tabla que calculé,
vuelvo a medir las alineaciones y de vuelta con esas nuevas alineaciones vuelvo a calcular
la tabla. Entonces, aunque no me crean, esto después de muchas iteraciones va convergiendo
a algo. Y parece mágico, ¿no? Parece como que, en realidad si yo no tengo ninguno de
dos valores, no debería nada, debería como dar fruta. Pero voy a tratar de comenzarlos
de que, en realidad, esto sí funciona, con un ejemplito. Bien, tenemos. Entonces, vamos
a construir un sistema que es de traducción entre francés y el inglés donde hay un cuerpo
muy grande, pero bueno, nos vamos a concentrar solo en tres pequeñas oraciones citas que
dicen la mesón se traduce como de House, la mesón blue se traduce como de Blue House
y la flea se traduce como de Flower. Entonces, al principio lo que hago es decir, bueno,
todas las traducciones entre todas las palabras son equiprobables, así que lo que me va a
quedar es cuando reparten entre las alineaciones, todas van a tener el mismo peso. Entre la y
mesón, la probabilidad de que la se traduzca como D o que se traduzca como House va a ser
la misma, en realidad porque todas las alineaciones son equiprobables. En la mesón blue también
pasa lo mismo, la probabilidad de traducir la como D como Blue o como House va a ser
la misma y en la flea pasa igual. Entonces, eso es la primera, el primer paso, digamos,
en el primer paso yo voy a tener todas las alineaciones equiprobables y todas las valores
de las palabras iguales.
Entonces, en mi algoritmo yo empecé con una tabla de traducción que era toda uniforme,
digamos, yo tenía la probabilidad de traducir cualquier palabra en cualquier otra, era la
misma. A partir de eso yo me construí estas alineaciones que también parece que son
todas equiprobables y parece que no tienen como mucha información. Entonces lo que voy
a hacer ahora, a partir de esto, es tratar de construirme de vuelta la tabla de traducciones
pero mirando estas nuevas alineaciones que hay. Entonces lo que voy a construir es una
tabla que tiene todas las palabras del lado de francés, tiene la mesón blue flower y
de House blue flower.
Y para llenar esta nueva tabla, lo que tengo que hacer es iterar sobre las alineaciones,
mirar cada una de las palabras cuantas veces está alineada con las otras y contar, o sea,
y sumar los pesos de cada una de las alineaciones. Entonces la alineación entre la y de. En total,
mirando ese ejemplo de corpus, ¿cuánto me daría? ¿Cuál sería el peso de esa alineación?
Para verlo, en realidad lo que hago es contar, miro cuantas veces la y de están alineados.
Entonces tengo 0.5 de peso en la primera, en la segunda tengo 0.33 y en la última tengo
0.5 de vuelta. Así que en total tengo como 1.33 de peso entre la y de. Después miro,
entre la y House, ¿cuánto peso tengo? ¿cuánta masa de probabilidad tengo? Bueno, tengo 0.5
en la primera relación, 0.33 en la segunda y nada en la tercera. Por lo tanto en total tengo 0.83
de probabilidades entre la y House. Después miro, entre la y blue, ¿cuánto peso tengo?
Solamente 0.33 solo está en la del medio y entre la y flair, ¿cuánto tengo? No, entre la y
flower, ¿cuánto tengo? 0.5 solo aparece en la del final. Bien, completemos la siguiente,
entre mesón y de, ¿cuánto tendría? 0.83, está en la primera y en la segunda, entre
mesón y House, entre mesón y House y 0.83 porque aparece en las dos. Bien, entre mesón y blue,
solamente aparece en la segunda, así que voy a tener 0.33 y entre mesón y flower no tengo nada.
Después entre blue y de, solamente aparece en la segunda, así que voy a tener 0.33, entre blue
y House, creo que de vuelta tengo 0.33 y entre blue y blue también 0.33 y no aparece junto con
flower. Y para después, para flair tengo 0.5 con de, 0 con House, 0 con blue y 0.5 con flower.
Bien, entonces hice una pasada por todas las alineaciones y me calculé cuáles son los
pesos relativos de cada una de estos pares. Lo siguiente que hago, como esto va a ser una
probabilidad, es normalizar. Entonces me voy a construir una tabla, digamos normalizando por,
digamos, voy a sumar en cada fila y voy a dividir entre la cantidad que aparece para cada fila,
así que de vuelta me construyo la tabla, que me queda la mesón blue flower y de este lado de
House acá, de House y blue flower. Y lo que voy a hacer es normalizar, entonces si yo sumo estos
de acá, creo que me da 2 en total, no, 3 en total. Tengo los valores acá, no tengo que
hacer los cálculos, pero sí, me da 3 en total. Entonces lo que pasa cuando yo normalizo es que
acá me queda 0.44, acá me queda 0.28, acá me queda 0.11 y acá me queda 0.17. Pues el segundo,
también lo normalizo esta vez entre 2 y me queda 0.42, 0.42, 0.16, 0. El tercero ya suma 1,
así que me queda 0.23, 0.33, 0.33, 0 y el último también queda igual, 0.5, 0, 0, 0.5. Bien,
entonces me construí una nueva tabla de probabilidad de traducción, dado que ahora las
alineaciones serían estas. Y noten lo que pasó acá, si yo miro la fila correspondiente a la,
¿qué es lo que pasa ahora con esa fila? Recuerda que yo empecé teniendo todas las
probabilidades de traducción de que parecen palabras, eran equiprobables. Si yo ahora miro la
fila de la, ¿qué es lo que pasa? Exacto, aparece claramente que la asociación entre la y de es
más fuerte, tengo un 0.44 de probabilidad de traducir la como de y tengo bastante menos en
los otros, tengo 0.28, 0.11, 0.17. Y yo había empezado diciendo que eran equiprobables, entonces yo
probablemente tenía 0.25, 0.25, 0.25, 0.25 en cada una. Y después de un paso de la iteración,
descubrió que la ID tiene más chance de ser una traducción de la otra, en vez de traducir la
como House o la como Blue o la como Flower. Eso pasa en el primer paso, en la primera iteración
el tipo descubre, el algoritmo descubre que la asociación entre la ID es bastante más fuerte.
Como pasa eso, lo que va a pasar es que cuando yo reparta de vuelta en las alineaciones estas
líneas que se corresponden a la asociación entre la ID van a estar más fuertes, van a tener un
poco más de peso y como esto es una distribución de probabilidades, esa masa que ganó la asociación
entre la ID se va a tener que sacar de otras alineaciones posibles, o sea si la está asociada
con D, entonces no está asociada con las otras que están alrededor. Entonces esa masa que se pierde,
digamos, o sea que gana en la D se tiene que repartir en las otras alineaciones posibles,
o sea en las que no son entre la ID. Entonces después de una iteración la asociación entre
la ID empieza a ser más fuerte y como pasa eso en la siguiente iteración va a empezar a descubrir
que como la estaba alineado con D, entonces mesón tiene que estar alineado con House
y como mesón estaba alineado con House, digamos, esa misma masa de probabilidad se va a traducir,
a transferir a la segunda y lo mismo, como la estaba alineado con D, entonces Fleur tiene
que estar alineado con Flour. Entonces si yo sigo iterando en estos pasos, en cada paso lo que va
a pasar es que se va a mover un poco más de probabilidad hasta que al final va a terminar
descubriendo cuál es la alineación real de las palabras, o sea va a descubrir que la va,
o sea con D, mesón con House, Blue con Blue, Fleur con Flour. ¿Cómo es que va a descubrir eso?
Porque en cada paso lo que va pasando es que algunas de las asociaciones como están, como
aparecen, que ocurren digamos en más oraciones, tienen más fuerza que otras, entonces el peso
que esas asociaciones ganan lo va sacando de otro lado y eso hace que de otro lado se empicen
a generar otras alineaciones diferentes. Entonces al final esto termina convergiendo y termina
revelando lo que es la estructura suyacente de las palabras y cómo se alinean unas con otras.
Bueno, una vez que yo termine de hacer esto puedo agarrar y construirme efectivamente la tabla
final de traducciones que es simplemente busco cada una de las posibles traducciones, digamos de los
posibles pares y saco las probabilidades. ¿Y qué pasó acá? Mientras yo estaba construyendo mi
modelo de traducción, mientras yo estaba construyendo la tabla de traducciones además de como
efectos secundarios se construyó un corpus alineado, un corpus que está alineado a nivel de palabras.
Así que bueno, el algoritmo de Spectation Maximization funciona de esa manera, tiene siempre dos
pasos, un paso de Spectation y un paso de Maximization. En este caso el paso de Spectation se
trataba de agarro la tabla de probabilidad de traducción que tengo y con eso me armo alineaciones y
después el de Maximization es al revés agarro las alineaciones que acabo de construir y me
armo una nueva tabla y voy iterando todos esos pasos hasta que eventualmente converge.
Bien, dijimos que eran cinco modelos de IBM, no vamos a ver muy en detalle los otros, o sea,
solo mencionar que empiezan a crear complejidad. En este modelo uno habíamos dicho que todas las
alineaciones eran equiprobables, en el modelo dos abandonan esa noción y dicen bueno, en vez de
alineaciones equiprobables, yo voy a tener un modelo de reordenamiento de las palabras para decir,
bueno, tengo cierta probabilidad de que las palabras que están, si yo tengo I palabras en inglés,
J palabras en español, tengo cierta probabilidad de mover la palabra I y la palabra J y bueno,
y así siguen subiendo en complejidad hasta llegar al modelo cinco, que modelo cinco es el que anda
mejor, pero de todas maneras estos son modelos que ya no se usan, digamos, esto es del año 93 y en
general se han obtenido mejores resultados abandonando estos modelos. Entonces el que vamos a pasar a
ver a continuación es un modelo bastante más moderno que es lo que sí se utiliza hoy en día
en traductores como los de Google. Sí. Es que en realidad, claro, a ver, estos modelos
estadísticos no utilizan ningún tipo de analizador morfológico y nada para sacarlo. Hay otros modelos
que sí lo hacen, no vamos a dar ninguno en esta clase pero hay otros modelos que sí hacen
uso de esa información. Igual son como refinamientos, creo que ninguno lo tiene como en la base del
modelo, el uso de parto speech, pero sí cuando vos no sabes una palabra, digamos una palabra que
es desconocida, en realidad utilizar información sobre parto speech y eso probablemente te ayude.
En estos modelos por lo menos no lo habían tenido en cuenta. Bien, entonces sí, lo que vamos a ver
ahora es el modelo de frases que es algo más moderno y es, o sea, el Google Translate o Bing
Translate se basan en modelos de este estilo. Y bueno, y antes de ver cómo se modelo de frases,
volvamos un poco a lo que era la alineación entre palabras. Yo tenía esta frase clásica,
¿no? María no dio una bofetada de la bruja verde, en inglés era Mary did not slap de Greenwich y una
alineación entre esas dos oraciones en realidad se vería como algo así. Yo tengo que María se
alinea con Mary, no se alinea con did not, slap se alinea con daba una bofetada, de se alinea
con ala, podría ser solamente con la y el a que no está alineado nada. Green se alinea con verde
y bruja con wedge. ¿Qué diferencia tiene esto con la otra alineación que habíamos visto hoy?
A ver si se les ocurre algo distinto que tiene esta alineación y la que habíamos visto hoy.
Era not con no, sí. ¿Y qué es lo que cambia acá para que pase eso?
Lo que estaba pasando hoy era que yo partía de las palabras en español,
iba las palabras en inglés y yo tenía una función que me mapeaba las palabras en
español con las palabras en inglés. Entonces yo a cada palabra en español como máximo le
podía hacer corresponder una palabra en inglés. Entonces me quedaba que yo podía expresar cosas
como que daba una bofetada, daba, está asociado a slap, una está asociado a slap, bofetada está
asociado a slap, eso lo podía expresar, pero no podía expresar algo como esto, que no está
asociado did not, porque no sería una función. Yo no puedo asociar uno de los valores de la función
con dos cosas del lado del codominio. Y acá en realidad no puedo hacerlo ni en este sentido ni
en el otro sentido, con una función no me sirve porque de vuelta me pasa que slap está asociado
tres cosas. Entonces con una función de alineación yo no puedo construir este tipo de expresiones,
en realidad necesito algo como un poco más poderoso. Esto es lo que decíamos, los modelos
de IBM siempre usan un mapeo de uno a muchos, usan una función de alineación, mapeo uno a muchos,
pero en realidad lo que necesito para poder capturar realmente cómo funciona en el lenguaje es
mapeo de muchos a muchos. Yo voy a tener que un conjunto de palabras se va a traducir en otro
conjunto de palabras. En definitiva lo que pasa es que pequeñas frases se traducen como otras
pequeñas frases, por eso necesito un mapeo de muchos a muchos. Entonces bueno hay algoritmos que
agarran estos mapeos que como construimos recién, el mapeo de uno a muchos en las dos
direcciones digamos y a partir de eso construyen este mapeo de muchos a muchos. Por ejemplo,
el algoritmo de la herramienta quizá más más, lo que hace es decir bueno yo tengo un corpus en
inglés y en español alineo utilizando los los modelos de IBM digamos voy alineo por un lado
de inglés español y por otro lado de español inglés y acá me quedan dos mapeos de uno a
n digamos dos mapeos con funciones y después lo que hago es intersectar esos dos esas dos
alineaciones que me quedaron y unirlas. Cuando las intersecto obtengo lo que se conoce como
puntos de alta confianza, los puntos negros son los puntos de alta confianza que son los de la
intersección y los puntos grises son los que están en la unión, o sea los que pertenecían a algunos
de los dos modelos. Entonces la herramienta de lo que hace es decir bueno una vez que yo tengo la
intersección y la unión hago crecer los puntos que están en la intersección, colonizando otros
puntos que estén en la unión, hasta que al final termino completando digamos toda la imagen. Este
punto que quedó solito ahí ese no sería parte de la alineación al final, sólo los que podéis
llegar moviéndote a través de puntos ya conocidos. Entonces bueno eso es una forma que utiliza se
llama el algoritmo de Oginey que partiendo alineaciones unidireccionales digamos me permite
construir una alineación completa muchos a muchos entre las palabras. Bien eso le quería mencionar
acerca de las alineaciones entre palabras y ahora sí vamos a ver cómo funciona un modelo
basado en frases. Un modelo basado en frases tiene cierta semejanza con el modelo anterior que
habíamos visto pero es un poco más expresivo en realidad yo parto de una oración por ejemplo en
alemán que decía Morgan Fligge y Gnaskana de la Sur Conference. Lo primero que hace el modelo
cuando quiere traducir digamos en este caso es decir bueno yo voy a segmentar esa oración de
origen en cierta cantidad de frases. Después voy a traducir cada una de esas frases usando una
tabla de traducción y esta vez no es una tabla de traducción de palabras sino que es una tabla
de traducción de frases que me dice para acá frases con que otra frase se corresponde y una vez
que yo traduje cada una de esas frases las voy a reordenar de alguna manera buscando que suene
lo más natural posible buscando aumentar la fluidez de esa oración. Entonces como que la
historia de generación es un poco más simple que la otra no tenía que ir sorteando cosas simplemente
digo separo mi oración en segmentos que les voy a llamar frases los traduzco y los reordeno.
Esa segmentación en frases no tiene porque tener un significado lingüístico yo no voy a separarlas
en grupo nominal, grupo verbal, grupo profesional, etcétera. No tengo por qué o sea capaz que yo
segmento las frases y justo me queda un grupo preposicional capaz que no. Lo único que tiene que
pasar es que estos segmentos que yo construyo tienen que estar en mi tabla de traducción de frases
alcanza con eso como para que yo pueda utilizarlos en mi traducción pero no tienen por qué tener
una motivación lingüística. Bueno entonces un modelo basado en frases tiene estos componentes
parecido al anterior porque de vuelta yo lo que quiero hacer es encontrar la probabilidad de
pdf dado e digamos sigo teniendo la misma ecuación fundamental de la traducción automática estadística
la quiero resolver necesito pdf dado e y pdf solo que ahora el pdf dado e lo voy a calcular de una
manera distinta voy a decir que para calcular esto tengo un modelo de traducción de frases y un modelo
de reordenamiento un modelo de una gran tabla de frases que me dice cada frase con qué probabilidad
la traduzco en otra y después una forma de decir cómo reordeno esas frases para tener mejores
oraciones y bueno y como siempre voy a tener otro componente que es el que mide la la fluidez que es
el modelo del lenguaje porque los modelos de frases funcionan mejor que los modelos basados en
palabras porque la frase ya tiene cierto contexto la frases en realidad son como pequeños grupos de
palabras que yo puedo traducir uno uno en el otro entonces cosas como dar la mano dar una
bofetada a tomar el pelo etcétera todas esas cosas como expresiones son mucho más fáciles de traducir
si en realidad yo ya sé que esta expresión que son tres cuatro palabras la puedo traducir en esta
otra expresión que son tres cuatro palabras es como más expresivo entonces se puede aprender más
cosas y bueno obviamente cuanto más cuanto más datos tenga cuanto más largo sea el cuerpo que yo
tengo yo puedo aprender frases más largas mejores probabilidades y mejores frases
bueno acá hay un ejemplo de cómo sería una tabla de traducción de frases o sea es parecido a la
tabla de traducción de palabras o es lo que acá tengo de en borschlag o sea si yo busco la fila
asociada en borschlag o sea encontraría todas estas traducciones de propósal con 62 por ciento
de probabilidad posesivo propósal con 10 por ciento a propósal con 3 por ciento etcétera o
sea como ven se traducen frases en frases bueno y cómo hago para aprender una tabla de traducción
de frases yo parto de esta alineación de palabras digamos esta alineación completa que ya no es
una función sino que es digamos una alineación de muchos a muchos y voy a tratar de encontrar todos
los todas las frases todos los pares de frases que son consistentes con la alineación a qué me
refiero con que son consistentes acá hay ejemplos yo quiero decir que mariano y maría did not son
es son un par de frases que son consistentes con esta alineación en cambio mariano y maría did
como es que miro esto lo que pasa es que cuando yo tengo mariano y maría did la palabra no está
alineada con did not y el did not digamos el no no pertenece hasta alineación que yo estoy
tratando de decir entonces digo que es no consistente lo mismo pasa con si yo tato alinear mariano
daba y maría did not lo que pasa ahí es que daba no está digamos los puntos de alineación de daba
no están dentro de este cuadrante que estoy tratando de buscar entonces en definitiva digo que no es
consistente las alineaciones consistentes correctas son las que consideran todos los
puntos dentro de ese cuadrante entonces mariano está asociado con maría did not y es así es
consistente así que como aprendo frases consistentes empiezo por las alineaciones digamos
empiezo por la alineación de palabra después busco de una palabra y digo bueno me quedo
con todas esas traducciones de palabras y las pongo en mi tabla de frases y después voy
tomando de a dos y me quedo con todas esas otras frases y las voy agregando mi tabla de frases
después me puedo avanzar en uno y tomar de a tres tomar de a cuatro y llegar a tomar incluso
toda la oración como frases entonces a partir de estas oraciones que tenían no sé este 1 2 3
4 5 6 7 8 9 palabras yo termino aprendiendo como 17 frases digamos cada vez más grandes
y bueno hoy voy sacando esto de todo el corpus y calculando mi tabla de probabilidades
de qué manera calculo esas probabilidades yo lo que puedo hacer es como siempre ver cuántas
veces aparece en el corpus y contar o si no si yo tenía construido el modelo anterior el modelo
de la tabla de traducciones de palabra a palabra en realidad lo que puedo hacer es aprovechar ese
modelo de traducción de palabra a palabra y decir bueno me armo una traducción entre un par de
frases basándome en las traduciones palabra a palabra son como dos formas distintas de
construirlo y a veces hasta complementarias bien eso fue el modelo de frases los modelos
de frases son los más usados hoy en día en realidad en lo que es la traducción automática son los
que han dado mejores resultados y bueno y nos faltaba una cosa para terminar el toda la imagen de
lo que es la traducción automática estadística que es la decodificación
entonces damos un resumen de lo que teníamos hasta ahora
hasta ahora yo partí de yo quería resolver la cocción fundamental de la traducción automática
estadística y yo tenía un corpus paralelo que tenía texto en el idioma origen y el idioma
destino y a partir de ciento análisis estadístico yo me construí un modelo de traducción que es
lo que vimos en esta clase además yo tenía cierto cierta cantidad de texto en el idioma
destino y a partir de cierto análisis estadístico me construí un modelo de lenguaje que me dice
que tan fluido es una oración en el lenguaje destino entonces ahora lo que me falta recuerden
que yo lo que tenía que hacer era iterar sobre todas las oraciones del lenguaje destino y pasarlas
a través del modelo de traducción y del modelo de lenguaje para que me dé la probabilidad de esa
oración bueno lo que me falta es el algoritmo de codificación que en vez de probar con todas
las oraciones del lenguaje destino me va a decir unas cuantas oraciones para probar capa que me
dice 150 oraciones para probar sobre las cuales utilizar el modelo de traducción y el modelo
de lenguaje entonces esto es como un diagrama de de módulos en los cuales el algoritmo de
codificación utiliza los dos módulos tanto el de traducción como el de lenguaje
bueno cómo funciona el algoritmo de codificación el que vamos a ver es un algoritmo de codificación
de tipo beam search y bueno funciona de la siguiente manera yo tengo la oración maría no
dio una bofetada a la bruja verde y la quiero traducir al inglés y tengo una tabla de traducción de
frases entonces mi oración maría no dio una bofetada a la bruja verde yo busco en la tabla
de frases cuáles de esas de digamos cuáles segmentos cuáles subsegmentos de esa oración yo
puedo encontrar en la tabla de traducción de frases entonces voy a encontrar por ejemplo que maría lo
puedo traducir como mary no lo busco en la tabla y lo puedo traducir como not como did not o como
no dio lo puedo traducir como git pero además no dio esa frase entera yo lo busco en la tabla y
me parece que la puedo traducir como did not give dio una bofetada toda esa frase lo puedo traducir
como slap una bofetada lo puedo decir como a slap y bueno y otras cosas bruja lo puedo decir como
witch verde como green pero además en algún lado de la tabla tengo que bruja verde lo puedo
traducir como green witch y así digamos yo puedo encontrar tengo diferentes maneras de segmentar
la oración y además para cada uno de esos segmentos puedo encontrar distintas formas de
traducirlo en el lenguaje destino con mi tabla de frases entonces el algoritmo de codificación
funciona de la siguiente manera empezamos teniendo en cada paso del algoritmo vamos a tener un conjunto
de hipótesis de traducción se llega a ver ahí lo que dice ahi ojo más o menos
bien
acá quedaron mal los cuadraditos bueno en cada uno de los pasos yo voy a tener un conjunto de hipótesis
de traducción al principio del algoritmo voy a empezar con lo con una hipótesis vacía como
se le está hipótesis dice que lo importante de leer es la parte de la f que tiene un montón de
guiones significa que no hay ninguna palabra del español cubierta esas son todas las 9 creo 9
palabras en español ninguna está cubierta y esta hipótesis tiene probabilidad 1 entonces en
cada paso del algoritmo lo que voy a hacer es elegir un par de frases tal que una es traducción de
la otra y voy a crear una hipótesis nueva a partir de una que ya tengo entonces en este paso lo que
hice fue decir el hijo el par de frases maría mary y ahí me creo una nueva hipótesis que cubre
la primera palabra por eso parece una serie con este caso elige la frase en inglés mary y ahora
tiene una probabilidad de 0.564 ese número de esa probabilidad va a servir para guiar un poco en el
algoritmo pero vamos a ver después cómo es que se calcula por ahora quédense solamente con el número
bien pero entonces yo tenía otra opción en realidad yo podía haber elegido empezar en
vez de traducir maría por mary podía haber elegido empezar por traducir bruja por witch
y ahí me crearía otra hipótesis de traducción donde cubro la penúltima de las de las palabras
en español agarro la palabra witch delijo la palabra witch y tiene una probabilidad de 0.182
entonces en cada paso del algoritmo lo que hace es elegir una hipótesis que tiene elegir un par
de frases y expandir así que lo siguiente que puedo hacer es elegir la frase did not expandirla a
partir de la hipótesis que tenía con mary y bueno eso me cubre ahora dos palabras en español y me
tiene me me dio otra probabilidad y después sigo avanzando y sigo avanzando hasta que llegó a cubrir
en algún momento si yo sigo avanzando y sigo agregando hipótesis en algún momento voy a
llegar a cubrir todas las palabras del idioma español todas las palabras de la oración en idioma
entonces hay una vez que yo cubrí todas las palabras digo bueno esto es una hipótesis completa
y esto lo devuelvo como una potencial candidata digamos una oración candidata a traducción
pero claro a medida que yo fui avanzando una cosa que pasó es que fui dejando hipótesis colgadas
y esas hipótesis podrían tener otras traducciones posibles yo acá lo que devolí era una posible
traducción pero a medida que yo tenía las otras hipótesis si yo hubiera seguido por las otras
hipótesis hubiera podido devolver otras cosas entonces yo necesito hacer un backtracking para
poder devolver todas las posibilidades poder volver a ver las hipótesis a revisitar las hipótesis
que había dejado colgadas y volver a explorar los otros caminos entonces necesitaría hacer un
backtracking para recorrerlas todas y si hago un backtracking lo que va a pasar es que voy a
va a ocurrir una explosión de exponencial del espacio de búsqueda porque en realidad todas las
las posibilidades que se abren son exponenciales y ahí esto como que se vuelve bastante lento entonces
yo quería un decodificador para volver este problema un problema tratable en vez de agarrar
las infinitas oraciones del idioma me quedo con algunas que sean más probables con este algoritmo
de codificación logré reducir de infinito a algo finito pero aún así es demasiado lento porque
hay una explosión combinación combinatoria digamos de la hipótesis y me queda una cantidad
exponencial de hipótesis entonces como es tan grande este problema digamos como la cantidad
de hipótesis exponencial y este es un problema NP completo entonces se utilizan técnicas para
reducir el espacio de búsqueda y hay como dos tipos de técnicas algunas son con riesgo y otras
son sin riesgo las técnicas sin riesgo lo que quiere decir es que si yo aplica una técnica de
reducción de hipótesis sin riesgo la solución ideal que yo tenía dentro de mi búsqueda no la
voy a perder utilizando una técnica sin riesgo en cambio en la con riesgo si yo podría llegar a
perder la solución óptima bien entonces la técnica sin riesgo que conocemos es la de recombinación de
hipótesis que dice que si yo tengo dos hipótesis voy avanzando por dos caminos dentro del algoritmo
y llevo a dos hipótesis iguales por lo menos dos hipótesis que cubren las mismas palabras entonces
me puedo quedar con la que tiene mayor probabilidad de las dos y descartar la otra porque porque a
medida que yo voy a seguir avanzando en el algoritmo lo que va a pasar es que van a bajar las
probabilidades digamos yo eligiendo más palabras y eligiendo más frases me va a bajar la probabilidad
y nunca me va a pasar que la una de las hipótesis que tenía menos probabilidad vaya a subir en
realidad siempre va a tener menos entonces en definitiva yo puedo con seguridad descartar la
que tiene menos probabilidad bueno esa es recombinación de hipótesis pero ni siquiera con
eso alcanza digamos para reducir el espacio de búsqueda lo suficiente aún queda muchísimas hipótesis
entonces suele utilizar técnicas de podado con riesgo la técnica del histograma la técnica del
umbral el histograma significa que a cada paso digamos en cada paso del algoritmo yo me quedo
con los n las n hipótesis de traducción más probables y descarto las otras y la técnica con
un umbral dice que a cada paso del algoritmo me quedó con la hipótesis de mayor probabilidad y
las que estén a una distancia alfa máximo de esa cuál es el riesgo de las las técnicas de podado
que si la mejor traducción y la traducción óptima tenía algunas frases muy poco probables al
principio entonces probablemente yo descarte esa solución en los primeros pasos y no llegan
a encontrar la solución óptima digamos la perdí por el hecho de haber podado
sin embargo bueno tiene como como ventaja que en realidad reduce muchísimo el espacio de búsqueda
y vuelve vuelve este problema un problema tratable bueno y ahora sí qué significaba esa probabilidad
que estaba viendo en cada una de las hipótesis o sea el podado necesita tener las mejores hipótesis
y bueno y para la recombinación también necesito saber la probabilidad de la hipótesis bueno la forma
de calcular la probabilidad de hipótesis se divide en dos digamos tengo lo que encontré hasta el
momento la hipótesis lleva cubierta cierta cantidad de palabras entonces para esa cantidad de palabras
que ya llevo cubiertas utilizo los tres modelos el modelo de traducción el modelo de reordenamiento
del modelo de lenguaje utilizo los tres modelos para calcular la probabilidad de la frase hasta
el momento pero para lo que me falta traducir yo no puedo utilizar todo porque no tengo toda la
información de traducción entonces lo que hago es utilizar solamente el modelo de traducción y el
modelo de lenguaje descarto el modelo de reordenamiento y bueno entonces hago calcular una
probabilidad que es una parte con todos los tres modelos y otra parte sin el modelo de
reordenamiento bien este algoritmo que acabamos de escribir que hace esta búsqueda basándose en
hipótesis que utiliza recombinación hipodado hipótesis y bueno el calcula de las probabilidades
de esta manera se conoce como algoritmo búsqueda asterisco es un algoritmo de bin search que se
usa muchísimo en lo que es traducción automática estadística por ejemplo el sistema mouses acá
tenemos este ejemplos de herramientas open source o gratuitas que sirven para construcción de de
traductores automáticos el sistema mouses es un sistema open source para desarrollar este tipo
de traductores automáticos estadísticos e implementa este algoritmo de codificación de
búsqueda asterisco y bueno lo que tiene el sistema mouses de bueno es que en realidad lo que hace
además de implementar el decodificadores utiliza a los otros sistemas y los integra de alguna manera
entonces integra este otro sistema el irs tlm que es una herramienta para crear modelos de lenguaje
basados en n en n gramas y el otro sistema se quiza más más que lo habíamos mencionado hoy que es
el sistema que me permite alinear corpus de oraciones en los distintos idiomas llegando
los modelos del 1 al 5 de traducción de bm bueno entonces estas tres herramientas sirven si uno
quiere construir un traductor automático estadístico entre cualquier par de idiomas puede utilizar
estas tres herramientas y teniendo un corpus paralelo y un corpus monolingüe puede construirse un
traductor pero bueno además otra cosa que mencionamos en la clase pasada pero este eran los
sistemas basados en reglas los sistemas basados en reglas han caído un poco este digamos no tienen
tanta popularidad como antes sin embargo algunos se siguen usando y el sistema apertium es un sistema
opensource para construir sistema de traducción basados en reglas que tiene como un montón de
pares de lenguajes y bueno ya anda relativamente bien digamos entonces se sigue desarrollando
hasta hoy entonces es una alternativa opensource que está basada en reglas en vez de estar basado en
estadísticas y bueno esta es un resumen de lo que vimos así que dejamos por acá
