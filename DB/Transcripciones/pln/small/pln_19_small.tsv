start	end	text
0	23920	Una vez que elegí con el paso 1, elegí cuántas palabras en español le voy a usar, en el paso
23920	27960	2 lo que voy a elegir es una alineación, una función de alineación que me dice cada
27960	31600	palabra con cuál se va a corresponder, cada palabra del lado del español con qué palabra
31600	37920	en inglés se va a corresponder. Este modelo asume de manera muy naiv que todas las alineaciones
37920	45080	que yo puedo tener son equiprobables. O sea, asume que yo voy a tener un conjunto de alineaciones
45080	50160	posibles y todas van a tener la misma probabilidad. Bien, entonces, la probabilidad de elegir
50160	55440	una alineación en particular, si yo tengo un montón de alineaciones, digamos, la probabilidad
55440	60560	de elegir una alineación en particular, va a ser uno sobre la cantidad de alineaciones
60560	66080	que tengo, porque en realidad todas van a ser equiprobables. Bien, entonces, ¿cuántas
66080	70040	alineaciones puedo tener entre dos oraciones, una oración en inglés que tiene largo I y
70040	73720	una oración en español que tiene largo J? ¿Cómo puedo calcular cuántas alineaciones
73720	90480	existen? Más o menos, sí, casi la J. Recuerden que del lado de inglés yo tenía ciertas
90480	101520	palabras, en inglés tenía la palabra E1, E2 hasta, subí, y en español tenía las palabras
101520	112120	F1, F2 hasta F subj. Entonces, yo podía trazar líneas para alinear, pero además en inglés
112120	117040	yo siempre he considerado que tenía un token nul, entonces todas las palabras que no estaban
117040	123000	alineadas del lado del español iban a parar ahí. Así que en inglés en realidad no tengo
123000	127480	i posibilidades, tengo una más, tengo i más uno. Entonces, ¿cuántas formas tengo yo
127480	134040	de mapear estas J posibilidades en español con las I de inglés? Exacto, i más uno a
134040	137920	la J, porque yo tengo i más un opciones para la primera y más una opciones para la segunda,
137920	146920	etcétera, hasta que llevo al final. Así que son i más uno a las J alineaciones posibles.
146920	163280	Ojo, el nul es como una pisadita que hago yo para alinear cosas que no tienen un correspondiente.
163280	171400	O sea, yo tenía una palabra en español que… Varias de las Fes pueden estar alineadas
171400	179080	de nul, no importa en qué orden están. Eso. Bien, entonces eran i más uno a las J posibles
179080	189320	alineaciones, por lo tanto, la probabilidad de elegir una alineación A dada la oración
189320	194640	en inglés, la probabilidad de elegir una alineación cualquiera dada la oración en
194640	201040	inglés va a ser el producto de la probabilidad de haber sorteado un valor J primero, que era
201040	206280	epsilon por la probabilidad de elegir una alineación cualquiera para ese J, que es
206280	213560	uno sobre i más uno a la J. Bien, entonces esto lo resumimos como epsilon sobre i más
213560	225160	uno a la J. Epsilon sobre i más uno a la J es la probabilidad de, dada una oración
225160	231560	en inglés, elegir cierta alineación que yo voy a utilizar. Bien, ese fue el segundo
231560	238160	paso. El tercer paso es una vez que ya tengo la alineación, voy mirando cada palabra del
238160	243880	lado en inglés y le voy poniendo una palabra correspondiente del lado español. Para acá
243880	247960	voy a asumir que yo tengo una tabla de traducción, una tabla de traducción que me dice que tiene
247960	251360	de un lado todas las palabras en español y del otro lado todas las palabras en inglés,
251360	258200	entonces mi tabla va a tener una forma como, por ejemplo, hacer una tabla así que de un
258200	267360	lado va a decir las palabras en español como banco, perro, gato y más cosas y del otro
267360	275000	lado va a tener las correspondientes en inglés como bank, bench, cut, tree y más cosas.
275000	279400	Y entonces esta tabla va a decir la probabilidad de traducir una cosa en la gota. Entonces banco
279400	284400	probablemente tenga cierta probabilidad para bank y cierta probabilidad para bench, 0.4
284400	293000	y 0.6, 0.06 puse. Y para cut no va a tener ninguna probabilidad y para tree tampoco
293000	298720	y después perro no va a tener nada de esto, pero sí después y cut va a ser, no sé 0.8
298720	303000	en este caso, etcétera. Voy a tener una tabla bastante grande que tiene todas las posibilidades
303000	312040	de traducir una palabra como otra. Entonces si yo tengo esa tabla, lo que puedo decir
312040	320440	es que la forma de calcular la probabilidad de esa oración final que yo traduje va a
320440	323520	depender de cuáles son las palabras que yo elija, va a depender de cuáles son las palabras
323520	331080	que yo haya puesto dentro de mi oración para traducir. Entonces esa tabla que está
331080	338320	ahí definida, le llamamos acá en la slide, aparece como t de f sub x sub y dice que la
338320	348280	probabilidad de traducir la palabra es sub y como f sub x. Entonces saca de una cosa importante.
348280	357120	Si tenemos la oración en inglés, la oración en inglés recuerdan que tenía las palabras
357120	363560	f sub 1, f sub 2 hasta f sub n, la oración en español tenía las palabras f sub 1, f
363560	370080	sub 1, f sub 2 hasta f sub j. Y yo tenía en el medio una función de alineación que
370080	379600	me decía qué palabra se correspondía con cuál. Entonces no era f sub n ni f sub j.
379600	392200	Era f sub i y f sub j grande. Esto era f sub i y esto era f sub j grande. Entonces si yo
392200	399560	tengo una palabra cualquiera dentro de la oración en español, tengo un f sub j chica
399560	405160	dentro de la oración en español, esto se va a corresponder con algún f sub i chica
405160	409720	en la oración en inglés, digamos. Yo sé que esto se cumple por la función de alineación
409720	412560	porque agarra y mapea todas las palabras que están en español con algo que está del
412560	417160	lado del inglés, potencialmente con el token vacío nul.
417160	422240	Bien, entonces tengo una palabra del lado del español que es f sub j y una palabra del
422240	427520	lado del inglés que es f sub i. ¿Cuál es la relación entre f sub j y f sub i? ¿Cómo
427520	440000	es la relación entre sí, digamos? Yo puedo decir que el i es igual a algo de j. ¿De alguna
440000	447920	manera? La función de alineación, ahí está. O sea, el i es igual a la función de alineación
447920	455080	aplicada j. Como la i, el índice de este de acá es igual a la función de alineación
455080	463160	aplicada j. Entonces, yo puedo decir que la palabra f sub i es igual a la palabra e sub
463160	468600	a sub j. Así que puedo decir que, en realidad, los que están alineados son la palabra f
468600	475240	sub j está alineada con la palabra e sub a sub j. Y ahí me saqué el i de encima, digamos.
475240	481840	Simplemente, iterando sobre las palabras, iterando sobre la j puedo establecer la correspondencia
481960	490960	entre las dos palabras. Y eso es un poco lo que dice acá para terminar de armar lo que
490960	493800	es el modelo de traducción. Para terminar de armar el modelo de traducción dicen que
493800	497720	en el tercer paso yo voy a elegir cuáles son las palabras. Entonces, lo que voy a hacer
497720	505120	es iterar sobre todas las palabras y haciendo el producto de todas las probabilidades. O
505120	511520	sea, el producto de dado que yo tenía la palabra f sub j, perdón, dado que yo tenía la palabra
511600	517520	e sub a sub j en inglés, entonces elegir la palabra f sub j en español. Eso hago una
517520	525760	productoria con todos los valores de las distintas palabras. Bien, entonces ahí llegué
525760	535040	a el último de los valores que quería calcular, que es la probabilidad de f dado que conozco
535040	545560	ahí es igual a la productoria con j igual a 1 hasta j grande de el valor de la tabla
545560	559720	de traducción, que es t sub f sub j, t de f sub j e sub a sub j. Bueno, ahí tengo cómo
559720	565320	en cada paso fui calculando cosas, este se correspondía al paso uno del modelo, paso
565320	569040	uno, este se corresponde con el paso dos del modelo, en realidad este ya tiene el paso
569040	572360	uno y el paso dos juntos porque ya tengo el éxilón acá y este se corresponde con el
572360	581840	paso tres del modelo. El paso tres de la historia de generación. Mi objetivo con todos estos
581840	592280	valores que están acá es calcular p de f dado e. ¿Qué parámetros introduce? ¿Qué
592280	596440	parámetros fueron surgiendo a medida que yo iba iterando sobre estos pasos? Bueno, en
596440	600320	primer lugar el éxilón aquel que estábamos viendo, este es un valor que yo tendría que
600320	606440	estimar a partir de mirar en los corpus como son los largos de las oraciones relativos
606440	609920	y el otro parámetro importante es aquella tabla de allá, aquella tabla de traducción
609920	613800	es que me dice banco, con qué probabilidad lo puedo traducir como bank, con qué probabilidad
613800	618760	lo puedo traducir como bench, etcétera, etcétera. Esa tabla en realidad es un parámetro del
618760	622040	modelo, es un parámetro del sistema que si yo lo tuviera me alcanzaría con eso para
622040	627360	poder construirme este modelo y calcular la probabilidad de cualquier par de oraciones.
627360	639200	Bien, y entonces antes de continuar vamos a terminar de armar cuál es la imagen de esto,
639200	644240	que es decir yo en realidad lo que quería calcular era p de f dado e, que eso va a ser
644240	649640	mi modelo de traducción y de hecho va a ser el encargado de medir la adecuación de una
649640	655560	frase. P de f dado e lo puedo calcular con esta descomposición de pasos que hice acá
655560	679000	en realidad porque lo hago de la siguiente manera. Yo quiero calcular p de f dado e y
679000	683760	entonces voy a mirar lo que dice acá, p de f dado e es igual a la sumatoriana de p de f
683760	690960	dado e. ¿Qué significa eso? Que para traducir entre una oración en español y una oración
690960	695560	en inglés, o más bien para traducir entre una oración en inglés y una oración en
695560	700760	español hay muchas formas de alinear las palabras entre el inglés y en español y una
700760	704440	vez que yo elegí una forma de alinear hay muchas formas de elegir las palabras que vienen
704440	709120	después digamos yo miro la tarjeta de traducción y capaz que hay varias maneras de elegir distintas
709120	714320	palabras. Entonces lo que eso significa es que no existe una sola manera de traducir una
714320	717800	oración en inglés a una oración en español. Yo puedo encontrar varias formas de alinear
717800	721880	las palabras y varias formas de elegir las palabras de manera que muchas alineaciones son
721880	731000	posibles. Entonces para saber cuál es la probabilidad de traducir p de f dado e, entonces
731000	734320	yo voy a tener que sumar sobre todas las alineaciones posibles sobre todas las formas
734320	740080	de alinear las dos oraciones f y e, voy a tener que iterar sobre eso y para cada una
740080	745720	voy a tener que calcular la probabilidad parcial. Entonces digamos yo tengo cinco formas de
745720	750000	alinear las dos oraciones, cinco es un número un poco raro pero digamos tengo n formas de
750000	754800	alinear las dos oraciones, voy a tener que mirar bueno para la primera alineación cuál
754800	759920	es la probabilidad de encontrar la oración f para la segunda alineación cuál es la
759920	762840	probabilidad de encontrar la oración f para la tercera oración y así hasta llegar al
762840	768840	final y agarro y sumo todo eso. Eso lo puedo hacer porque las alineaciones son una descomposición
768840	772520	del espacio de probabilidades. En realidad yo puedo descomponer el espacio de probabilidades
772520	777480	en pedacitos disjuntos y cada alineación va a ser uno de ellos. Así que digamos que
777480	782040	para calcular el modelo de traducción p de f dado e necesito sumar sobre todas las alineaciones
782040	788960	posibles. Ahora lo que me falta es saber cómo calculo este valor de acá. Así que lo que
789040	794840	estoy diciendo es que la probabilidad de f dado e es la suma sobre las alineaciones
794840	800840	de la probabilidad de f y esa alineación dado e. Eso es simplemente lo que dice ahí
800840	805560	en la slide. Lo que me falta calcular entonces es esta parte de acá y esa parte de acá la
805560	811800	calculo de esta manera. Yo digo que la probabilidad de f dado e es igual, ahí está más o menos
811800	818800	el resultado final pero podemos sacar qué es lo que tendría que poner de este lado.
819440	826440	Ahora sí me acuerdo bien. Ah, ahí está. Por definición de probabilidad condicional.
826440	833440	Eso. p de f dado e, le voy a dar varias maneras a hacerlo pero esto se puede definir como
843480	850480	p de f a e sobre p de e. No? Por definición de probabilidad condicional. Pero además
851480	858480	esto si quiero podría llegar a decir esto es lo mismo que p de f a e sobre p de e por
861320	868320	la cualidad que me faltaba. No. A e. Por p de a e sobre p de a e. Era esto lo quería.
880480	887480	O sea, yo puedo agarrar esta probabilidad que está acá y multiplicarla y dividirla por
888120	891920	el mismo número, que sé que son mayores que cero, así que en definitiva esa división
891920	897920	me va a dar uno. Y ahí yo puedo tomar y asigno este con este y este con este. En definitiva
897920	904920	lo que me queda es si asocio estos dos me va a quedar p de f dado a e y si asocio estos
904920	911920	dos de acá me va a quedar p de a dado e. ¿Qué es lo que dice allá? La probabilidad
916160	922680	de p de f a dado e, bueno, sí, de los dos, de f y a dado e es igual a la probabilidad
922680	928960	de f dado a e por la probabilidad de a dado e. Bien, y estos dos valores que están acá
928960	932600	no los elegí por casualidad sino que son los valores que tenía antes en el modelo.
932600	939600	O sea, yo tenía que el p de a dado e era igual a epsilon sobre y más uno a la j y el
941520	948520	otro era la productoria desde j igual a 1 hasta j grande de las valores de traducción,
950320	957320	el f sub j y el e sub a sub j. Entonces, en definitiva puedo calcular p de f a dado e
958000	961400	y además puedo calcular haciendo una suma sobre todas las alineaciones posibles, puedo
961400	968400	calcular el p de f dado e. Bien, con eso y con todo ese montón de cocciones llegamos
969920	974720	a construir lo que es un modelo de traducción, o sea, solamente teniendo una tabla de traducciones
974720	979400	que me diga cuál es la probabilidad de traducir una palabra. Como otra palabra, yo puedo llegar
979400	986400	a definirme cuál es la probabilidad de traducir una oración dada otra oración. Bien, y hay
986400	993400	una cosa más, bueno, esto ya lo estuvimos viendo que aplicamos en cada paso, y hay una
994240	1001240	cosa más que es si yo tuviera las dos oraciones, digamos, la oración en inglés y la oración
1002120	1007080	en español y además tuviera la tabla esta con todas las probabilidades, yo podría hacer
1007080	1010800	un algoritmo de programación dinámica, un algoritmo estilo Viterbi que vaya recorriendo
1010800	1015400	alineaciones y me diga cuál es la alineación más probable. No vamos a ver los detalles
1015400	1018800	del algoritmo, pero hay una forma de decir, bueno, voy recorriendo las dos oraciones y
1018800	1024000	me voy quedando con las subsecciones más probables y al final me termina devolviendo
1024000	1029280	cuál es la alineación más probable dada esas oraciones. O sea, que si yo tuviera ya
1029280	1034520	esa tabla de traducciones, esa tabla de probabilidad de traducción, podría construirme las alineaciones
1034520	1041520	del corpus. Así que bueno, hasta el momento decíamos, bueno, suponemos que tenemos esta
1041600	1047360	tabla de traducción que me dice para bank si se traduce, perdón, para banco si se traduce
1047360	1053400	como bank o como bench, etc. Estaba diciendo que tenía esa tabla, pero en realidad la
1053400	1058040	realidad es que no tengo esa tabla y me gustaría poder construirla. Entonces, nos gustaría
1058040	1062560	poder estimar esas probabilidades para poder construirme esa tabla. Si yo tuviera un corpus
1062560	1066040	paralelo, simplemente podría ir recorriendo el corpus y contando cuántas veces aparece
1066040	1070720	banco alineado con bench y cuántas veces aparece alineado con bank y ahí sacaría
1070800	1078280	una probabilidad, pero no tengo las alineaciones. Y por lo que vimos, digamos, recién, si yo
1078280	1082320	tuviera la tabla, entonces yo además podría ir recorriendo el corpus y construirme las
1082320	1087560	alineaciones. Así que si yo tuviera las alineaciones podría contar y sacar la tabla, si yo tuviera
1087560	1092800	la tabla podría pasarle un algoritmo y construir las alineaciones. Pero la verdad que no tengo
1092800	1097120	ninguna de las dos cosas. Entonces se vuelve un problema de huevo y la gallina. O sea,
1097120	1101160	si yo tuviera las alineaciones construiría el modelo, construiría la tabla de probabilidades,
1101160	1105720	si yo tuviera la tabla de probabilidades podría construir las alineaciones. Para este tipo
1105720	1111360	de problemas, en los cuales yo tengo como dos variables interdependientes y no conozco
1111360	1114960	exactamente el valor de ninguna de las dos, se utiliza lo que se conoce como el algoritmo
1114960	1120760	expectation maximization o maximización de la esperanza. Y bueno, es un algoritmo que
1120760	1125680	sirve exactamente para este tipo de problemas. En realidad lo que va a hacer el algoritmo
1125840	1131040	iterar es un algoritmo iterativo que va tratando de converger una solución y lo que hace es
1131040	1139320	decir, bueno, yo no tengo ninguno de los dos valores. O sea, si yo tuviera mi tabla de
1139320	1143520	probabilidad de traducción me podría calcular las alineaciones y tuviera mis alineaciones
1143520	1147280	me podría calcular la probabilidad de traducción. Entonces lo que hace es decir, bueno, asumo
1147280	1152360	que mi tabla de traducción va a ser uniforme, digamos. Cualquier palabra se puede traducir
1152480	1157520	como cualquier otra palabra con la misma probabilidad. A partir de eso calculo alineaciones y a partir
1157520	1164520	de esas nuevas alineaciones calculo otra vez la tabla. Y de vuelta, con esa tabla que calculé,
1164960	1169360	vuelvo a medir las alineaciones y de vuelta con esas nuevas alineaciones vuelvo a calcular
1169360	1174960	la tabla. Entonces, aunque no me crean, esto después de muchas iteraciones va convergiendo
1174960	1179440	a algo. Y parece mágico, ¿no? Parece como que, en realidad si yo no tengo ninguno de
1179520	1186000	dos valores, no debería nada, debería como dar fruta. Pero voy a tratar de comenzarlos
1186000	1193000	de que, en realidad, esto sí funciona, con un ejemplito. Bien, tenemos. Entonces, vamos
1194400	1199680	a construir un sistema que es de traducción entre francés y el inglés donde hay un cuerpo
1199680	1203000	muy grande, pero bueno, nos vamos a concentrar solo en tres pequeñas oraciones citas que
1203000	1207000	dicen la mesón se traduce como de House, la mesón blue se traduce como de Blue House
1207160	1212160	y la flea se traduce como de Flower. Entonces, al principio lo que hago es decir, bueno,
1212160	1216760	todas las traducciones entre todas las palabras son equiprobables, así que lo que me va a
1216760	1221240	quedar es cuando reparten entre las alineaciones, todas van a tener el mismo peso. Entre la y
1221240	1225760	mesón, la probabilidad de que la se traduzca como D o que se traduzca como House va a ser
1225760	1230720	la misma, en realidad porque todas las alineaciones son equiprobables. En la mesón blue también
1230720	1234200	pasa lo mismo, la probabilidad de traducir la como D como Blue o como House va a ser
1234360	1244360	la misma y en la flea pasa igual. Entonces, eso es la primera, el primer paso, digamos,
1244360	1249640	en el primer paso yo voy a tener todas las alineaciones equiprobables y todas las valores
1249640	1250760	de las palabras iguales.
1263760	1270920	Entonces, en mi algoritmo yo empecé con una tabla de traducción que era toda uniforme,
1270920	1274520	digamos, yo tenía la probabilidad de traducir cualquier palabra en cualquier otra, era la
1274520	1280240	misma. A partir de eso yo me construí estas alineaciones que también parece que son
1280240	1283760	todas equiprobables y parece que no tienen como mucha información. Entonces lo que voy
1283760	1288280	a hacer ahora, a partir de esto, es tratar de construirme de vuelta la tabla de traducciones
1288280	1292160	pero mirando estas nuevas alineaciones que hay. Entonces lo que voy a construir es una
1292160	1302800	tabla que tiene todas las palabras del lado de francés, tiene la mesón blue flower y
1302800	1308680	de House blue flower.
1308680	1317520	Y para llenar esta nueva tabla, lo que tengo que hacer es iterar sobre las alineaciones,
1317520	1320920	mirar cada una de las palabras cuantas veces está alineada con las otras y contar, o sea,
1321680	1328120	y sumar los pesos de cada una de las alineaciones. Entonces la alineación entre la y de. En total,
1328120	1332160	mirando ese ejemplo de corpus, ¿cuánto me daría? ¿Cuál sería el peso de esa alineación?
1334800	1339640	Para verlo, en realidad lo que hago es contar, miro cuantas veces la y de están alineados.
1339640	1346240	Entonces tengo 0.5 de peso en la primera, en la segunda tengo 0.33 y en la última tengo
1346240	1354320	0.5 de vuelta. Así que en total tengo como 1.33 de peso entre la y de. Después miro,
1354320	1360680	entre la y House, ¿cuánto peso tengo? ¿cuánta masa de probabilidad tengo? Bueno, tengo 0.5
1360680	1368160	en la primera relación, 0.33 en la segunda y nada en la tercera. Por lo tanto en total tengo 0.83
1368880	1375280	de probabilidades entre la y House. Después miro, entre la y blue, ¿cuánto peso tengo?
1379600	1385480	Solamente 0.33 solo está en la del medio y entre la y flair, ¿cuánto tengo? No, entre la y
1385480	1391280	flower, ¿cuánto tengo? 0.5 solo aparece en la del final. Bien, completemos la siguiente,
1391280	1401480	entre mesón y de, ¿cuánto tendría? 0.83, está en la primera y en la segunda, entre
1401480	1414720	mesón y House, entre mesón y House y 0.83 porque aparece en las dos. Bien, entre mesón y blue,
1414720	1419360	solamente aparece en la segunda, así que voy a tener 0.33 y entre mesón y flower no tengo nada.
1420000	1426360	Después entre blue y de, solamente aparece en la segunda, así que voy a tener 0.33, entre blue
1426360	1433040	y House, creo que de vuelta tengo 0.33 y entre blue y blue también 0.33 y no aparece junto con
1433040	1441800	flower. Y para después, para flair tengo 0.5 con de, 0 con House, 0 con blue y 0.5 con flower.
1442800	1447920	Bien, entonces hice una pasada por todas las alineaciones y me calculé cuáles son los
1447920	1452320	pesos relativos de cada una de estos pares. Lo siguiente que hago, como esto va a ser una
1452320	1456440	probabilidad, es normalizar. Entonces me voy a construir una tabla, digamos normalizando por,
1456440	1462040	digamos, voy a sumar en cada fila y voy a dividir entre la cantidad que aparece para cada fila,
1462040	1472120	así que de vuelta me construyo la tabla, que me queda la mesón blue flower y de este lado de
1475560	1486600	House acá, de House y blue flower. Y lo que voy a hacer es normalizar, entonces si yo sumo estos
1486600	1495240	de acá, creo que me da 2 en total, no, 3 en total. Tengo los valores acá, no tengo que
1495240	1500080	hacer los cálculos, pero sí, me da 3 en total. Entonces lo que pasa cuando yo normalizo es que
1500080	1509440	acá me queda 0.44, acá me queda 0.28, acá me queda 0.11 y acá me queda 0.17. Pues el segundo,
1509440	1518720	también lo normalizo esta vez entre 2 y me queda 0.42, 0.42, 0.16, 0. El tercero ya suma 1,
1518720	1531160	así que me queda 0.23, 0.33, 0.33, 0 y el último también queda igual, 0.5, 0, 0, 0.5. Bien,
1531440	1539240	entonces me construí una nueva tabla de probabilidad de traducción, dado que ahora las
1539240	1544560	alineaciones serían estas. Y noten lo que pasó acá, si yo miro la fila correspondiente a la,
1544560	1557520	¿qué es lo que pasa ahora con esa fila? Recuerda que yo empecé teniendo todas las
1557520	1561680	probabilidades de traducción de que parecen palabras, eran equiprobables. Si yo ahora miro la
1561680	1576360	fila de la, ¿qué es lo que pasa? Exacto, aparece claramente que la asociación entre la y de es
1576360	1582160	más fuerte, tengo un 0.44 de probabilidad de traducir la como de y tengo bastante menos en
1582160	1587160	los otros, tengo 0.28, 0.11, 0.17. Y yo había empezado diciendo que eran equiprobables, entonces yo
1587200	1595080	probablemente tenía 0.25, 0.25, 0.25, 0.25 en cada una. Y después de un paso de la iteración,
1595080	1602800	descubrió que la ID tiene más chance de ser una traducción de la otra, en vez de traducir la
1602800	1608080	como House o la como Blue o la como Flower. Eso pasa en el primer paso, en la primera iteración
1608080	1613720	el tipo descubre, el algoritmo descubre que la asociación entre la ID es bastante más fuerte.
1614440	1621160	Como pasa eso, lo que va a pasar es que cuando yo reparta de vuelta en las alineaciones estas
1621160	1625760	líneas que se corresponden a la asociación entre la ID van a estar más fuertes, van a tener un
1625760	1632560	poco más de peso y como esto es una distribución de probabilidades, esa masa que ganó la asociación
1632560	1636360	entre la ID se va a tener que sacar de otras alineaciones posibles, o sea si la está asociada
1636360	1641880	con D, entonces no está asociada con las otras que están alrededor. Entonces esa masa que se pierde,
1641880	1649720	digamos, o sea que gana en la D se tiene que repartir en las otras alineaciones posibles,
1649720	1656200	o sea en las que no son entre la ID. Entonces después de una iteración la asociación entre
1656200	1663880	la ID empieza a ser más fuerte y como pasa eso en la siguiente iteración va a empezar a descubrir
1663880	1668280	que como la estaba alineado con D, entonces mesón tiene que estar alineado con House
1668280	1675760	y como mesón estaba alineado con House, digamos, esa misma masa de probabilidad se va a traducir,
1675760	1681120	a transferir a la segunda y lo mismo, como la estaba alineado con D, entonces Fleur tiene
1681120	1687560	que estar alineado con Flour. Entonces si yo sigo iterando en estos pasos, en cada paso lo que va
1687560	1691520	a pasar es que se va a mover un poco más de probabilidad hasta que al final va a terminar
1691520	1697120	descubriendo cuál es la alineación real de las palabras, o sea va a descubrir que la va,
1697120	1703080	o sea con D, mesón con House, Blue con Blue, Fleur con Flour. ¿Cómo es que va a descubrir eso?
1703080	1707160	Porque en cada paso lo que va pasando es que algunas de las asociaciones como están, como
1707160	1712640	aparecen, que ocurren digamos en más oraciones, tienen más fuerza que otras, entonces el peso
1712640	1717360	que esas asociaciones ganan lo va sacando de otro lado y eso hace que de otro lado se empicen
1717360	1724160	a generar otras alineaciones diferentes. Entonces al final esto termina convergiendo y termina
1724200	1729120	revelando lo que es la estructura suyacente de las palabras y cómo se alinean unas con otras.
1729120	1734920	Bueno, una vez que yo termine de hacer esto puedo agarrar y construirme efectivamente la tabla
1734920	1740640	final de traducciones que es simplemente busco cada una de las posibles traducciones, digamos de los
1740640	1748760	posibles pares y saco las probabilidades. ¿Y qué pasó acá? Mientras yo estaba construyendo mi
1748760	1754000	modelo de traducción, mientras yo estaba construyendo la tabla de traducciones además de como
1754000	1759080	efectos secundarios se construyó un corpus alineado, un corpus que está alineado a nivel de palabras.
1764000	1772120	Así que bueno, el algoritmo de Spectation Maximization funciona de esa manera, tiene siempre dos
1772120	1781640	pasos, un paso de Spectation y un paso de Maximization. En este caso el paso de Spectation se
1781640	1788560	trataba de agarro la tabla de probabilidad de traducción que tengo y con eso me armo alineaciones y
1788560	1792360	después el de Maximization es al revés agarro las alineaciones que acabo de construir y me
1792360	1796520	armo una nueva tabla y voy iterando todos esos pasos hasta que eventualmente converge.
1796520	1803800	Bien, dijimos que eran cinco modelos de IBM, no vamos a ver muy en detalle los otros, o sea,
1803800	1809360	solo mencionar que empiezan a crear complejidad. En este modelo uno habíamos dicho que todas las
1809360	1814960	alineaciones eran equiprobables, en el modelo dos abandonan esa noción y dicen bueno, en vez de
1814960	1820440	alineaciones equiprobables, yo voy a tener un modelo de reordenamiento de las palabras para decir,
1820440	1825320	bueno, tengo cierta probabilidad de que las palabras que están, si yo tengo I palabras en inglés,
1825320	1830520	J palabras en español, tengo cierta probabilidad de mover la palabra I y la palabra J y bueno,
1830520	1835800	y así siguen subiendo en complejidad hasta llegar al modelo cinco, que modelo cinco es el que anda
1835800	1842640	mejor, pero de todas maneras estos son modelos que ya no se usan, digamos, esto es del año 93 y en
1842640	1848640	general se han obtenido mejores resultados abandonando estos modelos. Entonces el que vamos a pasar a
1848640	1853600	ver a continuación es un modelo bastante más moderno que es lo que sí se utiliza hoy en día
1853600	1871040	en traductores como los de Google. Sí. Es que en realidad, claro, a ver, estos modelos
1871040	1876160	estadísticos no utilizan ningún tipo de analizador morfológico y nada para sacarlo. Hay otros modelos
1876160	1880280	que sí lo hacen, no vamos a dar ninguno en esta clase pero hay otros modelos que sí hacen
1880280	1886120	uso de esa información. Igual son como refinamientos, creo que ninguno lo tiene como en la base del
1886120	1892480	modelo, el uso de parto speech, pero sí cuando vos no sabes una palabra, digamos una palabra que
1892480	1897880	es desconocida, en realidad utilizar información sobre parto speech y eso probablemente te ayude.
1897880	1903360	En estos modelos por lo menos no lo habían tenido en cuenta. Bien, entonces sí, lo que vamos a ver
1903360	1907880	ahora es el modelo de frases que es algo más moderno y es, o sea, el Google Translate o Bing
1908040	1912280	Translate se basan en modelos de este estilo. Y bueno, y antes de ver cómo se modelo de frases,
1912280	1916760	volvamos un poco a lo que era la alineación entre palabras. Yo tenía esta frase clásica,
1916760	1923440	¿no? María no dio una bofetada de la bruja verde, en inglés era Mary did not slap de Greenwich y una
1923440	1927760	alineación entre esas dos oraciones en realidad se vería como algo así. Yo tengo que María se
1927760	1933440	alinea con Mary, no se alinea con did not, slap se alinea con daba una bofetada, de se alinea
1933440	1938760	con ala, podría ser solamente con la y el a que no está alineado nada. Green se alinea con verde
1938760	1945040	y bruja con wedge. ¿Qué diferencia tiene esto con la otra alineación que habíamos visto hoy?
1946400	1950600	A ver si se les ocurre algo distinto que tiene esta alineación y la que habíamos visto hoy.
1954800	1959800	Era not con no, sí. ¿Y qué es lo que cambia acá para que pase eso?
1963440	1971720	Lo que estaba pasando hoy era que yo partía de las palabras en español,
1971720	1975080	iba las palabras en inglés y yo tenía una función que me mapeaba las palabras en
1975080	1979000	español con las palabras en inglés. Entonces yo a cada palabra en español como máximo le
1979000	1983720	podía hacer corresponder una palabra en inglés. Entonces me quedaba que yo podía expresar cosas
1983720	1988760	como que daba una bofetada, daba, está asociado a slap, una está asociado a slap, bofetada está
1988760	1993840	asociado a slap, eso lo podía expresar, pero no podía expresar algo como esto, que no está
1993840	1998360	asociado did not, porque no sería una función. Yo no puedo asociar uno de los valores de la función
1998360	2005480	con dos cosas del lado del codominio. Y acá en realidad no puedo hacerlo ni en este sentido ni
2005480	2009000	en el otro sentido, con una función no me sirve porque de vuelta me pasa que slap está asociado
2009000	2014880	tres cosas. Entonces con una función de alineación yo no puedo construir este tipo de expresiones,
2014880	2020760	en realidad necesito algo como un poco más poderoso. Esto es lo que decíamos, los modelos
2020760	2024920	de IBM siempre usan un mapeo de uno a muchos, usan una función de alineación, mapeo uno a muchos,
2024920	2029400	pero en realidad lo que necesito para poder capturar realmente cómo funciona en el lenguaje es
2029400	2033080	mapeo de muchos a muchos. Yo voy a tener que un conjunto de palabras se va a traducir en otro
2033080	2038480	conjunto de palabras. En definitiva lo que pasa es que pequeñas frases se traducen como otras
2038480	2044960	pequeñas frases, por eso necesito un mapeo de muchos a muchos. Entonces bueno hay algoritmos que
2044960	2051280	agarran estos mapeos que como construimos recién, el mapeo de uno a muchos en las dos
2051280	2055520	direcciones digamos y a partir de eso construyen este mapeo de muchos a muchos. Por ejemplo,
2055520	2059960	el algoritmo de la herramienta quizá más más, lo que hace es decir bueno yo tengo un corpus en
2059960	2066840	inglés y en español alineo utilizando los los modelos de IBM digamos voy alineo por un lado
2066840	2071880	de inglés español y por otro lado de español inglés y acá me quedan dos mapeos de uno a
2071880	2077040	n digamos dos mapeos con funciones y después lo que hago es intersectar esos dos esas dos
2077040	2083000	alineaciones que me quedaron y unirlas. Cuando las intersecto obtengo lo que se conoce como
2083000	2090680	puntos de alta confianza, los puntos negros son los puntos de alta confianza que son los de la
2090680	2095700	intersección y los puntos grises son los que están en la unión, o sea los que pertenecían a algunos
2095700	2099020	de los dos modelos. Entonces la herramienta de lo que hace es decir bueno una vez que yo tengo la
2099020	2104500	intersección y la unión hago crecer los puntos que están en la intersección, colonizando otros
2104500	2108740	puntos que estén en la unión, hasta que al final termino completando digamos toda la imagen. Este
2108740	2114320	punto que quedó solito ahí ese no sería parte de la alineación al final, sólo los que podéis
2114320	2123160	llegar moviéndote a través de puntos ya conocidos. Entonces bueno eso es una forma que utiliza se
2123160	2128960	llama el algoritmo de Oginey que partiendo alineaciones unidireccionales digamos me permite
2128960	2134640	construir una alineación completa muchos a muchos entre las palabras. Bien eso le quería mencionar
2134640	2138800	acerca de las alineaciones entre palabras y ahora sí vamos a ver cómo funciona un modelo
2138800	2145120	basado en frases. Un modelo basado en frases tiene cierta semejanza con el modelo anterior que
2145120	2149840	habíamos visto pero es un poco más expresivo en realidad yo parto de una oración por ejemplo en
2149840	2154920	alemán que decía Morgan Fligge y Gnaskana de la Sur Conference. Lo primero que hace el modelo
2154920	2159920	cuando quiere traducir digamos en este caso es decir bueno yo voy a segmentar esa oración de
2159920	2165760	origen en cierta cantidad de frases. Después voy a traducir cada una de esas frases usando una
2165760	2169280	tabla de traducción y esta vez no es una tabla de traducción de palabras sino que es una tabla
2169280	2175040	de traducción de frases que me dice para acá frases con que otra frase se corresponde y una vez
2175040	2179640	que yo traduje cada una de esas frases las voy a reordenar de alguna manera buscando que suene
2179640	2185080	lo más natural posible buscando aumentar la fluidez de esa oración. Entonces como que la
2185080	2188760	historia de generación es un poco más simple que la otra no tenía que ir sorteando cosas simplemente
2188760	2196560	digo separo mi oración en segmentos que les voy a llamar frases los traduzco y los reordeno.
2196560	2203160	Esa segmentación en frases no tiene porque tener un significado lingüístico yo no voy a separarlas
2203160	2208240	en grupo nominal, grupo verbal, grupo profesional, etcétera. No tengo por qué o sea capaz que yo
2208240	2213200	segmento las frases y justo me queda un grupo preposicional capaz que no. Lo único que tiene que
2213200	2217800	pasar es que estos segmentos que yo construyo tienen que estar en mi tabla de traducción de frases
2217800	2222080	alcanza con eso como para que yo pueda utilizarlos en mi traducción pero no tienen por qué tener
2222080	2229680	una motivación lingüística. Bueno entonces un modelo basado en frases tiene estos componentes
2229680	2234400	parecido al anterior porque de vuelta yo lo que quiero hacer es encontrar la probabilidad de
2234400	2240200	pdf dado e digamos sigo teniendo la misma ecuación fundamental de la traducción automática estadística
2240200	2246240	la quiero resolver necesito pdf dado e y pdf solo que ahora el pdf dado e lo voy a calcular de una
2246240	2250960	manera distinta voy a decir que para calcular esto tengo un modelo de traducción de frases y un modelo
2250960	2255600	de reordenamiento un modelo de una gran tabla de frases que me dice cada frase con qué probabilidad
2255600	2261240	la traduzco en otra y después una forma de decir cómo reordeno esas frases para tener mejores
2261240	2267720	oraciones y bueno y como siempre voy a tener otro componente que es el que mide la la fluidez que es
2267720	2273920	el modelo del lenguaje porque los modelos de frases funcionan mejor que los modelos basados en
2273920	2280280	palabras porque la frase ya tiene cierto contexto la frases en realidad son como pequeños grupos de
2280280	2288040	palabras que yo puedo traducir uno uno en el otro entonces cosas como dar la mano dar una
2288040	2292720	bofetada a tomar el pelo etcétera todas esas cosas como expresiones son mucho más fáciles de traducir
2292720	2296360	si en realidad yo ya sé que esta expresión que son tres cuatro palabras la puedo traducir en esta
2296360	2300200	otra expresión que son tres cuatro palabras es como más expresivo entonces se puede aprender más
2300200	2305440	cosas y bueno obviamente cuanto más cuanto más datos tenga cuanto más largo sea el cuerpo que yo
2305440	2309400	tengo yo puedo aprender frases más largas mejores probabilidades y mejores frases
2311400	2315920	bueno acá hay un ejemplo de cómo sería una tabla de traducción de frases o sea es parecido a la
2315920	2320240	tabla de traducción de palabras o es lo que acá tengo de en borschlag o sea si yo busco la fila
2320240	2325160	asociada en borschlag o sea encontraría todas estas traducciones de propósal con 62 por ciento
2325160	2331400	de probabilidad posesivo propósal con 10 por ciento a propósal con 3 por ciento etcétera o
2331400	2336680	sea como ven se traducen frases en frases bueno y cómo hago para aprender una tabla de traducción
2336680	2344240	de frases yo parto de esta alineación de palabras digamos esta alineación completa que ya no es
2344240	2349880	una función sino que es digamos una alineación de muchos a muchos y voy a tratar de encontrar todos
2349880	2355560	los todas las frases todos los pares de frases que son consistentes con la alineación a qué me
2355560	2364760	refiero con que son consistentes acá hay ejemplos yo quiero decir que mariano y maría did not son
2364760	2370440	es son un par de frases que son consistentes con esta alineación en cambio mariano y maría did
2370840	2376240	como es que miro esto lo que pasa es que cuando yo tengo mariano y maría did la palabra no está
2376240	2381400	alineada con did not y el did not digamos el no no pertenece hasta alineación que yo estoy
2381400	2386720	tratando de decir entonces digo que es no consistente lo mismo pasa con si yo tato alinear mariano
2386720	2393040	daba y maría did not lo que pasa ahí es que daba no está digamos los puntos de alineación de daba
2393040	2396920	no están dentro de este cuadrante que estoy tratando de buscar entonces en definitiva digo que no es
2396920	2401280	consistente las alineaciones consistentes correctas son las que consideran todos los
2401280	2406080	puntos dentro de ese cuadrante entonces mariano está asociado con maría did not y es así es
2406080	2416160	consistente así que como aprendo frases consistentes empiezo por las alineaciones digamos
2416160	2419400	empiezo por la alineación de palabra después busco de una palabra y digo bueno me quedo
2419400	2424320	con todas esas traducciones de palabras y las pongo en mi tabla de frases y después voy
2424320	2428840	tomando de a dos y me quedo con todas esas otras frases y las voy agregando mi tabla de frases
2428840	2435120	después me puedo avanzar en uno y tomar de a tres tomar de a cuatro y llegar a tomar incluso
2435120	2440160	toda la oración como frases entonces a partir de estas oraciones que tenían no sé este 1 2 3
2440160	2446120	4 5 6 7 8 9 palabras yo termino aprendiendo como 17 frases digamos cada vez más grandes
2447760	2452800	y bueno hoy voy sacando esto de todo el corpus y calculando mi tabla de probabilidades
2454360	2459480	de qué manera calculo esas probabilidades yo lo que puedo hacer es como siempre ver cuántas
2459480	2465560	veces aparece en el corpus y contar o si no si yo tenía construido el modelo anterior el modelo
2465560	2470400	de la tabla de traducciones de palabra a palabra en realidad lo que puedo hacer es aprovechar ese
2470400	2474920	modelo de traducción de palabra a palabra y decir bueno me armo una traducción entre un par de
2474920	2479120	frases basándome en las traduciones palabra a palabra son como dos formas distintas de
2479120	2487920	construirlo y a veces hasta complementarias bien eso fue el modelo de frases los modelos
2487920	2493200	de frases son los más usados hoy en día en realidad en lo que es la traducción automática son los
2493200	2499160	que han dado mejores resultados y bueno y nos faltaba una cosa para terminar el toda la imagen de
2499160	2502200	lo que es la traducción automática estadística que es la decodificación
2502200	2508160	entonces damos un resumen de lo que teníamos hasta ahora
2509360	2514920	hasta ahora yo partí de yo quería resolver la cocción fundamental de la traducción automática
2514920	2520720	estadística y yo tenía un corpus paralelo que tenía texto en el idioma origen y el idioma
2520720	2524960	destino y a partir de ciento análisis estadístico yo me construí un modelo de traducción que es
2524960	2530920	lo que vimos en esta clase además yo tenía cierto cierta cantidad de texto en el idioma
2530920	2535360	destino y a partir de cierto análisis estadístico me construí un modelo de lenguaje que me dice
2535360	2542440	que tan fluido es una oración en el lenguaje destino entonces ahora lo que me falta recuerden
2542440	2546920	que yo lo que tenía que hacer era iterar sobre todas las oraciones del lenguaje destino y pasarlas
2546920	2550320	a través del modelo de traducción y del modelo de lenguaje para que me dé la probabilidad de esa
2550320	2556440	oración bueno lo que me falta es el algoritmo de codificación que en vez de probar con todas
2556440	2560680	las oraciones del lenguaje destino me va a decir unas cuantas oraciones para probar capa que me
2560680	2566120	dice 150 oraciones para probar sobre las cuales utilizar el modelo de traducción y el modelo
2566120	2571000	de lenguaje entonces esto es como un diagrama de de módulos en los cuales el algoritmo de
2571000	2575640	codificación utiliza los dos módulos tanto el de traducción como el de lenguaje
2577640	2584560	bueno cómo funciona el algoritmo de codificación el que vamos a ver es un algoritmo de codificación
2584560	2591200	de tipo beam search y bueno funciona de la siguiente manera yo tengo la oración maría no
2591200	2596440	dio una bofetada a la bruja verde y la quiero traducir al inglés y tengo una tabla de traducción de
2596440	2603960	frases entonces mi oración maría no dio una bofetada a la bruja verde yo busco en la tabla
2603960	2610200	de frases cuáles de esas de digamos cuáles segmentos cuáles subsegmentos de esa oración yo
2610200	2614280	puedo encontrar en la tabla de traducción de frases entonces voy a encontrar por ejemplo que maría lo
2614280	2619240	puedo traducir como mary no lo busco en la tabla y lo puedo traducir como not como did not o como
2619240	2625640	no dio lo puedo traducir como git pero además no dio esa frase entera yo lo busco en la tabla y
2625640	2630720	me parece que la puedo traducir como did not give dio una bofetada toda esa frase lo puedo traducir
2630720	2638840	como slap una bofetada lo puedo decir como a slap y bueno y otras cosas bruja lo puedo decir como
2638840	2642160	witch verde como green pero además en algún lado de la tabla tengo que bruja verde lo puedo
2642160	2648560	traducir como green witch y así digamos yo puedo encontrar tengo diferentes maneras de segmentar
2648560	2652560	la oración y además para cada uno de esos segmentos puedo encontrar distintas formas de
2652560	2660280	traducirlo en el lenguaje destino con mi tabla de frases entonces el algoritmo de codificación
2660280	2665040	funciona de la siguiente manera empezamos teniendo en cada paso del algoritmo vamos a tener un conjunto
2665040	2670480	de hipótesis de traducción se llega a ver ahí lo que dice ahi ojo más o menos
2677160	2677660	bien
2680000	2685520	acá quedaron mal los cuadraditos bueno en cada uno de los pasos yo voy a tener un conjunto de hipótesis
2685520	2692440	de traducción al principio del algoritmo voy a empezar con lo con una hipótesis vacía como
2692440	2696880	se le está hipótesis dice que lo importante de leer es la parte de la f que tiene un montón de
2696880	2701560	guiones significa que no hay ninguna palabra del español cubierta esas son todas las 9 creo 9
2701560	2707160	palabras en español ninguna está cubierta y esta hipótesis tiene probabilidad 1 entonces en
2707160	2712800	cada paso del algoritmo lo que voy a hacer es elegir un par de frases tal que una es traducción de
2712800	2717840	la otra y voy a crear una hipótesis nueva a partir de una que ya tengo entonces en este paso lo que
2717840	2725720	hice fue decir el hijo el par de frases maría mary y ahí me creo una nueva hipótesis que cubre
2725720	2730840	la primera palabra por eso parece una serie con este caso elige la frase en inglés mary y ahora
2730840	2736840	tiene una probabilidad de 0.564 ese número de esa probabilidad va a servir para guiar un poco en el
2736840	2740680	algoritmo pero vamos a ver después cómo es que se calcula por ahora quédense solamente con el número
2742040	2746840	bien pero entonces yo tenía otra opción en realidad yo podía haber elegido empezar en
2746840	2750760	vez de traducir maría por mary podía haber elegido empezar por traducir bruja por witch
2751760	2760280	y ahí me crearía otra hipótesis de traducción donde cubro la penúltima de las de las palabras
2760280	2766120	en español agarro la palabra witch delijo la palabra witch y tiene una probabilidad de 0.182
2767880	2772480	entonces en cada paso del algoritmo lo que hace es elegir una hipótesis que tiene elegir un par
2772480	2778840	de frases y expandir así que lo siguiente que puedo hacer es elegir la frase did not expandirla a
2778840	2783920	partir de la hipótesis que tenía con mary y bueno eso me cubre ahora dos palabras en español y me
2783920	2791320	tiene me me dio otra probabilidad y después sigo avanzando y sigo avanzando hasta que llegó a cubrir
2791320	2795480	en algún momento si yo sigo avanzando y sigo agregando hipótesis en algún momento voy a
2795480	2801640	llegar a cubrir todas las palabras del idioma español todas las palabras de la oración en idioma
2802040	2806960	entonces hay una vez que yo cubrí todas las palabras digo bueno esto es una hipótesis completa
2806960	2812880	y esto lo devuelvo como una potencial candidata digamos una oración candidata a traducción
2812880	2818680	pero claro a medida que yo fui avanzando una cosa que pasó es que fui dejando hipótesis colgadas
2818680	2824360	y esas hipótesis podrían tener otras traducciones posibles yo acá lo que devolí era una posible
2824360	2828200	traducción pero a medida que yo tenía las otras hipótesis si yo hubiera seguido por las otras
2828200	2834200	hipótesis hubiera podido devolver otras cosas entonces yo necesito hacer un backtracking para
2834200	2839160	poder devolver todas las posibilidades poder volver a ver las hipótesis a revisitar las hipótesis
2839160	2844160	que había dejado colgadas y volver a explorar los otros caminos entonces necesitaría hacer un
2844160	2851400	backtracking para recorrerlas todas y si hago un backtracking lo que va a pasar es que voy a
2852360	2858520	va a ocurrir una explosión de exponencial del espacio de búsqueda porque en realidad todas las
2858520	2864760	las posibilidades que se abren son exponenciales y ahí esto como que se vuelve bastante lento entonces
2865920	2870280	yo quería un decodificador para volver este problema un problema tratable en vez de agarrar
2870280	2875080	las infinitas oraciones del idioma me quedo con algunas que sean más probables con este algoritmo
2875080	2881080	de codificación logré reducir de infinito a algo finito pero aún así es demasiado lento porque
2881080	2886440	hay una explosión combinación combinatoria digamos de la hipótesis y me queda una cantidad
2886440	2893160	exponencial de hipótesis entonces como es tan grande este problema digamos como la cantidad
2893160	2897800	de hipótesis exponencial y este es un problema NP completo entonces se utilizan técnicas para
2897800	2903280	reducir el espacio de búsqueda y hay como dos tipos de técnicas algunas son con riesgo y otras
2903280	2908440	son sin riesgo las técnicas sin riesgo lo que quiere decir es que si yo aplica una técnica de
2908440	2914640	reducción de hipótesis sin riesgo la solución ideal que yo tenía dentro de mi búsqueda no la
2914640	2919280	voy a perder utilizando una técnica sin riesgo en cambio en la con riesgo si yo podría llegar a
2919280	2924840	perder la solución óptima bien entonces la técnica sin riesgo que conocemos es la de recombinación de
2924840	2930280	hipótesis que dice que si yo tengo dos hipótesis voy avanzando por dos caminos dentro del algoritmo
2930280	2935440	y llevo a dos hipótesis iguales por lo menos dos hipótesis que cubren las mismas palabras entonces
2935440	2940880	me puedo quedar con la que tiene mayor probabilidad de las dos y descartar la otra porque porque a
2940880	2944160	medida que yo voy a seguir avanzando en el algoritmo lo que va a pasar es que van a bajar las
2944160	2948560	probabilidades digamos yo eligiendo más palabras y eligiendo más frases me va a bajar la probabilidad
2948560	2954400	y nunca me va a pasar que la una de las hipótesis que tenía menos probabilidad vaya a subir en
2954400	2960280	realidad siempre va a tener menos entonces en definitiva yo puedo con seguridad descartar la
2960280	2966000	que tiene menos probabilidad bueno esa es recombinación de hipótesis pero ni siquiera con
2966000	2970960	eso alcanza digamos para reducir el espacio de búsqueda lo suficiente aún queda muchísimas hipótesis
2970960	2976360	entonces suele utilizar técnicas de podado con riesgo la técnica del histograma la técnica del
2976360	2980840	umbral el histograma significa que a cada paso digamos en cada paso del algoritmo yo me quedo
2980840	2986560	con los n las n hipótesis de traducción más probables y descarto las otras y la técnica con
2986560	2992000	un umbral dice que a cada paso del algoritmo me quedó con la hipótesis de mayor probabilidad y
2992000	2999400	las que estén a una distancia alfa máximo de esa cuál es el riesgo de las las técnicas de podado
2999400	3004600	que si la mejor traducción y la traducción óptima tenía algunas frases muy poco probables al
3004600	3010880	principio entonces probablemente yo descarte esa solución en los primeros pasos y no llegan
3010880	3013880	a encontrar la solución óptima digamos la perdí por el hecho de haber podado
3015440	3020160	sin embargo bueno tiene como como ventaja que en realidad reduce muchísimo el espacio de búsqueda
3020160	3028120	y vuelve vuelve este problema un problema tratable bueno y ahora sí qué significaba esa probabilidad
3028120	3033080	que estaba viendo en cada una de las hipótesis o sea el podado necesita tener las mejores hipótesis
3033080	3038600	y bueno y para la recombinación también necesito saber la probabilidad de la hipótesis bueno la forma
3038600	3043360	de calcular la probabilidad de hipótesis se divide en dos digamos tengo lo que encontré hasta el
3043360	3047840	momento la hipótesis lleva cubierta cierta cantidad de palabras entonces para esa cantidad de palabras
3047840	3053080	que ya llevo cubiertas utilizo los tres modelos el modelo de traducción el modelo de reordenamiento
3053080	3058000	del modelo de lenguaje utilizo los tres modelos para calcular la probabilidad de la frase hasta
3058000	3063760	el momento pero para lo que me falta traducir yo no puedo utilizar todo porque no tengo toda la
3063760	3067880	información de traducción entonces lo que hago es utilizar solamente el modelo de traducción y el
3067920	3072840	modelo de lenguaje descarto el modelo de reordenamiento y bueno entonces hago calcular una
3072840	3076480	probabilidad que es una parte con todos los tres modelos y otra parte sin el modelo de
3076480	3083600	reordenamiento bien este algoritmo que acabamos de escribir que hace esta búsqueda basándose en
3083600	3088880	hipótesis que utiliza recombinación hipodado hipótesis y bueno el calcula de las probabilidades
3088880	3093760	de esta manera se conoce como algoritmo búsqueda asterisco es un algoritmo de bin search que se
3093760	3101040	usa muchísimo en lo que es traducción automática estadística por ejemplo el sistema mouses acá
3101040	3107680	tenemos este ejemplos de herramientas open source o gratuitas que sirven para construcción de de
3107680	3113200	traductores automáticos el sistema mouses es un sistema open source para desarrollar este tipo
3113200	3119280	de traductores automáticos estadísticos e implementa este algoritmo de codificación de
3119280	3125160	búsqueda asterisco y bueno lo que tiene el sistema mouses de bueno es que en realidad lo que hace
3125160	3130600	además de implementar el decodificadores utiliza a los otros sistemas y los integra de alguna manera
3130600	3136160	entonces integra este otro sistema el irs tlm que es una herramienta para crear modelos de lenguaje
3136160	3140800	basados en n en n gramas y el otro sistema se quiza más más que lo habíamos mencionado hoy que es
3140800	3148640	el sistema que me permite alinear corpus de oraciones en los distintos idiomas llegando
3148640	3153840	los modelos del 1 al 5 de traducción de bm bueno entonces estas tres herramientas sirven si uno
3153840	3158000	quiere construir un traductor automático estadístico entre cualquier par de idiomas puede utilizar
3158000	3164480	estas tres herramientas y teniendo un corpus paralelo y un corpus monolingüe puede construirse un
3164480	3170560	traductor pero bueno además otra cosa que mencionamos en la clase pasada pero este eran los
3170560	3175320	sistemas basados en reglas los sistemas basados en reglas han caído un poco este digamos no tienen
3175320	3180160	tanta popularidad como antes sin embargo algunos se siguen usando y el sistema apertium es un sistema
3180160	3184960	opensource para construir sistema de traducción basados en reglas que tiene como un montón de
3184960	3190080	pares de lenguajes y bueno ya anda relativamente bien digamos entonces se sigue desarrollando
3190080	3195960	hasta hoy entonces es una alternativa opensource que está basada en reglas en vez de estar basado en
3195960	3204400	estadísticas y bueno esta es un resumen de lo que vimos así que dejamos por acá
