La clase pasada estuvimos viendo una metodología de clasificación en general, así para cualquier
problema de clasificación, especialmente cómo separar el corpus, qué medidas utilizar. Una
cantidad de aspectos metodológicos que son muy importantes y que lo hicimos independiente del
dominio en el que estamos, que es el depresamiento de lenguajas natural porque aplica para cualquier
problema de clasificación. Problemas de clasificación y los métodos a aplicar a utilizar se pueden
definir en general. De hecho, en el curso de aprendizaje automático, ustedes aprenden con más
detalle lo que vimos en parte del curso de aprendizaje automático, aprenden con más detalle lo que
ayer vimos en una clase sola. Porque se ven diferentes métodos, excluimos cuál era el método en
particular y hablamos en general un clasificador, un clasificador supervisado, dijimos aquel caso
donde yo tengo un conjunto de instancias de cosas, un conjunto de clases discreto y tengo que
asignarle a cada instancia, la tarea de desasignarle a cada una de esas instancias uno del grupo de
clases. Si yo tengo un conjunto de documentos y quiero saber en qué idioma está lo que estoy
haciendo es un problema de clasificación. Tengo el conjunto de los documentos, tengo las clases
que son los idiomas posibles y yo tengo que a cada uno asociarle una clase. Podría eventualmente
ser más de una clase, podemos tener un problema multiclase, es decir hay variantes ¿no? Yo podría
decir que a cada documento era signo más de una clase, por ejemplo si lo quiero clasificar el
tópico de un documento, esto puede ser de espectáculos y de deportes o estamos hablando de
guandanar a poner. En la clase de hoy lo que vamos a ver es vamos a hablar de los métodos que hay
de clasificación de algunos métodos y de cómo se aplican algunas tareas del procesamiento de
lenguaje natural. Vamos a hablar un poco de las características del método y de cómo intanciarlo
en algún caso de ejemplo. Como yo decía en la clase pasada, los métodos de clasificación
están muy difundidos en todos los diferentes análisis porque generalmente los elementos
de dominio con lo que trabajamos son discretos, las palabras, las oraciones, los documentos,
los tweets son todas cosas discretas. Entonces en general vamos a ver métodos de clasificación
supervisadas. Si yo quisiera por ejemplo un ejemplo concreto, un proyecto que tuvimos el año
pasado que era que clasificaba un tweet si era un chiste o no, esa era una tarea de clasificación,
una tarea que también encaramos aunque no con demasiado éxito, era la de calificar el chiste
en un rango, en un, por vacinar un valor de qué tan bueno estaba, digamos, si se podía llegar
a capturar eso y ahí si yo como lo planteamos nosotros era que vos le podías poner una,
dos, tres, cuatro, cinco estrellas, eso sigue siendo un problema de clasificación supervisada,
pero si esto yo lo considerara un continuo, ahí tendríamos un problema de regresión,
no son usuales, los problemas de regresión de pasamiento no van a que natural porque nuestros
niños generalmente son discretos. Bueno, pero vamos a método de clasificación supervisada y en
particular vamos a hablar de métodos probabilistas. Los métodos probabilistas en general tenemos
la instancia, o sea yo no voy a volver sobre la terminología que vimos hace pasada, tenemos la
instancia representada por atributos y queremos asignarlo a una clase, pero además los métodos
probabilistas lo que hacen es asignarle una probabilidad a cada clase posible. Entonces yo no
solo te digo esta instancia, esta instancia, este tweet es humorístico, sino que te digo este
tweet tiene un 85% de chances en humorístico y no humorístico un 15%. Y esto por supuesto
tiene que ser una distribución de probabilidad, sumar uno y tal, mayor que cero. Entonces
y además los métodos probabilistas intentan obtener una distribución sobre las clases
dado en los atributos.
Y por supuesto clasificar en general va a ser, uno va a elegir la clase con la probabilidad más alta.
Así es que no quiere simplemente dejar de volver esa distribución para que otra etapa
del proceso lo utilice. Yo tengo la posibilidad de hacer eso. Los métodos generativos, que son
uno de los tipos de métodos que hay, lo que intentan es, son los que hemos estado viendo
hasta ahora en general y es lo que tratan de modelar la distribución conjunta, es decir,
la clase junto con los atributos, ¿sí? Y las etiquetas, ¿de acuerdo? ¿Por qué? Porque es lo que
necesitan para, a partir de la regla de Valle. Es decir, yo quiero la clase dada del conjunto de
features, ¿se acuerdan que la feature era nuestra representación del documento, ¿no?
Característica que, Valle a la redundancia caracterizaban al documento. Entonces, la probabilidad
de la clase dada de los atributos es igual, la probabilidad conjunta dividida de la probabilidad
de los atributos, ¿sí? Por definición, por la definición de probabilidad condicional.
¿De acuerdo? Entonces lo que tratan de modelar es esto. ¿Por qué lo hacen? ¿Por qué esta
probabilidad generalmente son más fáciles de estimar que las otras? ¿Por qué la puedo
estimar contando más fácilmente? ¿Por qué? Porque fíjense que yo como condiciono en dada
de la clase, digamos, yo, por ejemplo, puedo asumir independencia entre las variables aleatorias esta,
o sea, entre los atributos y puedo decir, si estas son independientes, p de x1 dado c
por p de x2 dado c, ¿no? ¿Esto no lo puedo hacer de este lado? Yo no puedo decir p de c
dado x1, porque no funciona así la probabilidad, digamos. La independencia la puedo dejar de acá
al lado. Y cualquier propiedad de dependencia entre variables aleatorias se mira de este lado,
¿no? Eso genera toda una teoría que se llama la de los modelos gráficos, que por supuesto no
vamos a hablar acá, pero que me dicen, bueno, ¿cuál es la estructura que yo supongo en términos
de dependencia? Es decir, esta variable depende de esta, esta no, y así. Y puedo modelarlo con
un gráfico, como va a tomar. Entonces, llegan a esto, ¿no? La probabilidad de la clase,
dado los atributos, la probabilidad de la clase por la probabilidad de los atributos a la clase.
Esto es valles, ¿no? Y ya lo hemos visto varias veces en el curso, no estamos inventando nada.
Dividido la probabilidad de los atributos. Y bueno, y nada, lo que hemos hecho hasta ahora,
tanto la probabilidad priori, la PC como la probabilidad de verosimilitud, esta la puedo
estimar a partir de los datos. Esto ya lo hemos hecho. Pero vamos a tener que simplificar el problema.
El método nai valles lo que hace es asumir que los atributos son independientes entre sí,
lo cual es una barbaridad conceptual, si por ejemplo estamos hablando de un texto y los atributos son
las palabras que tiene. Realmente las palabras vienen acompañadas, se hacen amigas entre ellas,
digamos, ¿no? Si hay muy palabras positivas, muy probable que haya otras palabras positivas.
Bueno, valles dice, bueno, no sé, no sé. La probabilidad de una palabra solo depende de la clase.
Y por lo tanto eso hace que pueda partir la probabilidad, porque como son independientes,
la probabilidad de x1 dado x1 por xn dado c, la probabilidad de x1 dado c por la probabilidad de
x2 dado c, bla, bla. Y bueno, ¿y cómo construye un clasificador a partir de esto? Y bueno,
maximizo lo de arriba, busco la clase que maximice lo de arriba. Lo de abajo es independiente de la
clase. Entonces busco la clase que maximice lo de arriba y ahí tengo un clasificador. ¿De acuerdo?
Es muy sencillo, tomo todos los atributos que se me ocurren, los considero independientes. Ahora
lo moveremos en algún ejemplo y busco la clase que maximiza. El método Ney Valle funciona muy bien
como base para un clasificador y por poca plata uno hace un clasificador como la gente que capaz
que hasta le pueden llamar un AI en la prensa. Yo no sé de cuándo es el método de Ney Valle,
me suena como de los años 60, si bien se basa en el teoría de Valle que de 1700, pero funciona
muy bien. En general, como primera aproximación rápida o algo, uno puede usar Ney Valle sin mucho
cargo de conciencia y funciona en general muy bien. El método de Ney Valle es aplicado a la
clasificación de documentos. Utiliza una de las formas de darlo es utilizando lo que
se llama una aproximación vago words. Es el ejemplo, es como el ejemplo canónico de
clasificación, digamos, el vago word. Yo digo tengo todo esto, es un documento que tiene una
estructura, que tiene un orden entre las palabras, que tiene una sintaxis, que tiene
relaciones bien formadas, con una semántica, yo no le hago caso a nada de eso. Y lo que hago
solamente es considero que esto es una bolsa de palabras. La bolsa de se acuerdan, bolsa es
como un set, pero que puede tener elemento repetido. Una bolsa de palabras y tengo el conteo de
cantidad de veces que una palabra aparece en ese documento. Mi representación del documento es esto.
Me features son estos. Entonces, cómo hago clasificación, esto fue lo que hubo en la laboratoria
del año pasado. Entonces, cómo se instancia Ney Valle para el problema de clasificación de documento?
Bueno, las posiciones son todas las posiciones que tengo en el documento que quiero evaluar.
Yo quiero evaluar en la clase. Aguardo, quiero evaluar la clase en un documento,
entonces tengo las posiciones, que son todos los tokens que aparecen en cada palabra, en el documento.
Y la clase, según Ney Valle, es la clase que maximiza, quería comentar algo acá.
Esto en realidad es un conteo, pero yo acá la voy a contar seis veces. Por eso es un bug of words.
En las posiciones considero todas las posiciones posibles, como decía, y calculo la clase como la
clase que maximiza la probabilidad de cada palabra que aparece en el documento dado a esa clase.
¿Se entiende? Es la clase que hace más probable, considerando independencia,
que esa palabra es T en ese documento, digamos, ¿no? La probabilidad de W subida o C.
¿Y cómo hago para hacer eso? Y bueno, para calcular esos valores, para estimar esos valores,
yo digo, bueno, nuestro mejor estimador, este corrito quiere decir nuestro estimador,
nuestro mejor estimador de la clase, de la probabilidad priori, de la probabilidad,
estamos hablando de la probabilidad de la clase, si no tuvieramos la palabra, es decir,
yo puedo tener una distribución, yo tengo documentos que son o de deporte o de música,
vamos a suponer que son exclusivos, ¿tá? La probabilidad de la clase es el número de
documentos de deporte sobre el total, o sea, mi probabilidad priori, ¿se acuerdan de Valle,
¿no? Yo tengo una probabilidad priori que lo que pienso antes de empezar a ver el documento y
antes de ver el documento yo puedo decir, bueno, el 90% de los documentos son de deporte,
entonces mi probabilidad a priori es 0.9, ¿te acuerdo? Es mucho más probable a priori que sea un
documento de deporte, yo voy a ajustar esa probabilidad con la probabilidad de las palabras de
cada una, ¿te acuerdo? Entonces, yo estimo esa probabilidad priori con el número de
clase de documentos, que tienen la clase dividido el total de documentos. Y, similarmente,
estimo por conteo la probabilidad de cada palabra de la clase contando del total de
veces que aparecen todas las palabras en los documentos de esa clase, o sea, de todas las
palabras que aparecen en los documentos de deporte, ¿cuántas veces aparece esa palabra en la de
deporte? Tiene sentido, ¿no? Es una palabra común en un dominio de deportes, esta es lo que se
pregunta, y multiplica a todas esas probabilidades, que seguramente operativamente tengamos que usar
un logaritmo y sumar, porque si no nos va a dar todavía muy chiquita, pero conceptualmente lo
mismo. ¿Se entiende? ¿Por qué, en vez de usar esto, tengo que usar esto?
¿Por qué tengo que hacer eso?
¿Por qué tengo que hacer esto? ¿Qué es esto?
La plaza, le agrego uno, acaba contador para que no tenga el problema de que, porque si una de
estas probabilidades, lo mismo que nos pasó con los engramas, si le suena conocido, porque es lo
mismo, si una de aquella probabilidad de da cero, se me cancela toda la clase, la probabilidad de
clase va a ser cero. Entonces para eso hacemos la plaza, hacemos smoothing, suavizado, agregándole
uno a cada contador. Por ejemplo, bueno todo esto que yo estoy diciendo está en el capítulo 7,
más o menos, que es general, del capítulo 7 del libro de Martin Yurashki. El libro de Martin Yurashki
está online, los capítulos nuevos, de hecho todos los capítulos correspondientes a clases que
hemos dado están online, yo realmente les recomiendo leerlos un libro que está muy claro, no va a
tener mucha más dificultad que lo que vemos en la clase, por lo menos no se, uno pierde perspectiva,
no? Está claro, pero, pero... ¿Qué le pasa? Le agrajo de si, si me giro nada, si, si, y ahí pueden
chequear y hay algunos detalles más que me parecen muy interesantes, si a ustedes les interesa. Bueno,
supongamos que nosotros tenemos el cuerpo de entrenamiento que tenemos arriba, las oraciones que
están arriba y con una categoría negativa o positiva, algún tipo, en este caso estamos haciendo
sentimenta análisis, es decir, analizar si la percepción es positiva o negativa sobre un documento.
En el cuerpo de los tweets hacíamos algo así, algo parecido, es decir, yo necesito saber si la clase
del cuerpo del tweet es de humor o no humor. Bueno, y ahí tenemos algunos ejemplos negativos y otros
positivos y queremos saber qué pasa con predictable with no originality. Entonces,
la probabilidad priori de la clase cuál es y es el total de documentos hay 1, 2, 3, 4, 5,
de los cuales tres son negativas y dos son positivas, o sea, que estas son nuestra probabilidad
priori. Y luego entramos a buscar la probabilidad de cada palabra. La probabilidad de predictable,
dado que la clase es negativa, es 1 que es la ocurrencia de predictable,
predictable solo aparece en la segunda oración y en un contexto negativo.
Entonces, a cada 1 y a cada tenemos el más 20 es para normalizar, para la plaza,
o sea, 1 más 1 y 14, que es el total de palabras más 20, 14 es el total de palabras diferente.
¿De acuerdo?
De las palabras diferentes. ¿La palabras? ¿Cómo fáciles verlo acá?
Sí, es la clase, ¿no?
De la clase. ¿La cantidad de palabras que hay en la clase? No, no son diferentes, son todas.
¿Del total de palabras que hay? ¿Voy a contar? Positivo, 1, 2, 3, 4, 5, 6, 7, 8, 9.
Son todas, porque acá yo estoy considerando todas las ocurrencias. Es una de las cosas que se
le critican, ahí vayan, es general, es eso, que si yo repito muchas veces algo, le sumo probabilidad.
Que a veces no es lo que se quiere, digamos. Si hay atributos que reiteran cosas,
es como que están muy relacionados y no están aportando información.
Entonces, acá están todas las probabilidades de las diferentes palabras. Fíjense,
bueno, ahora nos fijamos en el ejemplo. Con esas probabilidades, esa es nuestra,
es como entrenamos nuestro clasificador, esencialmente. ¿De acuerdo? Es decir, a partir del
cuerpo de entrenamiento, yo calculo esta probabilidad y lo que estoy haciendo es entrenar.
Como ustedes ven, son cuentas muy sencillas de hacer. El clasificador, no hay vaya,
la ventaja que tiene, es que es muy rápido, muy, muy rápido. Tanto para entrenar como para
evaluar. Entonces, cuando uno quiera acercarse a un problema y ver, ¿qué tan difícil es
clasificar un cuerpo de humor? Entonces, se le arrima con un método de esto,
que lo entrenan dos patadas, y más o menos tiene una idea. Dice, ah, mirad, que pude clasificar
el 75, 80% del olor. O sea, lo que es un problema que tiene para mejorar un poco,
tampoco es que es horrible y difícil. Y luego, sí, empieza a afinar, a ajustar parámetros,
a cambiar el método, capaz que le mete una red o agregarle datos, le mete una red neuronal
que está una semana entrenando. Pero con esto tiene una primera aproximación, por lo menos.
A mí se alcanza, pasa alguien en los medios. Porque depende la tarea que estamos haciendo.
Bueno, ¿pero qué pasa? Entonces, ¿cómo clasifico? Y bueno, si la palabra es prevista,
volvió en no originality, yo tengo la probabilidad de la oración dada la categoría negativa por
la probabilidad de la categoría negativa. O sea, que es 3 dividido 5 por las diferentes
probabilidades de las palabras que aparecen en la categoría negativa. Si se fijan acá estos
1 es porque no aparecían. Y acá, fíjense que originality es una palabra montinando a positiva,
¿no? Es 1 sobre 29 contra 1 sobre 34. O sea, que está mejor en la positiva que en la negativa.
¿Sí? ¿Por qué? Porque aparecen contextos positivos, realmente. Acá el problema que
tienen no, adelante. Que es uno de los problemas que ahora vamos a ver. Pero de todos modos,
multiplicando las probabilidades de cada palabra, llega que es más probable que sea negativa.
¿Y por qué? Porque dice predíctabel, seguramente. ¿Por qué dice no?
En realidad esto es number crunching, ¿no? Es porque hay un motivo, digamos, uno de las
aplicaciones son siempre aposteriores en estas cosas, ¿no? Es decir, bueno, pasó esto, pero en
realidad esto es un motivo de sus cuentas, inicialmente. ¿Se entiende? ¿Se entiende acá?
Si nosotros queremos hacer sentimentanálisis, para el caso particular de clasificación de
documentos que se llama sentimentanálisis, que es ver la impresión respecto a algo,
a un documento, hay algunas reglas que permiten mejorar la performance.
Es lo mismo, es exactamente lo mismo, las clases son las mismas, pero se puede hacer
alguna modificación. Por ejemplo, no contar múltiples ocurrencias en la palabra en el mismo
documento. Esto que yo les decía hoy, cuento una vez olas. Y se dice, es muy, muy linda,
linda, linda, cuento una vez olas. Eso se llama binary navages. El manejo de la
innovación es todo un tema, es todo un tema, el manejo de la innovación. Y una aproximación
muy, muy sencilla, muy naí, pero que mejora las cosas, pues bueno, yo a todo lo que dice
después de didn't, lo clasifico no como like, sino como not like, invento una palabra nueva.
Podría llegar a hacer alguna cosa un poco más elaborada si tuviera un parser, porque si yo
tengo un parser, tengo el árbol y tengo una rama que dice no todo lo que hay abajo.
Entonces yo sé el alcance de no. Ahí igual el problema está en cómo hacer el parsing,
pero si yo le agrego información de parsing, la cosa puede mejorar. De parsing vamos a
hablar la semana que viene, pero yo diría que... Háganme acordar que hable el final
de esto, del parsing. Esta, pero esta es una primera aproximación, ¿entiendes? Creo unas
palabras nuevas ahí y ahora el like se cuenta como not like. Es muy naí porque dice todo lo que
está después de didn't, pero podría haber otras cosas en el medio. No, no es tan sencillo, digamos,
pues las oraciones son más complicadas. No, creo que pienses que, y hay un que ahí con oración
subordinada, puede ser más complejo que esto, pero no da rimamos. Y otra aproximación, por supuesto,
es usar lo que se llama lexicones de sentimiento, que son listas de palabras positivas y listas
de palabras negativas. Tengo una lista recolectada, ¿sí?
En el caso de que está mostrando como se llena la palabra, ¿no? Sí. Como no estaba, no tenía
supexicon, cualquier cosa que pusiera, de originales, ¿no? Cualquier cosa que pusiera
sento para originales y que no estuvieran entre niches, la primera, ¿no? Ah, sí, sí, claro, claro,
claro, claro, claro. De todos modos se supone que vos, en todo este tipo de métodos, justamente lo
que supone es que como vos tenés grandes volúmenes, si no, no funcionan. Claro, claro, claro. Es decir,
que lo que hacen es capturar algo a partir de muchas ocurrencias. Pero sí, si no parece,
si tenés cero, es la misma para todas. En el lexicón, entonces vos lo que podés hacer
es agregar, en tu clasificador, simplemente una fitur que dice la cantidad de palabras en
un lexicón positivo y la cantidad de palabras en un lexicón negativo. Es decir, tiene tres
palabras negativas, es un X, Xn más 1 y Xn más 2, son dos atributos nomás, ¿sí? Y la
cantidad de palabras en un lexicón negativo. Le agrego dos atributos que, si recordamos
en la clase pasada, van a seguramente estar más correlacionados con la clase y nos van
a poder dar una pista de su comportamiento. Si llegara a hacer un método de regla que
dice, bueno, el que tiene más palabra positiva gana, porque juegan todas, intervienen mucho
en la clasificación, ¿sí? ¿De acuerdo?
Otro ejemplo, ¿cómo puedo hacer para calcular un tag de part of pitch si tengo la palabra
y los postage de las palabras anteriores y siguientes? Y bueno, de la misma forma, ¿no?
La probabilidad de que sea un adjetivo, dado que la anterior es un determinante, el siguiente
es un nombre y la palabra es blanco, es la probabilidad de que la clase sea un adjetivo
a priori, esto lo hago por conteo, la probabilidad de que una palabra sea blanco como adjetivo,
es decir, de todas las veces que hubo blanco, cuántas veces, miento, de todos los adjetivos
cual era blanco, cuántas veces pasó que antes de un adjetivo hubieron determinantes
por la probabilidad de que el siguiente sea un nombre si este es un adjetivo. ¿Se entiende?
Simplemente hago conteo de todas las veces que aparecieron cosas antes y las considero
independiente entre ellas, lo cual sabemos que no es cierto, pero es lo que hay, es lo
que puedo computar. Y bueno, y como yo estoy calculando la probabilidad conjunta de esto,
podría llegar a generar ejemplos con la distribución calculada, eso me puede ser útil para hacer
generación de texto, todos estos métodos me permiten, los métodos de, por ejemplo,
de engrama me permiten generar también texto, que es la forma que hacen los generadores,
que escriben parecido a alguien, digamos. Bueno, bueno, atacar los métodos generativos
que son estos, es, como nadie valles. Un método generativo es ese que busca una distribución
de todas las clases, prueba todas las clases y computa la distribución conjunta con los
atributos. Los métodos discriminativos son un poco diferentes porque en lugar de,
en lugar de calcular la probabilidad de la conjunta dicen, bueno, no, de todo ejemplo,
cuál de los dos es mejor, cuál clase es mejor para este ejemplo, sin tratar de modelar
todas las clases posibles. Es decir, modelamos directamente la probabilidad, intento modelar
directamente la probabilidad condicional, la probabilidad de la clase daba los atributos,
¿sí? Voy derecho a eso, ¿qué es más probable dado de todos estos atributos? Nada más.
Y hay varias aproximaciones, algunas que son probabilísticas como entropía máxima y otras
no. A ver, el preceptor de su porvector machine, ahora vamos a ver, no, vamos a ver la definición
de su porvector machine. Pero esencialmente lo que te dicen es, bueno, esto está de tal lado.
Si yo tengo estos puntos así,
entreno y después te digo, bueno, este está de este, si este punto está del lado de los
redonditos. No sé qué tan del lado está de los, esto no es probabilista, por ejemplo.
O puedo hacer lo probabilista, pero igual lo único que respondo es acá y de qué lado está.
Bueno, entonces vamos a ver uno que es el modelo de entropía máxima,
que es como lo que vamos a ver, es como la versión discriminativa del método de Ney Valle.
O también conocido como regresión multinomial logística, que vamos a ver por qué se llama así,
y son modelos lo lineales para clasificación, es decir, yo quiero la clase, si tengo una serie de
tributos. Hago.
E, ahora vamos a ver qué es esto, ¿no? F su I, son las features, son como un indicador de algunas,
son features a partir de los atributos, ahora vamos a ver cómo lo abrimos eso,
pero son derivadas de estos atributos. Los W son los pesos, una serie de pesos que yo
voy a intentar calcular, son los pesos de mi modelo, lo que yo voy a entrenar,
aprender son los pesos de mi modelo. Y este es el producto, el dot product de ambos, es decir,
esto va a ser W1 por F1 más W2 por F2 más W3 por F3, etcétera. ¿De acuerdo? Entonces yo,
la feature esta, que según mi ejemplo, cuando yo vaya a evaluar, según mi ejemplo,
esto va a valer algo, lo multiplico por un número fijo que va a depender de lo que yo entrené,
es decir, lo que yo quiero aprender es W, ¿de acuerdo? Eso, elevó E a la suma de eso,
ahora vamos a ver por qué hago esto. Y esta Z es simplemente un factor de normalización,
es decir, de todos los W subí que tengo, o sea, esto no necesariamente genera una distribución
de probabilidad, entonces este Z es como la suma de todos los casos posibles para llevarlo a una
probabilidad, a que la suma me dé uno, aquellos que hablábamos unas clases atrás, bueno,
generalizado acá. Esa es la famosa Z, que parece una pavada, pero es lo más difícil de computar,
porque yo tengo que calcular este valor para todos los atributos posibles para que me dé una distribución.
Entonces, los modelos de entropía al máximo calculan la probabilidad de la clase utilizando
esta fórmula. Ahora vamos a ver por qué. Pero antes vamos a hablar de otra cosa para llegar a eso,
y es de regresión lineal. Un problema de regresión lineal, que era lo que yo le decía hoy,
es cuando uno intenta, un problema de regresión es cuando uno intenta calcular un valor,
de algún valor real, un valor real, ¿sí? Entonces yo, si yo quiero saber, supongamos que yo
tengo estos puntos acá,
¿sí? Cuando yo hago regresión lineal, lo que hago es trazar, buscar una línea que separe los
ejemplos. Eso esencialmente es, si esto es, va a ser una cosa como, si yo supongo que pasa por el
origen, esta recta, vamos a suponer que pasa por el origen, y si no, vemos cómo se corrige.
Vamos a llamarle X1, X2.
Esto es la recta que representa esto, ¿no? Es decir, el W1 y W2 me van a determinar la
legislación de la recta. Acá está pasando por el origen porque no tiene elemento independiente,
yo puedo inventar un W0 con un X0 que vale siempre 1, para agregarle, vamos a moverla a la recta.
¿De acuerdo? Entonces, lo que yo, cuando digo que hago regresión lineal, lo que digo es bueno,
mis puntos yo asumo que son separables por una recta. Ah, perdón, yo quiero estimar X2 dado
X1, ¿de acuerdo? Entonces yo obtengo la recta para un nuevo X, vengo acá y calculo el I.
Entonces, I va a ser igual al WI por FI, que es esto, la sumatoria de los WI por FI es el dot
product de WI con F. ¿De acuerdo? Simplemente estoy haciendo un estimador lineal de esto.
¿Y cómo hago para, como encuentro esta recta? Bueno, una de las formas más usuales es la que
minimiza la suma de los cuadrados de la diferencia entre valores y predicciones, o sea,
esta recta minimiza esta distancia, ¿de acuerdo? La distancia, yo busco la recta que tenga la
distancia mínima de esto al cuadrado y esto al cuadrado, ¿sí? Lo hago al cuadrado para que la
suma sea positiva, para que no me afecte si estoy de un lado o del otro. La vieja regresa
en un lineal, ¿sí? Entonces, no voy a entrar en detalles, pero yo calculo la fórmula de los
mínimos cuadrados que son, si lo piensan son todo multiplicaciones de cosas al cuadrado, más
cosas al cuadrado. O sea, que esto es positivo, es una función positiva y convexa y entonces yo lo que
trato de buscar es el mínimo de esa función. El año que viene lo voy a escribir a eso,
porque no sé si queda claro. Este, a ustedes no les importa. Pero la cuestión es que yo termino
minimizando una función convexa, una función convexa y una función que es así. Así es una
función convexa, ¿no? Que cualquier, cualquier par de puntos que yo una pasan todo por adentro del,
no? Vamos, acá, esto es una función convexa. Sí, yo puedo unir acá, ¿de acuerdo? ¿Cómo es
una función convexa? ¿Qué característica tiene las funciones convexas? ¿Cuál es
qué característica tiene la función convexa? Ah, no se acuerdo. Las funciones convexas tienen el
tema de que cuando yo encuentro un mínimo local es un mínimo global. Si una función es así,
yo puedo quedarme, buscar el mínimo acá y encontrarme con este mínimo local y buscar acá.
Sí, sí, claro, puedo llegar a quedar atascado acá. Si yo tengo una función convexa,
esto es informativo, si quieren hacer curso de prensa automático, esto lo ven en detalle.
Este, si yo tengo una función convexa, yo puedo buscar un punto cualquiera y empezar a
calcular en la derivada y avanzar en la, en la dirección de la derivada y al final, al final del
día voy a encontrar si hago las cosas bien el mínimo de función. Eso llama descenso por
gradiente, ¿sí? Y debería enseñarse en primer año.
Hay otro método de minimización. Son métodos de minimización numérica, ¿no? Son calculos numéricos.
Quiero decir, no hay una fórmula cerrada para eso. Para el método de mínimo cuadrado sí hay
una fórmula cerrada, es decir, una fórmula calcular, pero es más fácil de hacer descenso
por gradiente, pues más rápido. Bueno, cuestión, que nosotros podemos saber cómo hacer esto,
es decir, que yo puedo aprender los WB, o sea, los WB que minimizan, esos son los WB que queremos,
está claro, ¿no? Es decir, yo calculo a partir del cuerpo de entrenamiento,
esos WB, y lo uso luego. Eso se trata de aprender. Entonces, este es un problema de
regresión, donde yo quiero calcular un número, pero acá estoy en un problema de clasificación,
o sea, que lo que yo quiero aprender es una categoría, una probabilidad. Entonces, mi primera
aproximación es, perdón, es bueno, yo digo esto, la probabilidad, el número que yo quiero estimar
es la probabilidad de que sea clase, acá tenemos un caso positivo o negativo, ¿no? La probabilidad
de que I va a ir a true, o sea, de la clase dado mi X. Entonces, yo lo que digo es bueno, hago
regresión, hago regresión, pero en lugar de calcular un número, o sea, sigo calculando
un número que es el valor de la probabilidad. Ahora, ¿qué problema tiene esto? El problema
que tiene esto es que no es una distribución de probabilidad, no es un valor de probabilidad,
porque la probabilidad tiene que estar entre 0 y 1, ¿de acuerdo? Entonces, esto no me sirve
a aplicarlo directamente, porque me puede dar cualquier cosa, yo quiero una probabilidad.
Entonces, lo que digo es bueno, pruebo con los odds, los odds que no sé cómo se traduce,
los odds son como las chances, como las apuestas, ¿no? Tenés 2 a 1, 1 a 2,
que es, esencialmente, la probabilidad de que sea verdadero comparado con la probabilidad de
que no lo sea. Esto está un poco mejor, porque este resultado está entre 0 e infinito,
pero si es sin estar entre 0 y 1, entre, perdón, yo quiero llevarlo a algo que esté
entre menos infinito y más infinito que es esto, ¿no? Está claro, está claro. Esto está entre
menos infinito y más infinito, el W por F, cualquier cosa. Acá yo lo reduzco a una cosa que
está entre 0 y infinito, mejor. Bueno, pero para que esto quede entre, entre 0 y 1, lo que hago
es, le aplico el logaritmo, para que quede, perdón, dije al revés, para que quede entre
1 y infinito y más infinito, le aplico el logaritmo. Implico el logaritmo y entonces digo, bueno,
esto es lo que buscábamos, yo quiero estimar el logaritmo de la probabilidad de las odds,
y por eso llegó a esa fórmula tan rara con E, porque cuando yo despejo, y esto se lo dejo de
ver, cuando yo despejo P igual true, es fácil, ¿no? Digamos, este logaritmo se transforma en un E
a la W por F, ¿sí? Bueno, se lo dejo de ver. Cuestión, ¿qué queda de sí? E a la W por F dividido 1 más
E a la W por F. Las odds, se transforma en que es esta función, ¿sí? Entonces, llegué a una función
que me dice la probabilidad de que sea verdadero da la clase, a partir de haciendo unas cosas raras
con las features, nada menos. O sea, algo parecido al lineal, pero que la corrijo con esta función.
Esto es lo mismo que hace la red neuronal. No mismo. Esa función se llama función logística y tiene
este aspecto. ¿Cuál es la característica de la función logística? Y bueno, que parece un escalón,
es parecida una cosa que vale cero, si es negativo y uno si es positivo, pero que es continua.
Es una linda función, es una función smooth. Pero sigue pareciendo un escalón. Si yo logro,
si estoy de este lado, más seguramente sea negativo y si estoy de este lado sea positivo. Pero puedo
derivar a las esas cosas. Las red neuronal usan mucho eso. Y hacemos el chiste de no se puede entrar.
No, es para que se sientan más. Bueno.
Y bueno, ¿y cómo clasificamos? Muy es fácil. Si la probabilidad de que sea verdadero mayor que
la probabilidad que sea falso, dada el atributo, es lo mismo que decir que e a la w por f es mayor que
1. Por esto. ¿Sí? Que es lo mismo que decir que w por f sea mayor que 0. Entonces clasificar es muy
fácil con este método, porque lo único que toca hacer es multiplicar w por f con los pesos que
calculé por la feature y si me da mayor que 0 quiere decir que positivo y sino negativo. Eso es la
regresión logística. Se llama regresión, aunque se llama regresión es un método de clasificación.
¿De acuerdo?
Y la pregunta es bueno, pero acá yo todavía no respondí. ¿Cómo estimaba los pesos que
me iba allí? Se era contando. Acá tengo que hacer algunas cosas un poco más raras.
Digo que mi w estimado es el que maximiza este producto de probabilidades de las diferentes
clases. Y me queda esta función súper rara, súper fea, súper complicada, pero que adivinen que es
convexa. Y como es convexa, bueno, yo quiero buscar el máximo de una función convexa,
lo mismo que le decía hoy. Aplico desde eso por la diente o algún otro método de numérico.
Entonces tengo una forma de estimar esos w, el asunto que tengo es la forma de estimar.
Y ¿qué pasa si tengo más de dos clases? Y bueno, tengo que hacer una cosa así,
calcular la feature a partir de cada clase con cada tributo.
Metarlo dentro de la fórmula y volver a normalizar. Y por eso nuestro método se llama
multinomial logistic regression, porque es una extensión de la regresión logística a un caso de
múltiple clases. Y por supuesto no vamos a quedar con la clase que maximiza la probabilidad de
este tributo. ¿De acuerdo?
Esta clase es un poquito más, entra más en detalles matemáticos que el resto. Me parece
importante entender por qué esos atributos aparecen y por qué aparecen todas esas cosas con
e. Y las cosas con e generalmente son para cambiar la curva. ¿Qué pasa si es paiguiente?
Y por último, como comentario, ¿por qué se llaman modelos de entropía máxima? La entropía,
no sé si hablamos algo de entropía en alguna clase. La entropía es una medida que trata de ver
qué tan parecido son los elementos de algo. Entonces, el principio de entropía máxima dice,
bueno, yo si tengo muchas distribuciones posibles, candidatas, algo, el hijo,
la que tiene entropía máxima, es decir, la que da dos mil datos, la que solo asume lo que los datos
te dicen. ¿Qué quiero decir eso? Si yo no conozco nada sobre un documento en el caso del 90-20, 90-10,
asumo 90-10 porque puedo asumir a partir de los datos. Yo podría asumir 0802 por
algún motivo, pero si yo no sé más que eso, estoy utilizando, o si no sé nada,
si yo no sé nada sobre un documento, no sé nada, no tengo ninguna información a priori.
Y te doy un documento y te digo de qué clase es, es de deporte o es de espectáculo. ¿Qué harían
ustedes? Si yo te digo 50-50, eso es aplicar el principio de entropía máxima, es decir,
bueno, yo no tengo información, es todo equiprobable. ¿Se acuerdan que la entropía es máxima cuando
son todo equiprobable? Si yo agrego un poco de información y yo tengo un dado, pero yo te
aseguro que el 6 no sale nunca. ¿Cuál es la probabilidad de sacar un 1? Un quinto. O sea,
paso de ser un sexto, un quinto, porque tengo más información, pero siempre no debo un cuarto,
porque no puedo sacarlo de ningún dato. Eso es el principio de entropía máxima. Si yo,
cuando elijo estas distribuciones posibles, aplico solo lo que los atributos me dicen,
aplicando el principio, el que tenga máxima entropía, a lo que llego es exactamente al
mismo modelo que presenté antes. Por eso también los modelos se llaman modelos de entropía máxima,
es porque son dos formas diferentes de llegar a los mismos. Si usted quiere en el detalle,
en la literatura está eso. No sé si les interesa, pero al que les voy a interesar está muy bien.
Coinciden con eso. Coinciden con una distribución de probabilidad para un modelo logístico
lupinional cuyo peso maximiza la verosimilitud en los datos de entrenamiento.
Por ejemplo, si yo quiero aplicar un modelo de entropía máxima al ejemplo del post time,
la feature van a lucir así. Tengo una feature 1 que dice vale f1, vale 1 si la palabra es
reis y la clase es nombre y si no vale 0. Otra feature va a ser 1 si la anterior es tú y la clase
es verbo y si no es 0. Otra feature y como se imaginarán la feature son, estamos hablando de
miles o de millones de features, pues son todas las posibles, las relevantes. Así lucen las features
un modelo de entropía máxima, de un montón de features y yo lo que voy a hacer y además son
indicadores, eso generalmente vale 1 o 0, usualmente. Y lo que yo voy a hacer es, calculando a través de
contando, sí, voy a calcular los W, con aquello que hablamos hoy de la fórmula de minimizarla,
o sea que cada feature va a tener un peso indicando qué tanto afecta la feature corresponde para el
tag ese. Por ejemplo, si yo tengo aquello, se acuerdan que queríamos saber qué era reis en el post
tag, ¿no? Que era la única palabra que no sabíamos lo que era. Entonces, estos son los pesos que yo
entrené, lo que me dicen es que, fíjense que los pesos que tenemos acá, lo que me dicen es
que la feature más importante es la size, en el caso, para que sea, si es un nombre, esta es muy
negativa, o sea que resta valor y esta es muy positiva, f2 para un verbo, no me pregunten si son
números reales o no más guardas, f2 es, ah, si la previa es tú, si, la previa es tú, pesa muy
positivamente para que eso sea un verbo, tú reis, ¿no? Y la f6, ¿qué es? Y pesa muy negativamente para
un nombre, fíjense que hay features diferentes según la clase, porque son más de 1, más de 2,
¿de acuerdo? Entonces, yo hago las cuentas, la probabilidad de que sea un nombre dado las
features es, es, es exactamente aplicar las features relevantes, acá es 0,8 porque
multiplica la prim, la segunda y la sexta, porque son las que aplican a nn, ¿no? Si no valen 0.
La 2, dijimos, y la 6, son las de tú, 0,8. No, es la 1 y la 6, la 1 y la 6, correcto. O sea,
si reis la palabra, porque la otra no aplica, fíjense que como valen 0 no pasa nada con la
multiplicación, porque yo estoy diciendo e al a eso, ¿no? No me molesta el 0 en este caso. Y este
es el factor de normalización, es simplemente para que esto de 0,20 y 0,8. Sumo esto más esto,
sumo todas y bueno, entonces yo busco la clase que más se inicia que en este caso es verbo, ¿de acuerdo?
Bueno, esos son los modelos de entropia máxima. Hay otros modelos discriminativos que lo voy a
mencionar rápidamente, porque en la forma de aplicarlos es la misma, lo único que hay acá de
diferente es que es diferente la forma de elegir clasificador, es decir, si acá lo hacíamos por
regresión logística, el support vector machines, que es un método que se puso muy de moda en
principio de este siglo, en la década pasada digamos, es un método que lo que hace es buscar
separar linealmente, pero en vez de hacerlo por mínimos cuadrados, lo que dice es buscar la recta
que separa más, que queda más en el medio digamos, la intuición atrás de support vector
machines que yo busco, si yo tengo los ejemplos así, tengo muchas rectas que pasan, ¿sí? Yo trato
de encontrar la que maximiza el margen de los que están más cerca y queda en el medio, ¿sí,
por eso se llama, los support vectors son estos, son los que están más cerca,
los demás, si se fijan, no importan para el clasificador. ¿Cuál es la hipótesis del support
vector machines y por qué son tan robustos y por qué, como están justo en el medio? Quiero decir,
si yo meto uno que está acá, si un ejemplo a clasificar queda muy cerquita del borde, me puedo
equivocar, ¿se entiende? Es más probable que me esté equivocando, en cambio yo le pongo en el
medio y bueno, quedan bastante lejos digamos, y de hecho funcionan muy bien clasificando. Fueron
toda una revolución en la support vector machines, ahora como ahora están de moda la red neuronal
en la support vector machines, hicieron lo mismo a principios, agarraron, fueron los primeros
métodos de discriminativo clasificación que empezaron a batir todos los récords digamos de
diferentes tareas, hasta que pasaron de moda con el tema de las, si bien se usan mucho pasaron
de moda con el tema de las red neuronales que volvieron a batirle los récords, pero esencialmente
el método cómo se aplica es el mismo, así la diferencia es como teóricamente como se calcula
que está ahí, hay otros métodos de clasificación, hacen el aprendizaje automático, los aprenden,
vecinos más cercanos, los caníres, que es, clasifico un documento buscando los que están más cerca
del punto de vista tribu, calculo una distancia entre documentos y me quedo con los que están más
cercanos, ¿no? Es como la idea de, bueno si este está acá, ¿quiénes son los vecinos más cercanos?
Y bueno, supongamos que los tres vecinos más cercanos son estos, en este caso los tres son
circulitos, o sea que eso seguramente sea un circulito, podemos tener problemas cuando estamos
los métodos de vecinos más cercanos tienen la ventaja, obviamente, de que pueden reconocer,
pueden reconocer clásteres,
los métodos de vecinos más cercanos definen una cosa así,
no, pero,
la cosa así, pueden reconocer cosas que no son lineales, tienen el problema de que a veces
sobreajustan demasiado, árboles de decisión que no son muy realizados en el procesamiento de
la imaginación, rando fores que son como una, hay muchos, muchos métodos de clasificación,
pero en todos lo que tienen en común es que las medidas para realizar, para el métodología
es la que hay en la clasificación pasada. Y eso desde el punto de vista de los métodos de clasificación
puros, pero también se acuerdan que habíamos visto los métodos de clasificación secuencial,
cuando yo quiero asignar una secuencia de tangs, de clases, asumo que mi atributo tiene una secuencia,
mi instancia es una secuencia, por ejemplo, una oración, que es una secuencia de palabra,
y quiero asignar una secuencia de tangs, bueno, hay versiones generativas, en el caso de los,
la versión generativa de Naive Bayes son los hidden Marco Models, que lo vimos bastante en detalle
en alguna clase anterior, y hay una versión también de clasificadores secuenciales,
que estos son por lejos los que bandan mejor, que son los temas secuenciales, que son los
conditions al random fields, los conditions al random field también fueron una novedad en los temas
de clasificación secuencial, porque andan mucho mejor el general que los hidden Marco Models,
y tienen una, son como una versión, una versión secuencial del modelo entropiés máximo,
no, no, no, no esperen que entren detalle, tampoco conozco mucho la detalle, el matemático del
del conditional random fields, pero como herramienta digamos para el clasificación de secuencias
funciona muy bien. Yo diría que si uno va a, a ver si me queda algo más, no, acá tienen
un poco de show jugar, sí, de estas cosas. Hueca, sirve para jugar, pero es juguete en general.
Salkill Learnes es una herramienta bastante, una librería bastante polenta de, en Python y
que está bastante de moda. Y acá me faltan, me faltan todas las nuevas bolas de bibliotecas de
Dib Learning, ¿no?, de que son de Red Lunar y que son Torch, este, Teano, Keras,
TensorFlow. Pero bueno, Salkill Learnes es una biblioteca de, de genérica, de Machine Learning
en Python, Orange también. En Ileteká es más de procedimiento en lenguaje natural, pero tiene
por ejemplo un plazificador exceciano. CRF más más es un Toolkit para Condition Random Fields.
PyBrain, creo que no, no, no corre más o no sé, que es Red Neuronal y Homebiteon.
SMelite era la herramienta de Support Vector Machines, cuando estaba en moda.
SMelite es el 99 para que se dieron una idea y estaba bastante estable porque no hay mucho para,
es muy sencillo el modelo de la Support Vector Machines, por lo grande como me lo aplico.
Yo diría que, que si, si, si vamos a lo que, a lo que es el procedimiento en lenguaje natural a nivel
de, a nivel de, como decir, de mercado o de herramienta o de, no me sabe la palabra, de industria,
digamos, a nivel industrial, no es la palabra correcta pero está. Yo diría que el procedimiento en
lenguaje natural está en lo que hemos aprendido hasta ahora. No? Es decir, todas estas cosas que
hemos aprendido en las clases hasta ahora ya se encuentra a nivel industrial. A nivel industrial
estoy hablando de las compañías de Intermed, no? Reconocimiento de, en, reconocimiento de
caracteres mal escrito, clasificación, clasificación de, sentimenta análisis, de todo lo que hemos
hablado hasta ahora, no? En el grama y todas esas cosas. También, a ver, no es lo único, no?
Machine Translation es un ejemplo de cosas que andan muy bien. Este, pero utilizan métodos más
o menos hasta acá. Lo que quiero decir es que, y bueno, y ahí hay algún componente semántico
también que lo van a ver después con Luis, pero esas cosas más avanzadas, digamos, recién,
recién se está empezando a hablar, pero en algunas cosas de, por ejemplo,
reconocimiento de entidades, no? El otro día lo veía en un diario, digamos, unos dos años atrás,
que algo que decía que era una aplicación que reconocía a partir del New York Times lugares,
bueno, Google lo hace, no? Lugares y cuando arma las citas, cuando a partir de un correo te lo
meten en el calendario, ahí lo que está haciendo es reconocimiento de entidades. Está
reconociendo que dice el jueves 23, cena con tal y lo está viendo, está haciendo clasificación
secuencial, pero eso son cosas que en el academia están como hace como 10 años, digamos, los
condillos hablando de FIT tienen como 10 años, recién están empezando como entrares, el tipo
costo. Y hay cosas que todavía está por verse cómo se van a incorporar, que son las que vamos
a ver de ahora en adelante, que son el parsing, o sea, análisis más complejo, ni que hablar de
análisis semántico más allá de la semántica de palabras, son cosas que vamos a ir viendo después,
o sea, hay mucho todavía para mejorar y en el academia también, porque no está en todo,
resuelto ni mucho menos. Por ejemplo, en el poder analizar semánticamente las cosas,
estamos bastante lejos. Pero lo que quería transmitir es que esto de la clasificación es
lo más que anda en la vuelta, digamos, ¿no? Y que con esto se va a hacer un montón de cosas.
Bueno, clases que vienen arrancamos con parci, ¿sí? Gracias.
