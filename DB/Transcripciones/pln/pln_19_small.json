{"text": " Una vez que eleg\u00ed con el paso 1, eleg\u00ed cu\u00e1ntas palabras en espa\u00f1ol le voy a usar, en el paso 2 lo que voy a elegir es una alineaci\u00f3n, una funci\u00f3n de alineaci\u00f3n que me dice cada palabra con cu\u00e1l se va a corresponder, cada palabra del lado del espa\u00f1ol con qu\u00e9 palabra en ingl\u00e9s se va a corresponder. Este modelo asume de manera muy naiv que todas las alineaciones que yo puedo tener son equiprobables. O sea, asume que yo voy a tener un conjunto de alineaciones posibles y todas van a tener la misma probabilidad. Bien, entonces, la probabilidad de elegir una alineaci\u00f3n en particular, si yo tengo un mont\u00f3n de alineaciones, digamos, la probabilidad de elegir una alineaci\u00f3n en particular, va a ser uno sobre la cantidad de alineaciones que tengo, porque en realidad todas van a ser equiprobables. Bien, entonces, \u00bfcu\u00e1ntas alineaciones puedo tener entre dos oraciones, una oraci\u00f3n en ingl\u00e9s que tiene largo I y una oraci\u00f3n en espa\u00f1ol que tiene largo J? \u00bfC\u00f3mo puedo calcular cu\u00e1ntas alineaciones existen? M\u00e1s o menos, s\u00ed, casi la J. Recuerden que del lado de ingl\u00e9s yo ten\u00eda ciertas palabras, en ingl\u00e9s ten\u00eda la palabra E1, E2 hasta, sub\u00ed, y en espa\u00f1ol ten\u00eda las palabras F1, F2 hasta F subj. Entonces, yo pod\u00eda trazar l\u00edneas para alinear, pero adem\u00e1s en ingl\u00e9s yo siempre he considerado que ten\u00eda un token nul, entonces todas las palabras que no estaban alineadas del lado del espa\u00f1ol iban a parar ah\u00ed. As\u00ed que en ingl\u00e9s en realidad no tengo i posibilidades, tengo una m\u00e1s, tengo i m\u00e1s uno. Entonces, \u00bfcu\u00e1ntas formas tengo yo de mapear estas J posibilidades en espa\u00f1ol con las I de ingl\u00e9s? Exacto, i m\u00e1s uno a la J, porque yo tengo i m\u00e1s un opciones para la primera y m\u00e1s una opciones para la segunda, etc\u00e9tera, hasta que llevo al final. As\u00ed que son i m\u00e1s uno a las J alineaciones posibles. Ojo, el nul es como una pisadita que hago yo para alinear cosas que no tienen un correspondiente. O sea, yo ten\u00eda una palabra en espa\u00f1ol que\u2026 Varias de las Fes pueden estar alineadas de nul, no importa en qu\u00e9 orden est\u00e1n. Eso. Bien, entonces eran i m\u00e1s uno a las J posibles alineaciones, por lo tanto, la probabilidad de elegir una alineaci\u00f3n A dada la oraci\u00f3n en ingl\u00e9s, la probabilidad de elegir una alineaci\u00f3n cualquiera dada la oraci\u00f3n en ingl\u00e9s va a ser el producto de la probabilidad de haber sorteado un valor J primero, que era epsilon por la probabilidad de elegir una alineaci\u00f3n cualquiera para ese J, que es uno sobre i m\u00e1s uno a la J. Bien, entonces esto lo resumimos como epsilon sobre i m\u00e1s uno a la J. Epsilon sobre i m\u00e1s uno a la J es la probabilidad de, dada una oraci\u00f3n en ingl\u00e9s, elegir cierta alineaci\u00f3n que yo voy a utilizar. Bien, ese fue el segundo paso. El tercer paso es una vez que ya tengo la alineaci\u00f3n, voy mirando cada palabra del lado en ingl\u00e9s y le voy poniendo una palabra correspondiente del lado espa\u00f1ol. Para ac\u00e1 voy a asumir que yo tengo una tabla de traducci\u00f3n, una tabla de traducci\u00f3n que me dice que tiene de un lado todas las palabras en espa\u00f1ol y del otro lado todas las palabras en ingl\u00e9s, entonces mi tabla va a tener una forma como, por ejemplo, hacer una tabla as\u00ed que de un lado va a decir las palabras en espa\u00f1ol como banco, perro, gato y m\u00e1s cosas y del otro lado va a tener las correspondientes en ingl\u00e9s como bank, bench, cut, tree y m\u00e1s cosas. Y entonces esta tabla va a decir la probabilidad de traducir una cosa en la gota. Entonces banco probablemente tenga cierta probabilidad para bank y cierta probabilidad para bench, 0.4 y 0.6, 0.06 puse. Y para cut no va a tener ninguna probabilidad y para tree tampoco y despu\u00e9s perro no va a tener nada de esto, pero s\u00ed despu\u00e9s y cut va a ser, no s\u00e9 0.8 en este caso, etc\u00e9tera. Voy a tener una tabla bastante grande que tiene todas las posibilidades de traducir una palabra como otra. Entonces si yo tengo esa tabla, lo que puedo decir es que la forma de calcular la probabilidad de esa oraci\u00f3n final que yo traduje va a depender de cu\u00e1les son las palabras que yo elija, va a depender de cu\u00e1les son las palabras que yo haya puesto dentro de mi oraci\u00f3n para traducir. Entonces esa tabla que est\u00e1 ah\u00ed definida, le llamamos ac\u00e1 en la slide, aparece como t de f sub x sub y dice que la probabilidad de traducir la palabra es sub y como f sub x. Entonces saca de una cosa importante. Si tenemos la oraci\u00f3n en ingl\u00e9s, la oraci\u00f3n en ingl\u00e9s recuerdan que ten\u00eda las palabras f sub 1, f sub 2 hasta f sub n, la oraci\u00f3n en espa\u00f1ol ten\u00eda las palabras f sub 1, f sub 1, f sub 2 hasta f sub j. Y yo ten\u00eda en el medio una funci\u00f3n de alineaci\u00f3n que me dec\u00eda qu\u00e9 palabra se correspond\u00eda con cu\u00e1l. Entonces no era f sub n ni f sub j. Era f sub i y f sub j grande. Esto era f sub i y esto era f sub j grande. Entonces si yo tengo una palabra cualquiera dentro de la oraci\u00f3n en espa\u00f1ol, tengo un f sub j chica dentro de la oraci\u00f3n en espa\u00f1ol, esto se va a corresponder con alg\u00fan f sub i chica en la oraci\u00f3n en ingl\u00e9s, digamos. Yo s\u00e9 que esto se cumple por la funci\u00f3n de alineaci\u00f3n porque agarra y mapea todas las palabras que est\u00e1n en espa\u00f1ol con algo que est\u00e1 del lado del ingl\u00e9s, potencialmente con el token vac\u00edo nul. Bien, entonces tengo una palabra del lado del espa\u00f1ol que es f sub j y una palabra del lado del ingl\u00e9s que es f sub i. \u00bfCu\u00e1l es la relaci\u00f3n entre f sub j y f sub i? \u00bfC\u00f3mo es la relaci\u00f3n entre s\u00ed, digamos? Yo puedo decir que el i es igual a algo de j. \u00bfDe alguna manera? La funci\u00f3n de alineaci\u00f3n, ah\u00ed est\u00e1. O sea, el i es igual a la funci\u00f3n de alineaci\u00f3n aplicada j. Como la i, el \u00edndice de este de ac\u00e1 es igual a la funci\u00f3n de alineaci\u00f3n aplicada j. Entonces, yo puedo decir que la palabra f sub i es igual a la palabra e sub a sub j. As\u00ed que puedo decir que, en realidad, los que est\u00e1n alineados son la palabra f sub j est\u00e1 alineada con la palabra e sub a sub j. Y ah\u00ed me saqu\u00e9 el i de encima, digamos. Simplemente, iterando sobre las palabras, iterando sobre la j puedo establecer la correspondencia entre las dos palabras. Y eso es un poco lo que dice ac\u00e1 para terminar de armar lo que es el modelo de traducci\u00f3n. Para terminar de armar el modelo de traducci\u00f3n dicen que en el tercer paso yo voy a elegir cu\u00e1les son las palabras. Entonces, lo que voy a hacer es iterar sobre todas las palabras y haciendo el producto de todas las probabilidades. O sea, el producto de dado que yo ten\u00eda la palabra f sub j, perd\u00f3n, dado que yo ten\u00eda la palabra e sub a sub j en ingl\u00e9s, entonces elegir la palabra f sub j en espa\u00f1ol. Eso hago una productoria con todos los valores de las distintas palabras. Bien, entonces ah\u00ed llegu\u00e9 a el \u00faltimo de los valores que quer\u00eda calcular, que es la probabilidad de f dado que conozco ah\u00ed es igual a la productoria con j igual a 1 hasta j grande de el valor de la tabla de traducci\u00f3n, que es t sub f sub j, t de f sub j e sub a sub j. Bueno, ah\u00ed tengo c\u00f3mo en cada paso fui calculando cosas, este se correspond\u00eda al paso uno del modelo, paso uno, este se corresponde con el paso dos del modelo, en realidad este ya tiene el paso uno y el paso dos juntos porque ya tengo el \u00e9xil\u00f3n ac\u00e1 y este se corresponde con el paso tres del modelo. El paso tres de la historia de generaci\u00f3n. Mi objetivo con todos estos valores que est\u00e1n ac\u00e1 es calcular p de f dado e. \u00bfQu\u00e9 par\u00e1metros introduce? \u00bfQu\u00e9 par\u00e1metros fueron surgiendo a medida que yo iba iterando sobre estos pasos? Bueno, en primer lugar el \u00e9xil\u00f3n aquel que est\u00e1bamos viendo, este es un valor que yo tendr\u00eda que estimar a partir de mirar en los corpus como son los largos de las oraciones relativos y el otro par\u00e1metro importante es aquella tabla de all\u00e1, aquella tabla de traducci\u00f3n es que me dice banco, con qu\u00e9 probabilidad lo puedo traducir como bank, con qu\u00e9 probabilidad lo puedo traducir como bench, etc\u00e9tera, etc\u00e9tera. Esa tabla en realidad es un par\u00e1metro del modelo, es un par\u00e1metro del sistema que si yo lo tuviera me alcanzar\u00eda con eso para poder construirme este modelo y calcular la probabilidad de cualquier par de oraciones. Bien, y entonces antes de continuar vamos a terminar de armar cu\u00e1l es la imagen de esto, que es decir yo en realidad lo que quer\u00eda calcular era p de f dado e, que eso va a ser mi modelo de traducci\u00f3n y de hecho va a ser el encargado de medir la adecuaci\u00f3n de una frase. P de f dado e lo puedo calcular con esta descomposici\u00f3n de pasos que hice ac\u00e1 en realidad porque lo hago de la siguiente manera. Yo quiero calcular p de f dado e y entonces voy a mirar lo que dice ac\u00e1, p de f dado e es igual a la sumatoriana de p de f dado e. \u00bfQu\u00e9 significa eso? Que para traducir entre una oraci\u00f3n en espa\u00f1ol y una oraci\u00f3n en ingl\u00e9s, o m\u00e1s bien para traducir entre una oraci\u00f3n en ingl\u00e9s y una oraci\u00f3n en espa\u00f1ol hay muchas formas de alinear las palabras entre el ingl\u00e9s y en espa\u00f1ol y una vez que yo eleg\u00ed una forma de alinear hay muchas formas de elegir las palabras que vienen despu\u00e9s digamos yo miro la tarjeta de traducci\u00f3n y capaz que hay varias maneras de elegir distintas palabras. Entonces lo que eso significa es que no existe una sola manera de traducir una oraci\u00f3n en ingl\u00e9s a una oraci\u00f3n en espa\u00f1ol. Yo puedo encontrar varias formas de alinear las palabras y varias formas de elegir las palabras de manera que muchas alineaciones son posibles. Entonces para saber cu\u00e1l es la probabilidad de traducir p de f dado e, entonces yo voy a tener que sumar sobre todas las alineaciones posibles sobre todas las formas de alinear las dos oraciones f y e, voy a tener que iterar sobre eso y para cada una voy a tener que calcular la probabilidad parcial. Entonces digamos yo tengo cinco formas de alinear las dos oraciones, cinco es un n\u00famero un poco raro pero digamos tengo n formas de alinear las dos oraciones, voy a tener que mirar bueno para la primera alineaci\u00f3n cu\u00e1l es la probabilidad de encontrar la oraci\u00f3n f para la segunda alineaci\u00f3n cu\u00e1l es la probabilidad de encontrar la oraci\u00f3n f para la tercera oraci\u00f3n y as\u00ed hasta llegar al final y agarro y sumo todo eso. Eso lo puedo hacer porque las alineaciones son una descomposici\u00f3n del espacio de probabilidades. En realidad yo puedo descomponer el espacio de probabilidades en pedacitos disjuntos y cada alineaci\u00f3n va a ser uno de ellos. As\u00ed que digamos que para calcular el modelo de traducci\u00f3n p de f dado e necesito sumar sobre todas las alineaciones posibles. Ahora lo que me falta es saber c\u00f3mo calculo este valor de ac\u00e1. As\u00ed que lo que estoy diciendo es que la probabilidad de f dado e es la suma sobre las alineaciones de la probabilidad de f y esa alineaci\u00f3n dado e. Eso es simplemente lo que dice ah\u00ed en la slide. Lo que me falta calcular entonces es esta parte de ac\u00e1 y esa parte de ac\u00e1 la calculo de esta manera. Yo digo que la probabilidad de f dado e es igual, ah\u00ed est\u00e1 m\u00e1s o menos el resultado final pero podemos sacar qu\u00e9 es lo que tendr\u00eda que poner de este lado. Ahora s\u00ed me acuerdo bien. Ah, ah\u00ed est\u00e1. Por definici\u00f3n de probabilidad condicional. Eso. p de f dado e, le voy a dar varias maneras a hacerlo pero esto se puede definir como p de f a e sobre p de e. No? Por definici\u00f3n de probabilidad condicional. Pero adem\u00e1s esto si quiero podr\u00eda llegar a decir esto es lo mismo que p de f a e sobre p de e por la cualidad que me faltaba. No. A e. Por p de a e sobre p de a e. Era esto lo quer\u00eda. O sea, yo puedo agarrar esta probabilidad que est\u00e1 ac\u00e1 y multiplicarla y dividirla por el mismo n\u00famero, que s\u00e9 que son mayores que cero, as\u00ed que en definitiva esa divisi\u00f3n me va a dar uno. Y ah\u00ed yo puedo tomar y asigno este con este y este con este. En definitiva lo que me queda es si asocio estos dos me va a quedar p de f dado a e y si asocio estos dos de ac\u00e1 me va a quedar p de a dado e. \u00bfQu\u00e9 es lo que dice all\u00e1? La probabilidad de p de f a dado e, bueno, s\u00ed, de los dos, de f y a dado e es igual a la probabilidad de f dado a e por la probabilidad de a dado e. Bien, y estos dos valores que est\u00e1n ac\u00e1 no los eleg\u00ed por casualidad sino que son los valores que ten\u00eda antes en el modelo. O sea, yo ten\u00eda que el p de a dado e era igual a epsilon sobre y m\u00e1s uno a la j y el otro era la productoria desde j igual a 1 hasta j grande de las valores de traducci\u00f3n, el f sub j y el e sub a sub j. Entonces, en definitiva puedo calcular p de f a dado e y adem\u00e1s puedo calcular haciendo una suma sobre todas las alineaciones posibles, puedo calcular el p de f dado e. Bien, con eso y con todo ese mont\u00f3n de cocciones llegamos a construir lo que es un modelo de traducci\u00f3n, o sea, solamente teniendo una tabla de traducciones que me diga cu\u00e1l es la probabilidad de traducir una palabra. Como otra palabra, yo puedo llegar a definirme cu\u00e1l es la probabilidad de traducir una oraci\u00f3n dada otra oraci\u00f3n. Bien, y hay una cosa m\u00e1s, bueno, esto ya lo estuvimos viendo que aplicamos en cada paso, y hay una cosa m\u00e1s que es si yo tuviera las dos oraciones, digamos, la oraci\u00f3n en ingl\u00e9s y la oraci\u00f3n en espa\u00f1ol y adem\u00e1s tuviera la tabla esta con todas las probabilidades, yo podr\u00eda hacer un algoritmo de programaci\u00f3n din\u00e1mica, un algoritmo estilo Viterbi que vaya recorriendo alineaciones y me diga cu\u00e1l es la alineaci\u00f3n m\u00e1s probable. No vamos a ver los detalles del algoritmo, pero hay una forma de decir, bueno, voy recorriendo las dos oraciones y me voy quedando con las subsecciones m\u00e1s probables y al final me termina devolviendo cu\u00e1l es la alineaci\u00f3n m\u00e1s probable dada esas oraciones. O sea, que si yo tuviera ya esa tabla de traducciones, esa tabla de probabilidad de traducci\u00f3n, podr\u00eda construirme las alineaciones del corpus. As\u00ed que bueno, hasta el momento dec\u00edamos, bueno, suponemos que tenemos esta tabla de traducci\u00f3n que me dice para bank si se traduce, perd\u00f3n, para banco si se traduce como bank o como bench, etc. Estaba diciendo que ten\u00eda esa tabla, pero en realidad la realidad es que no tengo esa tabla y me gustar\u00eda poder construirla. Entonces, nos gustar\u00eda poder estimar esas probabilidades para poder construirme esa tabla. Si yo tuviera un corpus paralelo, simplemente podr\u00eda ir recorriendo el corpus y contando cu\u00e1ntas veces aparece banco alineado con bench y cu\u00e1ntas veces aparece alineado con bank y ah\u00ed sacar\u00eda una probabilidad, pero no tengo las alineaciones. Y por lo que vimos, digamos, reci\u00e9n, si yo tuviera la tabla, entonces yo adem\u00e1s podr\u00eda ir recorriendo el corpus y construirme las alineaciones. As\u00ed que si yo tuviera las alineaciones podr\u00eda contar y sacar la tabla, si yo tuviera la tabla podr\u00eda pasarle un algoritmo y construir las alineaciones. Pero la verdad que no tengo ninguna de las dos cosas. Entonces se vuelve un problema de huevo y la gallina. O sea, si yo tuviera las alineaciones construir\u00eda el modelo, construir\u00eda la tabla de probabilidades, si yo tuviera la tabla de probabilidades podr\u00eda construir las alineaciones. Para este tipo de problemas, en los cuales yo tengo como dos variables interdependientes y no conozco exactamente el valor de ninguna de las dos, se utiliza lo que se conoce como el algoritmo expectation maximization o maximizaci\u00f3n de la esperanza. Y bueno, es un algoritmo que sirve exactamente para este tipo de problemas. En realidad lo que va a hacer el algoritmo iterar es un algoritmo iterativo que va tratando de converger una soluci\u00f3n y lo que hace es decir, bueno, yo no tengo ninguno de los dos valores. O sea, si yo tuviera mi tabla de probabilidad de traducci\u00f3n me podr\u00eda calcular las alineaciones y tuviera mis alineaciones me podr\u00eda calcular la probabilidad de traducci\u00f3n. Entonces lo que hace es decir, bueno, asumo que mi tabla de traducci\u00f3n va a ser uniforme, digamos. Cualquier palabra se puede traducir como cualquier otra palabra con la misma probabilidad. A partir de eso calculo alineaciones y a partir de esas nuevas alineaciones calculo otra vez la tabla. Y de vuelta, con esa tabla que calcul\u00e9, vuelvo a medir las alineaciones y de vuelta con esas nuevas alineaciones vuelvo a calcular la tabla. Entonces, aunque no me crean, esto despu\u00e9s de muchas iteraciones va convergiendo a algo. Y parece m\u00e1gico, \u00bfno? Parece como que, en realidad si yo no tengo ninguno de dos valores, no deber\u00eda nada, deber\u00eda como dar fruta. Pero voy a tratar de comenzarlos de que, en realidad, esto s\u00ed funciona, con un ejemplito. Bien, tenemos. Entonces, vamos a construir un sistema que es de traducci\u00f3n entre franc\u00e9s y el ingl\u00e9s donde hay un cuerpo muy grande, pero bueno, nos vamos a concentrar solo en tres peque\u00f1as oraciones citas que dicen la mes\u00f3n se traduce como de House, la mes\u00f3n blue se traduce como de Blue House y la flea se traduce como de Flower. Entonces, al principio lo que hago es decir, bueno, todas las traducciones entre todas las palabras son equiprobables, as\u00ed que lo que me va a quedar es cuando reparten entre las alineaciones, todas van a tener el mismo peso. Entre la y mes\u00f3n, la probabilidad de que la se traduzca como D o que se traduzca como House va a ser la misma, en realidad porque todas las alineaciones son equiprobables. En la mes\u00f3n blue tambi\u00e9n pasa lo mismo, la probabilidad de traducir la como D como Blue o como House va a ser la misma y en la flea pasa igual. Entonces, eso es la primera, el primer paso, digamos, en el primer paso yo voy a tener todas las alineaciones equiprobables y todas las valores de las palabras iguales. Entonces, en mi algoritmo yo empec\u00e9 con una tabla de traducci\u00f3n que era toda uniforme, digamos, yo ten\u00eda la probabilidad de traducir cualquier palabra en cualquier otra, era la misma. A partir de eso yo me constru\u00ed estas alineaciones que tambi\u00e9n parece que son todas equiprobables y parece que no tienen como mucha informaci\u00f3n. Entonces lo que voy a hacer ahora, a partir de esto, es tratar de construirme de vuelta la tabla de traducciones pero mirando estas nuevas alineaciones que hay. Entonces lo que voy a construir es una tabla que tiene todas las palabras del lado de franc\u00e9s, tiene la mes\u00f3n blue flower y de House blue flower. Y para llenar esta nueva tabla, lo que tengo que hacer es iterar sobre las alineaciones, mirar cada una de las palabras cuantas veces est\u00e1 alineada con las otras y contar, o sea, y sumar los pesos de cada una de las alineaciones. Entonces la alineaci\u00f3n entre la y de. En total, mirando ese ejemplo de corpus, \u00bfcu\u00e1nto me dar\u00eda? \u00bfCu\u00e1l ser\u00eda el peso de esa alineaci\u00f3n? Para verlo, en realidad lo que hago es contar, miro cuantas veces la y de est\u00e1n alineados. Entonces tengo 0.5 de peso en la primera, en la segunda tengo 0.33 y en la \u00faltima tengo 0.5 de vuelta. As\u00ed que en total tengo como 1.33 de peso entre la y de. Despu\u00e9s miro, entre la y House, \u00bfcu\u00e1nto peso tengo? \u00bfcu\u00e1nta masa de probabilidad tengo? Bueno, tengo 0.5 en la primera relaci\u00f3n, 0.33 en la segunda y nada en la tercera. Por lo tanto en total tengo 0.83 de probabilidades entre la y House. Despu\u00e9s miro, entre la y blue, \u00bfcu\u00e1nto peso tengo? Solamente 0.33 solo est\u00e1 en la del medio y entre la y flair, \u00bfcu\u00e1nto tengo? No, entre la y flower, \u00bfcu\u00e1nto tengo? 0.5 solo aparece en la del final. Bien, completemos la siguiente, entre mes\u00f3n y de, \u00bfcu\u00e1nto tendr\u00eda? 0.83, est\u00e1 en la primera y en la segunda, entre mes\u00f3n y House, entre mes\u00f3n y House y 0.83 porque aparece en las dos. Bien, entre mes\u00f3n y blue, solamente aparece en la segunda, as\u00ed que voy a tener 0.33 y entre mes\u00f3n y flower no tengo nada. Despu\u00e9s entre blue y de, solamente aparece en la segunda, as\u00ed que voy a tener 0.33, entre blue y House, creo que de vuelta tengo 0.33 y entre blue y blue tambi\u00e9n 0.33 y no aparece junto con flower. Y para despu\u00e9s, para flair tengo 0.5 con de, 0 con House, 0 con blue y 0.5 con flower. Bien, entonces hice una pasada por todas las alineaciones y me calcul\u00e9 cu\u00e1les son los pesos relativos de cada una de estos pares. Lo siguiente que hago, como esto va a ser una probabilidad, es normalizar. Entonces me voy a construir una tabla, digamos normalizando por, digamos, voy a sumar en cada fila y voy a dividir entre la cantidad que aparece para cada fila, as\u00ed que de vuelta me construyo la tabla, que me queda la mes\u00f3n blue flower y de este lado de House ac\u00e1, de House y blue flower. Y lo que voy a hacer es normalizar, entonces si yo sumo estos de ac\u00e1, creo que me da 2 en total, no, 3 en total. Tengo los valores ac\u00e1, no tengo que hacer los c\u00e1lculos, pero s\u00ed, me da 3 en total. Entonces lo que pasa cuando yo normalizo es que ac\u00e1 me queda 0.44, ac\u00e1 me queda 0.28, ac\u00e1 me queda 0.11 y ac\u00e1 me queda 0.17. Pues el segundo, tambi\u00e9n lo normalizo esta vez entre 2 y me queda 0.42, 0.42, 0.16, 0. El tercero ya suma 1, as\u00ed que me queda 0.23, 0.33, 0.33, 0 y el \u00faltimo tambi\u00e9n queda igual, 0.5, 0, 0, 0.5. Bien, entonces me constru\u00ed una nueva tabla de probabilidad de traducci\u00f3n, dado que ahora las alineaciones ser\u00edan estas. Y noten lo que pas\u00f3 ac\u00e1, si yo miro la fila correspondiente a la, \u00bfqu\u00e9 es lo que pasa ahora con esa fila? Recuerda que yo empec\u00e9 teniendo todas las probabilidades de traducci\u00f3n de que parecen palabras, eran equiprobables. Si yo ahora miro la fila de la, \u00bfqu\u00e9 es lo que pasa? Exacto, aparece claramente que la asociaci\u00f3n entre la y de es m\u00e1s fuerte, tengo un 0.44 de probabilidad de traducir la como de y tengo bastante menos en los otros, tengo 0.28, 0.11, 0.17. Y yo hab\u00eda empezado diciendo que eran equiprobables, entonces yo probablemente ten\u00eda 0.25, 0.25, 0.25, 0.25 en cada una. Y despu\u00e9s de un paso de la iteraci\u00f3n, descubri\u00f3 que la ID tiene m\u00e1s chance de ser una traducci\u00f3n de la otra, en vez de traducir la como House o la como Blue o la como Flower. Eso pasa en el primer paso, en la primera iteraci\u00f3n el tipo descubre, el algoritmo descubre que la asociaci\u00f3n entre la ID es bastante m\u00e1s fuerte. Como pasa eso, lo que va a pasar es que cuando yo reparta de vuelta en las alineaciones estas l\u00edneas que se corresponden a la asociaci\u00f3n entre la ID van a estar m\u00e1s fuertes, van a tener un poco m\u00e1s de peso y como esto es una distribuci\u00f3n de probabilidades, esa masa que gan\u00f3 la asociaci\u00f3n entre la ID se va a tener que sacar de otras alineaciones posibles, o sea si la est\u00e1 asociada con D, entonces no est\u00e1 asociada con las otras que est\u00e1n alrededor. Entonces esa masa que se pierde, digamos, o sea que gana en la D se tiene que repartir en las otras alineaciones posibles, o sea en las que no son entre la ID. Entonces despu\u00e9s de una iteraci\u00f3n la asociaci\u00f3n entre la ID empieza a ser m\u00e1s fuerte y como pasa eso en la siguiente iteraci\u00f3n va a empezar a descubrir que como la estaba alineado con D, entonces mes\u00f3n tiene que estar alineado con House y como mes\u00f3n estaba alineado con House, digamos, esa misma masa de probabilidad se va a traducir, a transferir a la segunda y lo mismo, como la estaba alineado con D, entonces Fleur tiene que estar alineado con Flour. Entonces si yo sigo iterando en estos pasos, en cada paso lo que va a pasar es que se va a mover un poco m\u00e1s de probabilidad hasta que al final va a terminar descubriendo cu\u00e1l es la alineaci\u00f3n real de las palabras, o sea va a descubrir que la va, o sea con D, mes\u00f3n con House, Blue con Blue, Fleur con Flour. \u00bfC\u00f3mo es que va a descubrir eso? Porque en cada paso lo que va pasando es que algunas de las asociaciones como est\u00e1n, como aparecen, que ocurren digamos en m\u00e1s oraciones, tienen m\u00e1s fuerza que otras, entonces el peso que esas asociaciones ganan lo va sacando de otro lado y eso hace que de otro lado se empicen a generar otras alineaciones diferentes. Entonces al final esto termina convergiendo y termina revelando lo que es la estructura suyacente de las palabras y c\u00f3mo se alinean unas con otras. Bueno, una vez que yo termine de hacer esto puedo agarrar y construirme efectivamente la tabla final de traducciones que es simplemente busco cada una de las posibles traducciones, digamos de los posibles pares y saco las probabilidades. \u00bfY qu\u00e9 pas\u00f3 ac\u00e1? Mientras yo estaba construyendo mi modelo de traducci\u00f3n, mientras yo estaba construyendo la tabla de traducciones adem\u00e1s de como efectos secundarios se construy\u00f3 un corpus alineado, un corpus que est\u00e1 alineado a nivel de palabras. As\u00ed que bueno, el algoritmo de Spectation Maximization funciona de esa manera, tiene siempre dos pasos, un paso de Spectation y un paso de Maximization. En este caso el paso de Spectation se trataba de agarro la tabla de probabilidad de traducci\u00f3n que tengo y con eso me armo alineaciones y despu\u00e9s el de Maximization es al rev\u00e9s agarro las alineaciones que acabo de construir y me armo una nueva tabla y voy iterando todos esos pasos hasta que eventualmente converge. Bien, dijimos que eran cinco modelos de IBM, no vamos a ver muy en detalle los otros, o sea, solo mencionar que empiezan a crear complejidad. En este modelo uno hab\u00edamos dicho que todas las alineaciones eran equiprobables, en el modelo dos abandonan esa noci\u00f3n y dicen bueno, en vez de alineaciones equiprobables, yo voy a tener un modelo de reordenamiento de las palabras para decir, bueno, tengo cierta probabilidad de que las palabras que est\u00e1n, si yo tengo I palabras en ingl\u00e9s, J palabras en espa\u00f1ol, tengo cierta probabilidad de mover la palabra I y la palabra J y bueno, y as\u00ed siguen subiendo en complejidad hasta llegar al modelo cinco, que modelo cinco es el que anda mejor, pero de todas maneras estos son modelos que ya no se usan, digamos, esto es del a\u00f1o 93 y en general se han obtenido mejores resultados abandonando estos modelos. Entonces el que vamos a pasar a ver a continuaci\u00f3n es un modelo bastante m\u00e1s moderno que es lo que s\u00ed se utiliza hoy en d\u00eda en traductores como los de Google. S\u00ed. Es que en realidad, claro, a ver, estos modelos estad\u00edsticos no utilizan ning\u00fan tipo de analizador morfol\u00f3gico y nada para sacarlo. Hay otros modelos que s\u00ed lo hacen, no vamos a dar ninguno en esta clase pero hay otros modelos que s\u00ed hacen uso de esa informaci\u00f3n. Igual son como refinamientos, creo que ninguno lo tiene como en la base del modelo, el uso de parto speech, pero s\u00ed cuando vos no sabes una palabra, digamos una palabra que es desconocida, en realidad utilizar informaci\u00f3n sobre parto speech y eso probablemente te ayude. En estos modelos por lo menos no lo hab\u00edan tenido en cuenta. Bien, entonces s\u00ed, lo que vamos a ver ahora es el modelo de frases que es algo m\u00e1s moderno y es, o sea, el Google Translate o Bing Translate se basan en modelos de este estilo. Y bueno, y antes de ver c\u00f3mo se modelo de frases, volvamos un poco a lo que era la alineaci\u00f3n entre palabras. Yo ten\u00eda esta frase cl\u00e1sica, \u00bfno? Mar\u00eda no dio una bofetada de la bruja verde, en ingl\u00e9s era Mary did not slap de Greenwich y una alineaci\u00f3n entre esas dos oraciones en realidad se ver\u00eda como algo as\u00ed. Yo tengo que Mar\u00eda se alinea con Mary, no se alinea con did not, slap se alinea con daba una bofetada, de se alinea con ala, podr\u00eda ser solamente con la y el a que no est\u00e1 alineado nada. Green se alinea con verde y bruja con wedge. \u00bfQu\u00e9 diferencia tiene esto con la otra alineaci\u00f3n que hab\u00edamos visto hoy? A ver si se les ocurre algo distinto que tiene esta alineaci\u00f3n y la que hab\u00edamos visto hoy. Era not con no, s\u00ed. \u00bfY qu\u00e9 es lo que cambia ac\u00e1 para que pase eso? Lo que estaba pasando hoy era que yo part\u00eda de las palabras en espa\u00f1ol, iba las palabras en ingl\u00e9s y yo ten\u00eda una funci\u00f3n que me mapeaba las palabras en espa\u00f1ol con las palabras en ingl\u00e9s. Entonces yo a cada palabra en espa\u00f1ol como m\u00e1ximo le pod\u00eda hacer corresponder una palabra en ingl\u00e9s. Entonces me quedaba que yo pod\u00eda expresar cosas como que daba una bofetada, daba, est\u00e1 asociado a slap, una est\u00e1 asociado a slap, bofetada est\u00e1 asociado a slap, eso lo pod\u00eda expresar, pero no pod\u00eda expresar algo como esto, que no est\u00e1 asociado did not, porque no ser\u00eda una funci\u00f3n. Yo no puedo asociar uno de los valores de la funci\u00f3n con dos cosas del lado del codominio. Y ac\u00e1 en realidad no puedo hacerlo ni en este sentido ni en el otro sentido, con una funci\u00f3n no me sirve porque de vuelta me pasa que slap est\u00e1 asociado tres cosas. Entonces con una funci\u00f3n de alineaci\u00f3n yo no puedo construir este tipo de expresiones, en realidad necesito algo como un poco m\u00e1s poderoso. Esto es lo que dec\u00edamos, los modelos de IBM siempre usan un mapeo de uno a muchos, usan una funci\u00f3n de alineaci\u00f3n, mapeo uno a muchos, pero en realidad lo que necesito para poder capturar realmente c\u00f3mo funciona en el lenguaje es mapeo de muchos a muchos. Yo voy a tener que un conjunto de palabras se va a traducir en otro conjunto de palabras. En definitiva lo que pasa es que peque\u00f1as frases se traducen como otras peque\u00f1as frases, por eso necesito un mapeo de muchos a muchos. Entonces bueno hay algoritmos que agarran estos mapeos que como construimos reci\u00e9n, el mapeo de uno a muchos en las dos direcciones digamos y a partir de eso construyen este mapeo de muchos a muchos. Por ejemplo, el algoritmo de la herramienta quiz\u00e1 m\u00e1s m\u00e1s, lo que hace es decir bueno yo tengo un corpus en ingl\u00e9s y en espa\u00f1ol alineo utilizando los los modelos de IBM digamos voy alineo por un lado de ingl\u00e9s espa\u00f1ol y por otro lado de espa\u00f1ol ingl\u00e9s y ac\u00e1 me quedan dos mapeos de uno a n digamos dos mapeos con funciones y despu\u00e9s lo que hago es intersectar esos dos esas dos alineaciones que me quedaron y unirlas. Cuando las intersecto obtengo lo que se conoce como puntos de alta confianza, los puntos negros son los puntos de alta confianza que son los de la intersecci\u00f3n y los puntos grises son los que est\u00e1n en la uni\u00f3n, o sea los que pertenec\u00edan a algunos de los dos modelos. Entonces la herramienta de lo que hace es decir bueno una vez que yo tengo la intersecci\u00f3n y la uni\u00f3n hago crecer los puntos que est\u00e1n en la intersecci\u00f3n, colonizando otros puntos que est\u00e9n en la uni\u00f3n, hasta que al final termino completando digamos toda la imagen. Este punto que qued\u00f3 solito ah\u00ed ese no ser\u00eda parte de la alineaci\u00f3n al final, s\u00f3lo los que pod\u00e9is llegar movi\u00e9ndote a trav\u00e9s de puntos ya conocidos. Entonces bueno eso es una forma que utiliza se llama el algoritmo de Oginey que partiendo alineaciones unidireccionales digamos me permite construir una alineaci\u00f3n completa muchos a muchos entre las palabras. Bien eso le quer\u00eda mencionar acerca de las alineaciones entre palabras y ahora s\u00ed vamos a ver c\u00f3mo funciona un modelo basado en frases. Un modelo basado en frases tiene cierta semejanza con el modelo anterior que hab\u00edamos visto pero es un poco m\u00e1s expresivo en realidad yo parto de una oraci\u00f3n por ejemplo en alem\u00e1n que dec\u00eda Morgan Fligge y Gnaskana de la Sur Conference. Lo primero que hace el modelo cuando quiere traducir digamos en este caso es decir bueno yo voy a segmentar esa oraci\u00f3n de origen en cierta cantidad de frases. Despu\u00e9s voy a traducir cada una de esas frases usando una tabla de traducci\u00f3n y esta vez no es una tabla de traducci\u00f3n de palabras sino que es una tabla de traducci\u00f3n de frases que me dice para ac\u00e1 frases con que otra frase se corresponde y una vez que yo traduje cada una de esas frases las voy a reordenar de alguna manera buscando que suene lo m\u00e1s natural posible buscando aumentar la fluidez de esa oraci\u00f3n. Entonces como que la historia de generaci\u00f3n es un poco m\u00e1s simple que la otra no ten\u00eda que ir sorteando cosas simplemente digo separo mi oraci\u00f3n en segmentos que les voy a llamar frases los traduzco y los reordeno. Esa segmentaci\u00f3n en frases no tiene porque tener un significado ling\u00fc\u00edstico yo no voy a separarlas en grupo nominal, grupo verbal, grupo profesional, etc\u00e9tera. No tengo por qu\u00e9 o sea capaz que yo segmento las frases y justo me queda un grupo preposicional capaz que no. Lo \u00fanico que tiene que pasar es que estos segmentos que yo construyo tienen que estar en mi tabla de traducci\u00f3n de frases alcanza con eso como para que yo pueda utilizarlos en mi traducci\u00f3n pero no tienen por qu\u00e9 tener una motivaci\u00f3n ling\u00fc\u00edstica. Bueno entonces un modelo basado en frases tiene estos componentes parecido al anterior porque de vuelta yo lo que quiero hacer es encontrar la probabilidad de pdf dado e digamos sigo teniendo la misma ecuaci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica estad\u00edstica la quiero resolver necesito pdf dado e y pdf solo que ahora el pdf dado e lo voy a calcular de una manera distinta voy a decir que para calcular esto tengo un modelo de traducci\u00f3n de frases y un modelo de reordenamiento un modelo de una gran tabla de frases que me dice cada frase con qu\u00e9 probabilidad la traduzco en otra y despu\u00e9s una forma de decir c\u00f3mo reordeno esas frases para tener mejores oraciones y bueno y como siempre voy a tener otro componente que es el que mide la la fluidez que es el modelo del lenguaje porque los modelos de frases funcionan mejor que los modelos basados en palabras porque la frase ya tiene cierto contexto la frases en realidad son como peque\u00f1os grupos de palabras que yo puedo traducir uno uno en el otro entonces cosas como dar la mano dar una bofetada a tomar el pelo etc\u00e9tera todas esas cosas como expresiones son mucho m\u00e1s f\u00e1ciles de traducir si en realidad yo ya s\u00e9 que esta expresi\u00f3n que son tres cuatro palabras la puedo traducir en esta otra expresi\u00f3n que son tres cuatro palabras es como m\u00e1s expresivo entonces se puede aprender m\u00e1s cosas y bueno obviamente cuanto m\u00e1s cuanto m\u00e1s datos tenga cuanto m\u00e1s largo sea el cuerpo que yo tengo yo puedo aprender frases m\u00e1s largas mejores probabilidades y mejores frases bueno ac\u00e1 hay un ejemplo de c\u00f3mo ser\u00eda una tabla de traducci\u00f3n de frases o sea es parecido a la tabla de traducci\u00f3n de palabras o es lo que ac\u00e1 tengo de en borschlag o sea si yo busco la fila asociada en borschlag o sea encontrar\u00eda todas estas traducciones de prop\u00f3sal con 62 por ciento de probabilidad posesivo prop\u00f3sal con 10 por ciento a prop\u00f3sal con 3 por ciento etc\u00e9tera o sea como ven se traducen frases en frases bueno y c\u00f3mo hago para aprender una tabla de traducci\u00f3n de frases yo parto de esta alineaci\u00f3n de palabras digamos esta alineaci\u00f3n completa que ya no es una funci\u00f3n sino que es digamos una alineaci\u00f3n de muchos a muchos y voy a tratar de encontrar todos los todas las frases todos los pares de frases que son consistentes con la alineaci\u00f3n a qu\u00e9 me refiero con que son consistentes ac\u00e1 hay ejemplos yo quiero decir que mariano y mar\u00eda did not son es son un par de frases que son consistentes con esta alineaci\u00f3n en cambio mariano y mar\u00eda did como es que miro esto lo que pasa es que cuando yo tengo mariano y mar\u00eda did la palabra no est\u00e1 alineada con did not y el did not digamos el no no pertenece hasta alineaci\u00f3n que yo estoy tratando de decir entonces digo que es no consistente lo mismo pasa con si yo tato alinear mariano daba y mar\u00eda did not lo que pasa ah\u00ed es que daba no est\u00e1 digamos los puntos de alineaci\u00f3n de daba no est\u00e1n dentro de este cuadrante que estoy tratando de buscar entonces en definitiva digo que no es consistente las alineaciones consistentes correctas son las que consideran todos los puntos dentro de ese cuadrante entonces mariano est\u00e1 asociado con mar\u00eda did not y es as\u00ed es consistente as\u00ed que como aprendo frases consistentes empiezo por las alineaciones digamos empiezo por la alineaci\u00f3n de palabra despu\u00e9s busco de una palabra y digo bueno me quedo con todas esas traducciones de palabras y las pongo en mi tabla de frases y despu\u00e9s voy tomando de a dos y me quedo con todas esas otras frases y las voy agregando mi tabla de frases despu\u00e9s me puedo avanzar en uno y tomar de a tres tomar de a cuatro y llegar a tomar incluso toda la oraci\u00f3n como frases entonces a partir de estas oraciones que ten\u00edan no s\u00e9 este 1 2 3 4 5 6 7 8 9 palabras yo termino aprendiendo como 17 frases digamos cada vez m\u00e1s grandes y bueno hoy voy sacando esto de todo el corpus y calculando mi tabla de probabilidades de qu\u00e9 manera calculo esas probabilidades yo lo que puedo hacer es como siempre ver cu\u00e1ntas veces aparece en el corpus y contar o si no si yo ten\u00eda construido el modelo anterior el modelo de la tabla de traducciones de palabra a palabra en realidad lo que puedo hacer es aprovechar ese modelo de traducci\u00f3n de palabra a palabra y decir bueno me armo una traducci\u00f3n entre un par de frases bas\u00e1ndome en las traduciones palabra a palabra son como dos formas distintas de construirlo y a veces hasta complementarias bien eso fue el modelo de frases los modelos de frases son los m\u00e1s usados hoy en d\u00eda en realidad en lo que es la traducci\u00f3n autom\u00e1tica son los que han dado mejores resultados y bueno y nos faltaba una cosa para terminar el toda la imagen de lo que es la traducci\u00f3n autom\u00e1tica estad\u00edstica que es la decodificaci\u00f3n entonces damos un resumen de lo que ten\u00edamos hasta ahora hasta ahora yo part\u00ed de yo quer\u00eda resolver la cocci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica estad\u00edstica y yo ten\u00eda un corpus paralelo que ten\u00eda texto en el idioma origen y el idioma destino y a partir de ciento an\u00e1lisis estad\u00edstico yo me constru\u00ed un modelo de traducci\u00f3n que es lo que vimos en esta clase adem\u00e1s yo ten\u00eda cierto cierta cantidad de texto en el idioma destino y a partir de cierto an\u00e1lisis estad\u00edstico me constru\u00ed un modelo de lenguaje que me dice que tan fluido es una oraci\u00f3n en el lenguaje destino entonces ahora lo que me falta recuerden que yo lo que ten\u00eda que hacer era iterar sobre todas las oraciones del lenguaje destino y pasarlas a trav\u00e9s del modelo de traducci\u00f3n y del modelo de lenguaje para que me d\u00e9 la probabilidad de esa oraci\u00f3n bueno lo que me falta es el algoritmo de codificaci\u00f3n que en vez de probar con todas las oraciones del lenguaje destino me va a decir unas cuantas oraciones para probar capa que me dice 150 oraciones para probar sobre las cuales utilizar el modelo de traducci\u00f3n y el modelo de lenguaje entonces esto es como un diagrama de de m\u00f3dulos en los cuales el algoritmo de codificaci\u00f3n utiliza los dos m\u00f3dulos tanto el de traducci\u00f3n como el de lenguaje bueno c\u00f3mo funciona el algoritmo de codificaci\u00f3n el que vamos a ver es un algoritmo de codificaci\u00f3n de tipo beam search y bueno funciona de la siguiente manera yo tengo la oraci\u00f3n mar\u00eda no dio una bofetada a la bruja verde y la quiero traducir al ingl\u00e9s y tengo una tabla de traducci\u00f3n de frases entonces mi oraci\u00f3n mar\u00eda no dio una bofetada a la bruja verde yo busco en la tabla de frases cu\u00e1les de esas de digamos cu\u00e1les segmentos cu\u00e1les subsegmentos de esa oraci\u00f3n yo puedo encontrar en la tabla de traducci\u00f3n de frases entonces voy a encontrar por ejemplo que mar\u00eda lo puedo traducir como mary no lo busco en la tabla y lo puedo traducir como not como did not o como no dio lo puedo traducir como git pero adem\u00e1s no dio esa frase entera yo lo busco en la tabla y me parece que la puedo traducir como did not give dio una bofetada toda esa frase lo puedo traducir como slap una bofetada lo puedo decir como a slap y bueno y otras cosas bruja lo puedo decir como witch verde como green pero adem\u00e1s en alg\u00fan lado de la tabla tengo que bruja verde lo puedo traducir como green witch y as\u00ed digamos yo puedo encontrar tengo diferentes maneras de segmentar la oraci\u00f3n y adem\u00e1s para cada uno de esos segmentos puedo encontrar distintas formas de traducirlo en el lenguaje destino con mi tabla de frases entonces el algoritmo de codificaci\u00f3n funciona de la siguiente manera empezamos teniendo en cada paso del algoritmo vamos a tener un conjunto de hip\u00f3tesis de traducci\u00f3n se llega a ver ah\u00ed lo que dice ahi ojo m\u00e1s o menos bien ac\u00e1 quedaron mal los cuadraditos bueno en cada uno de los pasos yo voy a tener un conjunto de hip\u00f3tesis de traducci\u00f3n al principio del algoritmo voy a empezar con lo con una hip\u00f3tesis vac\u00eda como se le est\u00e1 hip\u00f3tesis dice que lo importante de leer es la parte de la f que tiene un mont\u00f3n de guiones significa que no hay ninguna palabra del espa\u00f1ol cubierta esas son todas las 9 creo 9 palabras en espa\u00f1ol ninguna est\u00e1 cubierta y esta hip\u00f3tesis tiene probabilidad 1 entonces en cada paso del algoritmo lo que voy a hacer es elegir un par de frases tal que una es traducci\u00f3n de la otra y voy a crear una hip\u00f3tesis nueva a partir de una que ya tengo entonces en este paso lo que hice fue decir el hijo el par de frases mar\u00eda mary y ah\u00ed me creo una nueva hip\u00f3tesis que cubre la primera palabra por eso parece una serie con este caso elige la frase en ingl\u00e9s mary y ahora tiene una probabilidad de 0.564 ese n\u00famero de esa probabilidad va a servir para guiar un poco en el algoritmo pero vamos a ver despu\u00e9s c\u00f3mo es que se calcula por ahora qu\u00e9dense solamente con el n\u00famero bien pero entonces yo ten\u00eda otra opci\u00f3n en realidad yo pod\u00eda haber elegido empezar en vez de traducir mar\u00eda por mary pod\u00eda haber elegido empezar por traducir bruja por witch y ah\u00ed me crear\u00eda otra hip\u00f3tesis de traducci\u00f3n donde cubro la pen\u00faltima de las de las palabras en espa\u00f1ol agarro la palabra witch delijo la palabra witch y tiene una probabilidad de 0.182 entonces en cada paso del algoritmo lo que hace es elegir una hip\u00f3tesis que tiene elegir un par de frases y expandir as\u00ed que lo siguiente que puedo hacer es elegir la frase did not expandirla a partir de la hip\u00f3tesis que ten\u00eda con mary y bueno eso me cubre ahora dos palabras en espa\u00f1ol y me tiene me me dio otra probabilidad y despu\u00e9s sigo avanzando y sigo avanzando hasta que lleg\u00f3 a cubrir en alg\u00fan momento si yo sigo avanzando y sigo agregando hip\u00f3tesis en alg\u00fan momento voy a llegar a cubrir todas las palabras del idioma espa\u00f1ol todas las palabras de la oraci\u00f3n en idioma entonces hay una vez que yo cubr\u00ed todas las palabras digo bueno esto es una hip\u00f3tesis completa y esto lo devuelvo como una potencial candidata digamos una oraci\u00f3n candidata a traducci\u00f3n pero claro a medida que yo fui avanzando una cosa que pas\u00f3 es que fui dejando hip\u00f3tesis colgadas y esas hip\u00f3tesis podr\u00edan tener otras traducciones posibles yo ac\u00e1 lo que devol\u00ed era una posible traducci\u00f3n pero a medida que yo ten\u00eda las otras hip\u00f3tesis si yo hubiera seguido por las otras hip\u00f3tesis hubiera podido devolver otras cosas entonces yo necesito hacer un backtracking para poder devolver todas las posibilidades poder volver a ver las hip\u00f3tesis a revisitar las hip\u00f3tesis que hab\u00eda dejado colgadas y volver a explorar los otros caminos entonces necesitar\u00eda hacer un backtracking para recorrerlas todas y si hago un backtracking lo que va a pasar es que voy a va a ocurrir una explosi\u00f3n de exponencial del espacio de b\u00fasqueda porque en realidad todas las las posibilidades que se abren son exponenciales y ah\u00ed esto como que se vuelve bastante lento entonces yo quer\u00eda un decodificador para volver este problema un problema tratable en vez de agarrar las infinitas oraciones del idioma me quedo con algunas que sean m\u00e1s probables con este algoritmo de codificaci\u00f3n logr\u00e9 reducir de infinito a algo finito pero a\u00fan as\u00ed es demasiado lento porque hay una explosi\u00f3n combinaci\u00f3n combinatoria digamos de la hip\u00f3tesis y me queda una cantidad exponencial de hip\u00f3tesis entonces como es tan grande este problema digamos como la cantidad de hip\u00f3tesis exponencial y este es un problema NP completo entonces se utilizan t\u00e9cnicas para reducir el espacio de b\u00fasqueda y hay como dos tipos de t\u00e9cnicas algunas son con riesgo y otras son sin riesgo las t\u00e9cnicas sin riesgo lo que quiere decir es que si yo aplica una t\u00e9cnica de reducci\u00f3n de hip\u00f3tesis sin riesgo la soluci\u00f3n ideal que yo ten\u00eda dentro de mi b\u00fasqueda no la voy a perder utilizando una t\u00e9cnica sin riesgo en cambio en la con riesgo si yo podr\u00eda llegar a perder la soluci\u00f3n \u00f3ptima bien entonces la t\u00e9cnica sin riesgo que conocemos es la de recombinaci\u00f3n de hip\u00f3tesis que dice que si yo tengo dos hip\u00f3tesis voy avanzando por dos caminos dentro del algoritmo y llevo a dos hip\u00f3tesis iguales por lo menos dos hip\u00f3tesis que cubren las mismas palabras entonces me puedo quedar con la que tiene mayor probabilidad de las dos y descartar la otra porque porque a medida que yo voy a seguir avanzando en el algoritmo lo que va a pasar es que van a bajar las probabilidades digamos yo eligiendo m\u00e1s palabras y eligiendo m\u00e1s frases me va a bajar la probabilidad y nunca me va a pasar que la una de las hip\u00f3tesis que ten\u00eda menos probabilidad vaya a subir en realidad siempre va a tener menos entonces en definitiva yo puedo con seguridad descartar la que tiene menos probabilidad bueno esa es recombinaci\u00f3n de hip\u00f3tesis pero ni siquiera con eso alcanza digamos para reducir el espacio de b\u00fasqueda lo suficiente a\u00fan queda much\u00edsimas hip\u00f3tesis entonces suele utilizar t\u00e9cnicas de podado con riesgo la t\u00e9cnica del histograma la t\u00e9cnica del umbral el histograma significa que a cada paso digamos en cada paso del algoritmo yo me quedo con los n las n hip\u00f3tesis de traducci\u00f3n m\u00e1s probables y descarto las otras y la t\u00e9cnica con un umbral dice que a cada paso del algoritmo me qued\u00f3 con la hip\u00f3tesis de mayor probabilidad y las que est\u00e9n a una distancia alfa m\u00e1ximo de esa cu\u00e1l es el riesgo de las las t\u00e9cnicas de podado que si la mejor traducci\u00f3n y la traducci\u00f3n \u00f3ptima ten\u00eda algunas frases muy poco probables al principio entonces probablemente yo descarte esa soluci\u00f3n en los primeros pasos y no llegan a encontrar la soluci\u00f3n \u00f3ptima digamos la perd\u00ed por el hecho de haber podado sin embargo bueno tiene como como ventaja que en realidad reduce much\u00edsimo el espacio de b\u00fasqueda y vuelve vuelve este problema un problema tratable bueno y ahora s\u00ed qu\u00e9 significaba esa probabilidad que estaba viendo en cada una de las hip\u00f3tesis o sea el podado necesita tener las mejores hip\u00f3tesis y bueno y para la recombinaci\u00f3n tambi\u00e9n necesito saber la probabilidad de la hip\u00f3tesis bueno la forma de calcular la probabilidad de hip\u00f3tesis se divide en dos digamos tengo lo que encontr\u00e9 hasta el momento la hip\u00f3tesis lleva cubierta cierta cantidad de palabras entonces para esa cantidad de palabras que ya llevo cubiertas utilizo los tres modelos el modelo de traducci\u00f3n el modelo de reordenamiento del modelo de lenguaje utilizo los tres modelos para calcular la probabilidad de la frase hasta el momento pero para lo que me falta traducir yo no puedo utilizar todo porque no tengo toda la informaci\u00f3n de traducci\u00f3n entonces lo que hago es utilizar solamente el modelo de traducci\u00f3n y el modelo de lenguaje descarto el modelo de reordenamiento y bueno entonces hago calcular una probabilidad que es una parte con todos los tres modelos y otra parte sin el modelo de reordenamiento bien este algoritmo que acabamos de escribir que hace esta b\u00fasqueda bas\u00e1ndose en hip\u00f3tesis que utiliza recombinaci\u00f3n hipodado hip\u00f3tesis y bueno el calcula de las probabilidades de esta manera se conoce como algoritmo b\u00fasqueda asterisco es un algoritmo de bin search que se usa much\u00edsimo en lo que es traducci\u00f3n autom\u00e1tica estad\u00edstica por ejemplo el sistema mouses ac\u00e1 tenemos este ejemplos de herramientas open source o gratuitas que sirven para construcci\u00f3n de de traductores autom\u00e1ticos el sistema mouses es un sistema open source para desarrollar este tipo de traductores autom\u00e1ticos estad\u00edsticos e implementa este algoritmo de codificaci\u00f3n de b\u00fasqueda asterisco y bueno lo que tiene el sistema mouses de bueno es que en realidad lo que hace adem\u00e1s de implementar el decodificadores utiliza a los otros sistemas y los integra de alguna manera entonces integra este otro sistema el irs tlm que es una herramienta para crear modelos de lenguaje basados en n en n gramas y el otro sistema se quiza m\u00e1s m\u00e1s que lo hab\u00edamos mencionado hoy que es el sistema que me permite alinear corpus de oraciones en los distintos idiomas llegando los modelos del 1 al 5 de traducci\u00f3n de bm bueno entonces estas tres herramientas sirven si uno quiere construir un traductor autom\u00e1tico estad\u00edstico entre cualquier par de idiomas puede utilizar estas tres herramientas y teniendo un corpus paralelo y un corpus monoling\u00fce puede construirse un traductor pero bueno adem\u00e1s otra cosa que mencionamos en la clase pasada pero este eran los sistemas basados en reglas los sistemas basados en reglas han ca\u00eddo un poco este digamos no tienen tanta popularidad como antes sin embargo algunos se siguen usando y el sistema apertium es un sistema opensource para construir sistema de traducci\u00f3n basados en reglas que tiene como un mont\u00f3n de pares de lenguajes y bueno ya anda relativamente bien digamos entonces se sigue desarrollando hasta hoy entonces es una alternativa opensource que est\u00e1 basada en reglas en vez de estar basado en estad\u00edsticas y bueno esta es un resumen de lo que vimos as\u00ed que dejamos por ac\u00e1", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 23.92, "text": " Una vez que eleg\u00ed con el paso 1, eleg\u00ed cu\u00e1ntas palabras en espa\u00f1ol le voy a usar, en el paso", "tokens": [50364, 15491, 5715, 631, 14459, 870, 416, 806, 29212, 502, 11, 14459, 870, 44256, 296, 35240, 465, 31177, 476, 7552, 257, 14745, 11, 465, 806, 29212, 51560], "temperature": 0.0, "avg_logprob": -0.24159696367051867, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.20441624522209167}, {"id": 1, "seek": 0, "start": 23.92, "end": 27.96, "text": " 2 lo que voy a elegir es una alineaci\u00f3n, una funci\u00f3n de alineaci\u00f3n que me dice cada", "tokens": [51560, 568, 450, 631, 7552, 257, 14459, 347, 785, 2002, 419, 533, 3482, 11, 2002, 43735, 368, 419, 533, 3482, 631, 385, 10313, 8411, 51762], "temperature": 0.0, "avg_logprob": -0.24159696367051867, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.20441624522209167}, {"id": 2, "seek": 2796, "start": 27.96, "end": 31.6, "text": " palabra con cu\u00e1l se va a corresponder, cada palabra del lado del espa\u00f1ol con qu\u00e9 palabra", "tokens": [50364, 31702, 416, 44318, 369, 2773, 257, 6805, 260, 11, 8411, 31702, 1103, 11631, 1103, 31177, 416, 8057, 31702, 50546], "temperature": 0.0, "avg_logprob": -0.17439826590115906, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.22477588057518005}, {"id": 3, "seek": 2796, "start": 31.6, "end": 37.92, "text": " en ingl\u00e9s se va a corresponder. Este modelo asume de manera muy naiv que todas las alineaciones", "tokens": [50546, 465, 49766, 369, 2773, 257, 6805, 260, 13, 16105, 27825, 382, 2540, 368, 13913, 5323, 1667, 592, 631, 10906, 2439, 419, 533, 9188, 50862], "temperature": 0.0, "avg_logprob": -0.17439826590115906, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.22477588057518005}, {"id": 4, "seek": 2796, "start": 37.92, "end": 45.08, "text": " que yo puedo tener son equiprobables. O sea, asume que yo voy a tener un conjunto de alineaciones", "tokens": [50862, 631, 5290, 21612, 11640, 1872, 5037, 16614, 2965, 13, 422, 4158, 11, 382, 2540, 631, 5290, 7552, 257, 11640, 517, 37776, 368, 419, 533, 9188, 51220], "temperature": 0.0, "avg_logprob": -0.17439826590115906, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.22477588057518005}, {"id": 5, "seek": 2796, "start": 45.08, "end": 50.16, "text": " posibles y todas van a tener la misma probabilidad. Bien, entonces, la probabilidad de elegir", "tokens": [51220, 1366, 14428, 288, 10906, 3161, 257, 11640, 635, 24946, 31959, 4580, 13, 16956, 11, 13003, 11, 635, 31959, 4580, 368, 14459, 347, 51474], "temperature": 0.0, "avg_logprob": -0.17439826590115906, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.22477588057518005}, {"id": 6, "seek": 2796, "start": 50.16, "end": 55.44, "text": " una alineaci\u00f3n en particular, si yo tengo un mont\u00f3n de alineaciones, digamos, la probabilidad", "tokens": [51474, 2002, 419, 533, 3482, 465, 1729, 11, 1511, 5290, 13989, 517, 45259, 368, 419, 533, 9188, 11, 36430, 11, 635, 31959, 4580, 51738], "temperature": 0.0, "avg_logprob": -0.17439826590115906, "compression_ratio": 1.8449612403100775, "no_speech_prob": 0.22477588057518005}, {"id": 7, "seek": 5544, "start": 55.44, "end": 60.559999999999995, "text": " de elegir una alineaci\u00f3n en particular, va a ser uno sobre la cantidad de alineaciones", "tokens": [50364, 368, 14459, 347, 2002, 419, 533, 3482, 465, 1729, 11, 2773, 257, 816, 8526, 5473, 635, 33757, 368, 419, 533, 9188, 50620], "temperature": 0.0, "avg_logprob": -0.17914815102854081, "compression_ratio": 1.7524752475247525, "no_speech_prob": 0.28601083159446716}, {"id": 8, "seek": 5544, "start": 60.559999999999995, "end": 66.08, "text": " que tengo, porque en realidad todas van a ser equiprobables. Bien, entonces, \u00bfcu\u00e1ntas", "tokens": [50620, 631, 13989, 11, 4021, 465, 25635, 10906, 3161, 257, 816, 5037, 16614, 2965, 13, 16956, 11, 13003, 11, 3841, 12032, 27525, 296, 50896], "temperature": 0.0, "avg_logprob": -0.17914815102854081, "compression_ratio": 1.7524752475247525, "no_speech_prob": 0.28601083159446716}, {"id": 9, "seek": 5544, "start": 66.08, "end": 70.03999999999999, "text": " alineaciones puedo tener entre dos oraciones, una oraci\u00f3n en ingl\u00e9s que tiene largo I y", "tokens": [50896, 419, 533, 9188, 21612, 11640, 3962, 4491, 420, 9188, 11, 2002, 420, 3482, 465, 49766, 631, 7066, 31245, 286, 288, 51094], "temperature": 0.0, "avg_logprob": -0.17914815102854081, "compression_ratio": 1.7524752475247525, "no_speech_prob": 0.28601083159446716}, {"id": 10, "seek": 5544, "start": 70.03999999999999, "end": 73.72, "text": " una oraci\u00f3n en espa\u00f1ol que tiene largo J? \u00bfC\u00f3mo puedo calcular cu\u00e1ntas alineaciones", "tokens": [51094, 2002, 420, 3482, 465, 31177, 631, 7066, 31245, 508, 30, 3841, 28342, 21612, 2104, 17792, 44256, 296, 419, 533, 9188, 51278], "temperature": 0.0, "avg_logprob": -0.17914815102854081, "compression_ratio": 1.7524752475247525, "no_speech_prob": 0.28601083159446716}, {"id": 11, "seek": 7372, "start": 73.72, "end": 90.48, "text": " existen? M\u00e1s o menos, s\u00ed, casi la J. Recuerden que del lado de ingl\u00e9s yo ten\u00eda ciertas", "tokens": [50364, 2514, 268, 30, 376, 2490, 277, 8902, 11, 8600, 11, 22567, 635, 508, 13, 9647, 5486, 1556, 631, 1103, 11631, 368, 49766, 5290, 23718, 49252, 296, 51202], "temperature": 0.0, "avg_logprob": -0.2788524280894886, "compression_ratio": 1.3049645390070923, "no_speech_prob": 0.6713851094245911}, {"id": 12, "seek": 7372, "start": 90.48, "end": 101.52, "text": " palabras, en ingl\u00e9s ten\u00eda la palabra E1, E2 hasta, sub\u00ed, y en espa\u00f1ol ten\u00eda las palabras", "tokens": [51202, 35240, 11, 465, 49766, 23718, 635, 31702, 462, 16, 11, 462, 17, 10764, 11, 1422, 870, 11, 288, 465, 31177, 23718, 2439, 35240, 51754], "temperature": 0.0, "avg_logprob": -0.2788524280894886, "compression_ratio": 1.3049645390070923, "no_speech_prob": 0.6713851094245911}, {"id": 13, "seek": 10152, "start": 101.52, "end": 112.11999999999999, "text": " F1, F2 hasta F subj. Entonces, yo pod\u00eda trazar l\u00edneas para alinear, pero adem\u00e1s en ingl\u00e9s", "tokens": [50364, 479, 16, 11, 479, 17, 10764, 479, 1422, 73, 13, 15097, 11, 5290, 45588, 944, 26236, 16118, 716, 296, 1690, 419, 533, 289, 11, 4768, 21251, 465, 49766, 50894], "temperature": 0.0, "avg_logprob": -0.1741241417308845, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.17995160818099976}, {"id": 14, "seek": 10152, "start": 112.11999999999999, "end": 117.03999999999999, "text": " yo siempre he considerado que ten\u00eda un token nul, entonces todas las palabras que no estaban", "tokens": [50894, 5290, 12758, 415, 1949, 1573, 631, 23718, 517, 14862, 297, 425, 11, 13003, 10906, 2439, 35240, 631, 572, 36713, 51140], "temperature": 0.0, "avg_logprob": -0.1741241417308845, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.17995160818099976}, {"id": 15, "seek": 10152, "start": 117.03999999999999, "end": 123.0, "text": " alineadas del lado del espa\u00f1ol iban a parar ah\u00ed. As\u00ed que en ingl\u00e9s en realidad no tengo", "tokens": [51140, 419, 533, 6872, 1103, 11631, 1103, 31177, 741, 5144, 257, 37193, 12571, 13, 17419, 631, 465, 49766, 465, 25635, 572, 13989, 51438], "temperature": 0.0, "avg_logprob": -0.1741241417308845, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.17995160818099976}, {"id": 16, "seek": 10152, "start": 123.0, "end": 127.47999999999999, "text": " i posibilidades, tengo una m\u00e1s, tengo i m\u00e1s uno. Entonces, \u00bfcu\u00e1ntas formas tengo yo", "tokens": [51438, 741, 1366, 11607, 10284, 11, 13989, 2002, 3573, 11, 13989, 741, 3573, 8526, 13, 15097, 11, 3841, 12032, 27525, 296, 33463, 13989, 5290, 51662], "temperature": 0.0, "avg_logprob": -0.1741241417308845, "compression_ratio": 1.5683760683760684, "no_speech_prob": 0.17995160818099976}, {"id": 17, "seek": 12748, "start": 127.48, "end": 134.04, "text": " de mapear estas J posibilidades en espa\u00f1ol con las I de ingl\u00e9s? Exacto, i m\u00e1s uno a", "tokens": [50364, 368, 463, 494, 289, 13897, 508, 1366, 11607, 10284, 465, 31177, 416, 2439, 286, 368, 49766, 30, 7199, 78, 11, 741, 3573, 8526, 257, 50692], "temperature": 0.0, "avg_logprob": -0.2954691427725333, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.5306950211524963}, {"id": 18, "seek": 12748, "start": 134.04, "end": 137.92000000000002, "text": " la J, porque yo tengo i m\u00e1s un opciones para la primera y m\u00e1s una opciones para la segunda,", "tokens": [50692, 635, 508, 11, 4021, 5290, 13989, 741, 3573, 517, 999, 23469, 1690, 635, 17382, 288, 3573, 2002, 999, 23469, 1690, 635, 21978, 11, 50886], "temperature": 0.0, "avg_logprob": -0.2954691427725333, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.5306950211524963}, {"id": 19, "seek": 12748, "start": 137.92000000000002, "end": 146.92000000000002, "text": " etc\u00e9tera, hasta que llevo al final. As\u00ed que son i m\u00e1s uno a las J alineaciones posibles.", "tokens": [50886, 5183, 526, 23833, 11, 10764, 631, 12038, 3080, 419, 2572, 13, 17419, 631, 1872, 741, 3573, 8526, 257, 2439, 508, 419, 533, 9188, 1366, 14428, 13, 51336], "temperature": 0.0, "avg_logprob": -0.2954691427725333, "compression_ratio": 1.511111111111111, "no_speech_prob": 0.5306950211524963}, {"id": 20, "seek": 14692, "start": 146.92, "end": 163.27999999999997, "text": " Ojo, el nul es como una pisadita que hago yo para alinear cosas que no tienen un correspondiente.", "tokens": [50364, 422, 5134, 11, 806, 297, 425, 785, 2617, 2002, 26584, 345, 2786, 631, 38721, 5290, 1690, 419, 533, 289, 12218, 631, 572, 12536, 517, 6805, 8413, 13, 51182], "temperature": 0.0, "avg_logprob": -0.30436030301180755, "compression_ratio": 1.3006993006993006, "no_speech_prob": 0.14947465062141418}, {"id": 21, "seek": 14692, "start": 163.27999999999997, "end": 171.39999999999998, "text": " O sea, yo ten\u00eda una palabra en espa\u00f1ol que\u2026 Varias de las Fes pueden estar alineadas", "tokens": [51182, 422, 4158, 11, 5290, 23718, 2002, 31702, 465, 31177, 631, 1260, 691, 35027, 368, 2439, 479, 279, 14714, 8755, 419, 533, 6872, 51588], "temperature": 0.0, "avg_logprob": -0.30436030301180755, "compression_ratio": 1.3006993006993006, "no_speech_prob": 0.14947465062141418}, {"id": 22, "seek": 17140, "start": 171.4, "end": 179.08, "text": " de nul, no importa en qu\u00e9 orden est\u00e1n. Eso. Bien, entonces eran i m\u00e1s uno a las J posibles", "tokens": [50364, 368, 297, 425, 11, 572, 33218, 465, 8057, 28615, 10368, 13, 27795, 13, 16956, 11, 13003, 32762, 741, 3573, 8526, 257, 2439, 508, 1366, 14428, 50748], "temperature": 0.0, "avg_logprob": -0.16094574125686495, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.5286911129951477}, {"id": 23, "seek": 17140, "start": 179.08, "end": 189.32, "text": " alineaciones, por lo tanto, la probabilidad de elegir una alineaci\u00f3n A dada la oraci\u00f3n", "tokens": [50748, 419, 533, 9188, 11, 1515, 450, 10331, 11, 635, 31959, 4580, 368, 14459, 347, 2002, 419, 533, 3482, 316, 274, 1538, 635, 420, 3482, 51260], "temperature": 0.0, "avg_logprob": -0.16094574125686495, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.5286911129951477}, {"id": 24, "seek": 17140, "start": 189.32, "end": 194.64000000000001, "text": " en ingl\u00e9s, la probabilidad de elegir una alineaci\u00f3n cualquiera dada la oraci\u00f3n en", "tokens": [51260, 465, 49766, 11, 635, 31959, 4580, 368, 14459, 347, 2002, 419, 533, 3482, 10911, 35134, 274, 1538, 635, 420, 3482, 465, 51526], "temperature": 0.0, "avg_logprob": -0.16094574125686495, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.5286911129951477}, {"id": 25, "seek": 17140, "start": 194.64000000000001, "end": 201.04000000000002, "text": " ingl\u00e9s va a ser el producto de la probabilidad de haber sorteado un valor J primero, que era", "tokens": [51526, 49766, 2773, 257, 816, 806, 47583, 368, 635, 31959, 4580, 368, 15811, 25559, 1573, 517, 15367, 508, 21289, 11, 631, 4249, 51846], "temperature": 0.0, "avg_logprob": -0.16094574125686495, "compression_ratio": 1.7960199004975124, "no_speech_prob": 0.5286911129951477}, {"id": 26, "seek": 20104, "start": 201.04, "end": 206.28, "text": " epsilon por la probabilidad de elegir una alineaci\u00f3n cualquiera para ese J, que es", "tokens": [50364, 17889, 1515, 635, 31959, 4580, 368, 14459, 347, 2002, 419, 533, 3482, 10911, 35134, 1690, 10167, 508, 11, 631, 785, 50626], "temperature": 0.0, "avg_logprob": -0.16390221913655598, "compression_ratio": 1.7414965986394557, "no_speech_prob": 0.038596123456954956}, {"id": 27, "seek": 20104, "start": 206.28, "end": 213.56, "text": " uno sobre i m\u00e1s uno a la J. Bien, entonces esto lo resumimos como epsilon sobre i m\u00e1s", "tokens": [50626, 8526, 5473, 741, 3573, 8526, 257, 635, 508, 13, 16956, 11, 13003, 7433, 450, 725, 449, 8372, 2617, 17889, 5473, 741, 3573, 50990], "temperature": 0.0, "avg_logprob": -0.16390221913655598, "compression_ratio": 1.7414965986394557, "no_speech_prob": 0.038596123456954956}, {"id": 28, "seek": 20104, "start": 213.56, "end": 225.16, "text": " uno a la J. Epsilon sobre i m\u00e1s uno a la J es la probabilidad de, dada una oraci\u00f3n", "tokens": [50990, 8526, 257, 635, 508, 13, 462, 16592, 5473, 741, 3573, 8526, 257, 635, 508, 785, 635, 31959, 4580, 368, 11, 274, 1538, 2002, 420, 3482, 51570], "temperature": 0.0, "avg_logprob": -0.16390221913655598, "compression_ratio": 1.7414965986394557, "no_speech_prob": 0.038596123456954956}, {"id": 29, "seek": 22516, "start": 225.16, "end": 231.56, "text": " en ingl\u00e9s, elegir cierta alineaci\u00f3n que yo voy a utilizar. Bien, ese fue el segundo", "tokens": [50364, 465, 49766, 11, 14459, 347, 39769, 1328, 419, 533, 3482, 631, 5290, 7552, 257, 24060, 13, 16956, 11, 10167, 9248, 806, 17954, 50684], "temperature": 0.0, "avg_logprob": -0.13608225186665854, "compression_ratio": 1.883817427385892, "no_speech_prob": 0.6291707158088684}, {"id": 30, "seek": 22516, "start": 231.56, "end": 238.16, "text": " paso. El tercer paso es una vez que ya tengo la alineaci\u00f3n, voy mirando cada palabra del", "tokens": [50684, 29212, 13, 2699, 38103, 29212, 785, 2002, 5715, 631, 2478, 13989, 635, 419, 533, 3482, 11, 7552, 3149, 1806, 8411, 31702, 1103, 51014], "temperature": 0.0, "avg_logprob": -0.13608225186665854, "compression_ratio": 1.883817427385892, "no_speech_prob": 0.6291707158088684}, {"id": 31, "seek": 22516, "start": 238.16, "end": 243.88, "text": " lado en ingl\u00e9s y le voy poniendo una palabra correspondiente del lado espa\u00f1ol. Para ac\u00e1", "tokens": [51014, 11631, 465, 49766, 288, 476, 7552, 9224, 7304, 2002, 31702, 6805, 8413, 1103, 11631, 31177, 13, 11107, 23496, 51300], "temperature": 0.0, "avg_logprob": -0.13608225186665854, "compression_ratio": 1.883817427385892, "no_speech_prob": 0.6291707158088684}, {"id": 32, "seek": 22516, "start": 243.88, "end": 247.96, "text": " voy a asumir que yo tengo una tabla de traducci\u00f3n, una tabla de traducci\u00f3n que me dice que tiene", "tokens": [51300, 7552, 257, 382, 449, 347, 631, 5290, 13989, 2002, 4421, 875, 368, 2479, 1311, 5687, 11, 2002, 4421, 875, 368, 2479, 1311, 5687, 631, 385, 10313, 631, 7066, 51504], "temperature": 0.0, "avg_logprob": -0.13608225186665854, "compression_ratio": 1.883817427385892, "no_speech_prob": 0.6291707158088684}, {"id": 33, "seek": 22516, "start": 247.96, "end": 251.35999999999999, "text": " de un lado todas las palabras en espa\u00f1ol y del otro lado todas las palabras en ingl\u00e9s,", "tokens": [51504, 368, 517, 11631, 10906, 2439, 35240, 465, 31177, 288, 1103, 11921, 11631, 10906, 2439, 35240, 465, 49766, 11, 51674], "temperature": 0.0, "avg_logprob": -0.13608225186665854, "compression_ratio": 1.883817427385892, "no_speech_prob": 0.6291707158088684}, {"id": 34, "seek": 25136, "start": 251.36, "end": 258.2, "text": " entonces mi tabla va a tener una forma como, por ejemplo, hacer una tabla as\u00ed que de un", "tokens": [50364, 13003, 2752, 4421, 875, 2773, 257, 11640, 2002, 8366, 2617, 11, 1515, 13358, 11, 6720, 2002, 4421, 875, 8582, 631, 368, 517, 50706], "temperature": 0.0, "avg_logprob": -0.1596191290653113, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.223649263381958}, {"id": 35, "seek": 25136, "start": 258.2, "end": 267.36, "text": " lado va a decir las palabras en espa\u00f1ol como banco, perro, gato y m\u00e1s cosas y del otro", "tokens": [50706, 11631, 2773, 257, 10235, 2439, 35240, 465, 31177, 2617, 45498, 11, 680, 340, 11, 290, 2513, 288, 3573, 12218, 288, 1103, 11921, 51164], "temperature": 0.0, "avg_logprob": -0.1596191290653113, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.223649263381958}, {"id": 36, "seek": 25136, "start": 267.36, "end": 275.0, "text": " lado va a tener las correspondientes en ingl\u00e9s como bank, bench, cut, tree y m\u00e1s cosas.", "tokens": [51164, 11631, 2773, 257, 11640, 2439, 6805, 20135, 465, 49766, 2617, 3765, 11, 10638, 11, 1723, 11, 4230, 288, 3573, 12218, 13, 51546], "temperature": 0.0, "avg_logprob": -0.1596191290653113, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.223649263381958}, {"id": 37, "seek": 25136, "start": 275.0, "end": 279.40000000000003, "text": " Y entonces esta tabla va a decir la probabilidad de traducir una cosa en la gota. Entonces banco", "tokens": [51546, 398, 13003, 5283, 4421, 875, 2773, 257, 10235, 635, 31959, 4580, 368, 2479, 1311, 347, 2002, 10163, 465, 635, 658, 64, 13, 15097, 45498, 51766], "temperature": 0.0, "avg_logprob": -0.1596191290653113, "compression_ratio": 1.6621004566210045, "no_speech_prob": 0.223649263381958}, {"id": 38, "seek": 27940, "start": 279.4, "end": 284.4, "text": " probablemente tenga cierta probabilidad para bank y cierta probabilidad para bench, 0.4", "tokens": [50364, 21759, 4082, 36031, 39769, 1328, 31959, 4580, 1690, 3765, 288, 39769, 1328, 31959, 4580, 1690, 10638, 11, 1958, 13, 19, 50614], "temperature": 0.0, "avg_logprob": -0.21616358023423415, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.06461798399686813}, {"id": 39, "seek": 27940, "start": 284.4, "end": 293.0, "text": " y 0.6, 0.06 puse. Y para cut no va a tener ninguna probabilidad y para tree tampoco", "tokens": [50614, 288, 1958, 13, 21, 11, 1958, 13, 12791, 280, 438, 13, 398, 1690, 1723, 572, 2773, 257, 11640, 36073, 31959, 4580, 288, 1690, 4230, 36838, 51044], "temperature": 0.0, "avg_logprob": -0.21616358023423415, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.06461798399686813}, {"id": 40, "seek": 27940, "start": 293.0, "end": 298.71999999999997, "text": " y despu\u00e9s perro no va a tener nada de esto, pero s\u00ed despu\u00e9s y cut va a ser, no s\u00e9 0.8", "tokens": [51044, 288, 15283, 680, 340, 572, 2773, 257, 11640, 8096, 368, 7433, 11, 4768, 8600, 15283, 288, 1723, 2773, 257, 816, 11, 572, 7910, 1958, 13, 23, 51330], "temperature": 0.0, "avg_logprob": -0.21616358023423415, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.06461798399686813}, {"id": 41, "seek": 27940, "start": 298.71999999999997, "end": 303.0, "text": " en este caso, etc\u00e9tera. Voy a tener una tabla bastante grande que tiene todas las posibilidades", "tokens": [51330, 465, 4065, 9666, 11, 5183, 526, 23833, 13, 25563, 257, 11640, 2002, 4421, 875, 14651, 8883, 631, 7066, 10906, 2439, 1366, 11607, 10284, 51544], "temperature": 0.0, "avg_logprob": -0.21616358023423415, "compression_ratio": 1.6886792452830188, "no_speech_prob": 0.06461798399686813}, {"id": 42, "seek": 30300, "start": 303.0, "end": 312.04, "text": " de traducir una palabra como otra. Entonces si yo tengo esa tabla, lo que puedo decir", "tokens": [50364, 368, 2479, 1311, 347, 2002, 31702, 2617, 13623, 13, 15097, 1511, 5290, 13989, 11342, 4421, 875, 11, 450, 631, 21612, 10235, 50816], "temperature": 0.0, "avg_logprob": -0.11665012910194003, "compression_ratio": 1.8368421052631578, "no_speech_prob": 0.5000380277633667}, {"id": 43, "seek": 30300, "start": 312.04, "end": 320.44, "text": " es que la forma de calcular la probabilidad de esa oraci\u00f3n final que yo traduje va a", "tokens": [50816, 785, 631, 635, 8366, 368, 2104, 17792, 635, 31959, 4580, 368, 11342, 420, 3482, 2572, 631, 5290, 2479, 13008, 2773, 257, 51236], "temperature": 0.0, "avg_logprob": -0.11665012910194003, "compression_ratio": 1.8368421052631578, "no_speech_prob": 0.5000380277633667}, {"id": 44, "seek": 30300, "start": 320.44, "end": 323.52, "text": " depender de cu\u00e1les son las palabras que yo elija, va a depender de cu\u00e1les son las palabras", "tokens": [51236, 1367, 3216, 368, 2702, 842, 904, 1872, 2439, 35240, 631, 5290, 806, 20642, 11, 2773, 257, 1367, 3216, 368, 2702, 842, 904, 1872, 2439, 35240, 51390], "temperature": 0.0, "avg_logprob": -0.11665012910194003, "compression_ratio": 1.8368421052631578, "no_speech_prob": 0.5000380277633667}, {"id": 45, "seek": 30300, "start": 323.52, "end": 331.08, "text": " que yo haya puesto dentro de mi oraci\u00f3n para traducir. Entonces esa tabla que est\u00e1", "tokens": [51390, 631, 5290, 24693, 35136, 10856, 368, 2752, 420, 3482, 1690, 2479, 1311, 347, 13, 15097, 11342, 4421, 875, 631, 3192, 51768], "temperature": 0.0, "avg_logprob": -0.11665012910194003, "compression_ratio": 1.8368421052631578, "no_speech_prob": 0.5000380277633667}, {"id": 46, "seek": 33108, "start": 331.08, "end": 338.32, "text": " ah\u00ed definida, le llamamos ac\u00e1 en la slide, aparece como t de f sub x sub y dice que la", "tokens": [50364, 12571, 1561, 2887, 11, 476, 16848, 2151, 23496, 465, 635, 4137, 11, 37863, 2617, 256, 368, 283, 1422, 2031, 1422, 288, 10313, 631, 635, 50726], "temperature": 0.0, "avg_logprob": -0.24805048624674478, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.22966501116752625}, {"id": 47, "seek": 33108, "start": 338.32, "end": 348.28, "text": " probabilidad de traducir la palabra es sub y como f sub x. Entonces saca de una cosa importante.", "tokens": [50726, 31959, 4580, 368, 2479, 1311, 347, 635, 31702, 785, 1422, 288, 2617, 283, 1422, 2031, 13, 15097, 4899, 64, 368, 2002, 10163, 9416, 13, 51224], "temperature": 0.0, "avg_logprob": -0.24805048624674478, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.22966501116752625}, {"id": 48, "seek": 33108, "start": 348.28, "end": 357.12, "text": " Si tenemos la oraci\u00f3n en ingl\u00e9s, la oraci\u00f3n en ingl\u00e9s recuerdan que ten\u00eda las palabras", "tokens": [51224, 4909, 9914, 635, 420, 3482, 465, 49766, 11, 635, 420, 3482, 465, 49766, 39092, 10312, 631, 23718, 2439, 35240, 51666], "temperature": 0.0, "avg_logprob": -0.24805048624674478, "compression_ratio": 1.547486033519553, "no_speech_prob": 0.22966501116752625}, {"id": 49, "seek": 35712, "start": 357.12, "end": 363.56, "text": " f sub 1, f sub 2 hasta f sub n, la oraci\u00f3n en espa\u00f1ol ten\u00eda las palabras f sub 1, f", "tokens": [50364, 283, 1422, 502, 11, 283, 1422, 568, 10764, 283, 1422, 297, 11, 635, 420, 3482, 465, 31177, 23718, 2439, 35240, 283, 1422, 502, 11, 283, 50686], "temperature": 0.0, "avg_logprob": -0.22685474004500952, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.021724971011281013}, {"id": 50, "seek": 35712, "start": 363.56, "end": 370.08, "text": " sub 1, f sub 2 hasta f sub j. Y yo ten\u00eda en el medio una funci\u00f3n de alineaci\u00f3n que", "tokens": [50686, 1422, 502, 11, 283, 1422, 568, 10764, 283, 1422, 361, 13, 398, 5290, 23718, 465, 806, 22123, 2002, 43735, 368, 419, 533, 3482, 631, 51012], "temperature": 0.0, "avg_logprob": -0.22685474004500952, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.021724971011281013}, {"id": 51, "seek": 35712, "start": 370.08, "end": 379.6, "text": " me dec\u00eda qu\u00e9 palabra se correspond\u00eda con cu\u00e1l. Entonces no era f sub n ni f sub j.", "tokens": [51012, 385, 37599, 8057, 31702, 369, 6805, 2686, 416, 44318, 13, 15097, 572, 4249, 283, 1422, 297, 3867, 283, 1422, 361, 13, 51488], "temperature": 0.0, "avg_logprob": -0.22685474004500952, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.021724971011281013}, {"id": 52, "seek": 37960, "start": 379.6, "end": 392.20000000000005, "text": " Era f sub i y f sub j grande. Esto era f sub i y esto era f sub j grande. Entonces si yo", "tokens": [50364, 23071, 283, 1422, 741, 288, 283, 1422, 361, 8883, 13, 20880, 4249, 283, 1422, 741, 288, 7433, 4249, 283, 1422, 361, 8883, 13, 15097, 1511, 5290, 50994], "temperature": 0.0, "avg_logprob": -0.17377759297688802, "compression_ratio": 1.8380281690140845, "no_speech_prob": 0.08940964192152023}, {"id": 53, "seek": 37960, "start": 392.20000000000005, "end": 399.56, "text": " tengo una palabra cualquiera dentro de la oraci\u00f3n en espa\u00f1ol, tengo un f sub j chica", "tokens": [50994, 13989, 2002, 31702, 10911, 35134, 10856, 368, 635, 420, 3482, 465, 31177, 11, 13989, 517, 283, 1422, 361, 417, 2262, 51362], "temperature": 0.0, "avg_logprob": -0.17377759297688802, "compression_ratio": 1.8380281690140845, "no_speech_prob": 0.08940964192152023}, {"id": 54, "seek": 37960, "start": 399.56, "end": 405.16, "text": " dentro de la oraci\u00f3n en espa\u00f1ol, esto se va a corresponder con alg\u00fan f sub i chica", "tokens": [51362, 10856, 368, 635, 420, 3482, 465, 31177, 11, 7433, 369, 2773, 257, 6805, 260, 416, 26300, 283, 1422, 741, 417, 2262, 51642], "temperature": 0.0, "avg_logprob": -0.17377759297688802, "compression_ratio": 1.8380281690140845, "no_speech_prob": 0.08940964192152023}, {"id": 55, "seek": 40516, "start": 405.16, "end": 409.72, "text": " en la oraci\u00f3n en ingl\u00e9s, digamos. Yo s\u00e9 que esto se cumple por la funci\u00f3n de alineaci\u00f3n", "tokens": [50364, 465, 635, 420, 3482, 465, 49766, 11, 36430, 13, 7616, 7910, 631, 7433, 369, 12713, 781, 1515, 635, 43735, 368, 419, 533, 3482, 50592], "temperature": 0.0, "avg_logprob": -0.20650008686801843, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.22880931198596954}, {"id": 56, "seek": 40516, "start": 409.72, "end": 412.56, "text": " porque agarra y mapea todas las palabras que est\u00e1n en espa\u00f1ol con algo que est\u00e1 del", "tokens": [50592, 4021, 623, 289, 424, 288, 463, 494, 64, 10906, 2439, 35240, 631, 10368, 465, 31177, 416, 8655, 631, 3192, 1103, 50734], "temperature": 0.0, "avg_logprob": -0.20650008686801843, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.22880931198596954}, {"id": 57, "seek": 40516, "start": 412.56, "end": 417.16, "text": " lado del ingl\u00e9s, potencialmente con el token vac\u00edo nul.", "tokens": [50734, 11631, 1103, 49766, 11, 48265, 4082, 416, 806, 14862, 2842, 20492, 297, 425, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20650008686801843, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.22880931198596954}, {"id": 58, "seek": 40516, "start": 417.16, "end": 422.24, "text": " Bien, entonces tengo una palabra del lado del espa\u00f1ol que es f sub j y una palabra del", "tokens": [50964, 16956, 11, 13003, 13989, 2002, 31702, 1103, 11631, 1103, 31177, 631, 785, 283, 1422, 361, 288, 2002, 31702, 1103, 51218], "temperature": 0.0, "avg_logprob": -0.20650008686801843, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.22880931198596954}, {"id": 59, "seek": 40516, "start": 422.24, "end": 427.52000000000004, "text": " lado del ingl\u00e9s que es f sub i. \u00bfCu\u00e1l es la relaci\u00f3n entre f sub j y f sub i? \u00bfC\u00f3mo", "tokens": [51218, 11631, 1103, 49766, 631, 785, 283, 1422, 741, 13, 3841, 35222, 11447, 785, 635, 37247, 3962, 283, 1422, 361, 288, 283, 1422, 741, 30, 3841, 28342, 51482], "temperature": 0.0, "avg_logprob": -0.20650008686801843, "compression_ratio": 1.7965367965367964, "no_speech_prob": 0.22880931198596954}, {"id": 60, "seek": 42752, "start": 427.52, "end": 440.0, "text": " es la relaci\u00f3n entre s\u00ed, digamos? Yo puedo decir que el i es igual a algo de j. \u00bfDe alguna", "tokens": [50364, 785, 635, 37247, 3962, 8600, 11, 36430, 30, 7616, 21612, 10235, 631, 806, 741, 785, 10953, 257, 8655, 368, 361, 13, 3841, 11089, 20651, 50988], "temperature": 0.0, "avg_logprob": -0.17849671697042074, "compression_ratio": 1.7329192546583851, "no_speech_prob": 0.37718257308006287}, {"id": 61, "seek": 42752, "start": 440.0, "end": 447.91999999999996, "text": " manera? La funci\u00f3n de alineaci\u00f3n, ah\u00ed est\u00e1. O sea, el i es igual a la funci\u00f3n de alineaci\u00f3n", "tokens": [50988, 13913, 30, 2369, 43735, 368, 419, 533, 3482, 11, 12571, 3192, 13, 422, 4158, 11, 806, 741, 785, 10953, 257, 635, 43735, 368, 419, 533, 3482, 51384], "temperature": 0.0, "avg_logprob": -0.17849671697042074, "compression_ratio": 1.7329192546583851, "no_speech_prob": 0.37718257308006287}, {"id": 62, "seek": 42752, "start": 447.91999999999996, "end": 455.08, "text": " aplicada j. Como la i, el \u00edndice de este de ac\u00e1 es igual a la funci\u00f3n de alineaci\u00f3n", "tokens": [51384, 18221, 1538, 361, 13, 11913, 635, 741, 11, 806, 18645, 273, 573, 368, 4065, 368, 23496, 785, 10953, 257, 635, 43735, 368, 419, 533, 3482, 51742], "temperature": 0.0, "avg_logprob": -0.17849671697042074, "compression_ratio": 1.7329192546583851, "no_speech_prob": 0.37718257308006287}, {"id": 63, "seek": 45508, "start": 455.08, "end": 463.15999999999997, "text": " aplicada j. Entonces, yo puedo decir que la palabra f sub i es igual a la palabra e sub", "tokens": [50364, 18221, 1538, 361, 13, 15097, 11, 5290, 21612, 10235, 631, 635, 31702, 283, 1422, 741, 785, 10953, 257, 635, 31702, 308, 1422, 50768], "temperature": 0.0, "avg_logprob": -0.19168481639787263, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.17862237989902496}, {"id": 64, "seek": 45508, "start": 463.15999999999997, "end": 468.59999999999997, "text": " a sub j. As\u00ed que puedo decir que, en realidad, los que est\u00e1n alineados son la palabra f", "tokens": [50768, 257, 1422, 361, 13, 17419, 631, 21612, 10235, 631, 11, 465, 25635, 11, 1750, 631, 10368, 419, 533, 4181, 1872, 635, 31702, 283, 51040], "temperature": 0.0, "avg_logprob": -0.19168481639787263, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.17862237989902496}, {"id": 65, "seek": 45508, "start": 468.59999999999997, "end": 475.24, "text": " sub j est\u00e1 alineada con la palabra e sub a sub j. Y ah\u00ed me saqu\u00e9 el i de encima, digamos.", "tokens": [51040, 1422, 361, 3192, 419, 533, 1538, 416, 635, 31702, 308, 1422, 257, 1422, 361, 13, 398, 12571, 385, 601, 16412, 806, 741, 368, 40265, 11, 36430, 13, 51372], "temperature": 0.0, "avg_logprob": -0.19168481639787263, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.17862237989902496}, {"id": 66, "seek": 45508, "start": 475.24, "end": 481.84, "text": " Simplemente, iterando sobre las palabras, iterando sobre la j puedo establecer la correspondencia", "tokens": [51372, 21532, 4082, 11, 17138, 1806, 5473, 2439, 35240, 11, 17138, 1806, 5473, 635, 361, 21612, 37444, 1776, 635, 6805, 10974, 51702], "temperature": 0.0, "avg_logprob": -0.19168481639787263, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.17862237989902496}, {"id": 67, "seek": 48184, "start": 481.96, "end": 490.96, "text": " entre las dos palabras. Y eso es un poco lo que dice ac\u00e1 para terminar de armar lo que", "tokens": [50370, 3962, 2439, 4491, 35240, 13, 398, 7287, 785, 517, 10639, 450, 631, 10313, 23496, 1690, 36246, 368, 3726, 289, 450, 631, 50820], "temperature": 0.0, "avg_logprob": -0.13482859956116236, "compression_ratio": 1.9396551724137931, "no_speech_prob": 0.5177727341651917}, {"id": 68, "seek": 48184, "start": 490.96, "end": 493.79999999999995, "text": " es el modelo de traducci\u00f3n. Para terminar de armar el modelo de traducci\u00f3n dicen que", "tokens": [50820, 785, 806, 27825, 368, 2479, 1311, 5687, 13, 11107, 36246, 368, 3726, 289, 806, 27825, 368, 2479, 1311, 5687, 33816, 631, 50962], "temperature": 0.0, "avg_logprob": -0.13482859956116236, "compression_ratio": 1.9396551724137931, "no_speech_prob": 0.5177727341651917}, {"id": 69, "seek": 48184, "start": 493.79999999999995, "end": 497.71999999999997, "text": " en el tercer paso yo voy a elegir cu\u00e1les son las palabras. Entonces, lo que voy a hacer", "tokens": [50962, 465, 806, 38103, 29212, 5290, 7552, 257, 14459, 347, 2702, 842, 904, 1872, 2439, 35240, 13, 15097, 11, 450, 631, 7552, 257, 6720, 51158], "temperature": 0.0, "avg_logprob": -0.13482859956116236, "compression_ratio": 1.9396551724137931, "no_speech_prob": 0.5177727341651917}, {"id": 70, "seek": 48184, "start": 497.71999999999997, "end": 505.12, "text": " es iterar sobre todas las palabras y haciendo el producto de todas las probabilidades. O", "tokens": [51158, 785, 17138, 289, 5473, 10906, 2439, 35240, 288, 20509, 806, 47583, 368, 10906, 2439, 31959, 10284, 13, 422, 51528], "temperature": 0.0, "avg_logprob": -0.13482859956116236, "compression_ratio": 1.9396551724137931, "no_speech_prob": 0.5177727341651917}, {"id": 71, "seek": 48184, "start": 505.12, "end": 511.52, "text": " sea, el producto de dado que yo ten\u00eda la palabra f sub j, perd\u00f3n, dado que yo ten\u00eda la palabra", "tokens": [51528, 4158, 11, 806, 47583, 368, 29568, 631, 5290, 23718, 635, 31702, 283, 1422, 361, 11, 12611, 1801, 11, 29568, 631, 5290, 23718, 635, 31702, 51848], "temperature": 0.0, "avg_logprob": -0.13482859956116236, "compression_ratio": 1.9396551724137931, "no_speech_prob": 0.5177727341651917}, {"id": 72, "seek": 51152, "start": 511.59999999999997, "end": 517.52, "text": " e sub a sub j en ingl\u00e9s, entonces elegir la palabra f sub j en espa\u00f1ol. Eso hago una", "tokens": [50368, 308, 1422, 257, 1422, 361, 465, 49766, 11, 13003, 14459, 347, 635, 31702, 283, 1422, 361, 465, 31177, 13, 27795, 38721, 2002, 50664], "temperature": 0.0, "avg_logprob": -0.15093955187730387, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.004039858002215624}, {"id": 73, "seek": 51152, "start": 517.52, "end": 525.76, "text": " productoria con todos los valores de las distintas palabras. Bien, entonces ah\u00ed llegu\u00e9", "tokens": [50664, 1674, 8172, 416, 6321, 1750, 38790, 368, 2439, 31489, 296, 35240, 13, 16956, 11, 13003, 12571, 11234, 42423, 51076], "temperature": 0.0, "avg_logprob": -0.15093955187730387, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.004039858002215624}, {"id": 74, "seek": 51152, "start": 525.76, "end": 535.04, "text": " a el \u00faltimo de los valores que quer\u00eda calcular, que es la probabilidad de f dado que conozco", "tokens": [51076, 257, 806, 21013, 368, 1750, 38790, 631, 37869, 2104, 17792, 11, 631, 785, 635, 31959, 4580, 368, 283, 29568, 631, 416, 15151, 1291, 51540], "temperature": 0.0, "avg_logprob": -0.15093955187730387, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.004039858002215624}, {"id": 75, "seek": 53504, "start": 535.04, "end": 545.56, "text": " ah\u00ed es igual a la productoria con j igual a 1 hasta j grande de el valor de la tabla", "tokens": [50364, 12571, 785, 10953, 257, 635, 1674, 8172, 416, 361, 10953, 257, 502, 10764, 361, 8883, 368, 806, 15367, 368, 635, 4421, 875, 50890], "temperature": 0.0, "avg_logprob": -0.23733562335633396, "compression_ratio": 1.3671875, "no_speech_prob": 0.004785146098583937}, {"id": 76, "seek": 53504, "start": 545.56, "end": 559.7199999999999, "text": " de traducci\u00f3n, que es t sub f sub j, t de f sub j e sub a sub j. Bueno, ah\u00ed tengo c\u00f3mo", "tokens": [50890, 368, 2479, 1311, 5687, 11, 631, 785, 256, 1422, 283, 1422, 361, 11, 256, 368, 283, 1422, 361, 308, 1422, 257, 1422, 361, 13, 16046, 11, 12571, 13989, 12826, 51598], "temperature": 0.0, "avg_logprob": -0.23733562335633396, "compression_ratio": 1.3671875, "no_speech_prob": 0.004785146098583937}, {"id": 77, "seek": 55972, "start": 559.72, "end": 565.32, "text": " en cada paso fui calculando cosas, este se correspond\u00eda al paso uno del modelo, paso", "tokens": [50364, 465, 8411, 29212, 27863, 4322, 1806, 12218, 11, 4065, 369, 6805, 2686, 419, 29212, 8526, 1103, 27825, 11, 29212, 50644], "temperature": 0.0, "avg_logprob": -0.18288601885785113, "compression_ratio": 1.877659574468085, "no_speech_prob": 0.05388989672064781}, {"id": 78, "seek": 55972, "start": 565.32, "end": 569.0400000000001, "text": " uno, este se corresponde con el paso dos del modelo, en realidad este ya tiene el paso", "tokens": [50644, 8526, 11, 4065, 369, 6805, 68, 416, 806, 29212, 4491, 1103, 27825, 11, 465, 25635, 4065, 2478, 7066, 806, 29212, 50830], "temperature": 0.0, "avg_logprob": -0.18288601885785113, "compression_ratio": 1.877659574468085, "no_speech_prob": 0.05388989672064781}, {"id": 79, "seek": 55972, "start": 569.0400000000001, "end": 572.36, "text": " uno y el paso dos juntos porque ya tengo el \u00e9xil\u00f3n ac\u00e1 y este se corresponde con el", "tokens": [50830, 8526, 288, 806, 29212, 4491, 33868, 4021, 2478, 13989, 806, 1136, 87, 388, 1801, 23496, 288, 4065, 369, 6805, 68, 416, 806, 50996], "temperature": 0.0, "avg_logprob": -0.18288601885785113, "compression_ratio": 1.877659574468085, "no_speech_prob": 0.05388989672064781}, {"id": 80, "seek": 55972, "start": 572.36, "end": 581.84, "text": " paso tres del modelo. El paso tres de la historia de generaci\u00f3n. Mi objetivo con todos estos", "tokens": [50996, 29212, 15890, 1103, 27825, 13, 2699, 29212, 15890, 368, 635, 18385, 368, 1337, 3482, 13, 10204, 29809, 416, 6321, 12585, 51470], "temperature": 0.0, "avg_logprob": -0.18288601885785113, "compression_ratio": 1.877659574468085, "no_speech_prob": 0.05388989672064781}, {"id": 81, "seek": 58184, "start": 581.84, "end": 592.2800000000001, "text": " valores que est\u00e1n ac\u00e1 es calcular p de f dado e. \u00bfQu\u00e9 par\u00e1metros introduce? \u00bfQu\u00e9", "tokens": [50364, 38790, 631, 10368, 23496, 785, 2104, 17792, 280, 368, 283, 29568, 308, 13, 3841, 15137, 971, 842, 29570, 5366, 30, 3841, 15137, 50886], "temperature": 0.0, "avg_logprob": -0.16181635087536228, "compression_ratio": 1.6793893129770991, "no_speech_prob": 0.3986620306968689}, {"id": 82, "seek": 58184, "start": 592.2800000000001, "end": 596.44, "text": " par\u00e1metros fueron surgiendo a medida que yo iba iterando sobre estos pasos? Bueno, en", "tokens": [50886, 971, 842, 29570, 28739, 19560, 7304, 257, 32984, 631, 5290, 33423, 17138, 1806, 5473, 12585, 1736, 329, 30, 16046, 11, 465, 51094], "temperature": 0.0, "avg_logprob": -0.16181635087536228, "compression_ratio": 1.6793893129770991, "no_speech_prob": 0.3986620306968689}, {"id": 83, "seek": 58184, "start": 596.44, "end": 600.32, "text": " primer lugar el \u00e9xil\u00f3n aquel que est\u00e1bamos viendo, este es un valor que yo tendr\u00eda que", "tokens": [51094, 12595, 11467, 806, 1136, 87, 388, 1801, 2373, 338, 631, 3192, 65, 2151, 34506, 11, 4065, 785, 517, 15367, 631, 5290, 3928, 37183, 631, 51288], "temperature": 0.0, "avg_logprob": -0.16181635087536228, "compression_ratio": 1.6793893129770991, "no_speech_prob": 0.3986620306968689}, {"id": 84, "seek": 58184, "start": 600.32, "end": 606.44, "text": " estimar a partir de mirar en los corpus como son los largos de las oraciones relativos", "tokens": [51288, 8017, 289, 257, 13906, 368, 3149, 289, 465, 1750, 1181, 31624, 2617, 1872, 1750, 11034, 329, 368, 2439, 420, 9188, 21960, 329, 51594], "temperature": 0.0, "avg_logprob": -0.16181635087536228, "compression_ratio": 1.6793893129770991, "no_speech_prob": 0.3986620306968689}, {"id": 85, "seek": 58184, "start": 606.44, "end": 609.9200000000001, "text": " y el otro par\u00e1metro importante es aquella tabla de all\u00e1, aquella tabla de traducci\u00f3n", "tokens": [51594, 288, 806, 11921, 971, 842, 45400, 9416, 785, 2373, 9885, 4421, 875, 368, 30642, 11, 2373, 9885, 4421, 875, 368, 2479, 1311, 5687, 51768], "temperature": 0.0, "avg_logprob": -0.16181635087536228, "compression_ratio": 1.6793893129770991, "no_speech_prob": 0.3986620306968689}, {"id": 86, "seek": 60992, "start": 609.92, "end": 613.8, "text": " es que me dice banco, con qu\u00e9 probabilidad lo puedo traducir como bank, con qu\u00e9 probabilidad", "tokens": [50364, 785, 631, 385, 10313, 45498, 11, 416, 8057, 31959, 4580, 450, 21612, 2479, 1311, 347, 2617, 3765, 11, 416, 8057, 31959, 4580, 50558], "temperature": 0.0, "avg_logprob": -0.1946911667332505, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.17707796394824982}, {"id": 87, "seek": 60992, "start": 613.8, "end": 618.76, "text": " lo puedo traducir como bench, etc\u00e9tera, etc\u00e9tera. Esa tabla en realidad es un par\u00e1metro del", "tokens": [50558, 450, 21612, 2479, 1311, 347, 2617, 10638, 11, 5183, 526, 23833, 11, 5183, 526, 23833, 13, 2313, 64, 4421, 875, 465, 25635, 785, 517, 971, 842, 45400, 1103, 50806], "temperature": 0.0, "avg_logprob": -0.1946911667332505, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.17707796394824982}, {"id": 88, "seek": 60992, "start": 618.76, "end": 622.04, "text": " modelo, es un par\u00e1metro del sistema que si yo lo tuviera me alcanzar\u00eda con eso para", "tokens": [50806, 27825, 11, 785, 517, 971, 842, 45400, 1103, 13245, 631, 1511, 5290, 450, 38177, 10609, 385, 50200, 21178, 416, 7287, 1690, 50970], "temperature": 0.0, "avg_logprob": -0.1946911667332505, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.17707796394824982}, {"id": 89, "seek": 60992, "start": 622.04, "end": 627.36, "text": " poder construirme este modelo y calcular la probabilidad de cualquier par de oraciones.", "tokens": [50970, 8152, 38445, 1398, 4065, 27825, 288, 2104, 17792, 635, 31959, 4580, 368, 21004, 971, 368, 420, 9188, 13, 51236], "temperature": 0.0, "avg_logprob": -0.1946911667332505, "compression_ratio": 1.8059701492537314, "no_speech_prob": 0.17707796394824982}, {"id": 90, "seek": 62736, "start": 627.36, "end": 639.2, "text": " Bien, y entonces antes de continuar vamos a terminar de armar cu\u00e1l es la imagen de esto,", "tokens": [50364, 16956, 11, 288, 13003, 11014, 368, 29980, 5295, 257, 36246, 368, 3726, 289, 44318, 785, 635, 40652, 368, 7433, 11, 50956], "temperature": 0.0, "avg_logprob": -0.14256982242359834, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.01472059078514576}, {"id": 91, "seek": 62736, "start": 639.2, "end": 644.24, "text": " que es decir yo en realidad lo que quer\u00eda calcular era p de f dado e, que eso va a ser", "tokens": [50956, 631, 785, 10235, 5290, 465, 25635, 450, 631, 37869, 2104, 17792, 4249, 280, 368, 283, 29568, 308, 11, 631, 7287, 2773, 257, 816, 51208], "temperature": 0.0, "avg_logprob": -0.14256982242359834, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.01472059078514576}, {"id": 92, "seek": 62736, "start": 644.24, "end": 649.64, "text": " mi modelo de traducci\u00f3n y de hecho va a ser el encargado de medir la adecuaci\u00f3n de una", "tokens": [51208, 2752, 27825, 368, 2479, 1311, 5687, 288, 368, 13064, 2773, 257, 816, 806, 2058, 289, 30135, 368, 1205, 347, 635, 614, 3045, 84, 3482, 368, 2002, 51478], "temperature": 0.0, "avg_logprob": -0.14256982242359834, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.01472059078514576}, {"id": 93, "seek": 62736, "start": 649.64, "end": 655.5600000000001, "text": " frase. P de f dado e lo puedo calcular con esta descomposici\u00f3n de pasos que hice ac\u00e1", "tokens": [51478, 38406, 13, 430, 368, 283, 29568, 308, 450, 21612, 2104, 17792, 416, 5283, 730, 21541, 329, 15534, 368, 1736, 329, 631, 50026, 23496, 51774], "temperature": 0.0, "avg_logprob": -0.14256982242359834, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.01472059078514576}, {"id": 94, "seek": 65556, "start": 655.56, "end": 679.0, "text": " en realidad porque lo hago de la siguiente manera. Yo quiero calcular p de f dado e y", "tokens": [50364, 465, 25635, 4021, 450, 38721, 368, 635, 25666, 13913, 13, 7616, 16811, 2104, 17792, 280, 368, 283, 29568, 308, 288, 51536], "temperature": 0.0, "avg_logprob": -0.24511377016703287, "compression_ratio": 1.103896103896104, "no_speech_prob": 0.10231040418148041}, {"id": 95, "seek": 67900, "start": 679.0, "end": 683.76, "text": " entonces voy a mirar lo que dice ac\u00e1, p de f dado e es igual a la sumatoriana de p de f", "tokens": [50364, 13003, 7552, 257, 3149, 289, 450, 631, 10313, 23496, 11, 280, 368, 283, 29568, 308, 785, 10953, 257, 635, 2408, 1639, 8497, 368, 280, 368, 283, 50602], "temperature": 0.0, "avg_logprob": -0.19402730957535672, "compression_ratio": 1.9605263157894737, "no_speech_prob": 0.4828905761241913}, {"id": 96, "seek": 67900, "start": 683.76, "end": 690.96, "text": " dado e. \u00bfQu\u00e9 significa eso? Que para traducir entre una oraci\u00f3n en espa\u00f1ol y una oraci\u00f3n", "tokens": [50602, 29568, 308, 13, 3841, 15137, 19957, 7287, 30, 4493, 1690, 2479, 1311, 347, 3962, 2002, 420, 3482, 465, 31177, 288, 2002, 420, 3482, 50962], "temperature": 0.0, "avg_logprob": -0.19402730957535672, "compression_ratio": 1.9605263157894737, "no_speech_prob": 0.4828905761241913}, {"id": 97, "seek": 67900, "start": 690.96, "end": 695.56, "text": " en ingl\u00e9s, o m\u00e1s bien para traducir entre una oraci\u00f3n en ingl\u00e9s y una oraci\u00f3n en", "tokens": [50962, 465, 49766, 11, 277, 3573, 3610, 1690, 2479, 1311, 347, 3962, 2002, 420, 3482, 465, 49766, 288, 2002, 420, 3482, 465, 51192], "temperature": 0.0, "avg_logprob": -0.19402730957535672, "compression_ratio": 1.9605263157894737, "no_speech_prob": 0.4828905761241913}, {"id": 98, "seek": 67900, "start": 695.56, "end": 700.76, "text": " espa\u00f1ol hay muchas formas de alinear las palabras entre el ingl\u00e9s y en espa\u00f1ol y una", "tokens": [51192, 31177, 4842, 16072, 33463, 368, 419, 533, 289, 2439, 35240, 3962, 806, 49766, 288, 465, 31177, 288, 2002, 51452], "temperature": 0.0, "avg_logprob": -0.19402730957535672, "compression_ratio": 1.9605263157894737, "no_speech_prob": 0.4828905761241913}, {"id": 99, "seek": 67900, "start": 700.76, "end": 704.44, "text": " vez que yo eleg\u00ed una forma de alinear hay muchas formas de elegir las palabras que vienen", "tokens": [51452, 5715, 631, 5290, 14459, 870, 2002, 8366, 368, 419, 533, 289, 4842, 16072, 33463, 368, 14459, 347, 2439, 35240, 631, 49298, 51636], "temperature": 0.0, "avg_logprob": -0.19402730957535672, "compression_ratio": 1.9605263157894737, "no_speech_prob": 0.4828905761241913}, {"id": 100, "seek": 70444, "start": 704.44, "end": 709.12, "text": " despu\u00e9s digamos yo miro la tarjeta de traducci\u00f3n y capaz que hay varias maneras de elegir distintas", "tokens": [50364, 15283, 36430, 5290, 2752, 340, 635, 3112, 7108, 64, 368, 2479, 1311, 5687, 288, 35453, 631, 4842, 37496, 587, 6985, 368, 14459, 347, 31489, 296, 50598], "temperature": 0.0, "avg_logprob": -0.18623488488858633, "compression_ratio": 1.9263157894736842, "no_speech_prob": 0.49475032091140747}, {"id": 101, "seek": 70444, "start": 709.12, "end": 714.32, "text": " palabras. Entonces lo que eso significa es que no existe una sola manera de traducir una", "tokens": [50598, 35240, 13, 15097, 450, 631, 7287, 19957, 785, 631, 572, 16304, 2002, 34162, 13913, 368, 2479, 1311, 347, 2002, 50858], "temperature": 0.0, "avg_logprob": -0.18623488488858633, "compression_ratio": 1.9263157894736842, "no_speech_prob": 0.49475032091140747}, {"id": 102, "seek": 70444, "start": 714.32, "end": 717.8000000000001, "text": " oraci\u00f3n en ingl\u00e9s a una oraci\u00f3n en espa\u00f1ol. Yo puedo encontrar varias formas de alinear", "tokens": [50858, 420, 3482, 465, 49766, 257, 2002, 420, 3482, 465, 31177, 13, 7616, 21612, 17525, 37496, 33463, 368, 419, 533, 289, 51032], "temperature": 0.0, "avg_logprob": -0.18623488488858633, "compression_ratio": 1.9263157894736842, "no_speech_prob": 0.49475032091140747}, {"id": 103, "seek": 70444, "start": 717.8000000000001, "end": 721.8800000000001, "text": " las palabras y varias formas de elegir las palabras de manera que muchas alineaciones son", "tokens": [51032, 2439, 35240, 288, 37496, 33463, 368, 14459, 347, 2439, 35240, 368, 13913, 631, 16072, 419, 533, 9188, 1872, 51236], "temperature": 0.0, "avg_logprob": -0.18623488488858633, "compression_ratio": 1.9263157894736842, "no_speech_prob": 0.49475032091140747}, {"id": 104, "seek": 70444, "start": 721.8800000000001, "end": 731.0, "text": " posibles. Entonces para saber cu\u00e1l es la probabilidad de traducir p de f dado e, entonces", "tokens": [51236, 1366, 14428, 13, 15097, 1690, 12489, 44318, 785, 635, 31959, 4580, 368, 2479, 1311, 347, 280, 368, 283, 29568, 308, 11, 13003, 51692], "temperature": 0.0, "avg_logprob": -0.18623488488858633, "compression_ratio": 1.9263157894736842, "no_speech_prob": 0.49475032091140747}, {"id": 105, "seek": 70444, "start": 731.0, "end": 734.32, "text": " yo voy a tener que sumar sobre todas las alineaciones posibles sobre todas las formas", "tokens": [51692, 5290, 7552, 257, 11640, 631, 2408, 289, 5473, 10906, 2439, 419, 533, 9188, 1366, 14428, 5473, 10906, 2439, 33463, 51858], "temperature": 0.0, "avg_logprob": -0.18623488488858633, "compression_ratio": 1.9263157894736842, "no_speech_prob": 0.49475032091140747}, {"id": 106, "seek": 73432, "start": 734.32, "end": 740.08, "text": " de alinear las dos oraciones f y e, voy a tener que iterar sobre eso y para cada una", "tokens": [50364, 368, 419, 533, 289, 2439, 4491, 420, 9188, 283, 288, 308, 11, 7552, 257, 11640, 631, 17138, 289, 5473, 7287, 288, 1690, 8411, 2002, 50652], "temperature": 0.0, "avg_logprob": -0.15870386576481002, "compression_ratio": 2.2844827586206895, "no_speech_prob": 0.04970284923911095}, {"id": 107, "seek": 73432, "start": 740.08, "end": 745.72, "text": " voy a tener que calcular la probabilidad parcial. Entonces digamos yo tengo cinco formas de", "tokens": [50652, 7552, 257, 11640, 631, 2104, 17792, 635, 31959, 4580, 971, 1013, 13, 15097, 36430, 5290, 13989, 21350, 33463, 368, 50934], "temperature": 0.0, "avg_logprob": -0.15870386576481002, "compression_ratio": 2.2844827586206895, "no_speech_prob": 0.04970284923911095}, {"id": 108, "seek": 73432, "start": 745.72, "end": 750.0, "text": " alinear las dos oraciones, cinco es un n\u00famero un poco raro pero digamos tengo n formas de", "tokens": [50934, 419, 533, 289, 2439, 4491, 420, 9188, 11, 21350, 785, 517, 14959, 517, 10639, 367, 9708, 4768, 36430, 13989, 297, 33463, 368, 51148], "temperature": 0.0, "avg_logprob": -0.15870386576481002, "compression_ratio": 2.2844827586206895, "no_speech_prob": 0.04970284923911095}, {"id": 109, "seek": 73432, "start": 750.0, "end": 754.8000000000001, "text": " alinear las dos oraciones, voy a tener que mirar bueno para la primera alineaci\u00f3n cu\u00e1l", "tokens": [51148, 419, 533, 289, 2439, 4491, 420, 9188, 11, 7552, 257, 11640, 631, 3149, 289, 11974, 1690, 635, 17382, 419, 533, 3482, 44318, 51388], "temperature": 0.0, "avg_logprob": -0.15870386576481002, "compression_ratio": 2.2844827586206895, "no_speech_prob": 0.04970284923911095}, {"id": 110, "seek": 73432, "start": 754.8000000000001, "end": 759.9200000000001, "text": " es la probabilidad de encontrar la oraci\u00f3n f para la segunda alineaci\u00f3n cu\u00e1l es la", "tokens": [51388, 785, 635, 31959, 4580, 368, 17525, 635, 420, 3482, 283, 1690, 635, 21978, 419, 533, 3482, 44318, 785, 635, 51644], "temperature": 0.0, "avg_logprob": -0.15870386576481002, "compression_ratio": 2.2844827586206895, "no_speech_prob": 0.04970284923911095}, {"id": 111, "seek": 73432, "start": 759.9200000000001, "end": 762.84, "text": " probabilidad de encontrar la oraci\u00f3n f para la tercera oraci\u00f3n y as\u00ed hasta llegar al", "tokens": [51644, 31959, 4580, 368, 17525, 635, 420, 3482, 283, 1690, 635, 1796, 41034, 420, 3482, 288, 8582, 10764, 24892, 419, 51790], "temperature": 0.0, "avg_logprob": -0.15870386576481002, "compression_ratio": 2.2844827586206895, "no_speech_prob": 0.04970284923911095}, {"id": 112, "seek": 76284, "start": 762.84, "end": 768.84, "text": " final y agarro y sumo todo eso. Eso lo puedo hacer porque las alineaciones son una descomposici\u00f3n", "tokens": [50364, 2572, 288, 623, 289, 340, 288, 2408, 78, 5149, 7287, 13, 27795, 450, 21612, 6720, 4021, 2439, 419, 533, 9188, 1872, 2002, 730, 21541, 329, 15534, 50664], "temperature": 0.0, "avg_logprob": -0.13185829586452907, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.17523984611034393}, {"id": 113, "seek": 76284, "start": 768.84, "end": 772.52, "text": " del espacio de probabilidades. En realidad yo puedo descomponer el espacio de probabilidades", "tokens": [50664, 1103, 33845, 368, 31959, 10284, 13, 2193, 25635, 5290, 21612, 730, 21541, 32949, 806, 33845, 368, 31959, 10284, 50848], "temperature": 0.0, "avg_logprob": -0.13185829586452907, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.17523984611034393}, {"id": 114, "seek": 76284, "start": 772.52, "end": 777.48, "text": " en pedacitos disjuntos y cada alineaci\u00f3n va a ser uno de ellos. As\u00ed que digamos que", "tokens": [50848, 465, 5670, 326, 11343, 717, 73, 2760, 329, 288, 8411, 419, 533, 3482, 2773, 257, 816, 8526, 368, 16353, 13, 17419, 631, 36430, 631, 51096], "temperature": 0.0, "avg_logprob": -0.13185829586452907, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.17523984611034393}, {"id": 115, "seek": 76284, "start": 777.48, "end": 782.0400000000001, "text": " para calcular el modelo de traducci\u00f3n p de f dado e necesito sumar sobre todas las alineaciones", "tokens": [51096, 1690, 2104, 17792, 806, 27825, 368, 2479, 1311, 5687, 280, 368, 283, 29568, 308, 11909, 3528, 2408, 289, 5473, 10906, 2439, 419, 533, 9188, 51324], "temperature": 0.0, "avg_logprob": -0.13185829586452907, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.17523984611034393}, {"id": 116, "seek": 76284, "start": 782.0400000000001, "end": 788.96, "text": " posibles. Ahora lo que me falta es saber c\u00f3mo calculo este valor de ac\u00e1. As\u00ed que lo que", "tokens": [51324, 1366, 14428, 13, 18840, 450, 631, 385, 22111, 785, 12489, 12826, 4322, 78, 4065, 15367, 368, 23496, 13, 17419, 631, 450, 631, 51670], "temperature": 0.0, "avg_logprob": -0.13185829586452907, "compression_ratio": 1.7884615384615385, "no_speech_prob": 0.17523984611034393}, {"id": 117, "seek": 78896, "start": 789.0400000000001, "end": 794.84, "text": " estoy diciendo es que la probabilidad de f dado e es la suma sobre las alineaciones", "tokens": [50368, 15796, 42797, 785, 631, 635, 31959, 4580, 368, 283, 29568, 308, 785, 635, 2408, 64, 5473, 2439, 419, 533, 9188, 50658], "temperature": 0.0, "avg_logprob": -0.1521337843432869, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.0671193078160286}, {"id": 118, "seek": 78896, "start": 794.84, "end": 800.84, "text": " de la probabilidad de f y esa alineaci\u00f3n dado e. Eso es simplemente lo que dice ah\u00ed", "tokens": [50658, 368, 635, 31959, 4580, 368, 283, 288, 11342, 419, 533, 3482, 29568, 308, 13, 27795, 785, 33190, 450, 631, 10313, 12571, 50958], "temperature": 0.0, "avg_logprob": -0.1521337843432869, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.0671193078160286}, {"id": 119, "seek": 78896, "start": 800.84, "end": 805.5600000000001, "text": " en la slide. Lo que me falta calcular entonces es esta parte de ac\u00e1 y esa parte de ac\u00e1 la", "tokens": [50958, 465, 635, 4137, 13, 6130, 631, 385, 22111, 2104, 17792, 13003, 785, 5283, 6975, 368, 23496, 288, 11342, 6975, 368, 23496, 635, 51194], "temperature": 0.0, "avg_logprob": -0.1521337843432869, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.0671193078160286}, {"id": 120, "seek": 78896, "start": 805.5600000000001, "end": 811.8000000000001, "text": " calculo de esta manera. Yo digo que la probabilidad de f dado e es igual, ah\u00ed est\u00e1 m\u00e1s o menos", "tokens": [51194, 4322, 78, 368, 5283, 13913, 13, 7616, 22990, 631, 635, 31959, 4580, 368, 283, 29568, 308, 785, 10953, 11, 12571, 3192, 3573, 277, 8902, 51506], "temperature": 0.0, "avg_logprob": -0.1521337843432869, "compression_ratio": 1.8131313131313131, "no_speech_prob": 0.0671193078160286}, {"id": 121, "seek": 81180, "start": 811.8, "end": 818.8, "text": " el resultado final pero podemos sacar qu\u00e9 es lo que tendr\u00eda que poner de este lado.", "tokens": [50364, 806, 28047, 2572, 4768, 12234, 43823, 8057, 785, 450, 631, 3928, 37183, 631, 19149, 368, 4065, 11631, 13, 50714], "temperature": 0.0, "avg_logprob": -0.4404606385664506, "compression_ratio": 1.2627737226277371, "no_speech_prob": 0.03939587622880936}, {"id": 122, "seek": 81180, "start": 819.4399999999999, "end": 826.4399999999999, "text": " Ahora s\u00ed me acuerdo bien. Ah, ah\u00ed est\u00e1. Por definici\u00f3n de probabilidad condicional.", "tokens": [50746, 18840, 8600, 385, 28113, 3610, 13, 2438, 11, 12571, 3192, 13, 5269, 1561, 15534, 368, 31959, 4580, 2224, 33010, 13, 51096], "temperature": 0.0, "avg_logprob": -0.4404606385664506, "compression_ratio": 1.2627737226277371, "no_speech_prob": 0.03939587622880936}, {"id": 123, "seek": 82644, "start": 826.44, "end": 833.44, "text": " Eso. p de f dado e, le voy a dar varias maneras a hacerlo pero esto se puede definir como", "tokens": [50364, 27795, 13, 280, 368, 283, 29568, 308, 11, 476, 7552, 257, 4072, 37496, 587, 6985, 257, 32039, 4768, 7433, 369, 8919, 1561, 347, 2617, 50714], "temperature": 0.0, "avg_logprob": -0.26263530299348653, "compression_ratio": 1.3037037037037038, "no_speech_prob": 0.002022377448156476}, {"id": 124, "seek": 82644, "start": 843.48, "end": 850.48, "text": " p de f a e sobre p de e. No? Por definici\u00f3n de probabilidad condicional. Pero adem\u00e1s", "tokens": [51216, 280, 368, 283, 257, 308, 5473, 280, 368, 308, 13, 883, 30, 5269, 1561, 15534, 368, 31959, 4580, 2224, 33010, 13, 9377, 21251, 51566], "temperature": 0.0, "avg_logprob": -0.26263530299348653, "compression_ratio": 1.3037037037037038, "no_speech_prob": 0.002022377448156476}, {"id": 125, "seek": 85048, "start": 851.48, "end": 858.48, "text": " esto si quiero podr\u00eda llegar a decir esto es lo mismo que p de f a e sobre p de e por", "tokens": [50414, 7433, 1511, 16811, 27246, 24892, 257, 10235, 7433, 785, 450, 12461, 631, 280, 368, 283, 257, 308, 5473, 280, 368, 308, 1515, 50764], "temperature": 0.0, "avg_logprob": -0.2998232500893729, "compression_ratio": 1.4180327868852458, "no_speech_prob": 0.001271535991691053}, {"id": 126, "seek": 85048, "start": 861.32, "end": 868.32, "text": " la cualidad que me faltaba. No. A e. Por p de a e sobre p de a e. Era esto lo quer\u00eda.", "tokens": [50906, 635, 10911, 4580, 631, 385, 37108, 5509, 13, 883, 13, 316, 308, 13, 5269, 280, 368, 257, 308, 5473, 280, 368, 257, 308, 13, 23071, 7433, 450, 37869, 13, 51256], "temperature": 0.0, "avg_logprob": -0.2998232500893729, "compression_ratio": 1.4180327868852458, "no_speech_prob": 0.001271535991691053}, {"id": 127, "seek": 88048, "start": 880.48, "end": 887.48, "text": " O sea, yo puedo agarrar esta probabilidad que est\u00e1 ac\u00e1 y multiplicarla y dividirla por", "tokens": [50364, 422, 4158, 11, 5290, 21612, 623, 2284, 289, 5283, 31959, 4580, 631, 3192, 23496, 288, 17596, 34148, 288, 4996, 347, 875, 1515, 50714], "temperature": 0.0, "avg_logprob": -0.19148368478935457, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.0041030095890164375}, {"id": 128, "seek": 88048, "start": 888.12, "end": 891.9200000000001, "text": " el mismo n\u00famero, que s\u00e9 que son mayores que cero, as\u00ed que en definitiva esa divisi\u00f3n", "tokens": [50746, 806, 12461, 14959, 11, 631, 7910, 631, 1872, 815, 2706, 631, 269, 2032, 11, 8582, 631, 465, 28781, 5931, 11342, 25974, 2560, 50936], "temperature": 0.0, "avg_logprob": -0.19148368478935457, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.0041030095890164375}, {"id": 129, "seek": 88048, "start": 891.9200000000001, "end": 897.9200000000001, "text": " me va a dar uno. Y ah\u00ed yo puedo tomar y asigno este con este y este con este. En definitiva", "tokens": [50936, 385, 2773, 257, 4072, 8526, 13, 398, 12571, 5290, 21612, 22048, 288, 382, 788, 78, 4065, 416, 4065, 288, 4065, 416, 4065, 13, 2193, 28781, 5931, 51236], "temperature": 0.0, "avg_logprob": -0.19148368478935457, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.0041030095890164375}, {"id": 130, "seek": 88048, "start": 897.9200000000001, "end": 904.9200000000001, "text": " lo que me queda es si asocio estos dos me va a quedar p de f dado a e y si asocio estos", "tokens": [51236, 450, 631, 385, 23314, 785, 1511, 382, 78, 8529, 12585, 4491, 385, 2773, 257, 39244, 280, 368, 283, 29568, 257, 308, 288, 1511, 382, 78, 8529, 12585, 51586], "temperature": 0.0, "avg_logprob": -0.19148368478935457, "compression_ratio": 1.6728971962616823, "no_speech_prob": 0.0041030095890164375}, {"id": 131, "seek": 90492, "start": 904.92, "end": 911.92, "text": " dos de ac\u00e1 me va a quedar p de a dado e. \u00bfQu\u00e9 es lo que dice all\u00e1? La probabilidad", "tokens": [50364, 4491, 368, 23496, 385, 2773, 257, 39244, 280, 368, 257, 29568, 308, 13, 3841, 15137, 785, 450, 631, 10313, 30642, 30, 2369, 31959, 4580, 50714], "temperature": 0.0, "avg_logprob": -0.2040995084322416, "compression_ratio": 1.7178217821782178, "no_speech_prob": 0.001631399616599083}, {"id": 132, "seek": 90492, "start": 916.16, "end": 922.68, "text": " de p de f a dado e, bueno, s\u00ed, de los dos, de f y a dado e es igual a la probabilidad", "tokens": [50926, 368, 280, 368, 283, 257, 29568, 308, 11, 11974, 11, 8600, 11, 368, 1750, 4491, 11, 368, 283, 288, 257, 29568, 308, 785, 10953, 257, 635, 31959, 4580, 51252], "temperature": 0.0, "avg_logprob": -0.2040995084322416, "compression_ratio": 1.7178217821782178, "no_speech_prob": 0.001631399616599083}, {"id": 133, "seek": 90492, "start": 922.68, "end": 928.9599999999999, "text": " de f dado a e por la probabilidad de a dado e. Bien, y estos dos valores que est\u00e1n ac\u00e1", "tokens": [51252, 368, 283, 29568, 257, 308, 1515, 635, 31959, 4580, 368, 257, 29568, 308, 13, 16956, 11, 288, 12585, 4491, 38790, 631, 10368, 23496, 51566], "temperature": 0.0, "avg_logprob": -0.2040995084322416, "compression_ratio": 1.7178217821782178, "no_speech_prob": 0.001631399616599083}, {"id": 134, "seek": 90492, "start": 928.9599999999999, "end": 932.5999999999999, "text": " no los eleg\u00ed por casualidad sino que son los valores que ten\u00eda antes en el modelo.", "tokens": [51566, 572, 1750, 14459, 870, 1515, 13052, 4580, 18108, 631, 1872, 1750, 38790, 631, 23718, 11014, 465, 806, 27825, 13, 51748], "temperature": 0.0, "avg_logprob": -0.2040995084322416, "compression_ratio": 1.7178217821782178, "no_speech_prob": 0.001631399616599083}, {"id": 135, "seek": 93260, "start": 932.6, "end": 939.6, "text": " O sea, yo ten\u00eda que el p de a dado e era igual a epsilon sobre y m\u00e1s uno a la j y el", "tokens": [50364, 422, 4158, 11, 5290, 23718, 631, 806, 280, 368, 257, 29568, 308, 4249, 10953, 257, 17889, 5473, 288, 3573, 8526, 257, 635, 361, 288, 806, 50714], "temperature": 0.0, "avg_logprob": -0.19950254836885056, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003425593487918377}, {"id": 136, "seek": 93260, "start": 941.52, "end": 948.52, "text": " otro era la productoria desde j igual a 1 hasta j grande de las valores de traducci\u00f3n,", "tokens": [50810, 11921, 4249, 635, 1674, 8172, 10188, 361, 10953, 257, 502, 10764, 361, 8883, 368, 2439, 38790, 368, 2479, 1311, 5687, 11, 51160], "temperature": 0.0, "avg_logprob": -0.19950254836885056, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003425593487918377}, {"id": 137, "seek": 93260, "start": 950.32, "end": 957.32, "text": " el f sub j y el e sub a sub j. Entonces, en definitiva puedo calcular p de f a dado e", "tokens": [51250, 806, 283, 1422, 361, 288, 806, 308, 1422, 257, 1422, 361, 13, 15097, 11, 465, 28781, 5931, 21612, 2104, 17792, 280, 368, 283, 257, 29568, 308, 51600], "temperature": 0.0, "avg_logprob": -0.19950254836885056, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003425593487918377}, {"id": 138, "seek": 93260, "start": 958.0, "end": 961.4, "text": " y adem\u00e1s puedo calcular haciendo una suma sobre todas las alineaciones posibles, puedo", "tokens": [51634, 288, 21251, 21612, 2104, 17792, 20509, 2002, 2408, 64, 5473, 10906, 2439, 419, 533, 9188, 1366, 14428, 11, 21612, 51804], "temperature": 0.0, "avg_logprob": -0.19950254836885056, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.003425593487918377}, {"id": 139, "seek": 96140, "start": 961.4, "end": 968.4, "text": " calcular el p de f dado e. Bien, con eso y con todo ese mont\u00f3n de cocciones llegamos", "tokens": [50364, 2104, 17792, 806, 280, 368, 283, 29568, 308, 13, 16956, 11, 416, 7287, 288, 416, 5149, 10167, 45259, 368, 598, 35560, 11234, 2151, 50714], "temperature": 0.0, "avg_logprob": -0.15653585273528767, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.004596956539899111}, {"id": 140, "seek": 96140, "start": 969.92, "end": 974.72, "text": " a construir lo que es un modelo de traducci\u00f3n, o sea, solamente teniendo una tabla de traducciones", "tokens": [50790, 257, 38445, 450, 631, 785, 517, 27825, 368, 2479, 1311, 5687, 11, 277, 4158, 11, 27814, 2064, 7304, 2002, 4421, 875, 368, 2479, 1311, 23469, 51030], "temperature": 0.0, "avg_logprob": -0.15653585273528767, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.004596956539899111}, {"id": 141, "seek": 96140, "start": 974.72, "end": 979.4, "text": " que me diga cu\u00e1l es la probabilidad de traducir una palabra. Como otra palabra, yo puedo llegar", "tokens": [51030, 631, 385, 2528, 64, 44318, 785, 635, 31959, 4580, 368, 2479, 1311, 347, 2002, 31702, 13, 11913, 13623, 31702, 11, 5290, 21612, 24892, 51264], "temperature": 0.0, "avg_logprob": -0.15653585273528767, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.004596956539899111}, {"id": 142, "seek": 96140, "start": 979.4, "end": 986.4, "text": " a definirme cu\u00e1l es la probabilidad de traducir una oraci\u00f3n dada otra oraci\u00f3n. Bien, y hay", "tokens": [51264, 257, 1561, 347, 1398, 44318, 785, 635, 31959, 4580, 368, 2479, 1311, 347, 2002, 420, 3482, 274, 1538, 13623, 420, 3482, 13, 16956, 11, 288, 4842, 51614], "temperature": 0.0, "avg_logprob": -0.15653585273528767, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.004596956539899111}, {"id": 143, "seek": 98640, "start": 986.4, "end": 993.4, "text": " una cosa m\u00e1s, bueno, esto ya lo estuvimos viendo que aplicamos en cada paso, y hay una", "tokens": [50364, 2002, 10163, 3573, 11, 11974, 11, 7433, 2478, 450, 49777, 8372, 34506, 631, 18221, 2151, 465, 8411, 29212, 11, 288, 4842, 2002, 50714], "temperature": 0.0, "avg_logprob": -0.1683939666748047, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0012072483077645302}, {"id": 144, "seek": 98640, "start": 994.24, "end": 1001.24, "text": " cosa m\u00e1s que es si yo tuviera las dos oraciones, digamos, la oraci\u00f3n en ingl\u00e9s y la oraci\u00f3n", "tokens": [50756, 10163, 3573, 631, 785, 1511, 5290, 38177, 10609, 2439, 4491, 420, 9188, 11, 36430, 11, 635, 420, 3482, 465, 49766, 288, 635, 420, 3482, 51106], "temperature": 0.0, "avg_logprob": -0.1683939666748047, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0012072483077645302}, {"id": 145, "seek": 98640, "start": 1002.12, "end": 1007.0799999999999, "text": " en espa\u00f1ol y adem\u00e1s tuviera la tabla esta con todas las probabilidades, yo podr\u00eda hacer", "tokens": [51150, 465, 31177, 288, 21251, 38177, 10609, 635, 4421, 875, 5283, 416, 10906, 2439, 31959, 10284, 11, 5290, 27246, 6720, 51398], "temperature": 0.0, "avg_logprob": -0.1683939666748047, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0012072483077645302}, {"id": 146, "seek": 98640, "start": 1007.0799999999999, "end": 1010.8, "text": " un algoritmo de programaci\u00f3n din\u00e1mica, un algoritmo estilo Viterbi que vaya recorriendo", "tokens": [51398, 517, 3501, 50017, 3280, 368, 1461, 3482, 3791, 19524, 2262, 11, 517, 3501, 50017, 3280, 37470, 691, 1681, 5614, 631, 47682, 850, 284, 470, 3999, 51584], "temperature": 0.0, "avg_logprob": -0.1683939666748047, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0012072483077645302}, {"id": 147, "seek": 98640, "start": 1010.8, "end": 1015.4, "text": " alineaciones y me diga cu\u00e1l es la alineaci\u00f3n m\u00e1s probable. No vamos a ver los detalles", "tokens": [51584, 419, 533, 9188, 288, 385, 2528, 64, 44318, 785, 635, 419, 533, 3482, 3573, 21759, 13, 883, 5295, 257, 1306, 1750, 1141, 37927, 51814], "temperature": 0.0, "avg_logprob": -0.1683939666748047, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.0012072483077645302}, {"id": 148, "seek": 101540, "start": 1015.4, "end": 1018.8, "text": " del algoritmo, pero hay una forma de decir, bueno, voy recorriendo las dos oraciones y", "tokens": [50364, 1103, 3501, 50017, 3280, 11, 4768, 4842, 2002, 8366, 368, 10235, 11, 11974, 11, 7552, 850, 284, 470, 3999, 2439, 4491, 420, 9188, 288, 50534], "temperature": 0.0, "avg_logprob": -0.1188580121180808, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.010905100032687187}, {"id": 149, "seek": 101540, "start": 1018.8, "end": 1024.0, "text": " me voy quedando con las subsecciones m\u00e1s probables y al final me termina devolviendo", "tokens": [50534, 385, 7552, 13617, 1806, 416, 2439, 1422, 405, 35560, 3573, 1239, 2965, 288, 419, 2572, 385, 1433, 1426, 1905, 401, 85, 7304, 50794], "temperature": 0.0, "avg_logprob": -0.1188580121180808, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.010905100032687187}, {"id": 150, "seek": 101540, "start": 1024.0, "end": 1029.28, "text": " cu\u00e1l es la alineaci\u00f3n m\u00e1s probable dada esas oraciones. O sea, que si yo tuviera ya", "tokens": [50794, 44318, 785, 635, 419, 533, 3482, 3573, 21759, 274, 1538, 23388, 420, 9188, 13, 422, 4158, 11, 631, 1511, 5290, 38177, 10609, 2478, 51058], "temperature": 0.0, "avg_logprob": -0.1188580121180808, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.010905100032687187}, {"id": 151, "seek": 101540, "start": 1029.28, "end": 1034.52, "text": " esa tabla de traducciones, esa tabla de probabilidad de traducci\u00f3n, podr\u00eda construirme las alineaciones", "tokens": [51058, 11342, 4421, 875, 368, 2479, 1311, 23469, 11, 11342, 4421, 875, 368, 31959, 4580, 368, 2479, 1311, 5687, 11, 27246, 38445, 1398, 2439, 419, 533, 9188, 51320], "temperature": 0.0, "avg_logprob": -0.1188580121180808, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.010905100032687187}, {"id": 152, "seek": 101540, "start": 1034.52, "end": 1041.52, "text": " del corpus. As\u00ed que bueno, hasta el momento dec\u00edamos, bueno, suponemos que tenemos esta", "tokens": [51320, 1103, 1181, 31624, 13, 17419, 631, 11974, 11, 10764, 806, 9333, 979, 16275, 11, 11974, 11, 9331, 266, 4485, 631, 9914, 5283, 51670], "temperature": 0.0, "avg_logprob": -0.1188580121180808, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.010905100032687187}, {"id": 153, "seek": 104152, "start": 1041.6, "end": 1047.36, "text": " tabla de traducci\u00f3n que me dice para bank si se traduce, perd\u00f3n, para banco si se traduce", "tokens": [50368, 4421, 875, 368, 2479, 1311, 5687, 631, 385, 10313, 1690, 3765, 1511, 369, 2479, 4176, 11, 12611, 1801, 11, 1690, 45498, 1511, 369, 2479, 4176, 50656], "temperature": 0.0, "avg_logprob": -0.17843796866280692, "compression_ratio": 1.949090909090909, "no_speech_prob": 0.027411555871367455}, {"id": 154, "seek": 104152, "start": 1047.36, "end": 1053.4, "text": " como bank o como bench, etc. Estaba diciendo que ten\u00eda esa tabla, pero en realidad la", "tokens": [50656, 2617, 3765, 277, 2617, 10638, 11, 5183, 13, 4410, 5509, 42797, 631, 23718, 11342, 4421, 875, 11, 4768, 465, 25635, 635, 50958], "temperature": 0.0, "avg_logprob": -0.17843796866280692, "compression_ratio": 1.949090909090909, "no_speech_prob": 0.027411555871367455}, {"id": 155, "seek": 104152, "start": 1053.4, "end": 1058.04, "text": " realidad es que no tengo esa tabla y me gustar\u00eda poder construirla. Entonces, nos gustar\u00eda", "tokens": [50958, 25635, 785, 631, 572, 13989, 11342, 4421, 875, 288, 385, 45896, 8152, 38445, 875, 13, 15097, 11, 3269, 45896, 51190], "temperature": 0.0, "avg_logprob": -0.17843796866280692, "compression_ratio": 1.949090909090909, "no_speech_prob": 0.027411555871367455}, {"id": 156, "seek": 104152, "start": 1058.04, "end": 1062.56, "text": " poder estimar esas probabilidades para poder construirme esa tabla. Si yo tuviera un corpus", "tokens": [51190, 8152, 8017, 289, 23388, 31959, 10284, 1690, 8152, 38445, 1398, 11342, 4421, 875, 13, 4909, 5290, 38177, 10609, 517, 1181, 31624, 51416], "temperature": 0.0, "avg_logprob": -0.17843796866280692, "compression_ratio": 1.949090909090909, "no_speech_prob": 0.027411555871367455}, {"id": 157, "seek": 104152, "start": 1062.56, "end": 1066.04, "text": " paralelo, simplemente podr\u00eda ir recorriendo el corpus y contando cu\u00e1ntas veces aparece", "tokens": [51416, 26009, 10590, 11, 33190, 27246, 3418, 850, 284, 470, 3999, 806, 1181, 31624, 288, 660, 1806, 44256, 296, 17054, 37863, 51590], "temperature": 0.0, "avg_logprob": -0.17843796866280692, "compression_ratio": 1.949090909090909, "no_speech_prob": 0.027411555871367455}, {"id": 158, "seek": 104152, "start": 1066.04, "end": 1070.72, "text": " banco alineado con bench y cu\u00e1ntas veces aparece alineado con bank y ah\u00ed sacar\u00eda", "tokens": [51590, 45498, 419, 533, 1573, 416, 10638, 288, 44256, 296, 17054, 37863, 419, 533, 1573, 416, 3765, 288, 12571, 4899, 21178, 51824], "temperature": 0.0, "avg_logprob": -0.17843796866280692, "compression_ratio": 1.949090909090909, "no_speech_prob": 0.027411555871367455}, {"id": 159, "seek": 107072, "start": 1070.8, "end": 1078.28, "text": " una probabilidad, pero no tengo las alineaciones. Y por lo que vimos, digamos, reci\u00e9n, si yo", "tokens": [50368, 2002, 31959, 4580, 11, 4768, 572, 13989, 2439, 419, 533, 9188, 13, 398, 1515, 450, 631, 49266, 11, 36430, 11, 4214, 3516, 11, 1511, 5290, 50742], "temperature": 0.0, "avg_logprob": -0.14388987126241204, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.06995023041963577}, {"id": 160, "seek": 107072, "start": 1078.28, "end": 1082.32, "text": " tuviera la tabla, entonces yo adem\u00e1s podr\u00eda ir recorriendo el corpus y construirme las", "tokens": [50742, 38177, 10609, 635, 4421, 875, 11, 13003, 5290, 21251, 27246, 3418, 850, 284, 470, 3999, 806, 1181, 31624, 288, 38445, 1398, 2439, 50944], "temperature": 0.0, "avg_logprob": -0.14388987126241204, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.06995023041963577}, {"id": 161, "seek": 107072, "start": 1082.32, "end": 1087.56, "text": " alineaciones. As\u00ed que si yo tuviera las alineaciones podr\u00eda contar y sacar la tabla, si yo tuviera", "tokens": [50944, 419, 533, 9188, 13, 17419, 631, 1511, 5290, 38177, 10609, 2439, 419, 533, 9188, 27246, 27045, 288, 43823, 635, 4421, 875, 11, 1511, 5290, 38177, 10609, 51206], "temperature": 0.0, "avg_logprob": -0.14388987126241204, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.06995023041963577}, {"id": 162, "seek": 107072, "start": 1087.56, "end": 1092.8, "text": " la tabla podr\u00eda pasarle un algoritmo y construir las alineaciones. Pero la verdad que no tengo", "tokens": [51206, 635, 4421, 875, 27246, 25344, 306, 517, 3501, 50017, 3280, 288, 38445, 2439, 419, 533, 9188, 13, 9377, 635, 13692, 631, 572, 13989, 51468], "temperature": 0.0, "avg_logprob": -0.14388987126241204, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.06995023041963577}, {"id": 163, "seek": 107072, "start": 1092.8, "end": 1097.1200000000001, "text": " ninguna de las dos cosas. Entonces se vuelve un problema de huevo y la gallina. O sea,", "tokens": [51468, 36073, 368, 2439, 4491, 12218, 13, 15097, 369, 20126, 303, 517, 12395, 368, 24967, 3080, 288, 635, 8527, 1426, 13, 422, 4158, 11, 51684], "temperature": 0.0, "avg_logprob": -0.14388987126241204, "compression_ratio": 1.8714859437751004, "no_speech_prob": 0.06995023041963577}, {"id": 164, "seek": 109712, "start": 1097.12, "end": 1101.1599999999999, "text": " si yo tuviera las alineaciones construir\u00eda el modelo, construir\u00eda la tabla de probabilidades,", "tokens": [50364, 1511, 5290, 38177, 10609, 2439, 419, 533, 9188, 38445, 2686, 806, 27825, 11, 38445, 2686, 635, 4421, 875, 368, 31959, 10284, 11, 50566], "temperature": 0.0, "avg_logprob": -0.12950480941438328, "compression_ratio": 1.9816849816849818, "no_speech_prob": 0.003715764731168747}, {"id": 165, "seek": 109712, "start": 1101.1599999999999, "end": 1105.7199999999998, "text": " si yo tuviera la tabla de probabilidades podr\u00eda construir las alineaciones. Para este tipo", "tokens": [50566, 1511, 5290, 38177, 10609, 635, 4421, 875, 368, 31959, 10284, 27246, 38445, 2439, 419, 533, 9188, 13, 11107, 4065, 9746, 50794], "temperature": 0.0, "avg_logprob": -0.12950480941438328, "compression_ratio": 1.9816849816849818, "no_speech_prob": 0.003715764731168747}, {"id": 166, "seek": 109712, "start": 1105.7199999999998, "end": 1111.36, "text": " de problemas, en los cuales yo tengo como dos variables interdependientes y no conozco", "tokens": [50794, 368, 20720, 11, 465, 1750, 46932, 5290, 13989, 2617, 4491, 9102, 728, 36763, 20135, 288, 572, 416, 15151, 1291, 51076], "temperature": 0.0, "avg_logprob": -0.12950480941438328, "compression_ratio": 1.9816849816849818, "no_speech_prob": 0.003715764731168747}, {"id": 167, "seek": 109712, "start": 1111.36, "end": 1114.9599999999998, "text": " exactamente el valor de ninguna de las dos, se utiliza lo que se conoce como el algoritmo", "tokens": [51076, 48686, 806, 15367, 368, 36073, 368, 2439, 4491, 11, 369, 4976, 13427, 450, 631, 369, 33029, 384, 2617, 806, 3501, 50017, 3280, 51256], "temperature": 0.0, "avg_logprob": -0.12950480941438328, "compression_ratio": 1.9816849816849818, "no_speech_prob": 0.003715764731168747}, {"id": 168, "seek": 109712, "start": 1114.9599999999998, "end": 1120.76, "text": " expectation maximization o maximizaci\u00f3n de la esperanza. Y bueno, es un algoritmo que", "tokens": [51256, 14334, 5138, 2144, 277, 5138, 27603, 368, 635, 10045, 20030, 13, 398, 11974, 11, 785, 517, 3501, 50017, 3280, 631, 51546], "temperature": 0.0, "avg_logprob": -0.12950480941438328, "compression_ratio": 1.9816849816849818, "no_speech_prob": 0.003715764731168747}, {"id": 169, "seek": 109712, "start": 1120.76, "end": 1125.6799999999998, "text": " sirve exactamente para este tipo de problemas. En realidad lo que va a hacer el algoritmo", "tokens": [51546, 4735, 303, 48686, 1690, 4065, 9746, 368, 20720, 13, 2193, 25635, 450, 631, 2773, 257, 6720, 806, 3501, 50017, 3280, 51792], "temperature": 0.0, "avg_logprob": -0.12950480941438328, "compression_ratio": 1.9816849816849818, "no_speech_prob": 0.003715764731168747}, {"id": 170, "seek": 112568, "start": 1125.8400000000001, "end": 1131.04, "text": " iterar es un algoritmo iterativo que va tratando de converger una soluci\u00f3n y lo que hace es", "tokens": [50372, 17138, 289, 785, 517, 3501, 50017, 3280, 17138, 18586, 631, 2773, 21507, 1806, 368, 9652, 1321, 2002, 24807, 5687, 288, 450, 631, 10032, 785, 50632], "temperature": 0.0, "avg_logprob": -0.15867871504563552, "compression_ratio": 1.9205020920502092, "no_speech_prob": 0.11916280537843704}, {"id": 171, "seek": 112568, "start": 1131.04, "end": 1139.3200000000002, "text": " decir, bueno, yo no tengo ninguno de los dos valores. O sea, si yo tuviera mi tabla de", "tokens": [50632, 10235, 11, 11974, 11, 5290, 572, 13989, 17210, 12638, 368, 1750, 4491, 38790, 13, 422, 4158, 11, 1511, 5290, 38177, 10609, 2752, 4421, 875, 368, 51046], "temperature": 0.0, "avg_logprob": -0.15867871504563552, "compression_ratio": 1.9205020920502092, "no_speech_prob": 0.11916280537843704}, {"id": 172, "seek": 112568, "start": 1139.3200000000002, "end": 1143.52, "text": " probabilidad de traducci\u00f3n me podr\u00eda calcular las alineaciones y tuviera mis alineaciones", "tokens": [51046, 31959, 4580, 368, 2479, 1311, 5687, 385, 27246, 2104, 17792, 2439, 419, 533, 9188, 288, 38177, 10609, 3346, 419, 533, 9188, 51256], "temperature": 0.0, "avg_logprob": -0.15867871504563552, "compression_ratio": 1.9205020920502092, "no_speech_prob": 0.11916280537843704}, {"id": 173, "seek": 112568, "start": 1143.52, "end": 1147.28, "text": " me podr\u00eda calcular la probabilidad de traducci\u00f3n. Entonces lo que hace es decir, bueno, asumo", "tokens": [51256, 385, 27246, 2104, 17792, 635, 31959, 4580, 368, 2479, 1311, 5687, 13, 15097, 450, 631, 10032, 785, 10235, 11, 11974, 11, 382, 40904, 51444], "temperature": 0.0, "avg_logprob": -0.15867871504563552, "compression_ratio": 1.9205020920502092, "no_speech_prob": 0.11916280537843704}, {"id": 174, "seek": 112568, "start": 1147.28, "end": 1152.3600000000001, "text": " que mi tabla de traducci\u00f3n va a ser uniforme, digamos. Cualquier palabra se puede traducir", "tokens": [51444, 631, 2752, 4421, 875, 368, 2479, 1311, 5687, 2773, 257, 816, 9452, 68, 11, 36430, 13, 383, 901, 16622, 31702, 369, 8919, 2479, 1311, 347, 51698], "temperature": 0.0, "avg_logprob": -0.15867871504563552, "compression_ratio": 1.9205020920502092, "no_speech_prob": 0.11916280537843704}, {"id": 175, "seek": 115236, "start": 1152.4799999999998, "end": 1157.52, "text": " como cualquier otra palabra con la misma probabilidad. A partir de eso calculo alineaciones y a partir", "tokens": [50370, 2617, 21004, 13623, 31702, 416, 635, 24946, 31959, 4580, 13, 316, 13906, 368, 7287, 4322, 78, 419, 533, 9188, 288, 257, 13906, 50622], "temperature": 0.0, "avg_logprob": -0.15026917090782752, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.02910243719816208}, {"id": 176, "seek": 115236, "start": 1157.52, "end": 1164.52, "text": " de esas nuevas alineaciones calculo otra vez la tabla. Y de vuelta, con esa tabla que calcul\u00e9,", "tokens": [50622, 368, 23388, 42817, 419, 533, 9188, 4322, 78, 13623, 5715, 635, 4421, 875, 13, 398, 368, 41542, 11, 416, 11342, 4421, 875, 631, 4322, 526, 11, 50972], "temperature": 0.0, "avg_logprob": -0.15026917090782752, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.02910243719816208}, {"id": 177, "seek": 115236, "start": 1164.9599999999998, "end": 1169.36, "text": " vuelvo a medir las alineaciones y de vuelta con esas nuevas alineaciones vuelvo a calcular", "tokens": [50994, 20126, 3080, 257, 1205, 347, 2439, 419, 533, 9188, 288, 368, 41542, 416, 23388, 42817, 419, 533, 9188, 20126, 3080, 257, 2104, 17792, 51214], "temperature": 0.0, "avg_logprob": -0.15026917090782752, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.02910243719816208}, {"id": 178, "seek": 115236, "start": 1169.36, "end": 1174.9599999999998, "text": " la tabla. Entonces, aunque no me crean, esto despu\u00e9s de muchas iteraciones va convergiendo", "tokens": [51214, 635, 4421, 875, 13, 15097, 11, 21962, 572, 385, 1197, 282, 11, 7433, 15283, 368, 16072, 17138, 9188, 2773, 9652, 70, 7304, 51494], "temperature": 0.0, "avg_logprob": -0.15026917090782752, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.02910243719816208}, {"id": 179, "seek": 115236, "start": 1174.9599999999998, "end": 1179.4399999999998, "text": " a algo. Y parece m\u00e1gico, \u00bfno? Parece como que, en realidad si yo no tengo ninguno de", "tokens": [51494, 257, 8655, 13, 398, 14120, 12228, 70, 2789, 11, 3841, 1771, 30, 45419, 2617, 631, 11, 465, 25635, 1511, 5290, 572, 13989, 17210, 12638, 368, 51718], "temperature": 0.0, "avg_logprob": -0.15026917090782752, "compression_ratio": 1.821011673151751, "no_speech_prob": 0.02910243719816208}, {"id": 180, "seek": 117944, "start": 1179.52, "end": 1186.0, "text": " dos valores, no deber\u00eda nada, deber\u00eda como dar fruta. Pero voy a tratar de comenzarlos", "tokens": [50368, 4491, 38790, 11, 572, 29671, 2686, 8096, 11, 29671, 2686, 2617, 4072, 431, 12093, 13, 9377, 7552, 257, 42549, 368, 29564, 39734, 50692], "temperature": 0.0, "avg_logprob": -0.2266899585723877, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.04621070995926857}, {"id": 181, "seek": 117944, "start": 1186.0, "end": 1193.0, "text": " de que, en realidad, esto s\u00ed funciona, con un ejemplito. Bien, tenemos. Entonces, vamos", "tokens": [50692, 368, 631, 11, 465, 25635, 11, 7433, 8600, 26210, 11, 416, 517, 10012, 5895, 3528, 13, 16956, 11, 9914, 13, 15097, 11, 5295, 51042], "temperature": 0.0, "avg_logprob": -0.2266899585723877, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.04621070995926857}, {"id": 182, "seek": 117944, "start": 1194.4, "end": 1199.68, "text": " a construir un sistema que es de traducci\u00f3n entre franc\u00e9s y el ingl\u00e9s donde hay un cuerpo", "tokens": [51112, 257, 38445, 517, 13245, 631, 785, 368, 2479, 1311, 5687, 3962, 30514, 2191, 288, 806, 49766, 10488, 4842, 517, 20264, 51376], "temperature": 0.0, "avg_logprob": -0.2266899585723877, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.04621070995926857}, {"id": 183, "seek": 117944, "start": 1199.68, "end": 1203.0, "text": " muy grande, pero bueno, nos vamos a concentrar solo en tres peque\u00f1as oraciones citas que", "tokens": [51376, 5323, 8883, 11, 4768, 11974, 11, 3269, 5295, 257, 5512, 5352, 6944, 465, 15890, 19132, 32448, 420, 9188, 4814, 296, 631, 51542], "temperature": 0.0, "avg_logprob": -0.2266899585723877, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.04621070995926857}, {"id": 184, "seek": 117944, "start": 1203.0, "end": 1207.0, "text": " dicen la mes\u00f3n se traduce como de House, la mes\u00f3n blue se traduce como de Blue House", "tokens": [51542, 33816, 635, 3813, 1801, 369, 2479, 4176, 2617, 368, 4928, 11, 635, 3813, 1801, 3344, 369, 2479, 4176, 2617, 368, 8510, 4928, 51742], "temperature": 0.0, "avg_logprob": -0.2266899585723877, "compression_ratio": 1.661710037174721, "no_speech_prob": 0.04621070995926857}, {"id": 185, "seek": 120700, "start": 1207.16, "end": 1212.16, "text": " y la flea se traduce como de Flower. Entonces, al principio lo que hago es decir, bueno,", "tokens": [50372, 288, 635, 7025, 64, 369, 2479, 4176, 2617, 368, 34993, 13, 15097, 11, 419, 34308, 450, 631, 38721, 785, 10235, 11, 11974, 11, 50622], "temperature": 0.0, "avg_logprob": -0.17230433146158855, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.007587185129523277}, {"id": 186, "seek": 120700, "start": 1212.16, "end": 1216.76, "text": " todas las traducciones entre todas las palabras son equiprobables, as\u00ed que lo que me va a", "tokens": [50622, 10906, 2439, 2479, 1311, 23469, 3962, 10906, 2439, 35240, 1872, 5037, 16614, 2965, 11, 8582, 631, 450, 631, 385, 2773, 257, 50852], "temperature": 0.0, "avg_logprob": -0.17230433146158855, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.007587185129523277}, {"id": 187, "seek": 120700, "start": 1216.76, "end": 1221.24, "text": " quedar es cuando reparten entre las alineaciones, todas van a tener el mismo peso. Entre la y", "tokens": [50852, 39244, 785, 7767, 1085, 11719, 3962, 2439, 419, 533, 9188, 11, 10906, 3161, 257, 11640, 806, 12461, 28149, 13, 27979, 635, 288, 51076], "temperature": 0.0, "avg_logprob": -0.17230433146158855, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.007587185129523277}, {"id": 188, "seek": 120700, "start": 1221.24, "end": 1225.76, "text": " mes\u00f3n, la probabilidad de que la se traduzca como D o que se traduzca como House va a ser", "tokens": [51076, 3813, 1801, 11, 635, 31959, 4580, 368, 631, 635, 369, 2479, 3334, 496, 2617, 413, 277, 631, 369, 2479, 3334, 496, 2617, 4928, 2773, 257, 816, 51302], "temperature": 0.0, "avg_logprob": -0.17230433146158855, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.007587185129523277}, {"id": 189, "seek": 120700, "start": 1225.76, "end": 1230.72, "text": " la misma, en realidad porque todas las alineaciones son equiprobables. En la mes\u00f3n blue tambi\u00e9n", "tokens": [51302, 635, 24946, 11, 465, 25635, 4021, 10906, 2439, 419, 533, 9188, 1872, 5037, 16614, 2965, 13, 2193, 635, 3813, 1801, 3344, 6407, 51550], "temperature": 0.0, "avg_logprob": -0.17230433146158855, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.007587185129523277}, {"id": 190, "seek": 120700, "start": 1230.72, "end": 1234.2, "text": " pasa lo mismo, la probabilidad de traducir la como D como Blue o como House va a ser", "tokens": [51550, 20260, 450, 12461, 11, 635, 31959, 4580, 368, 2479, 1311, 347, 635, 2617, 413, 2617, 8510, 277, 2617, 4928, 2773, 257, 816, 51724], "temperature": 0.0, "avg_logprob": -0.17230433146158855, "compression_ratio": 1.9963503649635037, "no_speech_prob": 0.007587185129523277}, {"id": 191, "seek": 123420, "start": 1234.3600000000001, "end": 1244.3600000000001, "text": " la misma y en la flea pasa igual. Entonces, eso es la primera, el primer paso, digamos,", "tokens": [50372, 635, 24946, 288, 465, 635, 7025, 64, 20260, 10953, 13, 15097, 11, 7287, 785, 635, 17382, 11, 806, 12595, 29212, 11, 36430, 11, 50872], "temperature": 0.0, "avg_logprob": -0.29804710466034556, "compression_ratio": 1.475, "no_speech_prob": 0.04471023380756378}, {"id": 192, "seek": 123420, "start": 1244.3600000000001, "end": 1249.64, "text": " en el primer paso yo voy a tener todas las alineaciones equiprobables y todas las valores", "tokens": [50872, 465, 806, 12595, 29212, 5290, 7552, 257, 11640, 10906, 2439, 419, 533, 9188, 5037, 16614, 2965, 288, 10906, 2439, 38790, 51136], "temperature": 0.0, "avg_logprob": -0.29804710466034556, "compression_ratio": 1.475, "no_speech_prob": 0.04471023380756378}, {"id": 193, "seek": 124964, "start": 1249.64, "end": 1250.76, "text": " de las palabras iguales.", "tokens": [50364, 368, 2439, 35240, 10953, 279, 13, 50420], "temperature": 0.0, "avg_logprob": -0.21722956361441775, "compression_ratio": 1.3972602739726028, "no_speech_prob": 0.023466724902391434}, {"id": 194, "seek": 124964, "start": 1263.76, "end": 1270.92, "text": " Entonces, en mi algoritmo yo empec\u00e9 con una tabla de traducci\u00f3n que era toda uniforme,", "tokens": [51070, 15097, 11, 465, 2752, 3501, 50017, 3280, 5290, 846, 494, 13523, 416, 2002, 4421, 875, 368, 2479, 1311, 5687, 631, 4249, 11687, 9452, 68, 11, 51428], "temperature": 0.0, "avg_logprob": -0.21722956361441775, "compression_ratio": 1.3972602739726028, "no_speech_prob": 0.023466724902391434}, {"id": 195, "seek": 124964, "start": 1270.92, "end": 1274.5200000000002, "text": " digamos, yo ten\u00eda la probabilidad de traducir cualquier palabra en cualquier otra, era la", "tokens": [51428, 36430, 11, 5290, 23718, 635, 31959, 4580, 368, 2479, 1311, 347, 21004, 31702, 465, 21004, 13623, 11, 4249, 635, 51608], "temperature": 0.0, "avg_logprob": -0.21722956361441775, "compression_ratio": 1.3972602739726028, "no_speech_prob": 0.023466724902391434}, {"id": 196, "seek": 127452, "start": 1274.52, "end": 1280.24, "text": " misma. A partir de eso yo me constru\u00ed estas alineaciones que tambi\u00e9n parece que son", "tokens": [50364, 24946, 13, 316, 13906, 368, 7287, 5290, 385, 12946, 870, 13897, 419, 533, 9188, 631, 6407, 14120, 631, 1872, 50650], "temperature": 0.0, "avg_logprob": -0.13492292707616632, "compression_ratio": 1.7303921568627452, "no_speech_prob": 0.05592169612646103}, {"id": 197, "seek": 127452, "start": 1280.24, "end": 1283.76, "text": " todas equiprobables y parece que no tienen como mucha informaci\u00f3n. Entonces lo que voy", "tokens": [50650, 10906, 5037, 16614, 2965, 288, 14120, 631, 572, 12536, 2617, 25248, 21660, 13, 15097, 450, 631, 7552, 50826], "temperature": 0.0, "avg_logprob": -0.13492292707616632, "compression_ratio": 1.7303921568627452, "no_speech_prob": 0.05592169612646103}, {"id": 198, "seek": 127452, "start": 1283.76, "end": 1288.28, "text": " a hacer ahora, a partir de esto, es tratar de construirme de vuelta la tabla de traducciones", "tokens": [50826, 257, 6720, 9923, 11, 257, 13906, 368, 7433, 11, 785, 42549, 368, 38445, 1398, 368, 41542, 635, 4421, 875, 368, 2479, 1311, 23469, 51052], "temperature": 0.0, "avg_logprob": -0.13492292707616632, "compression_ratio": 1.7303921568627452, "no_speech_prob": 0.05592169612646103}, {"id": 199, "seek": 127452, "start": 1288.28, "end": 1292.16, "text": " pero mirando estas nuevas alineaciones que hay. Entonces lo que voy a construir es una", "tokens": [51052, 4768, 3149, 1806, 13897, 42817, 419, 533, 9188, 631, 4842, 13, 15097, 450, 631, 7552, 257, 38445, 785, 2002, 51246], "temperature": 0.0, "avg_logprob": -0.13492292707616632, "compression_ratio": 1.7303921568627452, "no_speech_prob": 0.05592169612646103}, {"id": 200, "seek": 129216, "start": 1292.16, "end": 1302.8000000000002, "text": " tabla que tiene todas las palabras del lado de franc\u00e9s, tiene la mes\u00f3n blue flower y", "tokens": [50364, 4421, 875, 631, 7066, 10906, 2439, 35240, 1103, 11631, 368, 30514, 2191, 11, 7066, 635, 3813, 1801, 3344, 8617, 288, 50896], "temperature": 0.0, "avg_logprob": -0.27003232444204933, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.030557790771126747}, {"id": 201, "seek": 129216, "start": 1302.8000000000002, "end": 1308.68, "text": " de House blue flower.", "tokens": [50896, 368, 4928, 3344, 8617, 13, 51190], "temperature": 0.0, "avg_logprob": -0.27003232444204933, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.030557790771126747}, {"id": 202, "seek": 129216, "start": 1308.68, "end": 1317.52, "text": " Y para llenar esta nueva tabla, lo que tengo que hacer es iterar sobre las alineaciones,", "tokens": [51190, 398, 1690, 4849, 268, 289, 5283, 28963, 4421, 875, 11, 450, 631, 13989, 631, 6720, 785, 17138, 289, 5473, 2439, 419, 533, 9188, 11, 51632], "temperature": 0.0, "avg_logprob": -0.27003232444204933, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.030557790771126747}, {"id": 203, "seek": 129216, "start": 1317.52, "end": 1320.92, "text": " mirar cada una de las palabras cuantas veces est\u00e1 alineada con las otras y contar, o sea,", "tokens": [51632, 3149, 289, 8411, 2002, 368, 2439, 35240, 2702, 49153, 17054, 3192, 419, 533, 1538, 416, 2439, 20244, 288, 27045, 11, 277, 4158, 11, 51802], "temperature": 0.0, "avg_logprob": -0.27003232444204933, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.030557790771126747}, {"id": 204, "seek": 132092, "start": 1321.68, "end": 1328.1200000000001, "text": " y sumar los pesos de cada una de las alineaciones. Entonces la alineaci\u00f3n entre la y de. En total,", "tokens": [50402, 288, 2408, 289, 1750, 33204, 368, 8411, 2002, 368, 2439, 419, 533, 9188, 13, 15097, 635, 419, 533, 3482, 3962, 635, 288, 368, 13, 2193, 3217, 11, 50724], "temperature": 0.0, "avg_logprob": -0.20783125615752904, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.09191141277551651}, {"id": 205, "seek": 132092, "start": 1328.1200000000001, "end": 1332.16, "text": " mirando ese ejemplo de corpus, \u00bfcu\u00e1nto me dar\u00eda? \u00bfCu\u00e1l ser\u00eda el peso de esa alineaci\u00f3n?", "tokens": [50724, 3149, 1806, 10167, 13358, 368, 1181, 31624, 11, 3841, 12032, 27525, 78, 385, 4072, 2686, 30, 3841, 35222, 11447, 23679, 806, 28149, 368, 11342, 419, 533, 3482, 30, 50926], "temperature": 0.0, "avg_logprob": -0.20783125615752904, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.09191141277551651}, {"id": 206, "seek": 132092, "start": 1334.8000000000002, "end": 1339.64, "text": " Para verlo, en realidad lo que hago es contar, miro cuantas veces la y de est\u00e1n alineados.", "tokens": [51058, 11107, 1306, 752, 11, 465, 25635, 450, 631, 38721, 785, 27045, 11, 2752, 340, 2702, 49153, 17054, 635, 288, 368, 10368, 419, 533, 4181, 13, 51300], "temperature": 0.0, "avg_logprob": -0.20783125615752904, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.09191141277551651}, {"id": 207, "seek": 132092, "start": 1339.64, "end": 1346.24, "text": " Entonces tengo 0.5 de peso en la primera, en la segunda tengo 0.33 y en la \u00faltima tengo", "tokens": [51300, 15097, 13989, 1958, 13, 20, 368, 28149, 465, 635, 17382, 11, 465, 635, 21978, 13989, 1958, 13, 10191, 288, 465, 635, 28118, 13989, 51630], "temperature": 0.0, "avg_logprob": -0.20783125615752904, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.09191141277551651}, {"id": 208, "seek": 134624, "start": 1346.24, "end": 1354.32, "text": " 0.5 de vuelta. As\u00ed que en total tengo como 1.33 de peso entre la y de. Despu\u00e9s miro,", "tokens": [50364, 1958, 13, 20, 368, 41542, 13, 17419, 631, 465, 3217, 13989, 2617, 502, 13, 10191, 368, 28149, 3962, 635, 288, 368, 13, 40995, 2752, 340, 11, 50768], "temperature": 0.0, "avg_logprob": -0.17875636025760952, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0959814265370369}, {"id": 209, "seek": 134624, "start": 1354.32, "end": 1360.68, "text": " entre la y House, \u00bfcu\u00e1nto peso tengo? \u00bfcu\u00e1nta masa de probabilidad tengo? Bueno, tengo 0.5", "tokens": [50768, 3962, 635, 288, 4928, 11, 3841, 12032, 27525, 78, 28149, 13989, 30, 3841, 12032, 27525, 64, 29216, 368, 31959, 4580, 13989, 30, 16046, 11, 13989, 1958, 13, 20, 51086], "temperature": 0.0, "avg_logprob": -0.17875636025760952, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0959814265370369}, {"id": 210, "seek": 134624, "start": 1360.68, "end": 1368.16, "text": " en la primera relaci\u00f3n, 0.33 en la segunda y nada en la tercera. Por lo tanto en total tengo 0.83", "tokens": [51086, 465, 635, 17382, 37247, 11, 1958, 13, 10191, 465, 635, 21978, 288, 8096, 465, 635, 1796, 41034, 13, 5269, 450, 10331, 465, 3217, 13989, 1958, 13, 31849, 51460], "temperature": 0.0, "avg_logprob": -0.17875636025760952, "compression_ratio": 1.4736842105263157, "no_speech_prob": 0.0959814265370369}, {"id": 211, "seek": 136816, "start": 1368.88, "end": 1375.28, "text": " de probabilidades entre la y House. Despu\u00e9s miro, entre la y blue, \u00bfcu\u00e1nto peso tengo?", "tokens": [50400, 368, 31959, 10284, 3962, 635, 288, 4928, 13, 40995, 2752, 340, 11, 3962, 635, 288, 3344, 11, 3841, 12032, 27525, 78, 28149, 13989, 30, 50720], "temperature": 0.0, "avg_logprob": -0.25646444808605107, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.09188634157180786}, {"id": 212, "seek": 136816, "start": 1379.6000000000001, "end": 1385.48, "text": " Solamente 0.33 solo est\u00e1 en la del medio y entre la y flair, \u00bfcu\u00e1nto tengo? No, entre la y", "tokens": [50936, 7026, 3439, 1958, 13, 10191, 6944, 3192, 465, 635, 1103, 22123, 288, 3962, 635, 288, 283, 24319, 11, 3841, 12032, 27525, 78, 13989, 30, 883, 11, 3962, 635, 288, 51230], "temperature": 0.0, "avg_logprob": -0.25646444808605107, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.09188634157180786}, {"id": 213, "seek": 136816, "start": 1385.48, "end": 1391.28, "text": " flower, \u00bfcu\u00e1nto tengo? 0.5 solo aparece en la del final. Bien, completemos la siguiente,", "tokens": [51230, 8617, 11, 3841, 12032, 27525, 78, 13989, 30, 1958, 13, 20, 6944, 37863, 465, 635, 1103, 2572, 13, 16956, 11, 1557, 4485, 635, 25666, 11, 51520], "temperature": 0.0, "avg_logprob": -0.25646444808605107, "compression_ratio": 1.5657142857142856, "no_speech_prob": 0.09188634157180786}, {"id": 214, "seek": 139128, "start": 1391.28, "end": 1401.48, "text": " entre mes\u00f3n y de, \u00bfcu\u00e1nto tendr\u00eda? 0.83, est\u00e1 en la primera y en la segunda, entre", "tokens": [50364, 3962, 3813, 1801, 288, 368, 11, 3841, 12032, 27525, 78, 3928, 37183, 30, 1958, 13, 31849, 11, 3192, 465, 635, 17382, 288, 465, 635, 21978, 11, 3962, 50874], "temperature": 0.0, "avg_logprob": -0.22724515542216683, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.050572432577610016}, {"id": 215, "seek": 139128, "start": 1401.48, "end": 1414.72, "text": " mes\u00f3n y House, entre mes\u00f3n y House y 0.83 porque aparece en las dos. Bien, entre mes\u00f3n y blue,", "tokens": [50874, 3813, 1801, 288, 4928, 11, 3962, 3813, 1801, 288, 4928, 288, 1958, 13, 31849, 4021, 37863, 465, 2439, 4491, 13, 16956, 11, 3962, 3813, 1801, 288, 3344, 11, 51536], "temperature": 0.0, "avg_logprob": -0.22724515542216683, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.050572432577610016}, {"id": 216, "seek": 139128, "start": 1414.72, "end": 1419.36, "text": " solamente aparece en la segunda, as\u00ed que voy a tener 0.33 y entre mes\u00f3n y flower no tengo nada.", "tokens": [51536, 27814, 37863, 465, 635, 21978, 11, 8582, 631, 7552, 257, 11640, 1958, 13, 10191, 288, 3962, 3813, 1801, 288, 8617, 572, 13989, 8096, 13, 51768], "temperature": 0.0, "avg_logprob": -0.22724515542216683, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.050572432577610016}, {"id": 217, "seek": 141936, "start": 1420.0, "end": 1426.36, "text": " Despu\u00e9s entre blue y de, solamente aparece en la segunda, as\u00ed que voy a tener 0.33, entre blue", "tokens": [50396, 40995, 3962, 3344, 288, 368, 11, 27814, 37863, 465, 635, 21978, 11, 8582, 631, 7552, 257, 11640, 1958, 13, 10191, 11, 3962, 3344, 50714], "temperature": 0.0, "avg_logprob": -0.13336394049904562, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.002272051991894841}, {"id": 218, "seek": 141936, "start": 1426.36, "end": 1433.04, "text": " y House, creo que de vuelta tengo 0.33 y entre blue y blue tambi\u00e9n 0.33 y no aparece junto con", "tokens": [50714, 288, 4928, 11, 14336, 631, 368, 41542, 13989, 1958, 13, 10191, 288, 3962, 3344, 288, 3344, 6407, 1958, 13, 10191, 288, 572, 37863, 24663, 416, 51048], "temperature": 0.0, "avg_logprob": -0.13336394049904562, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.002272051991894841}, {"id": 219, "seek": 141936, "start": 1433.04, "end": 1441.8, "text": " flower. Y para despu\u00e9s, para flair tengo 0.5 con de, 0 con House, 0 con blue y 0.5 con flower.", "tokens": [51048, 8617, 13, 398, 1690, 15283, 11, 1690, 283, 24319, 13989, 1958, 13, 20, 416, 368, 11, 1958, 416, 4928, 11, 1958, 416, 3344, 288, 1958, 13, 20, 416, 8617, 13, 51486], "temperature": 0.0, "avg_logprob": -0.13336394049904562, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.002272051991894841}, {"id": 220, "seek": 141936, "start": 1442.8, "end": 1447.9199999999998, "text": " Bien, entonces hice una pasada por todas las alineaciones y me calcul\u00e9 cu\u00e1les son los", "tokens": [51536, 16956, 11, 13003, 50026, 2002, 1736, 1538, 1515, 10906, 2439, 419, 533, 9188, 288, 385, 4322, 526, 2702, 842, 904, 1872, 1750, 51792], "temperature": 0.0, "avg_logprob": -0.13336394049904562, "compression_ratio": 1.6711111111111112, "no_speech_prob": 0.002272051991894841}, {"id": 221, "seek": 144792, "start": 1447.92, "end": 1452.3200000000002, "text": " pesos relativos de cada una de estos pares. Lo siguiente que hago, como esto va a ser una", "tokens": [50364, 33204, 21960, 329, 368, 8411, 2002, 368, 12585, 2502, 495, 13, 6130, 25666, 631, 38721, 11, 2617, 7433, 2773, 257, 816, 2002, 50584], "temperature": 0.0, "avg_logprob": -0.16346232922046217, "compression_ratio": 1.6127167630057804, "no_speech_prob": 0.011670502834022045}, {"id": 222, "seek": 144792, "start": 1452.3200000000002, "end": 1456.44, "text": " probabilidad, es normalizar. Entonces me voy a construir una tabla, digamos normalizando por,", "tokens": [50584, 31959, 4580, 11, 785, 2710, 9736, 13, 15097, 385, 7552, 257, 38445, 2002, 4421, 875, 11, 36430, 2710, 590, 1806, 1515, 11, 50790], "temperature": 0.0, "avg_logprob": -0.16346232922046217, "compression_ratio": 1.6127167630057804, "no_speech_prob": 0.011670502834022045}, {"id": 223, "seek": 144792, "start": 1456.44, "end": 1462.04, "text": " digamos, voy a sumar en cada fila y voy a dividir entre la cantidad que aparece para cada fila,", "tokens": [50790, 36430, 11, 7552, 257, 2408, 289, 465, 8411, 1387, 64, 288, 7552, 257, 4996, 347, 3962, 635, 33757, 631, 37863, 1690, 8411, 1387, 64, 11, 51070], "temperature": 0.0, "avg_logprob": -0.16346232922046217, "compression_ratio": 1.6127167630057804, "no_speech_prob": 0.011670502834022045}, {"id": 224, "seek": 146204, "start": 1462.04, "end": 1472.12, "text": " as\u00ed que de vuelta me construyo la tabla, que me queda la mes\u00f3n blue flower y de este lado de", "tokens": [50364, 8582, 631, 368, 41542, 385, 12946, 8308, 635, 4421, 875, 11, 631, 385, 23314, 635, 3813, 1801, 3344, 8617, 288, 368, 4065, 11631, 368, 50868], "temperature": 0.0, "avg_logprob": -0.2818855805830522, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.05837453901767731}, {"id": 225, "seek": 146204, "start": 1475.56, "end": 1486.6, "text": " House ac\u00e1, de House y blue flower. Y lo que voy a hacer es normalizar, entonces si yo sumo estos", "tokens": [51040, 4928, 23496, 11, 368, 4928, 288, 3344, 8617, 13, 398, 450, 631, 7552, 257, 6720, 785, 2710, 9736, 11, 13003, 1511, 5290, 2408, 78, 12585, 51592], "temperature": 0.0, "avg_logprob": -0.2818855805830522, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.05837453901767731}, {"id": 226, "seek": 148660, "start": 1486.6, "end": 1495.24, "text": " de ac\u00e1, creo que me da 2 en total, no, 3 en total. Tengo los valores ac\u00e1, no tengo que", "tokens": [50364, 368, 23496, 11, 14336, 631, 385, 1120, 568, 465, 3217, 11, 572, 11, 805, 465, 3217, 13, 314, 30362, 1750, 38790, 23496, 11, 572, 13989, 631, 50796], "temperature": 0.0, "avg_logprob": -0.1625932651561695, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.0866839736700058}, {"id": 227, "seek": 148660, "start": 1495.24, "end": 1500.08, "text": " hacer los c\u00e1lculos, pero s\u00ed, me da 3 en total. Entonces lo que pasa cuando yo normalizo es que", "tokens": [50796, 6720, 1750, 6476, 75, 32397, 11, 4768, 8600, 11, 385, 1120, 805, 465, 3217, 13, 15097, 450, 631, 20260, 7767, 5290, 2710, 19055, 785, 631, 51038], "temperature": 0.0, "avg_logprob": -0.1625932651561695, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.0866839736700058}, {"id": 228, "seek": 148660, "start": 1500.08, "end": 1509.4399999999998, "text": " ac\u00e1 me queda 0.44, ac\u00e1 me queda 0.28, ac\u00e1 me queda 0.11 y ac\u00e1 me queda 0.17. Pues el segundo,", "tokens": [51038, 23496, 385, 23314, 1958, 13, 13912, 11, 23496, 385, 23314, 1958, 13, 11205, 11, 23496, 385, 23314, 1958, 13, 5348, 288, 23496, 385, 23314, 1958, 13, 7773, 13, 22386, 806, 17954, 11, 51506], "temperature": 0.0, "avg_logprob": -0.1625932651561695, "compression_ratio": 1.6745562130177514, "no_speech_prob": 0.0866839736700058}, {"id": 229, "seek": 150944, "start": 1509.44, "end": 1518.72, "text": " tambi\u00e9n lo normalizo esta vez entre 2 y me queda 0.42, 0.42, 0.16, 0. El tercero ya suma 1,", "tokens": [50364, 6407, 450, 2710, 19055, 5283, 5715, 3962, 568, 288, 385, 23314, 1958, 13, 15628, 11, 1958, 13, 15628, 11, 1958, 13, 6866, 11, 1958, 13, 2699, 38103, 78, 2478, 2408, 64, 502, 11, 50828], "temperature": 0.0, "avg_logprob": -0.15215659451175045, "compression_ratio": 1.385185185185185, "no_speech_prob": 0.04022637754678726}, {"id": 230, "seek": 150944, "start": 1518.72, "end": 1531.16, "text": " as\u00ed que me queda 0.23, 0.33, 0.33, 0 y el \u00faltimo tambi\u00e9n queda igual, 0.5, 0, 0, 0.5. Bien,", "tokens": [50828, 8582, 631, 385, 23314, 1958, 13, 9356, 11, 1958, 13, 10191, 11, 1958, 13, 10191, 11, 1958, 288, 806, 21013, 6407, 23314, 10953, 11, 1958, 13, 20, 11, 1958, 11, 1958, 11, 1958, 13, 20, 13, 16956, 11, 51450], "temperature": 0.0, "avg_logprob": -0.15215659451175045, "compression_ratio": 1.385185185185185, "no_speech_prob": 0.04022637754678726}, {"id": 231, "seek": 153116, "start": 1531.44, "end": 1539.24, "text": " entonces me constru\u00ed una nueva tabla de probabilidad de traducci\u00f3n, dado que ahora las", "tokens": [50378, 13003, 385, 12946, 870, 2002, 28963, 4421, 875, 368, 31959, 4580, 368, 2479, 1311, 5687, 11, 29568, 631, 9923, 2439, 50768], "temperature": 0.0, "avg_logprob": -0.1881486796125581, "compression_ratio": 1.4010416666666667, "no_speech_prob": 0.028377408161759377}, {"id": 232, "seek": 153116, "start": 1539.24, "end": 1544.5600000000002, "text": " alineaciones ser\u00edan estas. Y noten lo que pas\u00f3 ac\u00e1, si yo miro la fila correspondiente a la,", "tokens": [50768, 419, 533, 9188, 816, 11084, 13897, 13, 398, 406, 268, 450, 631, 41382, 23496, 11, 1511, 5290, 2752, 340, 635, 1387, 64, 6805, 8413, 257, 635, 11, 51034], "temperature": 0.0, "avg_logprob": -0.1881486796125581, "compression_ratio": 1.4010416666666667, "no_speech_prob": 0.028377408161759377}, {"id": 233, "seek": 153116, "start": 1544.5600000000002, "end": 1557.52, "text": " \u00bfqu\u00e9 es lo que pasa ahora con esa fila? Recuerda que yo empec\u00e9 teniendo todas las", "tokens": [51034, 3841, 16412, 785, 450, 631, 20260, 9923, 416, 11342, 1387, 64, 30, 9647, 5486, 2675, 631, 5290, 846, 494, 13523, 2064, 7304, 10906, 2439, 51682], "temperature": 0.0, "avg_logprob": -0.1881486796125581, "compression_ratio": 1.4010416666666667, "no_speech_prob": 0.028377408161759377}, {"id": 234, "seek": 155752, "start": 1557.52, "end": 1561.68, "text": " probabilidades de traducci\u00f3n de que parecen palabras, eran equiprobables. Si yo ahora miro la", "tokens": [50364, 31959, 10284, 368, 2479, 1311, 5687, 368, 631, 7448, 13037, 35240, 11, 32762, 5037, 16614, 2965, 13, 4909, 5290, 9923, 2752, 340, 635, 50572], "temperature": 0.0, "avg_logprob": -0.16352827652640964, "compression_ratio": 1.5461847389558232, "no_speech_prob": 0.02631559409201145}, {"id": 235, "seek": 155752, "start": 1561.68, "end": 1576.36, "text": " fila de la, \u00bfqu\u00e9 es lo que pasa? Exacto, aparece claramente que la asociaci\u00f3n entre la y de es", "tokens": [50572, 1387, 64, 368, 635, 11, 3841, 16412, 785, 450, 631, 20260, 30, 7199, 78, 11, 37863, 6093, 3439, 631, 635, 382, 78, 537, 3482, 3962, 635, 288, 368, 785, 51306], "temperature": 0.0, "avg_logprob": -0.16352827652640964, "compression_ratio": 1.5461847389558232, "no_speech_prob": 0.02631559409201145}, {"id": 236, "seek": 155752, "start": 1576.36, "end": 1582.16, "text": " m\u00e1s fuerte, tengo un 0.44 de probabilidad de traducir la como de y tengo bastante menos en", "tokens": [51306, 3573, 37129, 11, 13989, 517, 1958, 13, 13912, 368, 31959, 4580, 368, 2479, 1311, 347, 635, 2617, 368, 288, 13989, 14651, 8902, 465, 51596], "temperature": 0.0, "avg_logprob": -0.16352827652640964, "compression_ratio": 1.5461847389558232, "no_speech_prob": 0.02631559409201145}, {"id": 237, "seek": 155752, "start": 1582.16, "end": 1587.16, "text": " los otros, tengo 0.28, 0.11, 0.17. Y yo hab\u00eda empezado diciendo que eran equiprobables, entonces yo", "tokens": [51596, 1750, 16422, 11, 13989, 1958, 13, 11205, 11, 1958, 13, 5348, 11, 1958, 13, 7773, 13, 398, 5290, 16395, 18730, 1573, 42797, 631, 32762, 5037, 16614, 2965, 11, 13003, 5290, 51846], "temperature": 0.0, "avg_logprob": -0.16352827652640964, "compression_ratio": 1.5461847389558232, "no_speech_prob": 0.02631559409201145}, {"id": 238, "seek": 158716, "start": 1587.2, "end": 1595.0800000000002, "text": " probablemente ten\u00eda 0.25, 0.25, 0.25, 0.25 en cada una. Y despu\u00e9s de un paso de la iteraci\u00f3n,", "tokens": [50366, 21759, 4082, 23718, 1958, 13, 6074, 11, 1958, 13, 6074, 11, 1958, 13, 6074, 11, 1958, 13, 6074, 465, 8411, 2002, 13, 398, 15283, 368, 517, 29212, 368, 635, 17138, 3482, 11, 50760], "temperature": 0.0, "avg_logprob": -0.15247522437054178, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.010594397783279419}, {"id": 239, "seek": 158716, "start": 1595.0800000000002, "end": 1602.8000000000002, "text": " descubri\u00f3 que la ID tiene m\u00e1s chance de ser una traducci\u00f3n de la otra, en vez de traducir la", "tokens": [50760, 32592, 44802, 631, 635, 7348, 7066, 3573, 2931, 368, 816, 2002, 2479, 1311, 5687, 368, 635, 13623, 11, 465, 5715, 368, 2479, 1311, 347, 635, 51146], "temperature": 0.0, "avg_logprob": -0.15247522437054178, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.010594397783279419}, {"id": 240, "seek": 158716, "start": 1602.8000000000002, "end": 1608.0800000000002, "text": " como House o la como Blue o la como Flower. Eso pasa en el primer paso, en la primera iteraci\u00f3n", "tokens": [51146, 2617, 4928, 277, 635, 2617, 8510, 277, 635, 2617, 34993, 13, 27795, 20260, 465, 806, 12595, 29212, 11, 465, 635, 17382, 17138, 3482, 51410], "temperature": 0.0, "avg_logprob": -0.15247522437054178, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.010594397783279419}, {"id": 241, "seek": 158716, "start": 1608.0800000000002, "end": 1613.72, "text": " el tipo descubre, el algoritmo descubre que la asociaci\u00f3n entre la ID es bastante m\u00e1s fuerte.", "tokens": [51410, 806, 9746, 32592, 265, 11, 806, 3501, 50017, 3280, 32592, 265, 631, 635, 382, 78, 537, 3482, 3962, 635, 7348, 785, 14651, 3573, 37129, 13, 51692], "temperature": 0.0, "avg_logprob": -0.15247522437054178, "compression_ratio": 1.7035398230088497, "no_speech_prob": 0.010594397783279419}, {"id": 242, "seek": 161372, "start": 1614.44, "end": 1621.16, "text": " Como pasa eso, lo que va a pasar es que cuando yo reparta de vuelta en las alineaciones estas", "tokens": [50400, 11913, 20260, 7287, 11, 450, 631, 2773, 257, 25344, 785, 631, 7767, 5290, 1085, 19061, 368, 41542, 465, 2439, 419, 533, 9188, 13897, 50736], "temperature": 0.0, "avg_logprob": -0.11973619804107885, "compression_ratio": 1.8745247148288973, "no_speech_prob": 0.016538819298148155}, {"id": 243, "seek": 161372, "start": 1621.16, "end": 1625.76, "text": " l\u00edneas que se corresponden a la asociaci\u00f3n entre la ID van a estar m\u00e1s fuertes, van a tener un", "tokens": [50736, 16118, 716, 296, 631, 369, 6805, 268, 257, 635, 382, 78, 537, 3482, 3962, 635, 7348, 3161, 257, 8755, 3573, 8536, 911, 279, 11, 3161, 257, 11640, 517, 50966], "temperature": 0.0, "avg_logprob": -0.11973619804107885, "compression_ratio": 1.8745247148288973, "no_speech_prob": 0.016538819298148155}, {"id": 244, "seek": 161372, "start": 1625.76, "end": 1632.56, "text": " poco m\u00e1s de peso y como esto es una distribuci\u00f3n de probabilidades, esa masa que gan\u00f3 la asociaci\u00f3n", "tokens": [50966, 10639, 3573, 368, 28149, 288, 2617, 7433, 785, 2002, 4400, 30813, 368, 31959, 10284, 11, 11342, 29216, 631, 7574, 812, 635, 382, 78, 537, 3482, 51306], "temperature": 0.0, "avg_logprob": -0.11973619804107885, "compression_ratio": 1.8745247148288973, "no_speech_prob": 0.016538819298148155}, {"id": 245, "seek": 161372, "start": 1632.56, "end": 1636.3600000000001, "text": " entre la ID se va a tener que sacar de otras alineaciones posibles, o sea si la est\u00e1 asociada", "tokens": [51306, 3962, 635, 7348, 369, 2773, 257, 11640, 631, 43823, 368, 20244, 419, 533, 9188, 1366, 14428, 11, 277, 4158, 1511, 635, 3192, 382, 78, 537, 1538, 51496], "temperature": 0.0, "avg_logprob": -0.11973619804107885, "compression_ratio": 1.8745247148288973, "no_speech_prob": 0.016538819298148155}, {"id": 246, "seek": 161372, "start": 1636.3600000000001, "end": 1641.88, "text": " con D, entonces no est\u00e1 asociada con las otras que est\u00e1n alrededor. Entonces esa masa que se pierde,", "tokens": [51496, 416, 413, 11, 13003, 572, 3192, 382, 78, 537, 1538, 416, 2439, 20244, 631, 10368, 43663, 13, 15097, 11342, 29216, 631, 369, 9766, 1479, 11, 51772], "temperature": 0.0, "avg_logprob": -0.11973619804107885, "compression_ratio": 1.8745247148288973, "no_speech_prob": 0.016538819298148155}, {"id": 247, "seek": 164188, "start": 1641.88, "end": 1649.72, "text": " digamos, o sea que gana en la D se tiene que repartir en las otras alineaciones posibles,", "tokens": [50364, 36430, 11, 277, 4158, 631, 290, 2095, 465, 635, 413, 369, 7066, 631, 1085, 446, 347, 465, 2439, 20244, 419, 533, 9188, 1366, 14428, 11, 50756], "temperature": 0.0, "avg_logprob": -0.17490583363145884, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.004869782831519842}, {"id": 248, "seek": 164188, "start": 1649.72, "end": 1656.2, "text": " o sea en las que no son entre la ID. Entonces despu\u00e9s de una iteraci\u00f3n la asociaci\u00f3n entre", "tokens": [50756, 277, 4158, 465, 2439, 631, 572, 1872, 3962, 635, 7348, 13, 15097, 15283, 368, 2002, 17138, 3482, 635, 382, 78, 537, 3482, 3962, 51080], "temperature": 0.0, "avg_logprob": -0.17490583363145884, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.004869782831519842}, {"id": 249, "seek": 164188, "start": 1656.2, "end": 1663.88, "text": " la ID empieza a ser m\u00e1s fuerte y como pasa eso en la siguiente iteraci\u00f3n va a empezar a descubrir", "tokens": [51080, 635, 7348, 44577, 257, 816, 3573, 37129, 288, 2617, 20260, 7287, 465, 635, 25666, 17138, 3482, 2773, 257, 31168, 257, 32592, 10949, 51464], "temperature": 0.0, "avg_logprob": -0.17490583363145884, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.004869782831519842}, {"id": 250, "seek": 164188, "start": 1663.88, "end": 1668.2800000000002, "text": " que como la estaba alineado con D, entonces mes\u00f3n tiene que estar alineado con House", "tokens": [51464, 631, 2617, 635, 17544, 419, 533, 1573, 416, 413, 11, 13003, 3813, 1801, 7066, 631, 8755, 419, 533, 1573, 416, 4928, 51684], "temperature": 0.0, "avg_logprob": -0.17490583363145884, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.004869782831519842}, {"id": 251, "seek": 166828, "start": 1668.28, "end": 1675.76, "text": " y como mes\u00f3n estaba alineado con House, digamos, esa misma masa de probabilidad se va a traducir,", "tokens": [50364, 288, 2617, 3813, 1801, 17544, 419, 533, 1573, 416, 4928, 11, 36430, 11, 11342, 24946, 29216, 368, 31959, 4580, 369, 2773, 257, 2479, 1311, 347, 11, 50738], "temperature": 0.0, "avg_logprob": -0.18118026561306833, "compression_ratio": 1.8, "no_speech_prob": 0.005972648970782757}, {"id": 252, "seek": 166828, "start": 1675.76, "end": 1681.12, "text": " a transferir a la segunda y lo mismo, como la estaba alineado con D, entonces Fleur tiene", "tokens": [50738, 257, 5003, 347, 257, 635, 21978, 288, 450, 12461, 11, 2617, 635, 17544, 419, 533, 1573, 416, 413, 11, 13003, 18612, 374, 7066, 51006], "temperature": 0.0, "avg_logprob": -0.18118026561306833, "compression_ratio": 1.8, "no_speech_prob": 0.005972648970782757}, {"id": 253, "seek": 166828, "start": 1681.12, "end": 1687.56, "text": " que estar alineado con Flour. Entonces si yo sigo iterando en estos pasos, en cada paso lo que va", "tokens": [51006, 631, 8755, 419, 533, 1573, 416, 3235, 396, 13, 15097, 1511, 5290, 4556, 78, 17138, 1806, 465, 12585, 1736, 329, 11, 465, 8411, 29212, 450, 631, 2773, 51328], "temperature": 0.0, "avg_logprob": -0.18118026561306833, "compression_ratio": 1.8, "no_speech_prob": 0.005972648970782757}, {"id": 254, "seek": 166828, "start": 1687.56, "end": 1691.52, "text": " a pasar es que se va a mover un poco m\u00e1s de probabilidad hasta que al final va a terminar", "tokens": [51328, 257, 25344, 785, 631, 369, 2773, 257, 39945, 517, 10639, 3573, 368, 31959, 4580, 10764, 631, 419, 2572, 2773, 257, 36246, 51526], "temperature": 0.0, "avg_logprob": -0.18118026561306833, "compression_ratio": 1.8, "no_speech_prob": 0.005972648970782757}, {"id": 255, "seek": 166828, "start": 1691.52, "end": 1697.12, "text": " descubriendo cu\u00e1l es la alineaci\u00f3n real de las palabras, o sea va a descubrir que la va,", "tokens": [51526, 32592, 470, 3999, 44318, 785, 635, 419, 533, 3482, 957, 368, 2439, 35240, 11, 277, 4158, 2773, 257, 32592, 10949, 631, 635, 2773, 11, 51806], "temperature": 0.0, "avg_logprob": -0.18118026561306833, "compression_ratio": 1.8, "no_speech_prob": 0.005972648970782757}, {"id": 256, "seek": 169712, "start": 1697.12, "end": 1703.08, "text": " o sea con D, mes\u00f3n con House, Blue con Blue, Fleur con Flour. \u00bfC\u00f3mo es que va a descubrir eso?", "tokens": [50364, 277, 4158, 416, 413, 11, 3813, 1801, 416, 4928, 11, 8510, 416, 8510, 11, 18612, 374, 416, 3235, 396, 13, 3841, 28342, 785, 631, 2773, 257, 32592, 10949, 7287, 30, 50662], "temperature": 0.0, "avg_logprob": -0.1723344963015491, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.008473420515656471}, {"id": 257, "seek": 169712, "start": 1703.08, "end": 1707.1599999999999, "text": " Porque en cada paso lo que va pasando es que algunas de las asociaciones como est\u00e1n, como", "tokens": [50662, 11287, 465, 8411, 29212, 450, 631, 2773, 45412, 785, 631, 27316, 368, 2439, 382, 78, 537, 9188, 2617, 10368, 11, 2617, 50866], "temperature": 0.0, "avg_logprob": -0.1723344963015491, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.008473420515656471}, {"id": 258, "seek": 169712, "start": 1707.1599999999999, "end": 1712.6399999999999, "text": " aparecen, que ocurren digamos en m\u00e1s oraciones, tienen m\u00e1s fuerza que otras, entonces el peso", "tokens": [50866, 15004, 13037, 11, 631, 26430, 1095, 36430, 465, 3573, 420, 9188, 11, 12536, 3573, 39730, 631, 20244, 11, 13003, 806, 28149, 51140], "temperature": 0.0, "avg_logprob": -0.1723344963015491, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.008473420515656471}, {"id": 259, "seek": 169712, "start": 1712.6399999999999, "end": 1717.36, "text": " que esas asociaciones ganan lo va sacando de otro lado y eso hace que de otro lado se empicen", "tokens": [51140, 631, 23388, 382, 78, 537, 9188, 7574, 282, 450, 2773, 4899, 1806, 368, 11921, 11631, 288, 7287, 10032, 631, 368, 11921, 11631, 369, 4012, 299, 268, 51376], "temperature": 0.0, "avg_logprob": -0.1723344963015491, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.008473420515656471}, {"id": 260, "seek": 169712, "start": 1717.36, "end": 1724.1599999999999, "text": " a generar otras alineaciones diferentes. Entonces al final esto termina convergiendo y termina", "tokens": [51376, 257, 1337, 289, 20244, 419, 533, 9188, 17686, 13, 15097, 419, 2572, 7433, 1433, 1426, 9652, 70, 7304, 288, 1433, 1426, 51716], "temperature": 0.0, "avg_logprob": -0.1723344963015491, "compression_ratio": 1.7326007326007327, "no_speech_prob": 0.008473420515656471}, {"id": 261, "seek": 172416, "start": 1724.2, "end": 1729.1200000000001, "text": " revelando lo que es la estructura suyacente de las palabras y c\u00f3mo se alinean unas con otras.", "tokens": [50366, 15262, 1806, 450, 631, 785, 635, 43935, 2991, 459, 88, 326, 1576, 368, 2439, 35240, 288, 12826, 369, 419, 533, 282, 25405, 416, 20244, 13, 50612], "temperature": 0.0, "avg_logprob": -0.1551885964735499, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.013433435931801796}, {"id": 262, "seek": 172416, "start": 1729.1200000000001, "end": 1734.92, "text": " Bueno, una vez que yo termine de hacer esto puedo agarrar y construirme efectivamente la tabla", "tokens": [50612, 16046, 11, 2002, 5715, 631, 5290, 1433, 533, 368, 6720, 7433, 21612, 623, 2284, 289, 288, 38445, 1398, 22565, 23957, 635, 4421, 875, 50902], "temperature": 0.0, "avg_logprob": -0.1551885964735499, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.013433435931801796}, {"id": 263, "seek": 172416, "start": 1734.92, "end": 1740.64, "text": " final de traducciones que es simplemente busco cada una de las posibles traducciones, digamos de los", "tokens": [50902, 2572, 368, 2479, 1311, 23469, 631, 785, 33190, 1255, 1291, 8411, 2002, 368, 2439, 1366, 14428, 2479, 1311, 23469, 11, 36430, 368, 1750, 51188], "temperature": 0.0, "avg_logprob": -0.1551885964735499, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.013433435931801796}, {"id": 264, "seek": 172416, "start": 1740.64, "end": 1748.76, "text": " posibles pares y saco las probabilidades. \u00bfY qu\u00e9 pas\u00f3 ac\u00e1? Mientras yo estaba construyendo mi", "tokens": [51188, 1366, 14428, 2502, 495, 288, 4899, 78, 2439, 31959, 10284, 13, 3841, 56, 8057, 41382, 23496, 30, 376, 22148, 5290, 17544, 12946, 88, 3999, 2752, 51594], "temperature": 0.0, "avg_logprob": -0.1551885964735499, "compression_ratio": 1.5772357723577235, "no_speech_prob": 0.013433435931801796}, {"id": 265, "seek": 174876, "start": 1748.76, "end": 1754.0, "text": " modelo de traducci\u00f3n, mientras yo estaba construyendo la tabla de traducciones adem\u00e1s de como", "tokens": [50364, 27825, 368, 2479, 1311, 5687, 11, 26010, 5290, 17544, 12946, 88, 3999, 635, 4421, 875, 368, 2479, 1311, 23469, 21251, 368, 2617, 50626], "temperature": 0.0, "avg_logprob": -0.20963518619537352, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.002159487223252654}, {"id": 266, "seek": 174876, "start": 1754.0, "end": 1759.08, "text": " efectos secundarios se construy\u00f3 un corpus alineado, un corpus que est\u00e1 alineado a nivel de palabras.", "tokens": [50626, 22565, 329, 907, 997, 9720, 369, 12946, 88, 812, 517, 1181, 31624, 419, 533, 1573, 11, 517, 1181, 31624, 631, 3192, 419, 533, 1573, 257, 24423, 368, 35240, 13, 50880], "temperature": 0.0, "avg_logprob": -0.20963518619537352, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.002159487223252654}, {"id": 267, "seek": 174876, "start": 1764.0, "end": 1772.12, "text": " As\u00ed que bueno, el algoritmo de Spectation Maximization funciona de esa manera, tiene siempre dos", "tokens": [51126, 17419, 631, 11974, 11, 806, 3501, 50017, 3280, 368, 27078, 399, 29076, 2144, 26210, 368, 11342, 13913, 11, 7066, 12758, 4491, 51532], "temperature": 0.0, "avg_logprob": -0.20963518619537352, "compression_ratio": 1.5076142131979695, "no_speech_prob": 0.002159487223252654}, {"id": 268, "seek": 177212, "start": 1772.12, "end": 1781.6399999999999, "text": " pasos, un paso de Spectation y un paso de Maximization. En este caso el paso de Spectation se", "tokens": [50364, 1736, 329, 11, 517, 29212, 368, 27078, 399, 288, 517, 29212, 368, 29076, 2144, 13, 2193, 4065, 9666, 806, 29212, 368, 27078, 399, 369, 50840], "temperature": 0.0, "avg_logprob": -0.15780607259498453, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0448148176074028}, {"id": 269, "seek": 177212, "start": 1781.6399999999999, "end": 1788.56, "text": " trataba de agarro la tabla de probabilidad de traducci\u00f3n que tengo y con eso me armo alineaciones y", "tokens": [50840, 21507, 5509, 368, 623, 289, 340, 635, 4421, 875, 368, 31959, 4580, 368, 2479, 1311, 5687, 631, 13989, 288, 416, 7287, 385, 594, 3280, 419, 533, 9188, 288, 51186], "temperature": 0.0, "avg_logprob": -0.15780607259498453, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0448148176074028}, {"id": 270, "seek": 177212, "start": 1788.56, "end": 1792.36, "text": " despu\u00e9s el de Maximization es al rev\u00e9s agarro las alineaciones que acabo de construir y me", "tokens": [51186, 15283, 806, 368, 29076, 2144, 785, 419, 3698, 2191, 623, 289, 340, 2439, 419, 533, 9188, 631, 13281, 78, 368, 38445, 288, 385, 51376], "temperature": 0.0, "avg_logprob": -0.15780607259498453, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0448148176074028}, {"id": 271, "seek": 177212, "start": 1792.36, "end": 1796.52, "text": " armo una nueva tabla y voy iterando todos esos pasos hasta que eventualmente converge.", "tokens": [51376, 594, 3280, 2002, 28963, 4421, 875, 288, 7552, 17138, 1806, 6321, 22411, 1736, 329, 10764, 631, 33160, 4082, 9652, 432, 13, 51584], "temperature": 0.0, "avg_logprob": -0.15780607259498453, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.0448148176074028}, {"id": 272, "seek": 179652, "start": 1796.52, "end": 1803.8, "text": " Bien, dijimos que eran cinco modelos de IBM, no vamos a ver muy en detalle los otros, o sea,", "tokens": [50364, 16956, 11, 47709, 8372, 631, 32762, 21350, 2316, 329, 368, 23487, 11, 572, 5295, 257, 1306, 5323, 465, 1141, 11780, 1750, 16422, 11, 277, 4158, 11, 50728], "temperature": 0.0, "avg_logprob": -0.15962729820838342, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0028099301271140575}, {"id": 273, "seek": 179652, "start": 1803.8, "end": 1809.36, "text": " solo mencionar que empiezan a crear complejidad. En este modelo uno hab\u00edamos dicho que todas las", "tokens": [50728, 6944, 37030, 289, 631, 4012, 18812, 282, 257, 31984, 44424, 73, 4580, 13, 2193, 4065, 27825, 8526, 3025, 16275, 27346, 631, 10906, 2439, 51006], "temperature": 0.0, "avg_logprob": -0.15962729820838342, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0028099301271140575}, {"id": 274, "seek": 179652, "start": 1809.36, "end": 1814.96, "text": " alineaciones eran equiprobables, en el modelo dos abandonan esa noci\u00f3n y dicen bueno, en vez de", "tokens": [51006, 419, 533, 9188, 32762, 5037, 16614, 2965, 11, 465, 806, 27825, 4491, 9072, 282, 11342, 572, 5687, 288, 33816, 11974, 11, 465, 5715, 368, 51286], "temperature": 0.0, "avg_logprob": -0.15962729820838342, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0028099301271140575}, {"id": 275, "seek": 179652, "start": 1814.96, "end": 1820.44, "text": " alineaciones equiprobables, yo voy a tener un modelo de reordenamiento de las palabras para decir,", "tokens": [51286, 419, 533, 9188, 5037, 16614, 2965, 11, 5290, 7552, 257, 11640, 517, 27825, 368, 319, 19058, 16971, 368, 2439, 35240, 1690, 10235, 11, 51560], "temperature": 0.0, "avg_logprob": -0.15962729820838342, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0028099301271140575}, {"id": 276, "seek": 179652, "start": 1820.44, "end": 1825.32, "text": " bueno, tengo cierta probabilidad de que las palabras que est\u00e1n, si yo tengo I palabras en ingl\u00e9s,", "tokens": [51560, 11974, 11, 13989, 39769, 1328, 31959, 4580, 368, 631, 2439, 35240, 631, 10368, 11, 1511, 5290, 13989, 286, 35240, 465, 49766, 11, 51804], "temperature": 0.0, "avg_logprob": -0.15962729820838342, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.0028099301271140575}, {"id": 277, "seek": 182532, "start": 1825.32, "end": 1830.52, "text": " J palabras en espa\u00f1ol, tengo cierta probabilidad de mover la palabra I y la palabra J y bueno,", "tokens": [50364, 508, 35240, 465, 31177, 11, 13989, 39769, 1328, 31959, 4580, 368, 39945, 635, 31702, 286, 288, 635, 31702, 508, 288, 11974, 11, 50624], "temperature": 0.0, "avg_logprob": -0.15186742752317398, "compression_ratio": 1.7, "no_speech_prob": 0.005082803312689066}, {"id": 278, "seek": 182532, "start": 1830.52, "end": 1835.8, "text": " y as\u00ed siguen subiendo en complejidad hasta llegar al modelo cinco, que modelo cinco es el que anda", "tokens": [50624, 288, 8582, 4556, 7801, 1422, 7304, 465, 44424, 73, 4580, 10764, 24892, 419, 27825, 21350, 11, 631, 27825, 21350, 785, 806, 631, 21851, 50888], "temperature": 0.0, "avg_logprob": -0.15186742752317398, "compression_ratio": 1.7, "no_speech_prob": 0.005082803312689066}, {"id": 279, "seek": 182532, "start": 1835.8, "end": 1842.6399999999999, "text": " mejor, pero de todas maneras estos son modelos que ya no se usan, digamos, esto es del a\u00f1o 93 y en", "tokens": [50888, 11479, 11, 4768, 368, 10906, 587, 6985, 12585, 1872, 2316, 329, 631, 2478, 572, 369, 505, 282, 11, 36430, 11, 7433, 785, 1103, 15984, 28876, 288, 465, 51230], "temperature": 0.0, "avg_logprob": -0.15186742752317398, "compression_ratio": 1.7, "no_speech_prob": 0.005082803312689066}, {"id": 280, "seek": 182532, "start": 1842.6399999999999, "end": 1848.6399999999999, "text": " general se han obtenido mejores resultados abandonando estos modelos. Entonces el que vamos a pasar a", "tokens": [51230, 2674, 369, 7276, 28326, 2925, 42284, 36796, 9072, 1806, 12585, 2316, 329, 13, 15097, 806, 631, 5295, 257, 25344, 257, 51530], "temperature": 0.0, "avg_logprob": -0.15186742752317398, "compression_ratio": 1.7, "no_speech_prob": 0.005082803312689066}, {"id": 281, "seek": 182532, "start": 1848.6399999999999, "end": 1853.6, "text": " ver a continuaci\u00f3n es un modelo bastante m\u00e1s moderno que es lo que s\u00ed se utiliza hoy en d\u00eda", "tokens": [51530, 1306, 257, 2993, 3482, 785, 517, 27825, 14651, 3573, 4363, 78, 631, 785, 450, 631, 8600, 369, 4976, 13427, 13775, 465, 12271, 51778], "temperature": 0.0, "avg_logprob": -0.15186742752317398, "compression_ratio": 1.7, "no_speech_prob": 0.005082803312689066}, {"id": 282, "seek": 185360, "start": 1853.6, "end": 1871.04, "text": " en traductores como los de Google. S\u00ed. Es que en realidad, claro, a ver, estos modelos", "tokens": [50364, 465, 2479, 11130, 2706, 2617, 1750, 368, 3329, 13, 12375, 13, 2313, 631, 465, 25635, 11, 16742, 11, 257, 1306, 11, 12585, 2316, 329, 51236], "temperature": 0.0, "avg_logprob": -0.22963647842407225, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.022418923676013947}, {"id": 283, "seek": 185360, "start": 1871.04, "end": 1876.1599999999999, "text": " estad\u00edsticos no utilizan ning\u00fan tipo de analizador morfol\u00f3gico y nada para sacarlo. Hay otros modelos", "tokens": [51236, 39160, 19512, 9940, 572, 19906, 282, 30394, 9746, 368, 2624, 590, 5409, 1896, 7082, 14047, 2789, 288, 8096, 1690, 4899, 19457, 13, 8721, 16422, 2316, 329, 51492], "temperature": 0.0, "avg_logprob": -0.22963647842407225, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.022418923676013947}, {"id": 284, "seek": 185360, "start": 1876.1599999999999, "end": 1880.28, "text": " que s\u00ed lo hacen, no vamos a dar ninguno en esta clase pero hay otros modelos que s\u00ed hacen", "tokens": [51492, 631, 8600, 450, 27434, 11, 572, 5295, 257, 4072, 17210, 12638, 465, 5283, 44578, 4768, 4842, 16422, 2316, 329, 631, 8600, 27434, 51698], "temperature": 0.0, "avg_logprob": -0.22963647842407225, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.022418923676013947}, {"id": 285, "seek": 188028, "start": 1880.28, "end": 1886.12, "text": " uso de esa informaci\u00f3n. Igual son como refinamientos, creo que ninguno lo tiene como en la base del", "tokens": [50364, 22728, 368, 11342, 21660, 13, 19271, 901, 1872, 2617, 44395, 43466, 11, 14336, 631, 17210, 12638, 450, 7066, 2617, 465, 635, 3096, 1103, 50656], "temperature": 0.0, "avg_logprob": -0.1843004451961968, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.10268699377775192}, {"id": 286, "seek": 188028, "start": 1886.12, "end": 1892.48, "text": " modelo, el uso de parto speech, pero s\u00ed cuando vos no sabes una palabra, digamos una palabra que", "tokens": [50656, 27825, 11, 806, 22728, 368, 644, 78, 6218, 11, 4768, 8600, 7767, 13845, 572, 37790, 2002, 31702, 11, 36430, 2002, 31702, 631, 50974], "temperature": 0.0, "avg_logprob": -0.1843004451961968, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.10268699377775192}, {"id": 287, "seek": 188028, "start": 1892.48, "end": 1897.8799999999999, "text": " es desconocida, en realidad utilizar informaci\u00f3n sobre parto speech y eso probablemente te ayude.", "tokens": [50974, 785, 49801, 905, 2887, 11, 465, 25635, 24060, 21660, 5473, 644, 78, 6218, 288, 7287, 21759, 4082, 535, 7494, 2303, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1843004451961968, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.10268699377775192}, {"id": 288, "seek": 188028, "start": 1897.8799999999999, "end": 1903.36, "text": " En estos modelos por lo menos no lo hab\u00edan tenido en cuenta. Bien, entonces s\u00ed, lo que vamos a ver", "tokens": [51244, 2193, 12585, 2316, 329, 1515, 450, 8902, 572, 450, 44466, 33104, 465, 17868, 13, 16956, 11, 13003, 8600, 11, 450, 631, 5295, 257, 1306, 51518], "temperature": 0.0, "avg_logprob": -0.1843004451961968, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.10268699377775192}, {"id": 289, "seek": 188028, "start": 1903.36, "end": 1907.8799999999999, "text": " ahora es el modelo de frases que es algo m\u00e1s moderno y es, o sea, el Google Translate o Bing", "tokens": [51518, 9923, 785, 806, 27825, 368, 431, 1957, 631, 785, 8655, 3573, 4363, 78, 288, 785, 11, 277, 4158, 11, 806, 3329, 6531, 17593, 277, 30755, 51744], "temperature": 0.0, "avg_logprob": -0.1843004451961968, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.10268699377775192}, {"id": 290, "seek": 190788, "start": 1908.0400000000002, "end": 1912.2800000000002, "text": " Translate se basan en modelos de este estilo. Y bueno, y antes de ver c\u00f3mo se modelo de frases,", "tokens": [50372, 6531, 17593, 369, 987, 282, 465, 2316, 329, 368, 4065, 37470, 13, 398, 11974, 11, 288, 11014, 368, 1306, 12826, 369, 27825, 368, 431, 1957, 11, 50584], "temperature": 0.0, "avg_logprob": -0.2190105142727704, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.05818229168653488}, {"id": 291, "seek": 190788, "start": 1912.2800000000002, "end": 1916.7600000000002, "text": " volvamos un poco a lo que era la alineaci\u00f3n entre palabras. Yo ten\u00eda esta frase cl\u00e1sica,", "tokens": [50584, 1996, 85, 2151, 517, 10639, 257, 450, 631, 4249, 635, 419, 533, 3482, 3962, 35240, 13, 7616, 23718, 5283, 38406, 47434, 2262, 11, 50808], "temperature": 0.0, "avg_logprob": -0.2190105142727704, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.05818229168653488}, {"id": 292, "seek": 190788, "start": 1916.7600000000002, "end": 1923.44, "text": " \u00bfno? Mar\u00eda no dio una bofetada de la bruja verde, en ingl\u00e9s era Mary did not slap de Greenwich y una", "tokens": [50808, 3841, 1771, 30, 48472, 572, 31965, 2002, 748, 69, 302, 1538, 368, 635, 25267, 2938, 29653, 11, 465, 49766, 4249, 6059, 630, 406, 21075, 368, 6969, 9669, 288, 2002, 51142], "temperature": 0.0, "avg_logprob": -0.2190105142727704, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.05818229168653488}, {"id": 293, "seek": 190788, "start": 1923.44, "end": 1927.7600000000002, "text": " alineaci\u00f3n entre esas dos oraciones en realidad se ver\u00eda como algo as\u00ed. Yo tengo que Mar\u00eda se", "tokens": [51142, 419, 533, 3482, 3962, 23388, 4491, 420, 9188, 465, 25635, 369, 1306, 2686, 2617, 8655, 8582, 13, 7616, 13989, 631, 48472, 369, 51358], "temperature": 0.0, "avg_logprob": -0.2190105142727704, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.05818229168653488}, {"id": 294, "seek": 190788, "start": 1927.7600000000002, "end": 1933.44, "text": " alinea con Mary, no se alinea con did not, slap se alinea con daba una bofetada, de se alinea", "tokens": [51358, 419, 31940, 416, 6059, 11, 572, 369, 419, 31940, 416, 630, 406, 11, 21075, 369, 419, 31940, 416, 274, 5509, 2002, 748, 69, 302, 1538, 11, 368, 369, 419, 31940, 51642], "temperature": 0.0, "avg_logprob": -0.2190105142727704, "compression_ratio": 1.7664233576642336, "no_speech_prob": 0.05818229168653488}, {"id": 295, "seek": 193344, "start": 1933.44, "end": 1938.76, "text": " con ala, podr\u00eda ser solamente con la y el a que no est\u00e1 alineado nada. Green se alinea con verde", "tokens": [50364, 416, 419, 64, 11, 27246, 816, 27814, 416, 635, 288, 806, 257, 631, 572, 3192, 419, 533, 1573, 8096, 13, 6969, 369, 419, 31940, 416, 29653, 50630], "temperature": 0.0, "avg_logprob": -0.2234765950915883, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.08371791988611221}, {"id": 296, "seek": 193344, "start": 1938.76, "end": 1945.04, "text": " y bruja con wedge. \u00bfQu\u00e9 diferencia tiene esto con la otra alineaci\u00f3n que hab\u00edamos visto hoy?", "tokens": [50630, 288, 25267, 2938, 416, 34530, 13, 3841, 15137, 38844, 7066, 7433, 416, 635, 13623, 419, 533, 3482, 631, 3025, 16275, 17558, 13775, 30, 50944], "temperature": 0.0, "avg_logprob": -0.2234765950915883, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.08371791988611221}, {"id": 297, "seek": 193344, "start": 1946.4, "end": 1950.6000000000001, "text": " A ver si se les ocurre algo distinto que tiene esta alineaci\u00f3n y la que hab\u00edamos visto hoy.", "tokens": [51012, 316, 1306, 1511, 369, 1512, 26430, 265, 8655, 1483, 17246, 631, 7066, 5283, 419, 533, 3482, 288, 635, 631, 3025, 16275, 17558, 13775, 13, 51222], "temperature": 0.0, "avg_logprob": -0.2234765950915883, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.08371791988611221}, {"id": 298, "seek": 193344, "start": 1954.8, "end": 1959.8, "text": " Era not con no, s\u00ed. \u00bfY qu\u00e9 es lo que cambia ac\u00e1 para que pase eso?", "tokens": [51432, 23071, 406, 416, 572, 11, 8600, 13, 3841, 56, 8057, 785, 450, 631, 18751, 654, 23496, 1690, 631, 47125, 7287, 30, 51682], "temperature": 0.0, "avg_logprob": -0.2234765950915883, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.08371791988611221}, {"id": 299, "seek": 196344, "start": 1963.44, "end": 1971.72, "text": " Lo que estaba pasando hoy era que yo part\u00eda de las palabras en espa\u00f1ol,", "tokens": [50364, 6130, 631, 17544, 45412, 13775, 4249, 631, 5290, 644, 2686, 368, 2439, 35240, 465, 31177, 11, 50778], "temperature": 0.0, "avg_logprob": -0.15183743859967616, "compression_ratio": 2.008968609865471, "no_speech_prob": 0.0009341212571598589}, {"id": 300, "seek": 196344, "start": 1971.72, "end": 1975.0800000000002, "text": " iba las palabras en ingl\u00e9s y yo ten\u00eda una funci\u00f3n que me mapeaba las palabras en", "tokens": [50778, 33423, 2439, 35240, 465, 49766, 288, 5290, 23718, 2002, 43735, 631, 385, 463, 494, 5509, 2439, 35240, 465, 50946], "temperature": 0.0, "avg_logprob": -0.15183743859967616, "compression_ratio": 2.008968609865471, "no_speech_prob": 0.0009341212571598589}, {"id": 301, "seek": 196344, "start": 1975.0800000000002, "end": 1979.0, "text": " espa\u00f1ol con las palabras en ingl\u00e9s. Entonces yo a cada palabra en espa\u00f1ol como m\u00e1ximo le", "tokens": [50946, 31177, 416, 2439, 35240, 465, 49766, 13, 15097, 5290, 257, 8411, 31702, 465, 31177, 2617, 38876, 476, 51142], "temperature": 0.0, "avg_logprob": -0.15183743859967616, "compression_ratio": 2.008968609865471, "no_speech_prob": 0.0009341212571598589}, {"id": 302, "seek": 196344, "start": 1979.0, "end": 1983.72, "text": " pod\u00eda hacer corresponder una palabra en ingl\u00e9s. Entonces me quedaba que yo pod\u00eda expresar cosas", "tokens": [51142, 45588, 6720, 6805, 260, 2002, 31702, 465, 49766, 13, 15097, 385, 13617, 5509, 631, 5290, 45588, 33397, 289, 12218, 51378], "temperature": 0.0, "avg_logprob": -0.15183743859967616, "compression_ratio": 2.008968609865471, "no_speech_prob": 0.0009341212571598589}, {"id": 303, "seek": 196344, "start": 1983.72, "end": 1988.76, "text": " como que daba una bofetada, daba, est\u00e1 asociado a slap, una est\u00e1 asociado a slap, bofetada est\u00e1", "tokens": [51378, 2617, 631, 274, 5509, 2002, 748, 69, 302, 1538, 11, 274, 5509, 11, 3192, 382, 78, 537, 1573, 257, 21075, 11, 2002, 3192, 382, 78, 537, 1573, 257, 21075, 11, 748, 69, 302, 1538, 3192, 51630], "temperature": 0.0, "avg_logprob": -0.15183743859967616, "compression_ratio": 2.008968609865471, "no_speech_prob": 0.0009341212571598589}, {"id": 304, "seek": 198876, "start": 1988.76, "end": 1993.84, "text": " asociado a slap, eso lo pod\u00eda expresar, pero no pod\u00eda expresar algo como esto, que no est\u00e1", "tokens": [50364, 382, 78, 537, 1573, 257, 21075, 11, 7287, 450, 45588, 33397, 289, 11, 4768, 572, 45588, 33397, 289, 8655, 2617, 7433, 11, 631, 572, 3192, 50618], "temperature": 0.0, "avg_logprob": -0.13853841883535603, "compression_ratio": 1.9330708661417322, "no_speech_prob": 0.03313155844807625}, {"id": 305, "seek": 198876, "start": 1993.84, "end": 1998.36, "text": " asociado did not, porque no ser\u00eda una funci\u00f3n. Yo no puedo asociar uno de los valores de la funci\u00f3n", "tokens": [50618, 382, 78, 537, 1573, 630, 406, 11, 4021, 572, 23679, 2002, 43735, 13, 7616, 572, 21612, 382, 78, 537, 289, 8526, 368, 1750, 38790, 368, 635, 43735, 50844], "temperature": 0.0, "avg_logprob": -0.13853841883535603, "compression_ratio": 1.9330708661417322, "no_speech_prob": 0.03313155844807625}, {"id": 306, "seek": 198876, "start": 1998.36, "end": 2005.48, "text": " con dos cosas del lado del codominio. Y ac\u00e1 en realidad no puedo hacerlo ni en este sentido ni", "tokens": [50844, 416, 4491, 12218, 1103, 11631, 1103, 17656, 6981, 1004, 13, 398, 23496, 465, 25635, 572, 21612, 32039, 3867, 465, 4065, 19850, 3867, 51200], "temperature": 0.0, "avg_logprob": -0.13853841883535603, "compression_ratio": 1.9330708661417322, "no_speech_prob": 0.03313155844807625}, {"id": 307, "seek": 198876, "start": 2005.48, "end": 2009.0, "text": " en el otro sentido, con una funci\u00f3n no me sirve porque de vuelta me pasa que slap est\u00e1 asociado", "tokens": [51200, 465, 806, 11921, 19850, 11, 416, 2002, 43735, 572, 385, 4735, 303, 4021, 368, 41542, 385, 20260, 631, 21075, 3192, 382, 78, 537, 1573, 51376], "temperature": 0.0, "avg_logprob": -0.13853841883535603, "compression_ratio": 1.9330708661417322, "no_speech_prob": 0.03313155844807625}, {"id": 308, "seek": 198876, "start": 2009.0, "end": 2014.8799999999999, "text": " tres cosas. Entonces con una funci\u00f3n de alineaci\u00f3n yo no puedo construir este tipo de expresiones,", "tokens": [51376, 15890, 12218, 13, 15097, 416, 2002, 43735, 368, 419, 533, 3482, 5290, 572, 21612, 38445, 4065, 9746, 368, 33397, 5411, 11, 51670], "temperature": 0.0, "avg_logprob": -0.13853841883535603, "compression_ratio": 1.9330708661417322, "no_speech_prob": 0.03313155844807625}, {"id": 309, "seek": 201488, "start": 2014.88, "end": 2020.7600000000002, "text": " en realidad necesito algo como un poco m\u00e1s poderoso. Esto es lo que dec\u00edamos, los modelos", "tokens": [50364, 465, 25635, 11909, 3528, 8655, 2617, 517, 10639, 3573, 8152, 9869, 13, 20880, 785, 450, 631, 979, 16275, 11, 1750, 2316, 329, 50658], "temperature": 0.0, "avg_logprob": -0.13933541392552035, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.03573231026530266}, {"id": 310, "seek": 201488, "start": 2020.7600000000002, "end": 2024.92, "text": " de IBM siempre usan un mapeo de uno a muchos, usan una funci\u00f3n de alineaci\u00f3n, mapeo uno a muchos,", "tokens": [50658, 368, 23487, 12758, 505, 282, 517, 463, 494, 78, 368, 8526, 257, 17061, 11, 505, 282, 2002, 43735, 368, 419, 533, 3482, 11, 463, 494, 78, 8526, 257, 17061, 11, 50866], "temperature": 0.0, "avg_logprob": -0.13933541392552035, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.03573231026530266}, {"id": 311, "seek": 201488, "start": 2024.92, "end": 2029.4, "text": " pero en realidad lo que necesito para poder capturar realmente c\u00f3mo funciona en el lenguaje es", "tokens": [50866, 4768, 465, 25635, 450, 631, 11909, 3528, 1690, 8152, 3770, 28586, 14446, 12826, 26210, 465, 806, 35044, 84, 11153, 785, 51090], "temperature": 0.0, "avg_logprob": -0.13933541392552035, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.03573231026530266}, {"id": 312, "seek": 201488, "start": 2029.4, "end": 2033.0800000000002, "text": " mapeo de muchos a muchos. Yo voy a tener que un conjunto de palabras se va a traducir en otro", "tokens": [51090, 463, 494, 78, 368, 17061, 257, 17061, 13, 7616, 7552, 257, 11640, 631, 517, 37776, 368, 35240, 369, 2773, 257, 2479, 1311, 347, 465, 11921, 51274], "temperature": 0.0, "avg_logprob": -0.13933541392552035, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.03573231026530266}, {"id": 313, "seek": 201488, "start": 2033.0800000000002, "end": 2038.48, "text": " conjunto de palabras. En definitiva lo que pasa es que peque\u00f1as frases se traducen como otras", "tokens": [51274, 37776, 368, 35240, 13, 2193, 28781, 5931, 450, 631, 20260, 785, 631, 19132, 32448, 431, 1957, 369, 2479, 1311, 268, 2617, 20244, 51544], "temperature": 0.0, "avg_logprob": -0.13933541392552035, "compression_ratio": 1.7894736842105263, "no_speech_prob": 0.03573231026530266}, {"id": 314, "seek": 203848, "start": 2038.48, "end": 2044.96, "text": " peque\u00f1as frases, por eso necesito un mapeo de muchos a muchos. Entonces bueno hay algoritmos que", "tokens": [50364, 19132, 32448, 431, 1957, 11, 1515, 7287, 11909, 3528, 517, 463, 494, 78, 368, 17061, 257, 17061, 13, 15097, 11974, 4842, 3501, 50017, 3415, 631, 50688], "temperature": 0.0, "avg_logprob": -0.1691941636981386, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.14428366720676422}, {"id": 315, "seek": 203848, "start": 2044.96, "end": 2051.28, "text": " agarran estos mapeos que como construimos reci\u00e9n, el mapeo de uno a muchos en las dos", "tokens": [50688, 623, 2284, 282, 12585, 463, 494, 329, 631, 2617, 12946, 8372, 4214, 3516, 11, 806, 463, 494, 78, 368, 8526, 257, 17061, 465, 2439, 4491, 51004], "temperature": 0.0, "avg_logprob": -0.1691941636981386, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.14428366720676422}, {"id": 316, "seek": 203848, "start": 2051.28, "end": 2055.52, "text": " direcciones digamos y a partir de eso construyen este mapeo de muchos a muchos. Por ejemplo,", "tokens": [51004, 1264, 35560, 36430, 288, 257, 13906, 368, 7287, 12946, 16580, 4065, 463, 494, 78, 368, 17061, 257, 17061, 13, 5269, 13358, 11, 51216], "temperature": 0.0, "avg_logprob": -0.1691941636981386, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.14428366720676422}, {"id": 317, "seek": 203848, "start": 2055.52, "end": 2059.96, "text": " el algoritmo de la herramienta quiz\u00e1 m\u00e1s m\u00e1s, lo que hace es decir bueno yo tengo un corpus en", "tokens": [51216, 806, 3501, 50017, 3280, 368, 635, 38271, 64, 15450, 842, 3573, 3573, 11, 450, 631, 10032, 785, 10235, 11974, 5290, 13989, 517, 1181, 31624, 465, 51438], "temperature": 0.0, "avg_logprob": -0.1691941636981386, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.14428366720676422}, {"id": 318, "seek": 203848, "start": 2059.96, "end": 2066.84, "text": " ingl\u00e9s y en espa\u00f1ol alineo utilizando los los modelos de IBM digamos voy alineo por un lado", "tokens": [51438, 49766, 288, 465, 31177, 419, 533, 78, 19906, 1806, 1750, 1750, 2316, 329, 368, 23487, 36430, 7552, 419, 533, 78, 1515, 517, 11631, 51782], "temperature": 0.0, "avg_logprob": -0.1691941636981386, "compression_ratio": 1.7434944237918215, "no_speech_prob": 0.14428366720676422}, {"id": 319, "seek": 206684, "start": 2066.84, "end": 2071.88, "text": " de ingl\u00e9s espa\u00f1ol y por otro lado de espa\u00f1ol ingl\u00e9s y ac\u00e1 me quedan dos mapeos de uno a", "tokens": [50364, 368, 49766, 31177, 288, 1515, 11921, 11631, 368, 31177, 49766, 288, 23496, 385, 13617, 282, 4491, 463, 494, 329, 368, 8526, 257, 50616], "temperature": 0.0, "avg_logprob": -0.15032099938207819, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.019759496673941612}, {"id": 320, "seek": 206684, "start": 2071.88, "end": 2077.04, "text": " n digamos dos mapeos con funciones y despu\u00e9s lo que hago es intersectar esos dos esas dos", "tokens": [50616, 297, 36430, 4491, 463, 494, 329, 416, 1019, 23469, 288, 15283, 450, 631, 38721, 785, 27815, 289, 22411, 4491, 23388, 4491, 50874], "temperature": 0.0, "avg_logprob": -0.15032099938207819, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.019759496673941612}, {"id": 321, "seek": 206684, "start": 2077.04, "end": 2083.0, "text": " alineaciones que me quedaron y unirlas. Cuando las intersecto obtengo lo que se conoce como", "tokens": [50874, 419, 533, 9188, 631, 385, 13617, 6372, 288, 517, 1648, 296, 13, 21907, 2439, 27815, 78, 28326, 1571, 450, 631, 369, 33029, 384, 2617, 51172], "temperature": 0.0, "avg_logprob": -0.15032099938207819, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.019759496673941612}, {"id": 322, "seek": 206684, "start": 2083.0, "end": 2090.6800000000003, "text": " puntos de alta confianza, los puntos negros son los puntos de alta confianza que son los de la", "tokens": [51172, 34375, 368, 26495, 49081, 2394, 11, 1750, 34375, 408, 861, 329, 1872, 1750, 34375, 368, 26495, 49081, 2394, 631, 1872, 1750, 368, 635, 51556], "temperature": 0.0, "avg_logprob": -0.15032099938207819, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.019759496673941612}, {"id": 323, "seek": 206684, "start": 2090.6800000000003, "end": 2095.7000000000003, "text": " intersecci\u00f3n y los puntos grises son los que est\u00e1n en la uni\u00f3n, o sea los que pertenec\u00edan a algunos", "tokens": [51556, 728, 8159, 5687, 288, 1750, 34375, 677, 3598, 1872, 1750, 631, 10368, 465, 635, 517, 2560, 11, 277, 4158, 1750, 631, 680, 1147, 3045, 11084, 257, 21078, 51807], "temperature": 0.0, "avg_logprob": -0.15032099938207819, "compression_ratio": 1.9112903225806452, "no_speech_prob": 0.019759496673941612}, {"id": 324, "seek": 209570, "start": 2095.7, "end": 2099.02, "text": " de los dos modelos. Entonces la herramienta de lo que hace es decir bueno una vez que yo tengo la", "tokens": [50364, 368, 1750, 4491, 2316, 329, 13, 15097, 635, 38271, 64, 368, 450, 631, 10032, 785, 10235, 11974, 2002, 5715, 631, 5290, 13989, 635, 50530], "temperature": 0.0, "avg_logprob": -0.15118080377578735, "compression_ratio": 1.7615658362989324, "no_speech_prob": 0.004582867957651615}, {"id": 325, "seek": 209570, "start": 2099.02, "end": 2104.5, "text": " intersecci\u00f3n y la uni\u00f3n hago crecer los puntos que est\u00e1n en la intersecci\u00f3n, colonizando otros", "tokens": [50530, 728, 8159, 5687, 288, 635, 517, 2560, 38721, 1197, 1776, 1750, 34375, 631, 10368, 465, 635, 728, 8159, 5687, 11, 8255, 590, 1806, 16422, 50804], "temperature": 0.0, "avg_logprob": -0.15118080377578735, "compression_ratio": 1.7615658362989324, "no_speech_prob": 0.004582867957651615}, {"id": 326, "seek": 209570, "start": 2104.5, "end": 2108.74, "text": " puntos que est\u00e9n en la uni\u00f3n, hasta que al final termino completando digamos toda la imagen. Este", "tokens": [50804, 34375, 631, 871, 3516, 465, 635, 517, 2560, 11, 10764, 631, 419, 2572, 1433, 2982, 1557, 1806, 36430, 11687, 635, 40652, 13, 16105, 51016], "temperature": 0.0, "avg_logprob": -0.15118080377578735, "compression_ratio": 1.7615658362989324, "no_speech_prob": 0.004582867957651615}, {"id": 327, "seek": 209570, "start": 2108.74, "end": 2114.3199999999997, "text": " punto que qued\u00f3 solito ah\u00ed ese no ser\u00eda parte de la alineaci\u00f3n al final, s\u00f3lo los que pod\u00e9is", "tokens": [51016, 14326, 631, 13617, 812, 1404, 3528, 12571, 10167, 572, 23679, 6975, 368, 635, 419, 533, 3482, 419, 2572, 11, 22885, 1750, 631, 45728, 51295], "temperature": 0.0, "avg_logprob": -0.15118080377578735, "compression_ratio": 1.7615658362989324, "no_speech_prob": 0.004582867957651615}, {"id": 328, "seek": 209570, "start": 2114.3199999999997, "end": 2123.16, "text": " llegar movi\u00e9ndote a trav\u00e9s de puntos ya conocidos. Entonces bueno eso es una forma que utiliza se", "tokens": [51295, 24892, 2402, 72, 34577, 1370, 257, 24463, 368, 34375, 2478, 15871, 7895, 13, 15097, 11974, 7287, 785, 2002, 8366, 631, 4976, 13427, 369, 51737], "temperature": 0.0, "avg_logprob": -0.15118080377578735, "compression_ratio": 1.7615658362989324, "no_speech_prob": 0.004582867957651615}, {"id": 329, "seek": 212316, "start": 2123.16, "end": 2128.96, "text": " llama el algoritmo de Oginey que partiendo alineaciones unidireccionales digamos me permite", "tokens": [50364, 23272, 806, 3501, 50017, 3280, 368, 422, 1494, 2030, 631, 644, 7304, 419, 533, 9188, 517, 327, 621, 1914, 1966, 279, 36430, 385, 31105, 50654], "temperature": 0.0, "avg_logprob": -0.17416617075602214, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.07413550466299057}, {"id": 330, "seek": 212316, "start": 2128.96, "end": 2134.64, "text": " construir una alineaci\u00f3n completa muchos a muchos entre las palabras. Bien eso le quer\u00eda mencionar", "tokens": [50654, 38445, 2002, 419, 533, 3482, 46822, 17061, 257, 17061, 3962, 2439, 35240, 13, 16956, 7287, 476, 37869, 37030, 289, 50938], "temperature": 0.0, "avg_logprob": -0.17416617075602214, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.07413550466299057}, {"id": 331, "seek": 212316, "start": 2134.64, "end": 2138.7999999999997, "text": " acerca de las alineaciones entre palabras y ahora s\u00ed vamos a ver c\u00f3mo funciona un modelo", "tokens": [50938, 46321, 368, 2439, 419, 533, 9188, 3962, 35240, 288, 9923, 8600, 5295, 257, 1306, 12826, 26210, 517, 27825, 51146], "temperature": 0.0, "avg_logprob": -0.17416617075602214, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.07413550466299057}, {"id": 332, "seek": 212316, "start": 2138.7999999999997, "end": 2145.12, "text": " basado en frases. Un modelo basado en frases tiene cierta semejanza con el modelo anterior que", "tokens": [51146, 987, 1573, 465, 431, 1957, 13, 1156, 27825, 987, 1573, 465, 431, 1957, 7066, 39769, 1328, 369, 1398, 14763, 2394, 416, 806, 27825, 22272, 631, 51462], "temperature": 0.0, "avg_logprob": -0.17416617075602214, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.07413550466299057}, {"id": 333, "seek": 212316, "start": 2145.12, "end": 2149.8399999999997, "text": " hab\u00edamos visto pero es un poco m\u00e1s expresivo en realidad yo parto de una oraci\u00f3n por ejemplo en", "tokens": [51462, 3025, 16275, 17558, 4768, 785, 517, 10639, 3573, 33397, 6340, 465, 25635, 5290, 644, 78, 368, 2002, 420, 3482, 1515, 13358, 465, 51698], "temperature": 0.0, "avg_logprob": -0.17416617075602214, "compression_ratio": 1.7282608695652173, "no_speech_prob": 0.07413550466299057}, {"id": 334, "seek": 214984, "start": 2149.84, "end": 2154.92, "text": " alem\u00e1n que dec\u00eda Morgan Fligge y Gnaskana de la Sur Conference. Lo primero que hace el modelo", "tokens": [50364, 6775, 76, 7200, 631, 37599, 16724, 3235, 328, 432, 288, 460, 77, 3863, 2095, 368, 635, 6732, 22131, 13, 6130, 21289, 631, 10032, 806, 27825, 50618], "temperature": 0.0, "avg_logprob": -0.1631604941787234, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.055014919489622116}, {"id": 335, "seek": 214984, "start": 2154.92, "end": 2159.92, "text": " cuando quiere traducir digamos en este caso es decir bueno yo voy a segmentar esa oraci\u00f3n de", "tokens": [50618, 7767, 23877, 2479, 1311, 347, 36430, 465, 4065, 9666, 785, 10235, 11974, 5290, 7552, 257, 9469, 289, 11342, 420, 3482, 368, 50868], "temperature": 0.0, "avg_logprob": -0.1631604941787234, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.055014919489622116}, {"id": 336, "seek": 214984, "start": 2159.92, "end": 2165.76, "text": " origen en cierta cantidad de frases. Despu\u00e9s voy a traducir cada una de esas frases usando una", "tokens": [50868, 2349, 268, 465, 39769, 1328, 33757, 368, 431, 1957, 13, 40995, 7552, 257, 2479, 1311, 347, 8411, 2002, 368, 23388, 431, 1957, 29798, 2002, 51160], "temperature": 0.0, "avg_logprob": -0.1631604941787234, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.055014919489622116}, {"id": 337, "seek": 214984, "start": 2165.76, "end": 2169.28, "text": " tabla de traducci\u00f3n y esta vez no es una tabla de traducci\u00f3n de palabras sino que es una tabla", "tokens": [51160, 4421, 875, 368, 2479, 1311, 5687, 288, 5283, 5715, 572, 785, 2002, 4421, 875, 368, 2479, 1311, 5687, 368, 35240, 18108, 631, 785, 2002, 4421, 875, 51336], "temperature": 0.0, "avg_logprob": -0.1631604941787234, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.055014919489622116}, {"id": 338, "seek": 214984, "start": 2169.28, "end": 2175.04, "text": " de traducci\u00f3n de frases que me dice para ac\u00e1 frases con que otra frase se corresponde y una vez", "tokens": [51336, 368, 2479, 1311, 5687, 368, 431, 1957, 631, 385, 10313, 1690, 23496, 431, 1957, 416, 631, 13623, 38406, 369, 6805, 68, 288, 2002, 5715, 51624], "temperature": 0.0, "avg_logprob": -0.1631604941787234, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.055014919489622116}, {"id": 339, "seek": 214984, "start": 2175.04, "end": 2179.6400000000003, "text": " que yo traduje cada una de esas frases las voy a reordenar de alguna manera buscando que suene", "tokens": [51624, 631, 5290, 2479, 13008, 8411, 2002, 368, 23388, 431, 1957, 2439, 7552, 257, 319, 19058, 289, 368, 20651, 13913, 46804, 631, 459, 1450, 51854], "temperature": 0.0, "avg_logprob": -0.1631604941787234, "compression_ratio": 1.903973509933775, "no_speech_prob": 0.055014919489622116}, {"id": 340, "seek": 217964, "start": 2179.64, "end": 2185.08, "text": " lo m\u00e1s natural posible buscando aumentar la fluidez de esa oraci\u00f3n. Entonces como que la", "tokens": [50364, 450, 3573, 3303, 26644, 46804, 43504, 635, 5029, 45170, 368, 11342, 420, 3482, 13, 15097, 2617, 631, 635, 50636], "temperature": 0.0, "avg_logprob": -0.12924714660644532, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.0013564868131652474}, {"id": 341, "seek": 217964, "start": 2185.08, "end": 2188.7599999999998, "text": " historia de generaci\u00f3n es un poco m\u00e1s simple que la otra no ten\u00eda que ir sorteando cosas simplemente", "tokens": [50636, 18385, 368, 1337, 3482, 785, 517, 10639, 3573, 2199, 631, 635, 13623, 572, 23718, 631, 3418, 25559, 1806, 12218, 33190, 50820], "temperature": 0.0, "avg_logprob": -0.12924714660644532, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.0013564868131652474}, {"id": 342, "seek": 217964, "start": 2188.7599999999998, "end": 2196.56, "text": " digo separo mi oraci\u00f3n en segmentos que les voy a llamar frases los traduzco y los reordeno.", "tokens": [50820, 22990, 3128, 78, 2752, 420, 3482, 465, 9469, 329, 631, 1512, 7552, 257, 16848, 289, 431, 1957, 1750, 2479, 3334, 1291, 288, 1750, 319, 19058, 78, 13, 51210], "temperature": 0.0, "avg_logprob": -0.12924714660644532, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.0013564868131652474}, {"id": 343, "seek": 217964, "start": 2196.56, "end": 2203.16, "text": " Esa segmentaci\u00f3n en frases no tiene porque tener un significado ling\u00fc\u00edstico yo no voy a separarlas", "tokens": [51210, 2313, 64, 9469, 3482, 465, 431, 1957, 572, 7066, 4021, 11640, 517, 3350, 1573, 22949, 774, 19512, 2789, 5290, 572, 7552, 257, 3128, 6843, 296, 51540], "temperature": 0.0, "avg_logprob": -0.12924714660644532, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.0013564868131652474}, {"id": 344, "seek": 217964, "start": 2203.16, "end": 2208.24, "text": " en grupo nominal, grupo verbal, grupo profesional, etc\u00e9tera. No tengo por qu\u00e9 o sea capaz que yo", "tokens": [51540, 465, 20190, 41641, 11, 20190, 24781, 11, 20190, 42882, 11, 5183, 526, 23833, 13, 883, 13989, 1515, 8057, 277, 4158, 35453, 631, 5290, 51794], "temperature": 0.0, "avg_logprob": -0.12924714660644532, "compression_ratio": 1.7279151943462898, "no_speech_prob": 0.0013564868131652474}, {"id": 345, "seek": 220824, "start": 2208.24, "end": 2213.2, "text": " segmento las frases y justo me queda un grupo preposicional capaz que no. Lo \u00fanico que tiene que", "tokens": [50364, 9469, 78, 2439, 431, 1957, 288, 40534, 385, 23314, 517, 20190, 2666, 329, 33010, 35453, 631, 572, 13, 6130, 26113, 631, 7066, 631, 50612], "temperature": 0.0, "avg_logprob": -0.13414160673283348, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.03629232570528984}, {"id": 346, "seek": 220824, "start": 2213.2, "end": 2217.7999999999997, "text": " pasar es que estos segmentos que yo construyo tienen que estar en mi tabla de traducci\u00f3n de frases", "tokens": [50612, 25344, 785, 631, 12585, 9469, 329, 631, 5290, 12946, 8308, 12536, 631, 8755, 465, 2752, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 50842], "temperature": 0.0, "avg_logprob": -0.13414160673283348, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.03629232570528984}, {"id": 347, "seek": 220824, "start": 2217.7999999999997, "end": 2222.08, "text": " alcanza con eso como para que yo pueda utilizarlos en mi traducci\u00f3n pero no tienen por qu\u00e9 tener", "tokens": [50842, 419, 7035, 2394, 416, 7287, 2617, 1690, 631, 5290, 31907, 19906, 39734, 465, 2752, 2479, 1311, 5687, 4768, 572, 12536, 1515, 8057, 11640, 51056], "temperature": 0.0, "avg_logprob": -0.13414160673283348, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.03629232570528984}, {"id": 348, "seek": 220824, "start": 2222.08, "end": 2229.68, "text": " una motivaci\u00f3n ling\u00fc\u00edstica. Bueno entonces un modelo basado en frases tiene estos componentes", "tokens": [51056, 2002, 5426, 3482, 22949, 774, 19512, 2262, 13, 16046, 13003, 517, 27825, 987, 1573, 465, 431, 1957, 7066, 12585, 6542, 279, 51436], "temperature": 0.0, "avg_logprob": -0.13414160673283348, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.03629232570528984}, {"id": 349, "seek": 220824, "start": 2229.68, "end": 2234.3999999999996, "text": " parecido al anterior porque de vuelta yo lo que quiero hacer es encontrar la probabilidad de", "tokens": [51436, 7448, 17994, 419, 22272, 4021, 368, 41542, 5290, 450, 631, 16811, 6720, 785, 17525, 635, 31959, 4580, 368, 51672], "temperature": 0.0, "avg_logprob": -0.13414160673283348, "compression_ratio": 1.7173144876325088, "no_speech_prob": 0.03629232570528984}, {"id": 350, "seek": 223440, "start": 2234.4, "end": 2240.2000000000003, "text": " pdf dado e digamos sigo teniendo la misma ecuaci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica estad\u00edstica", "tokens": [50364, 280, 45953, 29568, 308, 36430, 4556, 78, 2064, 7304, 635, 24946, 11437, 84, 3482, 8088, 368, 635, 2479, 1311, 5687, 3553, 23432, 39160, 19512, 2262, 50654], "temperature": 0.0, "avg_logprob": -0.15072413853236608, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.3201821446418762}, {"id": 351, "seek": 223440, "start": 2240.2000000000003, "end": 2246.2400000000002, "text": " la quiero resolver necesito pdf dado e y pdf solo que ahora el pdf dado e lo voy a calcular de una", "tokens": [50654, 635, 16811, 34480, 11909, 3528, 280, 45953, 29568, 308, 288, 280, 45953, 6944, 631, 9923, 806, 280, 45953, 29568, 308, 450, 7552, 257, 2104, 17792, 368, 2002, 50956], "temperature": 0.0, "avg_logprob": -0.15072413853236608, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.3201821446418762}, {"id": 352, "seek": 223440, "start": 2246.2400000000002, "end": 2250.96, "text": " manera distinta voy a decir que para calcular esto tengo un modelo de traducci\u00f3n de frases y un modelo", "tokens": [50956, 13913, 1483, 16071, 7552, 257, 10235, 631, 1690, 2104, 17792, 7433, 13989, 517, 27825, 368, 2479, 1311, 5687, 368, 431, 1957, 288, 517, 27825, 51192], "temperature": 0.0, "avg_logprob": -0.15072413853236608, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.3201821446418762}, {"id": 353, "seek": 223440, "start": 2250.96, "end": 2255.6, "text": " de reordenamiento un modelo de una gran tabla de frases que me dice cada frase con qu\u00e9 probabilidad", "tokens": [51192, 368, 319, 19058, 16971, 517, 27825, 368, 2002, 9370, 4421, 875, 368, 431, 1957, 631, 385, 10313, 8411, 38406, 416, 8057, 31959, 4580, 51424], "temperature": 0.0, "avg_logprob": -0.15072413853236608, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.3201821446418762}, {"id": 354, "seek": 223440, "start": 2255.6, "end": 2261.2400000000002, "text": " la traduzco en otra y despu\u00e9s una forma de decir c\u00f3mo reordeno esas frases para tener mejores", "tokens": [51424, 635, 2479, 3334, 1291, 465, 13623, 288, 15283, 2002, 8366, 368, 10235, 12826, 319, 19058, 78, 23388, 431, 1957, 1690, 11640, 42284, 51706], "temperature": 0.0, "avg_logprob": -0.15072413853236608, "compression_ratio": 1.874074074074074, "no_speech_prob": 0.3201821446418762}, {"id": 355, "seek": 226124, "start": 2261.24, "end": 2267.72, "text": " oraciones y bueno y como siempre voy a tener otro componente que es el que mide la la fluidez que es", "tokens": [50364, 420, 9188, 288, 11974, 288, 2617, 12758, 7552, 257, 11640, 11921, 4026, 1576, 631, 785, 806, 631, 275, 482, 635, 635, 5029, 45170, 631, 785, 50688], "temperature": 0.0, "avg_logprob": -0.13534655030240716, "compression_ratio": 1.7953488372093023, "no_speech_prob": 0.009832375682890415}, {"id": 356, "seek": 226124, "start": 2267.72, "end": 2273.9199999999996, "text": " el modelo del lenguaje porque los modelos de frases funcionan mejor que los modelos basados en", "tokens": [50688, 806, 27825, 1103, 35044, 84, 11153, 4021, 1750, 2316, 329, 368, 431, 1957, 14186, 282, 11479, 631, 1750, 2316, 329, 987, 4181, 465, 50998], "temperature": 0.0, "avg_logprob": -0.13534655030240716, "compression_ratio": 1.7953488372093023, "no_speech_prob": 0.009832375682890415}, {"id": 357, "seek": 226124, "start": 2273.9199999999996, "end": 2280.2799999999997, "text": " palabras porque la frase ya tiene cierto contexto la frases en realidad son como peque\u00f1os grupos de", "tokens": [50998, 35240, 4021, 635, 38406, 2478, 7066, 28558, 47685, 635, 431, 1957, 465, 25635, 1872, 2617, 19132, 8242, 33758, 368, 51316], "temperature": 0.0, "avg_logprob": -0.13534655030240716, "compression_ratio": 1.7953488372093023, "no_speech_prob": 0.009832375682890415}, {"id": 358, "seek": 226124, "start": 2280.2799999999997, "end": 2288.04, "text": " palabras que yo puedo traducir uno uno en el otro entonces cosas como dar la mano dar una", "tokens": [51316, 35240, 631, 5290, 21612, 2479, 1311, 347, 8526, 8526, 465, 806, 11921, 13003, 12218, 2617, 4072, 635, 18384, 4072, 2002, 51704], "temperature": 0.0, "avg_logprob": -0.13534655030240716, "compression_ratio": 1.7953488372093023, "no_speech_prob": 0.009832375682890415}, {"id": 359, "seek": 228804, "start": 2288.04, "end": 2292.72, "text": " bofetada a tomar el pelo etc\u00e9tera todas esas cosas como expresiones son mucho m\u00e1s f\u00e1ciles de traducir", "tokens": [50364, 748, 69, 302, 1538, 257, 22048, 806, 12167, 5183, 526, 23833, 10906, 23388, 12218, 2617, 33397, 5411, 1872, 9824, 3573, 17474, 279, 368, 2479, 1311, 347, 50598], "temperature": 0.0, "avg_logprob": -0.17707872733795385, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.011438167653977871}, {"id": 360, "seek": 228804, "start": 2292.72, "end": 2296.36, "text": " si en realidad yo ya s\u00e9 que esta expresi\u00f3n que son tres cuatro palabras la puedo traducir en esta", "tokens": [50598, 1511, 465, 25635, 5290, 2478, 7910, 631, 5283, 33397, 2560, 631, 1872, 15890, 28795, 35240, 635, 21612, 2479, 1311, 347, 465, 5283, 50780], "temperature": 0.0, "avg_logprob": -0.17707872733795385, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.011438167653977871}, {"id": 361, "seek": 228804, "start": 2296.36, "end": 2300.2, "text": " otra expresi\u00f3n que son tres cuatro palabras es como m\u00e1s expresivo entonces se puede aprender m\u00e1s", "tokens": [50780, 13623, 33397, 2560, 631, 1872, 15890, 28795, 35240, 785, 2617, 3573, 33397, 6340, 13003, 369, 8919, 24916, 3573, 50972], "temperature": 0.0, "avg_logprob": -0.17707872733795385, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.011438167653977871}, {"id": 362, "seek": 228804, "start": 2300.2, "end": 2305.44, "text": " cosas y bueno obviamente cuanto m\u00e1s cuanto m\u00e1s datos tenga cuanto m\u00e1s largo sea el cuerpo que yo", "tokens": [50972, 12218, 288, 11974, 36325, 36685, 3573, 36685, 3573, 27721, 36031, 36685, 3573, 31245, 4158, 806, 20264, 631, 5290, 51234], "temperature": 0.0, "avg_logprob": -0.17707872733795385, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.011438167653977871}, {"id": 363, "seek": 228804, "start": 2305.44, "end": 2309.4, "text": " tengo yo puedo aprender frases m\u00e1s largas mejores probabilidades y mejores frases", "tokens": [51234, 13989, 5290, 21612, 24916, 431, 1957, 3573, 1613, 10549, 42284, 31959, 10284, 288, 42284, 431, 1957, 51432], "temperature": 0.0, "avg_logprob": -0.17707872733795385, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.011438167653977871}, {"id": 364, "seek": 228804, "start": 2311.4, "end": 2315.92, "text": " bueno ac\u00e1 hay un ejemplo de c\u00f3mo ser\u00eda una tabla de traducci\u00f3n de frases o sea es parecido a la", "tokens": [51532, 11974, 23496, 4842, 517, 13358, 368, 12826, 23679, 2002, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 277, 4158, 785, 7448, 17994, 257, 635, 51758], "temperature": 0.0, "avg_logprob": -0.17707872733795385, "compression_ratio": 1.9898305084745762, "no_speech_prob": 0.011438167653977871}, {"id": 365, "seek": 231592, "start": 2315.92, "end": 2320.2400000000002, "text": " tabla de traducci\u00f3n de palabras o es lo que ac\u00e1 tengo de en borschlag o sea si yo busco la fila", "tokens": [50364, 4421, 875, 368, 2479, 1311, 5687, 368, 35240, 277, 785, 450, 631, 23496, 13989, 368, 465, 272, 830, 40869, 277, 4158, 1511, 5290, 1255, 1291, 635, 1387, 64, 50580], "temperature": 0.0, "avg_logprob": -0.21123869401695083, "compression_ratio": 1.9596774193548387, "no_speech_prob": 0.0045801252126693726}, {"id": 366, "seek": 231592, "start": 2320.2400000000002, "end": 2325.16, "text": " asociada en borschlag o sea encontrar\u00eda todas estas traducciones de prop\u00f3sal con 62 por ciento", "tokens": [50580, 382, 78, 537, 1538, 465, 272, 830, 40869, 277, 4158, 17525, 2686, 10906, 13897, 2479, 1311, 23469, 368, 2365, 12994, 304, 416, 24536, 1515, 47361, 50826], "temperature": 0.0, "avg_logprob": -0.21123869401695083, "compression_ratio": 1.9596774193548387, "no_speech_prob": 0.0045801252126693726}, {"id": 367, "seek": 231592, "start": 2325.16, "end": 2331.4, "text": " de probabilidad posesivo prop\u00f3sal con 10 por ciento a prop\u00f3sal con 3 por ciento etc\u00e9tera o", "tokens": [50826, 368, 31959, 4580, 26059, 6340, 2365, 12994, 304, 416, 1266, 1515, 47361, 257, 2365, 12994, 304, 416, 805, 1515, 47361, 5183, 526, 23833, 277, 51138], "temperature": 0.0, "avg_logprob": -0.21123869401695083, "compression_ratio": 1.9596774193548387, "no_speech_prob": 0.0045801252126693726}, {"id": 368, "seek": 231592, "start": 2331.4, "end": 2336.6800000000003, "text": " sea como ven se traducen frases en frases bueno y c\u00f3mo hago para aprender una tabla de traducci\u00f3n", "tokens": [51138, 4158, 2617, 6138, 369, 2479, 1311, 268, 431, 1957, 465, 431, 1957, 11974, 288, 12826, 38721, 1690, 24916, 2002, 4421, 875, 368, 2479, 1311, 5687, 51402], "temperature": 0.0, "avg_logprob": -0.21123869401695083, "compression_ratio": 1.9596774193548387, "no_speech_prob": 0.0045801252126693726}, {"id": 369, "seek": 231592, "start": 2336.6800000000003, "end": 2344.2400000000002, "text": " de frases yo parto de esta alineaci\u00f3n de palabras digamos esta alineaci\u00f3n completa que ya no es", "tokens": [51402, 368, 431, 1957, 5290, 644, 78, 368, 5283, 419, 533, 3482, 368, 35240, 36430, 5283, 419, 533, 3482, 46822, 631, 2478, 572, 785, 51780], "temperature": 0.0, "avg_logprob": -0.21123869401695083, "compression_ratio": 1.9596774193548387, "no_speech_prob": 0.0045801252126693726}, {"id": 370, "seek": 234424, "start": 2344.24, "end": 2349.8799999999997, "text": " una funci\u00f3n sino que es digamos una alineaci\u00f3n de muchos a muchos y voy a tratar de encontrar todos", "tokens": [50364, 2002, 43735, 18108, 631, 785, 36430, 2002, 419, 533, 3482, 368, 17061, 257, 17061, 288, 7552, 257, 42549, 368, 17525, 6321, 50646], "temperature": 0.0, "avg_logprob": -0.13698034379088764, "compression_ratio": 2.0153061224489797, "no_speech_prob": 0.01798916608095169}, {"id": 371, "seek": 234424, "start": 2349.8799999999997, "end": 2355.56, "text": " los todas las frases todos los pares de frases que son consistentes con la alineaci\u00f3n a qu\u00e9 me", "tokens": [50646, 1750, 10906, 2439, 431, 1957, 6321, 1750, 2502, 495, 368, 431, 1957, 631, 1872, 8398, 279, 416, 635, 419, 533, 3482, 257, 8057, 385, 50930], "temperature": 0.0, "avg_logprob": -0.13698034379088764, "compression_ratio": 2.0153061224489797, "no_speech_prob": 0.01798916608095169}, {"id": 372, "seek": 234424, "start": 2355.56, "end": 2364.7599999999998, "text": " refiero con que son consistentes ac\u00e1 hay ejemplos yo quiero decir que mariano y mar\u00eda did not son", "tokens": [50930, 1895, 12030, 416, 631, 1872, 8398, 279, 23496, 4842, 10012, 5895, 329, 5290, 16811, 10235, 631, 1849, 6254, 288, 1849, 2686, 630, 406, 1872, 51390], "temperature": 0.0, "avg_logprob": -0.13698034379088764, "compression_ratio": 2.0153061224489797, "no_speech_prob": 0.01798916608095169}, {"id": 373, "seek": 234424, "start": 2364.7599999999998, "end": 2370.4399999999996, "text": " es son un par de frases que son consistentes con esta alineaci\u00f3n en cambio mariano y mar\u00eda did", "tokens": [51390, 785, 1872, 517, 971, 368, 431, 1957, 631, 1872, 8398, 279, 416, 5283, 419, 533, 3482, 465, 28731, 1849, 6254, 288, 1849, 2686, 630, 51674], "temperature": 0.0, "avg_logprob": -0.13698034379088764, "compression_ratio": 2.0153061224489797, "no_speech_prob": 0.01798916608095169}, {"id": 374, "seek": 237044, "start": 2370.84, "end": 2376.2400000000002, "text": " como es que miro esto lo que pasa es que cuando yo tengo mariano y mar\u00eda did la palabra no est\u00e1", "tokens": [50384, 2617, 785, 631, 2752, 340, 7433, 450, 631, 20260, 785, 631, 7767, 5290, 13989, 1849, 6254, 288, 1849, 2686, 630, 635, 31702, 572, 3192, 50654], "temperature": 0.0, "avg_logprob": -0.20819096067058507, "compression_ratio": 2.05, "no_speech_prob": 0.16456888616085052}, {"id": 375, "seek": 237044, "start": 2376.2400000000002, "end": 2381.4, "text": " alineada con did not y el did not digamos el no no pertenece hasta alineaci\u00f3n que yo estoy", "tokens": [50654, 419, 533, 1538, 416, 630, 406, 288, 806, 630, 406, 36430, 806, 572, 572, 680, 1147, 68, 384, 10764, 419, 533, 3482, 631, 5290, 15796, 50912], "temperature": 0.0, "avg_logprob": -0.20819096067058507, "compression_ratio": 2.05, "no_speech_prob": 0.16456888616085052}, {"id": 376, "seek": 237044, "start": 2381.4, "end": 2386.7200000000003, "text": " tratando de decir entonces digo que es no consistente lo mismo pasa con si yo tato alinear mariano", "tokens": [50912, 21507, 1806, 368, 10235, 13003, 22990, 631, 785, 572, 4603, 1576, 450, 12461, 20260, 416, 1511, 5290, 256, 2513, 419, 533, 289, 1849, 6254, 51178], "temperature": 0.0, "avg_logprob": -0.20819096067058507, "compression_ratio": 2.05, "no_speech_prob": 0.16456888616085052}, {"id": 377, "seek": 237044, "start": 2386.7200000000003, "end": 2393.04, "text": " daba y mar\u00eda did not lo que pasa ah\u00ed es que daba no est\u00e1 digamos los puntos de alineaci\u00f3n de daba", "tokens": [51178, 274, 5509, 288, 1849, 2686, 630, 406, 450, 631, 20260, 12571, 785, 631, 274, 5509, 572, 3192, 36430, 1750, 34375, 368, 419, 533, 3482, 368, 274, 5509, 51494], "temperature": 0.0, "avg_logprob": -0.20819096067058507, "compression_ratio": 2.05, "no_speech_prob": 0.16456888616085052}, {"id": 378, "seek": 237044, "start": 2393.04, "end": 2396.92, "text": " no est\u00e1n dentro de este cuadrante que estoy tratando de buscar entonces en definitiva digo que no es", "tokens": [51494, 572, 10368, 10856, 368, 4065, 34434, 81, 2879, 631, 15796, 21507, 1806, 368, 26170, 13003, 465, 28781, 5931, 22990, 631, 572, 785, 51688], "temperature": 0.0, "avg_logprob": -0.20819096067058507, "compression_ratio": 2.05, "no_speech_prob": 0.16456888616085052}, {"id": 379, "seek": 239692, "start": 2396.92, "end": 2401.28, "text": " consistente las alineaciones consistentes correctas son las que consideran todos los", "tokens": [50364, 4603, 1576, 2439, 419, 533, 9188, 8398, 279, 3006, 296, 1872, 2439, 631, 1949, 282, 6321, 1750, 50582], "temperature": 0.0, "avg_logprob": -0.17905650994716546, "compression_ratio": 1.9521739130434783, "no_speech_prob": 0.009805639274418354}, {"id": 380, "seek": 239692, "start": 2401.28, "end": 2406.08, "text": " puntos dentro de ese cuadrante entonces mariano est\u00e1 asociado con mar\u00eda did not y es as\u00ed es", "tokens": [50582, 34375, 10856, 368, 10167, 34434, 81, 2879, 13003, 1849, 6254, 3192, 382, 78, 537, 1573, 416, 1849, 2686, 630, 406, 288, 785, 8582, 785, 50822], "temperature": 0.0, "avg_logprob": -0.17905650994716546, "compression_ratio": 1.9521739130434783, "no_speech_prob": 0.009805639274418354}, {"id": 381, "seek": 239692, "start": 2406.08, "end": 2416.16, "text": " consistente as\u00ed que como aprendo frases consistentes empiezo por las alineaciones digamos", "tokens": [50822, 4603, 1576, 8582, 631, 2617, 21003, 78, 431, 1957, 8398, 279, 4012, 414, 4765, 1515, 2439, 419, 533, 9188, 36430, 51326], "temperature": 0.0, "avg_logprob": -0.17905650994716546, "compression_ratio": 1.9521739130434783, "no_speech_prob": 0.009805639274418354}, {"id": 382, "seek": 239692, "start": 2416.16, "end": 2419.4, "text": " empiezo por la alineaci\u00f3n de palabra despu\u00e9s busco de una palabra y digo bueno me quedo", "tokens": [51326, 4012, 414, 4765, 1515, 635, 419, 533, 3482, 368, 31702, 15283, 1255, 1291, 368, 2002, 31702, 288, 22990, 11974, 385, 13617, 78, 51488], "temperature": 0.0, "avg_logprob": -0.17905650994716546, "compression_ratio": 1.9521739130434783, "no_speech_prob": 0.009805639274418354}, {"id": 383, "seek": 239692, "start": 2419.4, "end": 2424.32, "text": " con todas esas traducciones de palabras y las pongo en mi tabla de frases y despu\u00e9s voy", "tokens": [51488, 416, 10906, 23388, 2479, 1311, 23469, 368, 35240, 288, 2439, 280, 25729, 465, 2752, 4421, 875, 368, 431, 1957, 288, 15283, 7552, 51734], "temperature": 0.0, "avg_logprob": -0.17905650994716546, "compression_ratio": 1.9521739130434783, "no_speech_prob": 0.009805639274418354}, {"id": 384, "seek": 242432, "start": 2424.32, "end": 2428.84, "text": " tomando de a dos y me quedo con todas esas otras frases y las voy agregando mi tabla de frases", "tokens": [50364, 2916, 1806, 368, 257, 4491, 288, 385, 13617, 78, 416, 10906, 23388, 20244, 431, 1957, 288, 2439, 7552, 623, 3375, 1806, 2752, 4421, 875, 368, 431, 1957, 50590], "temperature": 0.0, "avg_logprob": -0.1883899099458524, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.04721352830529213}, {"id": 385, "seek": 242432, "start": 2428.84, "end": 2435.1200000000003, "text": " despu\u00e9s me puedo avanzar en uno y tomar de a tres tomar de a cuatro y llegar a tomar incluso", "tokens": [50590, 15283, 385, 21612, 42444, 289, 465, 8526, 288, 22048, 368, 257, 15890, 22048, 368, 257, 28795, 288, 24892, 257, 22048, 24018, 50904], "temperature": 0.0, "avg_logprob": -0.1883899099458524, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.04721352830529213}, {"id": 386, "seek": 242432, "start": 2435.1200000000003, "end": 2440.1600000000003, "text": " toda la oraci\u00f3n como frases entonces a partir de estas oraciones que ten\u00edan no s\u00e9 este 1 2 3", "tokens": [50904, 11687, 635, 420, 3482, 2617, 431, 1957, 13003, 257, 13906, 368, 13897, 420, 9188, 631, 47596, 572, 7910, 4065, 502, 568, 805, 51156], "temperature": 0.0, "avg_logprob": -0.1883899099458524, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.04721352830529213}, {"id": 387, "seek": 242432, "start": 2440.1600000000003, "end": 2446.1200000000003, "text": " 4 5 6 7 8 9 palabras yo termino aprendiendo como 17 frases digamos cada vez m\u00e1s grandes", "tokens": [51156, 1017, 1025, 1386, 1614, 1649, 1722, 35240, 5290, 1433, 2982, 21003, 7304, 2617, 3282, 431, 1957, 36430, 8411, 5715, 3573, 16640, 51454], "temperature": 0.0, "avg_logprob": -0.1883899099458524, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.04721352830529213}, {"id": 388, "seek": 242432, "start": 2447.76, "end": 2452.8, "text": " y bueno hoy voy sacando esto de todo el corpus y calculando mi tabla de probabilidades", "tokens": [51536, 288, 11974, 13775, 7552, 4899, 1806, 7433, 368, 5149, 806, 1181, 31624, 288, 4322, 1806, 2752, 4421, 875, 368, 31959, 10284, 51788], "temperature": 0.0, "avg_logprob": -0.1883899099458524, "compression_ratio": 1.660649819494585, "no_speech_prob": 0.04721352830529213}, {"id": 389, "seek": 245432, "start": 2454.36, "end": 2459.48, "text": " de qu\u00e9 manera calculo esas probabilidades yo lo que puedo hacer es como siempre ver cu\u00e1ntas", "tokens": [50366, 368, 8057, 13913, 4322, 78, 23388, 31959, 10284, 5290, 450, 631, 21612, 6720, 785, 2617, 12758, 1306, 44256, 296, 50622], "temperature": 0.0, "avg_logprob": -0.11595926529321915, "compression_ratio": 1.9149797570850202, "no_speech_prob": 0.0010103173553943634}, {"id": 390, "seek": 245432, "start": 2459.48, "end": 2465.56, "text": " veces aparece en el corpus y contar o si no si yo ten\u00eda construido el modelo anterior el modelo", "tokens": [50622, 17054, 37863, 465, 806, 1181, 31624, 288, 27045, 277, 1511, 572, 1511, 5290, 23718, 12946, 2925, 806, 27825, 22272, 806, 27825, 50926], "temperature": 0.0, "avg_logprob": -0.11595926529321915, "compression_ratio": 1.9149797570850202, "no_speech_prob": 0.0010103173553943634}, {"id": 391, "seek": 245432, "start": 2465.56, "end": 2470.4, "text": " de la tabla de traducciones de palabra a palabra en realidad lo que puedo hacer es aprovechar ese", "tokens": [50926, 368, 635, 4421, 875, 368, 2479, 1311, 23469, 368, 31702, 257, 31702, 465, 25635, 450, 631, 21612, 6720, 785, 29015, 7374, 10167, 51168], "temperature": 0.0, "avg_logprob": -0.11595926529321915, "compression_ratio": 1.9149797570850202, "no_speech_prob": 0.0010103173553943634}, {"id": 392, "seek": 245432, "start": 2470.4, "end": 2474.92, "text": " modelo de traducci\u00f3n de palabra a palabra y decir bueno me armo una traducci\u00f3n entre un par de", "tokens": [51168, 27825, 368, 2479, 1311, 5687, 368, 31702, 257, 31702, 288, 10235, 11974, 385, 594, 3280, 2002, 2479, 1311, 5687, 3962, 517, 971, 368, 51394], "temperature": 0.0, "avg_logprob": -0.11595926529321915, "compression_ratio": 1.9149797570850202, "no_speech_prob": 0.0010103173553943634}, {"id": 393, "seek": 245432, "start": 2474.92, "end": 2479.1200000000003, "text": " frases bas\u00e1ndome en las traduciones palabra a palabra son como dos formas distintas de", "tokens": [51394, 431, 1957, 987, 18606, 423, 465, 2439, 2479, 1311, 5411, 31702, 257, 31702, 1872, 2617, 4491, 33463, 31489, 296, 368, 51604], "temperature": 0.0, "avg_logprob": -0.11595926529321915, "compression_ratio": 1.9149797570850202, "no_speech_prob": 0.0010103173553943634}, {"id": 394, "seek": 247912, "start": 2479.12, "end": 2487.92, "text": " construirlo y a veces hasta complementarias bien eso fue el modelo de frases los modelos", "tokens": [50364, 38445, 752, 288, 257, 17054, 10764, 17103, 35027, 3610, 7287, 9248, 806, 27825, 368, 431, 1957, 1750, 2316, 329, 50804], "temperature": 0.0, "avg_logprob": -0.14140389276587445, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.009901730343699455}, {"id": 395, "seek": 247912, "start": 2487.92, "end": 2493.2, "text": " de frases son los m\u00e1s usados hoy en d\u00eda en realidad en lo que es la traducci\u00f3n autom\u00e1tica son los", "tokens": [50804, 368, 431, 1957, 1872, 1750, 3573, 505, 4181, 13775, 465, 12271, 465, 25635, 465, 450, 631, 785, 635, 2479, 1311, 5687, 3553, 23432, 1872, 1750, 51068], "temperature": 0.0, "avg_logprob": -0.14140389276587445, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.009901730343699455}, {"id": 396, "seek": 247912, "start": 2493.2, "end": 2499.16, "text": " que han dado mejores resultados y bueno y nos faltaba una cosa para terminar el toda la imagen de", "tokens": [51068, 631, 7276, 29568, 42284, 36796, 288, 11974, 288, 3269, 37108, 5509, 2002, 10163, 1690, 36246, 806, 11687, 635, 40652, 368, 51366], "temperature": 0.0, "avg_logprob": -0.14140389276587445, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.009901730343699455}, {"id": 397, "seek": 247912, "start": 2499.16, "end": 2502.2, "text": " lo que es la traducci\u00f3n autom\u00e1tica estad\u00edstica que es la decodificaci\u00f3n", "tokens": [51366, 450, 631, 785, 635, 2479, 1311, 5687, 3553, 23432, 39160, 19512, 2262, 631, 785, 635, 979, 378, 40802, 51518], "temperature": 0.0, "avg_logprob": -0.14140389276587445, "compression_ratio": 1.793103448275862, "no_speech_prob": 0.009901730343699455}, {"id": 398, "seek": 250220, "start": 2502.2, "end": 2508.16, "text": " entonces damos un resumen de lo que ten\u00edamos hasta ahora", "tokens": [50364, 13003, 274, 2151, 517, 725, 16988, 368, 450, 631, 2064, 16275, 10764, 9923, 50662], "temperature": 0.0, "avg_logprob": -0.17051991394587926, "compression_ratio": 1.8922413793103448, "no_speech_prob": 0.0011863989057019353}, {"id": 399, "seek": 250220, "start": 2509.3599999999997, "end": 2514.9199999999996, "text": " hasta ahora yo part\u00ed de yo quer\u00eda resolver la cocci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica", "tokens": [50722, 10764, 9923, 5290, 644, 870, 368, 5290, 37869, 34480, 635, 598, 14735, 8088, 368, 635, 2479, 1311, 5687, 3553, 23432, 51000], "temperature": 0.0, "avg_logprob": -0.17051991394587926, "compression_ratio": 1.8922413793103448, "no_speech_prob": 0.0011863989057019353}, {"id": 400, "seek": 250220, "start": 2514.9199999999996, "end": 2520.72, "text": " estad\u00edstica y yo ten\u00eda un corpus paralelo que ten\u00eda texto en el idioma origen y el idioma", "tokens": [51000, 39160, 19512, 2262, 288, 5290, 23718, 517, 1181, 31624, 26009, 10590, 631, 23718, 35503, 465, 806, 18014, 6440, 2349, 268, 288, 806, 18014, 6440, 51290], "temperature": 0.0, "avg_logprob": -0.17051991394587926, "compression_ratio": 1.8922413793103448, "no_speech_prob": 0.0011863989057019353}, {"id": 401, "seek": 250220, "start": 2520.72, "end": 2524.96, "text": " destino y a partir de ciento an\u00e1lisis estad\u00edstico yo me constru\u00ed un modelo de traducci\u00f3n que es", "tokens": [51290, 2677, 2982, 288, 257, 13906, 368, 47361, 44113, 28436, 39160, 19512, 2789, 5290, 385, 12946, 870, 517, 27825, 368, 2479, 1311, 5687, 631, 785, 51502], "temperature": 0.0, "avg_logprob": -0.17051991394587926, "compression_ratio": 1.8922413793103448, "no_speech_prob": 0.0011863989057019353}, {"id": 402, "seek": 250220, "start": 2524.96, "end": 2530.9199999999996, "text": " lo que vimos en esta clase adem\u00e1s yo ten\u00eda cierto cierta cantidad de texto en el idioma", "tokens": [51502, 450, 631, 49266, 465, 5283, 44578, 21251, 5290, 23718, 28558, 39769, 1328, 33757, 368, 35503, 465, 806, 18014, 6440, 51800], "temperature": 0.0, "avg_logprob": -0.17051991394587926, "compression_ratio": 1.8922413793103448, "no_speech_prob": 0.0011863989057019353}, {"id": 403, "seek": 253092, "start": 2530.92, "end": 2535.36, "text": " destino y a partir de cierto an\u00e1lisis estad\u00edstico me constru\u00ed un modelo de lenguaje que me dice", "tokens": [50364, 2677, 2982, 288, 257, 13906, 368, 28558, 44113, 28436, 39160, 19512, 2789, 385, 12946, 870, 517, 27825, 368, 35044, 84, 11153, 631, 385, 10313, 50586], "temperature": 0.0, "avg_logprob": -0.1014076305341117, "compression_ratio": 2.0636042402826855, "no_speech_prob": 0.006035691127181053}, {"id": 404, "seek": 253092, "start": 2535.36, "end": 2542.44, "text": " que tan fluido es una oraci\u00f3n en el lenguaje destino entonces ahora lo que me falta recuerden", "tokens": [50586, 631, 7603, 5029, 2925, 785, 2002, 420, 3482, 465, 806, 35044, 84, 11153, 2677, 2982, 13003, 9923, 450, 631, 385, 22111, 39092, 1556, 50940], "temperature": 0.0, "avg_logprob": -0.1014076305341117, "compression_ratio": 2.0636042402826855, "no_speech_prob": 0.006035691127181053}, {"id": 405, "seek": 253092, "start": 2542.44, "end": 2546.92, "text": " que yo lo que ten\u00eda que hacer era iterar sobre todas las oraciones del lenguaje destino y pasarlas", "tokens": [50940, 631, 5290, 450, 631, 23718, 631, 6720, 4249, 17138, 289, 5473, 10906, 2439, 420, 9188, 1103, 35044, 84, 11153, 2677, 2982, 288, 1736, 6843, 296, 51164], "temperature": 0.0, "avg_logprob": -0.1014076305341117, "compression_ratio": 2.0636042402826855, "no_speech_prob": 0.006035691127181053}, {"id": 406, "seek": 253092, "start": 2546.92, "end": 2550.32, "text": " a trav\u00e9s del modelo de traducci\u00f3n y del modelo de lenguaje para que me d\u00e9 la probabilidad de esa", "tokens": [51164, 257, 24463, 1103, 27825, 368, 2479, 1311, 5687, 288, 1103, 27825, 368, 35044, 84, 11153, 1690, 631, 385, 2795, 635, 31959, 4580, 368, 11342, 51334], "temperature": 0.0, "avg_logprob": -0.1014076305341117, "compression_ratio": 2.0636042402826855, "no_speech_prob": 0.006035691127181053}, {"id": 407, "seek": 253092, "start": 2550.32, "end": 2556.44, "text": " oraci\u00f3n bueno lo que me falta es el algoritmo de codificaci\u00f3n que en vez de probar con todas", "tokens": [51334, 420, 3482, 11974, 450, 631, 385, 22111, 785, 806, 3501, 50017, 3280, 368, 17656, 40802, 631, 465, 5715, 368, 1239, 289, 416, 10906, 51640], "temperature": 0.0, "avg_logprob": -0.1014076305341117, "compression_ratio": 2.0636042402826855, "no_speech_prob": 0.006035691127181053}, {"id": 408, "seek": 253092, "start": 2556.44, "end": 2560.6800000000003, "text": " las oraciones del lenguaje destino me va a decir unas cuantas oraciones para probar capa que me", "tokens": [51640, 2439, 420, 9188, 1103, 35044, 84, 11153, 2677, 2982, 385, 2773, 257, 10235, 25405, 2702, 49153, 420, 9188, 1690, 1239, 289, 1410, 64, 631, 385, 51852], "temperature": 0.0, "avg_logprob": -0.1014076305341117, "compression_ratio": 2.0636042402826855, "no_speech_prob": 0.006035691127181053}, {"id": 409, "seek": 256068, "start": 2560.68, "end": 2566.12, "text": " dice 150 oraciones para probar sobre las cuales utilizar el modelo de traducci\u00f3n y el modelo", "tokens": [50364, 10313, 8451, 420, 9188, 1690, 1239, 289, 5473, 2439, 46932, 24060, 806, 27825, 368, 2479, 1311, 5687, 288, 806, 27825, 50636], "temperature": 0.0, "avg_logprob": -0.1457479243375817, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0005525161977857351}, {"id": 410, "seek": 256068, "start": 2566.12, "end": 2571.0, "text": " de lenguaje entonces esto es como un diagrama de de m\u00f3dulos en los cuales el algoritmo de", "tokens": [50636, 368, 35044, 84, 11153, 13003, 7433, 785, 2617, 517, 10686, 64, 368, 368, 275, 17081, 28348, 465, 1750, 46932, 806, 3501, 50017, 3280, 368, 50880], "temperature": 0.0, "avg_logprob": -0.1457479243375817, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0005525161977857351}, {"id": 411, "seek": 256068, "start": 2571.0, "end": 2575.64, "text": " codificaci\u00f3n utiliza los dos m\u00f3dulos tanto el de traducci\u00f3n como el de lenguaje", "tokens": [50880, 17656, 40802, 4976, 13427, 1750, 4491, 275, 17081, 28348, 10331, 806, 368, 2479, 1311, 5687, 2617, 806, 368, 35044, 84, 11153, 51112], "temperature": 0.0, "avg_logprob": -0.1457479243375817, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0005525161977857351}, {"id": 412, "seek": 256068, "start": 2577.64, "end": 2584.56, "text": " bueno c\u00f3mo funciona el algoritmo de codificaci\u00f3n el que vamos a ver es un algoritmo de codificaci\u00f3n", "tokens": [51212, 11974, 12826, 26210, 806, 3501, 50017, 3280, 368, 17656, 40802, 806, 631, 5295, 257, 1306, 785, 517, 3501, 50017, 3280, 368, 17656, 40802, 51558], "temperature": 0.0, "avg_logprob": -0.1457479243375817, "compression_ratio": 1.8974358974358974, "no_speech_prob": 0.0005525161977857351}, {"id": 413, "seek": 258456, "start": 2584.56, "end": 2591.2, "text": " de tipo beam search y bueno funciona de la siguiente manera yo tengo la oraci\u00f3n mar\u00eda no", "tokens": [50364, 368, 9746, 14269, 3164, 288, 11974, 26210, 368, 635, 25666, 13913, 5290, 13989, 635, 420, 3482, 1849, 2686, 572, 50696], "temperature": 0.0, "avg_logprob": -0.13818708823545137, "compression_ratio": 2.1228070175438596, "no_speech_prob": 0.30289357900619507}, {"id": 414, "seek": 258456, "start": 2591.2, "end": 2596.44, "text": " dio una bofetada a la bruja verde y la quiero traducir al ingl\u00e9s y tengo una tabla de traducci\u00f3n de", "tokens": [50696, 31965, 2002, 748, 69, 302, 1538, 257, 635, 25267, 2938, 29653, 288, 635, 16811, 2479, 1311, 347, 419, 49766, 288, 13989, 2002, 4421, 875, 368, 2479, 1311, 5687, 368, 50958], "temperature": 0.0, "avg_logprob": -0.13818708823545137, "compression_ratio": 2.1228070175438596, "no_speech_prob": 0.30289357900619507}, {"id": 415, "seek": 258456, "start": 2596.44, "end": 2603.96, "text": " frases entonces mi oraci\u00f3n mar\u00eda no dio una bofetada a la bruja verde yo busco en la tabla", "tokens": [50958, 431, 1957, 13003, 2752, 420, 3482, 1849, 2686, 572, 31965, 2002, 748, 69, 302, 1538, 257, 635, 25267, 2938, 29653, 5290, 1255, 1291, 465, 635, 4421, 875, 51334], "temperature": 0.0, "avg_logprob": -0.13818708823545137, "compression_ratio": 2.1228070175438596, "no_speech_prob": 0.30289357900619507}, {"id": 416, "seek": 258456, "start": 2603.96, "end": 2610.2, "text": " de frases cu\u00e1les de esas de digamos cu\u00e1les segmentos cu\u00e1les subsegmentos de esa oraci\u00f3n yo", "tokens": [51334, 368, 431, 1957, 2702, 842, 904, 368, 23388, 368, 36430, 2702, 842, 904, 9469, 329, 2702, 842, 904, 1422, 405, 10433, 329, 368, 11342, 420, 3482, 5290, 51646], "temperature": 0.0, "avg_logprob": -0.13818708823545137, "compression_ratio": 2.1228070175438596, "no_speech_prob": 0.30289357900619507}, {"id": 417, "seek": 258456, "start": 2610.2, "end": 2614.2799999999997, "text": " puedo encontrar en la tabla de traducci\u00f3n de frases entonces voy a encontrar por ejemplo que mar\u00eda lo", "tokens": [51646, 21612, 17525, 465, 635, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 13003, 7552, 257, 17525, 1515, 13358, 631, 1849, 2686, 450, 51850], "temperature": 0.0, "avg_logprob": -0.13818708823545137, "compression_ratio": 2.1228070175438596, "no_speech_prob": 0.30289357900619507}, {"id": 418, "seek": 261428, "start": 2614.28, "end": 2619.2400000000002, "text": " puedo traducir como mary no lo busco en la tabla y lo puedo traducir como not como did not o como", "tokens": [50364, 21612, 2479, 1311, 347, 2617, 275, 822, 572, 450, 1255, 1291, 465, 635, 4421, 875, 288, 450, 21612, 2479, 1311, 347, 2617, 406, 2617, 630, 406, 277, 2617, 50612], "temperature": 0.0, "avg_logprob": -0.16900077988119686, "compression_ratio": 2.3823529411764706, "no_speech_prob": 0.0066170948557555676}, {"id": 419, "seek": 261428, "start": 2619.2400000000002, "end": 2625.6400000000003, "text": " no dio lo puedo traducir como git pero adem\u00e1s no dio esa frase entera yo lo busco en la tabla y", "tokens": [50612, 572, 31965, 450, 21612, 2479, 1311, 347, 2617, 18331, 4768, 21251, 572, 31965, 11342, 38406, 948, 1663, 5290, 450, 1255, 1291, 465, 635, 4421, 875, 288, 50932], "temperature": 0.0, "avg_logprob": -0.16900077988119686, "compression_ratio": 2.3823529411764706, "no_speech_prob": 0.0066170948557555676}, {"id": 420, "seek": 261428, "start": 2625.6400000000003, "end": 2630.7200000000003, "text": " me parece que la puedo traducir como did not give dio una bofetada toda esa frase lo puedo traducir", "tokens": [50932, 385, 14120, 631, 635, 21612, 2479, 1311, 347, 2617, 630, 406, 976, 31965, 2002, 748, 69, 302, 1538, 11687, 11342, 38406, 450, 21612, 2479, 1311, 347, 51186], "temperature": 0.0, "avg_logprob": -0.16900077988119686, "compression_ratio": 2.3823529411764706, "no_speech_prob": 0.0066170948557555676}, {"id": 421, "seek": 261428, "start": 2630.7200000000003, "end": 2638.84, "text": " como slap una bofetada lo puedo decir como a slap y bueno y otras cosas bruja lo puedo decir como", "tokens": [51186, 2617, 21075, 2002, 748, 69, 302, 1538, 450, 21612, 10235, 2617, 257, 21075, 288, 11974, 288, 20244, 12218, 25267, 2938, 450, 21612, 10235, 2617, 51592], "temperature": 0.0, "avg_logprob": -0.16900077988119686, "compression_ratio": 2.3823529411764706, "no_speech_prob": 0.0066170948557555676}, {"id": 422, "seek": 261428, "start": 2638.84, "end": 2642.1600000000003, "text": " witch verde como green pero adem\u00e1s en alg\u00fan lado de la tabla tengo que bruja verde lo puedo", "tokens": [51592, 14867, 29653, 2617, 3092, 4768, 21251, 465, 26300, 11631, 368, 635, 4421, 875, 13989, 631, 25267, 2938, 29653, 450, 21612, 51758], "temperature": 0.0, "avg_logprob": -0.16900077988119686, "compression_ratio": 2.3823529411764706, "no_speech_prob": 0.0066170948557555676}, {"id": 423, "seek": 264216, "start": 2642.16, "end": 2648.56, "text": " traducir como green witch y as\u00ed digamos yo puedo encontrar tengo diferentes maneras de segmentar", "tokens": [50364, 2479, 1311, 347, 2617, 3092, 14867, 288, 8582, 36430, 5290, 21612, 17525, 13989, 17686, 587, 6985, 368, 9469, 289, 50684], "temperature": 0.0, "avg_logprob": -0.13838934391102892, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.0038984979037195444}, {"id": 424, "seek": 264216, "start": 2648.56, "end": 2652.56, "text": " la oraci\u00f3n y adem\u00e1s para cada uno de esos segmentos puedo encontrar distintas formas de", "tokens": [50684, 635, 420, 3482, 288, 21251, 1690, 8411, 8526, 368, 22411, 9469, 329, 21612, 17525, 31489, 296, 33463, 368, 50884], "temperature": 0.0, "avg_logprob": -0.13838934391102892, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.0038984979037195444}, {"id": 425, "seek": 264216, "start": 2652.56, "end": 2660.2799999999997, "text": " traducirlo en el lenguaje destino con mi tabla de frases entonces el algoritmo de codificaci\u00f3n", "tokens": [50884, 2479, 1311, 347, 752, 465, 806, 35044, 84, 11153, 2677, 2982, 416, 2752, 4421, 875, 368, 431, 1957, 13003, 806, 3501, 50017, 3280, 368, 17656, 40802, 51270], "temperature": 0.0, "avg_logprob": -0.13838934391102892, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.0038984979037195444}, {"id": 426, "seek": 264216, "start": 2660.2799999999997, "end": 2665.04, "text": " funciona de la siguiente manera empezamos teniendo en cada paso del algoritmo vamos a tener un conjunto", "tokens": [51270, 26210, 368, 635, 25666, 13913, 18730, 2151, 2064, 7304, 465, 8411, 29212, 1103, 3501, 50017, 3280, 5295, 257, 11640, 517, 37776, 51508], "temperature": 0.0, "avg_logprob": -0.13838934391102892, "compression_ratio": 1.7123893805309736, "no_speech_prob": 0.0038984979037195444}, {"id": 427, "seek": 266504, "start": 2665.04, "end": 2670.48, "text": " de hip\u00f3tesis de traducci\u00f3n se llega a ver ah\u00ed lo que dice ahi ojo m\u00e1s o menos", "tokens": [50364, 368, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 369, 40423, 257, 1306, 12571, 450, 631, 10313, 3716, 72, 277, 5134, 3573, 277, 8902, 50636], "temperature": 0.0, "avg_logprob": -0.2713781622953193, "compression_ratio": 1.625, "no_speech_prob": 0.04647274687886238}, {"id": 428, "seek": 266504, "start": 2677.16, "end": 2677.66, "text": " bien", "tokens": [50970, 3610, 50995], "temperature": 0.0, "avg_logprob": -0.2713781622953193, "compression_ratio": 1.625, "no_speech_prob": 0.04647274687886238}, {"id": 429, "seek": 266504, "start": 2680.0, "end": 2685.52, "text": " ac\u00e1 quedaron mal los cuadraditos bueno en cada uno de los pasos yo voy a tener un conjunto de hip\u00f3tesis", "tokens": [51112, 23496, 13617, 6372, 2806, 1750, 34434, 6206, 11343, 11974, 465, 8411, 8526, 368, 1750, 1736, 329, 5290, 7552, 257, 11640, 517, 37776, 368, 8103, 812, 7269, 271, 51388], "temperature": 0.0, "avg_logprob": -0.2713781622953193, "compression_ratio": 1.625, "no_speech_prob": 0.04647274687886238}, {"id": 430, "seek": 266504, "start": 2685.52, "end": 2692.44, "text": " de traducci\u00f3n al principio del algoritmo voy a empezar con lo con una hip\u00f3tesis vac\u00eda como", "tokens": [51388, 368, 2479, 1311, 5687, 419, 34308, 1103, 3501, 50017, 3280, 7552, 257, 31168, 416, 450, 416, 2002, 8103, 812, 7269, 271, 2842, 2686, 2617, 51734], "temperature": 0.0, "avg_logprob": -0.2713781622953193, "compression_ratio": 1.625, "no_speech_prob": 0.04647274687886238}, {"id": 431, "seek": 269244, "start": 2692.44, "end": 2696.88, "text": " se le est\u00e1 hip\u00f3tesis dice que lo importante de leer es la parte de la f que tiene un mont\u00f3n de", "tokens": [50364, 369, 476, 3192, 8103, 812, 7269, 271, 10313, 631, 450, 9416, 368, 34172, 785, 635, 6975, 368, 635, 283, 631, 7066, 517, 45259, 368, 50586], "temperature": 0.0, "avg_logprob": -0.13434039056301117, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.01523933932185173}, {"id": 432, "seek": 269244, "start": 2696.88, "end": 2701.56, "text": " guiones significa que no hay ninguna palabra del espa\u00f1ol cubierta esas son todas las 9 creo 9", "tokens": [50586, 695, 5411, 19957, 631, 572, 4842, 36073, 31702, 1103, 31177, 10057, 811, 1328, 23388, 1872, 10906, 2439, 1722, 14336, 1722, 50820], "temperature": 0.0, "avg_logprob": -0.13434039056301117, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.01523933932185173}, {"id": 433, "seek": 269244, "start": 2701.56, "end": 2707.16, "text": " palabras en espa\u00f1ol ninguna est\u00e1 cubierta y esta hip\u00f3tesis tiene probabilidad 1 entonces en", "tokens": [50820, 35240, 465, 31177, 36073, 3192, 10057, 811, 1328, 288, 5283, 8103, 812, 7269, 271, 7066, 31959, 4580, 502, 13003, 465, 51100], "temperature": 0.0, "avg_logprob": -0.13434039056301117, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.01523933932185173}, {"id": 434, "seek": 269244, "start": 2707.16, "end": 2712.8, "text": " cada paso del algoritmo lo que voy a hacer es elegir un par de frases tal que una es traducci\u00f3n de", "tokens": [51100, 8411, 29212, 1103, 3501, 50017, 3280, 450, 631, 7552, 257, 6720, 785, 14459, 347, 517, 971, 368, 431, 1957, 4023, 631, 2002, 785, 2479, 1311, 5687, 368, 51382], "temperature": 0.0, "avg_logprob": -0.13434039056301117, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.01523933932185173}, {"id": 435, "seek": 269244, "start": 2712.8, "end": 2717.84, "text": " la otra y voy a crear una hip\u00f3tesis nueva a partir de una que ya tengo entonces en este paso lo que", "tokens": [51382, 635, 13623, 288, 7552, 257, 31984, 2002, 8103, 812, 7269, 271, 28963, 257, 13906, 368, 2002, 631, 2478, 13989, 13003, 465, 4065, 29212, 450, 631, 51634], "temperature": 0.0, "avg_logprob": -0.13434039056301117, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.01523933932185173}, {"id": 436, "seek": 271784, "start": 2717.84, "end": 2725.7200000000003, "text": " hice fue decir el hijo el par de frases mar\u00eda mary y ah\u00ed me creo una nueva hip\u00f3tesis que cubre", "tokens": [50364, 50026, 9248, 10235, 806, 38390, 806, 971, 368, 431, 1957, 1849, 2686, 275, 822, 288, 12571, 385, 14336, 2002, 28963, 8103, 812, 7269, 271, 631, 10057, 265, 50758], "temperature": 0.0, "avg_logprob": -0.15547804260253906, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.01804562285542488}, {"id": 437, "seek": 271784, "start": 2725.7200000000003, "end": 2730.84, "text": " la primera palabra por eso parece una serie con este caso elige la frase en ingl\u00e9s mary y ahora", "tokens": [50758, 635, 17382, 31702, 1515, 7287, 14120, 2002, 23030, 416, 4065, 9666, 806, 3969, 635, 38406, 465, 49766, 275, 822, 288, 9923, 51014], "temperature": 0.0, "avg_logprob": -0.15547804260253906, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.01804562285542488}, {"id": 438, "seek": 271784, "start": 2730.84, "end": 2736.84, "text": " tiene una probabilidad de 0.564 ese n\u00famero de esa probabilidad va a servir para guiar un poco en el", "tokens": [51014, 7066, 2002, 31959, 4580, 368, 1958, 13, 20, 19395, 10167, 14959, 368, 11342, 31959, 4580, 2773, 257, 29463, 1690, 695, 9448, 517, 10639, 465, 806, 51314], "temperature": 0.0, "avg_logprob": -0.15547804260253906, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.01804562285542488}, {"id": 439, "seek": 271784, "start": 2736.84, "end": 2740.6800000000003, "text": " algoritmo pero vamos a ver despu\u00e9s c\u00f3mo es que se calcula por ahora qu\u00e9dense solamente con el n\u00famero", "tokens": [51314, 3501, 50017, 3280, 4768, 5295, 257, 1306, 15283, 12826, 785, 631, 369, 4322, 64, 1515, 9923, 421, 7811, 1288, 27814, 416, 806, 14959, 51506], "temperature": 0.0, "avg_logprob": -0.15547804260253906, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.01804562285542488}, {"id": 440, "seek": 271784, "start": 2742.04, "end": 2746.84, "text": " bien pero entonces yo ten\u00eda otra opci\u00f3n en realidad yo pod\u00eda haber elegido empezar en", "tokens": [51574, 3610, 4768, 13003, 5290, 23718, 13623, 999, 5687, 465, 25635, 5290, 45588, 15811, 14459, 2925, 31168, 465, 51814], "temperature": 0.0, "avg_logprob": -0.15547804260253906, "compression_ratio": 1.686206896551724, "no_speech_prob": 0.01804562285542488}, {"id": 441, "seek": 274684, "start": 2746.84, "end": 2750.76, "text": " vez de traducir mar\u00eda por mary pod\u00eda haber elegido empezar por traducir bruja por witch", "tokens": [50364, 5715, 368, 2479, 1311, 347, 1849, 2686, 1515, 275, 822, 45588, 15811, 14459, 2925, 31168, 1515, 2479, 1311, 347, 25267, 2938, 1515, 14867, 50560], "temperature": 0.0, "avg_logprob": -0.14881575218985013, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0023648221977055073}, {"id": 442, "seek": 274684, "start": 2751.76, "end": 2760.28, "text": " y ah\u00ed me crear\u00eda otra hip\u00f3tesis de traducci\u00f3n donde cubro la pen\u00faltima de las de las palabras", "tokens": [50610, 288, 12571, 385, 1197, 21178, 13623, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 10488, 10057, 340, 635, 3435, 43447, 4775, 368, 2439, 368, 2439, 35240, 51036], "temperature": 0.0, "avg_logprob": -0.14881575218985013, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0023648221977055073}, {"id": 443, "seek": 274684, "start": 2760.28, "end": 2766.1200000000003, "text": " en espa\u00f1ol agarro la palabra witch delijo la palabra witch y tiene una probabilidad de 0.182", "tokens": [51036, 465, 31177, 623, 289, 340, 635, 31702, 14867, 1103, 24510, 635, 31702, 14867, 288, 7066, 2002, 31959, 4580, 368, 1958, 13, 6494, 17, 51328], "temperature": 0.0, "avg_logprob": -0.14881575218985013, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0023648221977055073}, {"id": 444, "seek": 274684, "start": 2767.88, "end": 2772.48, "text": " entonces en cada paso del algoritmo lo que hace es elegir una hip\u00f3tesis que tiene elegir un par", "tokens": [51416, 13003, 465, 8411, 29212, 1103, 3501, 50017, 3280, 450, 631, 10032, 785, 14459, 347, 2002, 8103, 812, 7269, 271, 631, 7066, 14459, 347, 517, 971, 51646], "temperature": 0.0, "avg_logprob": -0.14881575218985013, "compression_ratio": 1.7072072072072073, "no_speech_prob": 0.0023648221977055073}, {"id": 445, "seek": 277248, "start": 2772.48, "end": 2778.84, "text": " de frases y expandir as\u00ed que lo siguiente que puedo hacer es elegir la frase did not expandirla a", "tokens": [50364, 368, 431, 1957, 288, 5268, 347, 8582, 631, 450, 25666, 631, 21612, 6720, 785, 14459, 347, 635, 38406, 630, 406, 5268, 347, 875, 257, 50682], "temperature": 0.0, "avg_logprob": -0.1255709721491887, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.13109847903251648}, {"id": 446, "seek": 277248, "start": 2778.84, "end": 2783.92, "text": " partir de la hip\u00f3tesis que ten\u00eda con mary y bueno eso me cubre ahora dos palabras en espa\u00f1ol y me", "tokens": [50682, 13906, 368, 635, 8103, 812, 7269, 271, 631, 23718, 416, 275, 822, 288, 11974, 7287, 385, 10057, 265, 9923, 4491, 35240, 465, 31177, 288, 385, 50936], "temperature": 0.0, "avg_logprob": -0.1255709721491887, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.13109847903251648}, {"id": 447, "seek": 277248, "start": 2783.92, "end": 2791.32, "text": " tiene me me dio otra probabilidad y despu\u00e9s sigo avanzando y sigo avanzando hasta que lleg\u00f3 a cubrir", "tokens": [50936, 7066, 385, 385, 31965, 13623, 31959, 4580, 288, 15283, 4556, 78, 42444, 1806, 288, 4556, 78, 42444, 1806, 10764, 631, 46182, 257, 10057, 10949, 51306], "temperature": 0.0, "avg_logprob": -0.1255709721491887, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.13109847903251648}, {"id": 448, "seek": 277248, "start": 2791.32, "end": 2795.48, "text": " en alg\u00fan momento si yo sigo avanzando y sigo agregando hip\u00f3tesis en alg\u00fan momento voy a", "tokens": [51306, 465, 26300, 9333, 1511, 5290, 4556, 78, 42444, 1806, 288, 4556, 78, 623, 3375, 1806, 8103, 812, 7269, 271, 465, 26300, 9333, 7552, 257, 51514], "temperature": 0.0, "avg_logprob": -0.1255709721491887, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.13109847903251648}, {"id": 449, "seek": 277248, "start": 2795.48, "end": 2801.64, "text": " llegar a cubrir todas las palabras del idioma espa\u00f1ol todas las palabras de la oraci\u00f3n en idioma", "tokens": [51514, 24892, 257, 10057, 10949, 10906, 2439, 35240, 1103, 18014, 6440, 31177, 10906, 2439, 35240, 368, 635, 420, 3482, 465, 18014, 6440, 51822], "temperature": 0.0, "avg_logprob": -0.1255709721491887, "compression_ratio": 1.937007874015748, "no_speech_prob": 0.13109847903251648}, {"id": 450, "seek": 280164, "start": 2802.04, "end": 2806.96, "text": " entonces hay una vez que yo cubr\u00ed todas las palabras digo bueno esto es una hip\u00f3tesis completa", "tokens": [50384, 13003, 4842, 2002, 5715, 631, 5290, 10057, 40577, 10906, 2439, 35240, 22990, 11974, 7433, 785, 2002, 8103, 812, 7269, 271, 46822, 50630], "temperature": 0.0, "avg_logprob": -0.11016374081373215, "compression_ratio": 1.91699604743083, "no_speech_prob": 0.003413092577829957}, {"id": 451, "seek": 280164, "start": 2806.96, "end": 2812.8799999999997, "text": " y esto lo devuelvo como una potencial candidata digamos una oraci\u00f3n candidata a traducci\u00f3n", "tokens": [50630, 288, 7433, 450, 1905, 3483, 3080, 2617, 2002, 48265, 6268, 3274, 36430, 2002, 420, 3482, 6268, 3274, 257, 2479, 1311, 5687, 50926], "temperature": 0.0, "avg_logprob": -0.11016374081373215, "compression_ratio": 1.91699604743083, "no_speech_prob": 0.003413092577829957}, {"id": 452, "seek": 280164, "start": 2812.8799999999997, "end": 2818.68, "text": " pero claro a medida que yo fui avanzando una cosa que pas\u00f3 es que fui dejando hip\u00f3tesis colgadas", "tokens": [50926, 4768, 16742, 257, 32984, 631, 5290, 27863, 42444, 1806, 2002, 10163, 631, 41382, 785, 631, 27863, 21259, 1806, 8103, 812, 7269, 271, 1173, 70, 6872, 51216], "temperature": 0.0, "avg_logprob": -0.11016374081373215, "compression_ratio": 1.91699604743083, "no_speech_prob": 0.003413092577829957}, {"id": 453, "seek": 280164, "start": 2818.68, "end": 2824.3599999999997, "text": " y esas hip\u00f3tesis podr\u00edan tener otras traducciones posibles yo ac\u00e1 lo que devol\u00ed era una posible", "tokens": [51216, 288, 23388, 8103, 812, 7269, 271, 15305, 11084, 11640, 20244, 2479, 1311, 23469, 1366, 14428, 5290, 23496, 450, 631, 1905, 401, 870, 4249, 2002, 26644, 51500], "temperature": 0.0, "avg_logprob": -0.11016374081373215, "compression_ratio": 1.91699604743083, "no_speech_prob": 0.003413092577829957}, {"id": 454, "seek": 280164, "start": 2824.3599999999997, "end": 2828.2, "text": " traducci\u00f3n pero a medida que yo ten\u00eda las otras hip\u00f3tesis si yo hubiera seguido por las otras", "tokens": [51500, 2479, 1311, 5687, 4768, 257, 32984, 631, 5290, 23718, 2439, 20244, 8103, 812, 7269, 271, 1511, 5290, 11838, 10609, 8878, 2925, 1515, 2439, 20244, 51692], "temperature": 0.0, "avg_logprob": -0.11016374081373215, "compression_ratio": 1.91699604743083, "no_speech_prob": 0.003413092577829957}, {"id": 455, "seek": 282820, "start": 2828.2, "end": 2834.2, "text": " hip\u00f3tesis hubiera podido devolver otras cosas entonces yo necesito hacer un backtracking para", "tokens": [50364, 8103, 812, 7269, 271, 11838, 10609, 2497, 2925, 1905, 401, 331, 20244, 12218, 13003, 5290, 11909, 3528, 6720, 517, 646, 6903, 14134, 1690, 50664], "temperature": 0.0, "avg_logprob": -0.12351878184192586, "compression_ratio": 1.9246231155778895, "no_speech_prob": 0.028178608044981956}, {"id": 456, "seek": 282820, "start": 2834.2, "end": 2839.16, "text": " poder devolver todas las posibilidades poder volver a ver las hip\u00f3tesis a revisitar las hip\u00f3tesis", "tokens": [50664, 8152, 1905, 401, 331, 10906, 2439, 1366, 11607, 10284, 8152, 33998, 257, 1306, 2439, 8103, 812, 7269, 271, 257, 20767, 3981, 2439, 8103, 812, 7269, 271, 50912], "temperature": 0.0, "avg_logprob": -0.12351878184192586, "compression_ratio": 1.9246231155778895, "no_speech_prob": 0.028178608044981956}, {"id": 457, "seek": 282820, "start": 2839.16, "end": 2844.16, "text": " que hab\u00eda dejado colgadas y volver a explorar los otros caminos entonces necesitar\u00eda hacer un", "tokens": [50912, 631, 16395, 21259, 1573, 1173, 70, 6872, 288, 33998, 257, 24765, 289, 1750, 16422, 1945, 15220, 13003, 11909, 3981, 2686, 6720, 517, 51162], "temperature": 0.0, "avg_logprob": -0.12351878184192586, "compression_ratio": 1.9246231155778895, "no_speech_prob": 0.028178608044981956}, {"id": 458, "seek": 282820, "start": 2844.16, "end": 2851.3999999999996, "text": " backtracking para recorrerlas todas y si hago un backtracking lo que va a pasar es que voy a", "tokens": [51162, 646, 6903, 14134, 1690, 850, 284, 9797, 7743, 10906, 288, 1511, 38721, 517, 646, 6903, 14134, 450, 631, 2773, 257, 25344, 785, 631, 7552, 257, 51524], "temperature": 0.0, "avg_logprob": -0.12351878184192586, "compression_ratio": 1.9246231155778895, "no_speech_prob": 0.028178608044981956}, {"id": 459, "seek": 285140, "start": 2852.36, "end": 2858.52, "text": " va a ocurrir una explosi\u00f3n de exponencial del espacio de b\u00fasqueda porque en realidad todas las", "tokens": [50412, 2773, 257, 26430, 10949, 2002, 9215, 2560, 368, 12680, 26567, 1103, 33845, 368, 272, 10227, 358, 8801, 4021, 465, 25635, 10906, 2439, 50720], "temperature": 0.0, "avg_logprob": -0.1296697893450337, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.09412967413663864}, {"id": 460, "seek": 285140, "start": 2858.52, "end": 2864.76, "text": " las posibilidades que se abren son exponenciales y ah\u00ed esto como que se vuelve bastante lento entonces", "tokens": [50720, 2439, 1366, 11607, 10284, 631, 369, 410, 1095, 1872, 12680, 26567, 279, 288, 12571, 7433, 2617, 631, 369, 20126, 303, 14651, 287, 15467, 13003, 51032], "temperature": 0.0, "avg_logprob": -0.1296697893450337, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.09412967413663864}, {"id": 461, "seek": 285140, "start": 2865.92, "end": 2870.28, "text": " yo quer\u00eda un decodificador para volver este problema un problema tratable en vez de agarrar", "tokens": [51090, 5290, 37869, 517, 979, 378, 1089, 5409, 1690, 33998, 4065, 12395, 517, 12395, 21507, 712, 465, 5715, 368, 623, 2284, 289, 51308], "temperature": 0.0, "avg_logprob": -0.1296697893450337, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.09412967413663864}, {"id": 462, "seek": 285140, "start": 2870.28, "end": 2875.08, "text": " las infinitas oraciones del idioma me quedo con algunas que sean m\u00e1s probables con este algoritmo", "tokens": [51308, 2439, 7193, 14182, 420, 9188, 1103, 18014, 6440, 385, 13617, 78, 416, 27316, 631, 37670, 3573, 1239, 2965, 416, 4065, 3501, 50017, 3280, 51548], "temperature": 0.0, "avg_logprob": -0.1296697893450337, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.09412967413663864}, {"id": 463, "seek": 285140, "start": 2875.08, "end": 2881.08, "text": " de codificaci\u00f3n logr\u00e9 reducir de infinito a algo finito pero a\u00fan as\u00ed es demasiado lento porque", "tokens": [51548, 368, 17656, 40802, 31013, 526, 2783, 23568, 368, 7193, 3528, 257, 8655, 962, 3528, 4768, 31676, 8582, 785, 39820, 287, 15467, 4021, 51848], "temperature": 0.0, "avg_logprob": -0.1296697893450337, "compression_ratio": 1.7789855072463767, "no_speech_prob": 0.09412967413663864}, {"id": 464, "seek": 288108, "start": 2881.08, "end": 2886.44, "text": " hay una explosi\u00f3n combinaci\u00f3n combinatoria digamos de la hip\u00f3tesis y me queda una cantidad", "tokens": [50364, 4842, 2002, 9215, 2560, 38514, 3482, 38514, 1639, 654, 36430, 368, 635, 8103, 812, 7269, 271, 288, 385, 23314, 2002, 33757, 50632], "temperature": 0.0, "avg_logprob": -0.11638259887695312, "compression_ratio": 1.954732510288066, "no_speech_prob": 0.000993202324025333}, {"id": 465, "seek": 288108, "start": 2886.44, "end": 2893.16, "text": " exponencial de hip\u00f3tesis entonces como es tan grande este problema digamos como la cantidad", "tokens": [50632, 12680, 26567, 368, 8103, 812, 7269, 271, 13003, 2617, 785, 7603, 8883, 4065, 12395, 36430, 2617, 635, 33757, 50968], "temperature": 0.0, "avg_logprob": -0.11638259887695312, "compression_ratio": 1.954732510288066, "no_speech_prob": 0.000993202324025333}, {"id": 466, "seek": 288108, "start": 2893.16, "end": 2897.7999999999997, "text": " de hip\u00f3tesis exponencial y este es un problema NP completo entonces se utilizan t\u00e9cnicas para", "tokens": [50968, 368, 8103, 812, 7269, 271, 12680, 26567, 288, 4065, 785, 517, 12395, 38611, 40135, 13003, 369, 19906, 282, 25564, 40672, 1690, 51200], "temperature": 0.0, "avg_logprob": -0.11638259887695312, "compression_ratio": 1.954732510288066, "no_speech_prob": 0.000993202324025333}, {"id": 467, "seek": 288108, "start": 2897.7999999999997, "end": 2903.2799999999997, "text": " reducir el espacio de b\u00fasqueda y hay como dos tipos de t\u00e9cnicas algunas son con riesgo y otras", "tokens": [51200, 2783, 23568, 806, 33845, 368, 272, 10227, 358, 8801, 288, 4842, 2617, 4491, 37105, 368, 25564, 40672, 27316, 1872, 416, 23932, 1571, 288, 20244, 51474], "temperature": 0.0, "avg_logprob": -0.11638259887695312, "compression_ratio": 1.954732510288066, "no_speech_prob": 0.000993202324025333}, {"id": 468, "seek": 288108, "start": 2903.2799999999997, "end": 2908.44, "text": " son sin riesgo las t\u00e9cnicas sin riesgo lo que quiere decir es que si yo aplica una t\u00e9cnica de", "tokens": [51474, 1872, 3343, 23932, 1571, 2439, 25564, 40672, 3343, 23932, 1571, 450, 631, 23877, 10235, 785, 631, 1511, 5290, 25522, 2262, 2002, 45411, 368, 51732], "temperature": 0.0, "avg_logprob": -0.11638259887695312, "compression_ratio": 1.954732510288066, "no_speech_prob": 0.000993202324025333}, {"id": 469, "seek": 290844, "start": 2908.44, "end": 2914.64, "text": " reducci\u00f3n de hip\u00f3tesis sin riesgo la soluci\u00f3n ideal que yo ten\u00eda dentro de mi b\u00fasqueda no la", "tokens": [50364, 2783, 14735, 368, 8103, 812, 7269, 271, 3343, 23932, 1571, 635, 24807, 5687, 7157, 631, 5290, 23718, 10856, 368, 2752, 272, 10227, 358, 8801, 572, 635, 50674], "temperature": 0.0, "avg_logprob": -0.10279979294152569, "compression_ratio": 1.992094861660079, "no_speech_prob": 0.10932890325784683}, {"id": 470, "seek": 290844, "start": 2914.64, "end": 2919.28, "text": " voy a perder utilizando una t\u00e9cnica sin riesgo en cambio en la con riesgo si yo podr\u00eda llegar a", "tokens": [50674, 7552, 257, 26971, 19906, 1806, 2002, 45411, 3343, 23932, 1571, 465, 28731, 465, 635, 416, 23932, 1571, 1511, 5290, 27246, 24892, 257, 50906], "temperature": 0.0, "avg_logprob": -0.10279979294152569, "compression_ratio": 1.992094861660079, "no_speech_prob": 0.10932890325784683}, {"id": 471, "seek": 290844, "start": 2919.28, "end": 2924.84, "text": " perder la soluci\u00f3n \u00f3ptima bien entonces la t\u00e9cnica sin riesgo que conocemos es la de recombinaci\u00f3n de", "tokens": [50906, 26971, 635, 24807, 5687, 11857, 662, 4775, 3610, 13003, 635, 45411, 3343, 23932, 1571, 631, 33029, 38173, 785, 635, 368, 850, 3548, 259, 3482, 368, 51184], "temperature": 0.0, "avg_logprob": -0.10279979294152569, "compression_ratio": 1.992094861660079, "no_speech_prob": 0.10932890325784683}, {"id": 472, "seek": 290844, "start": 2924.84, "end": 2930.28, "text": " hip\u00f3tesis que dice que si yo tengo dos hip\u00f3tesis voy avanzando por dos caminos dentro del algoritmo", "tokens": [51184, 8103, 812, 7269, 271, 631, 10313, 631, 1511, 5290, 13989, 4491, 8103, 812, 7269, 271, 7552, 42444, 1806, 1515, 4491, 1945, 15220, 10856, 1103, 3501, 50017, 3280, 51456], "temperature": 0.0, "avg_logprob": -0.10279979294152569, "compression_ratio": 1.992094861660079, "no_speech_prob": 0.10932890325784683}, {"id": 473, "seek": 290844, "start": 2930.28, "end": 2935.44, "text": " y llevo a dos hip\u00f3tesis iguales por lo menos dos hip\u00f3tesis que cubren las mismas palabras entonces", "tokens": [51456, 288, 12038, 3080, 257, 4491, 8103, 812, 7269, 271, 10953, 279, 1515, 450, 8902, 4491, 8103, 812, 7269, 271, 631, 10057, 1095, 2439, 23220, 296, 35240, 13003, 51714], "temperature": 0.0, "avg_logprob": -0.10279979294152569, "compression_ratio": 1.992094861660079, "no_speech_prob": 0.10932890325784683}, {"id": 474, "seek": 293544, "start": 2935.44, "end": 2940.88, "text": " me puedo quedar con la que tiene mayor probabilidad de las dos y descartar la otra porque porque a", "tokens": [50364, 385, 21612, 39244, 416, 635, 631, 7066, 10120, 31959, 4580, 368, 2439, 4491, 288, 7471, 446, 289, 635, 13623, 4021, 4021, 257, 50636], "temperature": 0.0, "avg_logprob": -0.09668717733243616, "compression_ratio": 1.9285714285714286, "no_speech_prob": 0.02586669661104679}, {"id": 475, "seek": 293544, "start": 2940.88, "end": 2944.16, "text": " medida que yo voy a seguir avanzando en el algoritmo lo que va a pasar es que van a bajar las", "tokens": [50636, 32984, 631, 5290, 7552, 257, 18584, 42444, 1806, 465, 806, 3501, 50017, 3280, 450, 631, 2773, 257, 25344, 785, 631, 3161, 257, 23589, 289, 2439, 50800], "temperature": 0.0, "avg_logprob": -0.09668717733243616, "compression_ratio": 1.9285714285714286, "no_speech_prob": 0.02586669661104679}, {"id": 476, "seek": 293544, "start": 2944.16, "end": 2948.56, "text": " probabilidades digamos yo eligiendo m\u00e1s palabras y eligiendo m\u00e1s frases me va a bajar la probabilidad", "tokens": [50800, 31959, 10284, 36430, 5290, 31089, 7304, 3573, 35240, 288, 31089, 7304, 3573, 431, 1957, 385, 2773, 257, 23589, 289, 635, 31959, 4580, 51020], "temperature": 0.0, "avg_logprob": -0.09668717733243616, "compression_ratio": 1.9285714285714286, "no_speech_prob": 0.02586669661104679}, {"id": 477, "seek": 293544, "start": 2948.56, "end": 2954.4, "text": " y nunca me va a pasar que la una de las hip\u00f3tesis que ten\u00eda menos probabilidad vaya a subir en", "tokens": [51020, 288, 13768, 385, 2773, 257, 25344, 631, 635, 2002, 368, 2439, 8103, 812, 7269, 271, 631, 23718, 8902, 31959, 4580, 47682, 257, 34785, 465, 51312], "temperature": 0.0, "avg_logprob": -0.09668717733243616, "compression_ratio": 1.9285714285714286, "no_speech_prob": 0.02586669661104679}, {"id": 478, "seek": 293544, "start": 2954.4, "end": 2960.28, "text": " realidad siempre va a tener menos entonces en definitiva yo puedo con seguridad descartar la", "tokens": [51312, 25635, 12758, 2773, 257, 11640, 8902, 13003, 465, 28781, 5931, 5290, 21612, 416, 35415, 7471, 446, 289, 635, 51606], "temperature": 0.0, "avg_logprob": -0.09668717733243616, "compression_ratio": 1.9285714285714286, "no_speech_prob": 0.02586669661104679}, {"id": 479, "seek": 296028, "start": 2960.28, "end": 2966.0, "text": " que tiene menos probabilidad bueno esa es recombinaci\u00f3n de hip\u00f3tesis pero ni siquiera con", "tokens": [50364, 631, 7066, 8902, 31959, 4580, 11974, 11342, 785, 850, 3548, 259, 3482, 368, 8103, 812, 7269, 271, 4768, 3867, 1511, 35134, 416, 50650], "temperature": 0.0, "avg_logprob": -0.132427245851547, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.0648493841290474}, {"id": 480, "seek": 296028, "start": 2966.0, "end": 2970.96, "text": " eso alcanza digamos para reducir el espacio de b\u00fasqueda lo suficiente a\u00fan queda much\u00edsimas hip\u00f3tesis", "tokens": [50650, 7287, 419, 7035, 2394, 36430, 1690, 2783, 23568, 806, 33845, 368, 272, 10227, 358, 8801, 450, 33958, 31676, 23314, 29353, 17957, 8103, 812, 7269, 271, 50898], "temperature": 0.0, "avg_logprob": -0.132427245851547, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.0648493841290474}, {"id": 481, "seek": 296028, "start": 2970.96, "end": 2976.36, "text": " entonces suele utilizar t\u00e9cnicas de podado con riesgo la t\u00e9cnica del histograma la t\u00e9cnica del", "tokens": [50898, 13003, 459, 16884, 24060, 25564, 40672, 368, 2497, 1573, 416, 23932, 1571, 635, 45411, 1103, 49816, 64, 635, 45411, 1103, 51168], "temperature": 0.0, "avg_logprob": -0.132427245851547, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.0648493841290474}, {"id": 482, "seek": 296028, "start": 2976.36, "end": 2980.84, "text": " umbral el histograma significa que a cada paso digamos en cada paso del algoritmo yo me quedo", "tokens": [51168, 1105, 32728, 806, 49816, 64, 19957, 631, 257, 8411, 29212, 36430, 465, 8411, 29212, 1103, 3501, 50017, 3280, 5290, 385, 13617, 78, 51392], "temperature": 0.0, "avg_logprob": -0.132427245851547, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.0648493841290474}, {"id": 483, "seek": 296028, "start": 2980.84, "end": 2986.5600000000004, "text": " con los n las n hip\u00f3tesis de traducci\u00f3n m\u00e1s probables y descarto las otras y la t\u00e9cnica con", "tokens": [51392, 416, 1750, 297, 2439, 297, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 3573, 1239, 2965, 288, 7471, 15864, 2439, 20244, 288, 635, 45411, 416, 51678], "temperature": 0.0, "avg_logprob": -0.132427245851547, "compression_ratio": 1.8127340823970037, "no_speech_prob": 0.0648493841290474}, {"id": 484, "seek": 298656, "start": 2986.56, "end": 2992.0, "text": " un umbral dice que a cada paso del algoritmo me qued\u00f3 con la hip\u00f3tesis de mayor probabilidad y", "tokens": [50364, 517, 1105, 32728, 10313, 631, 257, 8411, 29212, 1103, 3501, 50017, 3280, 385, 13617, 812, 416, 635, 8103, 812, 7269, 271, 368, 10120, 31959, 4580, 288, 50636], "temperature": 0.0, "avg_logprob": -0.12151232305562722, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.02016143873333931}, {"id": 485, "seek": 298656, "start": 2992.0, "end": 2999.4, "text": " las que est\u00e9n a una distancia alfa m\u00e1ximo de esa cu\u00e1l es el riesgo de las las t\u00e9cnicas de podado", "tokens": [50636, 2439, 631, 871, 3516, 257, 2002, 1483, 22862, 419, 11771, 38876, 368, 11342, 44318, 785, 806, 23932, 1571, 368, 2439, 2439, 25564, 40672, 368, 2497, 1573, 51006], "temperature": 0.0, "avg_logprob": -0.12151232305562722, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.02016143873333931}, {"id": 486, "seek": 298656, "start": 2999.4, "end": 3004.6, "text": " que si la mejor traducci\u00f3n y la traducci\u00f3n \u00f3ptima ten\u00eda algunas frases muy poco probables al", "tokens": [51006, 631, 1511, 635, 11479, 2479, 1311, 5687, 288, 635, 2479, 1311, 5687, 11857, 662, 4775, 23718, 27316, 431, 1957, 5323, 10639, 1239, 2965, 419, 51266], "temperature": 0.0, "avg_logprob": -0.12151232305562722, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.02016143873333931}, {"id": 487, "seek": 298656, "start": 3004.6, "end": 3010.88, "text": " principio entonces probablemente yo descarte esa soluci\u00f3n en los primeros pasos y no llegan", "tokens": [51266, 34308, 13003, 21759, 4082, 5290, 7471, 11026, 11342, 24807, 5687, 465, 1750, 12595, 329, 1736, 329, 288, 572, 11234, 282, 51580], "temperature": 0.0, "avg_logprob": -0.12151232305562722, "compression_ratio": 1.6329113924050633, "no_speech_prob": 0.02016143873333931}, {"id": 488, "seek": 301088, "start": 3010.88, "end": 3013.88, "text": " a encontrar la soluci\u00f3n \u00f3ptima digamos la perd\u00ed por el hecho de haber podado", "tokens": [50364, 257, 17525, 635, 24807, 5687, 11857, 662, 4775, 36430, 635, 12611, 870, 1515, 806, 13064, 368, 15811, 2497, 1573, 50514], "temperature": 0.0, "avg_logprob": -0.14253012860407593, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.037837497889995575}, {"id": 489, "seek": 301088, "start": 3015.44, "end": 3020.1600000000003, "text": " sin embargo bueno tiene como como ventaja que en realidad reduce much\u00edsimo el espacio de b\u00fasqueda", "tokens": [50592, 3343, 23955, 11974, 7066, 2617, 2617, 6931, 12908, 631, 465, 25635, 5407, 44722, 806, 33845, 368, 272, 10227, 358, 8801, 50828], "temperature": 0.0, "avg_logprob": -0.14253012860407593, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.037837497889995575}, {"id": 490, "seek": 301088, "start": 3020.1600000000003, "end": 3028.12, "text": " y vuelve vuelve este problema un problema tratable bueno y ahora s\u00ed qu\u00e9 significaba esa probabilidad", "tokens": [50828, 288, 20126, 303, 20126, 303, 4065, 12395, 517, 12395, 21507, 712, 11974, 288, 9923, 8600, 8057, 3350, 5509, 11342, 31959, 4580, 51226], "temperature": 0.0, "avg_logprob": -0.14253012860407593, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.037837497889995575}, {"id": 491, "seek": 301088, "start": 3028.12, "end": 3033.08, "text": " que estaba viendo en cada una de las hip\u00f3tesis o sea el podado necesita tener las mejores hip\u00f3tesis", "tokens": [51226, 631, 17544, 34506, 465, 8411, 2002, 368, 2439, 8103, 812, 7269, 271, 277, 4158, 806, 2497, 1573, 45485, 11640, 2439, 42284, 8103, 812, 7269, 271, 51474], "temperature": 0.0, "avg_logprob": -0.14253012860407593, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.037837497889995575}, {"id": 492, "seek": 301088, "start": 3033.08, "end": 3038.6, "text": " y bueno y para la recombinaci\u00f3n tambi\u00e9n necesito saber la probabilidad de la hip\u00f3tesis bueno la forma", "tokens": [51474, 288, 11974, 288, 1690, 635, 850, 3548, 259, 3482, 6407, 11909, 3528, 12489, 635, 31959, 4580, 368, 635, 8103, 812, 7269, 271, 11974, 635, 8366, 51750], "temperature": 0.0, "avg_logprob": -0.14253012860407593, "compression_ratio": 1.7717391304347827, "no_speech_prob": 0.037837497889995575}, {"id": 493, "seek": 303860, "start": 3038.6, "end": 3043.36, "text": " de calcular la probabilidad de hip\u00f3tesis se divide en dos digamos tengo lo que encontr\u00e9 hasta el", "tokens": [50364, 368, 2104, 17792, 635, 31959, 4580, 368, 8103, 812, 7269, 271, 369, 9845, 465, 4491, 36430, 13989, 450, 631, 10176, 10521, 10764, 806, 50602], "temperature": 0.0, "avg_logprob": -0.12162671023852205, "compression_ratio": 2.2661596958174903, "no_speech_prob": 0.030081188306212425}, {"id": 494, "seek": 303860, "start": 3043.36, "end": 3047.8399999999997, "text": " momento la hip\u00f3tesis lleva cubierta cierta cantidad de palabras entonces para esa cantidad de palabras", "tokens": [50602, 9333, 635, 8103, 812, 7269, 271, 37681, 10057, 811, 1328, 39769, 1328, 33757, 368, 35240, 13003, 1690, 11342, 33757, 368, 35240, 50826], "temperature": 0.0, "avg_logprob": -0.12162671023852205, "compression_ratio": 2.2661596958174903, "no_speech_prob": 0.030081188306212425}, {"id": 495, "seek": 303860, "start": 3047.8399999999997, "end": 3053.08, "text": " que ya llevo cubiertas utilizo los tres modelos el modelo de traducci\u00f3n el modelo de reordenamiento", "tokens": [50826, 631, 2478, 12038, 3080, 10057, 4859, 296, 4976, 19055, 1750, 15890, 2316, 329, 806, 27825, 368, 2479, 1311, 5687, 806, 27825, 368, 319, 19058, 16971, 51088], "temperature": 0.0, "avg_logprob": -0.12162671023852205, "compression_ratio": 2.2661596958174903, "no_speech_prob": 0.030081188306212425}, {"id": 496, "seek": 303860, "start": 3053.08, "end": 3058.0, "text": " del modelo de lenguaje utilizo los tres modelos para calcular la probabilidad de la frase hasta", "tokens": [51088, 1103, 27825, 368, 35044, 84, 11153, 4976, 19055, 1750, 15890, 2316, 329, 1690, 2104, 17792, 635, 31959, 4580, 368, 635, 38406, 10764, 51334], "temperature": 0.0, "avg_logprob": -0.12162671023852205, "compression_ratio": 2.2661596958174903, "no_speech_prob": 0.030081188306212425}, {"id": 497, "seek": 303860, "start": 3058.0, "end": 3063.7599999999998, "text": " el momento pero para lo que me falta traducir yo no puedo utilizar todo porque no tengo toda la", "tokens": [51334, 806, 9333, 4768, 1690, 450, 631, 385, 22111, 2479, 1311, 347, 5290, 572, 21612, 24060, 5149, 4021, 572, 13989, 11687, 635, 51622], "temperature": 0.0, "avg_logprob": -0.12162671023852205, "compression_ratio": 2.2661596958174903, "no_speech_prob": 0.030081188306212425}, {"id": 498, "seek": 303860, "start": 3063.7599999999998, "end": 3067.88, "text": " informaci\u00f3n de traducci\u00f3n entonces lo que hago es utilizar solamente el modelo de traducci\u00f3n y el", "tokens": [51622, 21660, 368, 2479, 1311, 5687, 13003, 450, 631, 38721, 785, 24060, 27814, 806, 27825, 368, 2479, 1311, 5687, 288, 806, 51828], "temperature": 0.0, "avg_logprob": -0.12162671023852205, "compression_ratio": 2.2661596958174903, "no_speech_prob": 0.030081188306212425}, {"id": 499, "seek": 306788, "start": 3067.92, "end": 3072.84, "text": " modelo de lenguaje descarto el modelo de reordenamiento y bueno entonces hago calcular una", "tokens": [50366, 27825, 368, 35044, 84, 11153, 7471, 15864, 806, 27825, 368, 319, 19058, 16971, 288, 11974, 13003, 38721, 2104, 17792, 2002, 50612], "temperature": 0.0, "avg_logprob": -0.13912916917067308, "compression_ratio": 1.9382716049382716, "no_speech_prob": 0.0019439634634181857}, {"id": 500, "seek": 306788, "start": 3072.84, "end": 3076.48, "text": " probabilidad que es una parte con todos los tres modelos y otra parte sin el modelo de", "tokens": [50612, 31959, 4580, 631, 785, 2002, 6975, 416, 6321, 1750, 15890, 2316, 329, 288, 13623, 6975, 3343, 806, 27825, 368, 50794], "temperature": 0.0, "avg_logprob": -0.13912916917067308, "compression_ratio": 1.9382716049382716, "no_speech_prob": 0.0019439634634181857}, {"id": 501, "seek": 306788, "start": 3076.48, "end": 3083.6, "text": " reordenamiento bien este algoritmo que acabamos de escribir que hace esta b\u00fasqueda bas\u00e1ndose en", "tokens": [50794, 319, 19058, 16971, 3610, 4065, 3501, 50017, 3280, 631, 13281, 2151, 368, 30598, 10119, 631, 10032, 5283, 272, 10227, 358, 8801, 987, 18606, 541, 465, 51150], "temperature": 0.0, "avg_logprob": -0.13912916917067308, "compression_ratio": 1.9382716049382716, "no_speech_prob": 0.0019439634634181857}, {"id": 502, "seek": 306788, "start": 3083.6, "end": 3088.88, "text": " hip\u00f3tesis que utiliza recombinaci\u00f3n hipodado hip\u00f3tesis y bueno el calcula de las probabilidades", "tokens": [51150, 8103, 812, 7269, 271, 631, 4976, 13427, 850, 3548, 259, 3482, 8103, 378, 1573, 8103, 812, 7269, 271, 288, 11974, 806, 4322, 64, 368, 2439, 31959, 10284, 51414], "temperature": 0.0, "avg_logprob": -0.13912916917067308, "compression_ratio": 1.9382716049382716, "no_speech_prob": 0.0019439634634181857}, {"id": 503, "seek": 306788, "start": 3088.88, "end": 3093.76, "text": " de esta manera se conoce como algoritmo b\u00fasqueda asterisco es un algoritmo de bin search que se", "tokens": [51414, 368, 5283, 13913, 369, 33029, 384, 2617, 3501, 50017, 3280, 272, 10227, 358, 8801, 257, 3120, 8610, 785, 517, 3501, 50017, 3280, 368, 5171, 3164, 631, 369, 51658], "temperature": 0.0, "avg_logprob": -0.13912916917067308, "compression_ratio": 1.9382716049382716, "no_speech_prob": 0.0019439634634181857}, {"id": 504, "seek": 309376, "start": 3093.76, "end": 3101.0400000000004, "text": " usa much\u00edsimo en lo que es traducci\u00f3n autom\u00e1tica estad\u00edstica por ejemplo el sistema mouses ac\u00e1", "tokens": [50364, 29909, 44722, 465, 450, 631, 785, 2479, 1311, 5687, 3553, 23432, 39160, 19512, 2262, 1515, 13358, 806, 13245, 275, 18562, 23496, 50728], "temperature": 0.0, "avg_logprob": -0.16919047707005552, "compression_ratio": 1.915, "no_speech_prob": 0.01079213060438633}, {"id": 505, "seek": 309376, "start": 3101.0400000000004, "end": 3107.6800000000003, "text": " tenemos este ejemplos de herramientas open source o gratuitas que sirven para construcci\u00f3n de de", "tokens": [50728, 9914, 4065, 10012, 5895, 329, 368, 38271, 296, 1269, 4009, 277, 38342, 296, 631, 4735, 553, 1690, 12946, 14735, 368, 368, 51060], "temperature": 0.0, "avg_logprob": -0.16919047707005552, "compression_ratio": 1.915, "no_speech_prob": 0.01079213060438633}, {"id": 506, "seek": 309376, "start": 3107.6800000000003, "end": 3113.2000000000003, "text": " traductores autom\u00e1ticos el sistema mouses es un sistema open source para desarrollar este tipo", "tokens": [51060, 2479, 84, 1672, 279, 3553, 7656, 9940, 806, 13245, 275, 18562, 785, 517, 13245, 1269, 4009, 1690, 32501, 289, 4065, 9746, 51336], "temperature": 0.0, "avg_logprob": -0.16919047707005552, "compression_ratio": 1.915, "no_speech_prob": 0.01079213060438633}, {"id": 507, "seek": 309376, "start": 3113.2000000000003, "end": 3119.28, "text": " de traductores autom\u00e1ticos estad\u00edsticos e implementa este algoritmo de codificaci\u00f3n de", "tokens": [51336, 368, 2479, 84, 1672, 279, 3553, 7656, 9940, 39160, 19512, 9940, 308, 4445, 64, 4065, 3501, 50017, 3280, 368, 17656, 40802, 368, 51640], "temperature": 0.0, "avg_logprob": -0.16919047707005552, "compression_ratio": 1.915, "no_speech_prob": 0.01079213060438633}, {"id": 508, "seek": 311928, "start": 3119.28, "end": 3125.1600000000003, "text": " b\u00fasqueda asterisco y bueno lo que tiene el sistema mouses de bueno es que en realidad lo que hace", "tokens": [50364, 272, 10227, 358, 8801, 257, 3120, 8610, 288, 11974, 450, 631, 7066, 806, 13245, 275, 18562, 368, 11974, 785, 631, 465, 25635, 450, 631, 10032, 50658], "temperature": 0.0, "avg_logprob": -0.20774861849271334, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.03327985107898712}, {"id": 509, "seek": 311928, "start": 3125.1600000000003, "end": 3130.6000000000004, "text": " adem\u00e1s de implementar el decodificadores utiliza a los otros sistemas y los integra de alguna manera", "tokens": [50658, 21251, 368, 4445, 289, 806, 979, 378, 1089, 11856, 4976, 13427, 257, 1750, 16422, 48720, 288, 1750, 16200, 424, 368, 20651, 13913, 50930], "temperature": 0.0, "avg_logprob": -0.20774861849271334, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.03327985107898712}, {"id": 510, "seek": 311928, "start": 3130.6000000000004, "end": 3136.1600000000003, "text": " entonces integra este otro sistema el irs tlm que es una herramienta para crear modelos de lenguaje", "tokens": [50930, 13003, 16200, 424, 4065, 11921, 13245, 806, 3418, 82, 256, 75, 76, 631, 785, 2002, 38271, 64, 1690, 31984, 2316, 329, 368, 35044, 84, 11153, 51208], "temperature": 0.0, "avg_logprob": -0.20774861849271334, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.03327985107898712}, {"id": 511, "seek": 311928, "start": 3136.1600000000003, "end": 3140.8, "text": " basados en n en n gramas y el otro sistema se quiza m\u00e1s m\u00e1s que lo hab\u00edamos mencionado hoy que es", "tokens": [51208, 987, 4181, 465, 297, 465, 297, 677, 19473, 288, 806, 11921, 13245, 369, 421, 13427, 3573, 3573, 631, 450, 3025, 16275, 37030, 1573, 13775, 631, 785, 51440], "temperature": 0.0, "avg_logprob": -0.20774861849271334, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.03327985107898712}, {"id": 512, "seek": 311928, "start": 3140.8, "end": 3148.6400000000003, "text": " el sistema que me permite alinear corpus de oraciones en los distintos idiomas llegando", "tokens": [51440, 806, 13245, 631, 385, 31105, 419, 533, 289, 1181, 31624, 368, 420, 9188, 465, 1750, 49337, 18014, 7092, 11234, 1806, 51832], "temperature": 0.0, "avg_logprob": -0.20774861849271334, "compression_ratio": 1.8178438661710037, "no_speech_prob": 0.03327985107898712}, {"id": 513, "seek": 314864, "start": 3148.64, "end": 3153.8399999999997, "text": " los modelos del 1 al 5 de traducci\u00f3n de bm bueno entonces estas tres herramientas sirven si uno", "tokens": [50364, 1750, 2316, 329, 1103, 502, 419, 1025, 368, 2479, 1311, 5687, 368, 272, 76, 11974, 13003, 13897, 15890, 38271, 296, 4735, 553, 1511, 8526, 50624], "temperature": 0.0, "avg_logprob": -0.1479456072948018, "compression_ratio": 1.9027237354085602, "no_speech_prob": 0.0021812438499182463}, {"id": 514, "seek": 314864, "start": 3153.8399999999997, "end": 3158.0, "text": " quiere construir un traductor autom\u00e1tico estad\u00edstico entre cualquier par de idiomas puede utilizar", "tokens": [50624, 23877, 38445, 517, 2479, 84, 1672, 3553, 28234, 39160, 19512, 2789, 3962, 21004, 971, 368, 18014, 7092, 8919, 24060, 50832], "temperature": 0.0, "avg_logprob": -0.1479456072948018, "compression_ratio": 1.9027237354085602, "no_speech_prob": 0.0021812438499182463}, {"id": 515, "seek": 314864, "start": 3158.0, "end": 3164.48, "text": " estas tres herramientas y teniendo un corpus paralelo y un corpus monoling\u00fce puede construirse un", "tokens": [50832, 13897, 15890, 38271, 296, 288, 2064, 7304, 517, 1181, 31624, 26009, 10590, 288, 517, 1181, 31624, 1108, 401, 278, 774, 68, 8919, 12946, 36097, 517, 51156], "temperature": 0.0, "avg_logprob": -0.1479456072948018, "compression_ratio": 1.9027237354085602, "no_speech_prob": 0.0021812438499182463}, {"id": 516, "seek": 314864, "start": 3164.48, "end": 3170.56, "text": " traductor pero bueno adem\u00e1s otra cosa que mencionamos en la clase pasada pero este eran los", "tokens": [51156, 2479, 84, 1672, 4768, 11974, 21251, 13623, 10163, 631, 37030, 2151, 465, 635, 44578, 1736, 1538, 4768, 4065, 32762, 1750, 51460], "temperature": 0.0, "avg_logprob": -0.1479456072948018, "compression_ratio": 1.9027237354085602, "no_speech_prob": 0.0021812438499182463}, {"id": 517, "seek": 314864, "start": 3170.56, "end": 3175.3199999999997, "text": " sistemas basados en reglas los sistemas basados en reglas han ca\u00eddo un poco este digamos no tienen", "tokens": [51460, 48720, 987, 4181, 465, 1121, 7743, 1750, 48720, 987, 4181, 465, 1121, 7743, 7276, 1335, 28470, 517, 10639, 4065, 36430, 572, 12536, 51698], "temperature": 0.0, "avg_logprob": -0.1479456072948018, "compression_ratio": 1.9027237354085602, "no_speech_prob": 0.0021812438499182463}, {"id": 518, "seek": 317532, "start": 3175.32, "end": 3180.1600000000003, "text": " tanta popularidad como antes sin embargo algunos se siguen usando y el sistema apertium es un sistema", "tokens": [50364, 40864, 3743, 4580, 2617, 11014, 3343, 23955, 21078, 369, 4556, 7801, 29798, 288, 806, 13245, 257, 15346, 2197, 785, 517, 13245, 50606], "temperature": 0.0, "avg_logprob": -0.16607580686870374, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.027009742334485054}, {"id": 519, "seek": 317532, "start": 3180.1600000000003, "end": 3184.96, "text": " opensource para construir sistema de traducci\u00f3n basados en reglas que tiene como un mont\u00f3n de", "tokens": [50606, 9870, 2948, 1690, 38445, 13245, 368, 2479, 1311, 5687, 987, 4181, 465, 1121, 7743, 631, 7066, 2617, 517, 45259, 368, 50846], "temperature": 0.0, "avg_logprob": -0.16607580686870374, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.027009742334485054}, {"id": 520, "seek": 317532, "start": 3184.96, "end": 3190.0800000000004, "text": " pares de lenguajes y bueno ya anda relativamente bien digamos entonces se sigue desarrollando", "tokens": [50846, 2502, 495, 368, 35044, 84, 29362, 288, 11974, 2478, 21851, 21960, 3439, 3610, 36430, 13003, 369, 34532, 32501, 1806, 51102], "temperature": 0.0, "avg_logprob": -0.16607580686870374, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.027009742334485054}, {"id": 521, "seek": 317532, "start": 3190.0800000000004, "end": 3195.96, "text": " hasta hoy entonces es una alternativa opensource que est\u00e1 basada en reglas en vez de estar basado en", "tokens": [51102, 10764, 13775, 13003, 785, 2002, 5400, 18740, 9870, 2948, 631, 3192, 987, 1538, 465, 1121, 7743, 465, 5715, 368, 8755, 987, 1573, 465, 51396], "temperature": 0.0, "avg_logprob": -0.16607580686870374, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.027009742334485054}, {"id": 522, "seek": 317532, "start": 3195.96, "end": 3204.4, "text": " estad\u00edsticas y bueno esta es un resumen de lo que vimos as\u00ed que dejamos por ac\u00e1", "tokens": [51396, 39160, 19512, 9150, 288, 11974, 5283, 785, 517, 725, 16988, 368, 450, 631, 49266, 8582, 631, 21259, 2151, 1515, 23496, 51818], "temperature": 0.0, "avg_logprob": -0.16607580686870374, "compression_ratio": 1.816793893129771, "no_speech_prob": 0.027009742334485054}], "language": "es"}