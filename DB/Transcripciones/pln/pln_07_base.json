{"text": " En la clase de hoy vamos a ver un tema nuevo que es el de los modelos del lenguaje. Si ya fueran en la clase pasada, vimos que era bastante diferente, el de los transductores para resolver el tema de la morfolog\u00eda de Taufinito, unos artefactos de Taufinito que permiten resolver temas a trav\u00e9s de un m\u00e9todo de reglas. Yo defino reglas de como se conforman las palabras, las combino de cierta forma y de esa forma resuelvo el tema de convertir de la palabra a su an\u00e1lisis y viceversa. Y despu\u00e9s vimos la segunda parte de un m\u00e9todo que era bastante diferente, su concepci\u00f3n, que es un m\u00e9todo estad\u00edstico, que lo que hac\u00eda era aplicando el modelo del canal ruidoso, aproximarse al problema de corregir el rojo de ortogr\u00e1fico. Cuando yo hablo un modelo probabilista, lo que estoy diciendo es que adem\u00e1s de, por ejemplo, clasificar o sugerir una soluci\u00f3n, lo que haces es asignarle probabilidades a las posibles respuestas. Un m\u00e9todo probabilista, t\u00edpicamente no da una respuesta, sino que devuelve una distribuci\u00f3n de probabilidad. Si yo tengo varios eventos posibles, una distribuci\u00f3n de probabilidad es un n\u00famero, entre 0 y 1, que yo asigno a cada evento posible, de forma que la suma de todos los eventos de en 1, eso es lo que llamamos una distribuci\u00f3n de probabilidad. Entre 0 y 1 son todos, son todos mayores o iguales que 0, menores iguales que 1 y adem\u00e1s su suma da 1, eso es una distribuci\u00f3n de probabilidad. 0, 5, 0, 25, 0, 25 es una distribuci\u00f3n de probabilidad. Si el evento 1 tiene probabilidad 0, 5, el otro es 0, 25 y el otro es 0, 25, eso es una distribuci\u00f3n de probabilidad. Si no suma 1, no son una distribuci\u00f3n de probabilidad. Y si yo, por ejemplo, tengo un evento que ocurre 10 veces, si por ejemplo hago conteo de frecuencia, por ejemplo no digo hay un evento 1, que ocurre 10 veces, hay un evento 2, que ocurre 5 y hay un evento 3, que ocurre 5, eso no es una distribuci\u00f3n de probabilidad, porque esto no est\u00e1 entre 0 y 1, porque no suman 1. \u00bfC\u00f3mo hago yo para convertir esto en una distribuci\u00f3n de probabilidad? Lo que hago es dividir por el total de ocurrencia, \u00bfverdad? Que en este caso es 20 y eso me da la proporci\u00f3n respecto a 1 y eso es siempre una distribuci\u00f3n de probabilidad. Entonces, se llama normalizar para obtener una probabilidad. Y ustedes lo van a ver que lo vamos a ver en varias veces. El m\u00e9todo de este de correcci\u00f3n utilizaba fuertemente la regla de valles para modelar la situaci\u00f3n. Hasta ahora hemos hablado en todas las cosas que hemos tratado de palabras aisladas, \u00bfno? La morfolog\u00eda estudia, en primero hablamos de c\u00f3mo separar las palabras y despu\u00e9s vimos c\u00f3mo analizaba la intamimente, pero siempre habl\u00e1bamos de palabras aisladas. Ac\u00e1 lo que vamos a empezar a mirar es \u00bfqu\u00e9 pasa cuando las palabras aparecen juntas? Es decir, nosotros lo que vamos a hablar es de la probabilidad de una secuencia de palabras. \u00bfPor qu\u00e9 esto importa? Porque como ustedes bien sabr\u00e1n, las palabras en el idioma pa\u00f1\u00f3n nos aparecen solas y no cualquier palabra as\u00ed o otra palabra. Nosotros tenemos una cantidad de reglas para expresar en el idioma que hace que el orden importe. Y de lo que se trata es ver c\u00f3mo se orden, c\u00f3mo tener en cuenta se orden, no puede ayudar a otra estaria. Creo que con alg\u00fan ejemplo lo vamos a ver m\u00e1s claro. Primero que nada vamos a recordar a Chonky, que esto yo lo comentaba en la primera clase, aquello de que Chonky dijo la noci\u00f3n de probabilidad de una oraci\u00f3n es completamente in\u00fatil bajo cualquier interpretaci\u00f3n de este t\u00e9rmino y tranc\u00f3 por 20 a\u00f1os la investigaci\u00f3n hasta que apareci\u00f3, Shellinet que volvi\u00f3 a revivir el tema de los m\u00e9todos probabilistas o basados en conteos para aproxim\u00e1rselo el problema de procedimiento en el lenguaje natural. Chonky lo que dec\u00eda esencialmente es cuando nosotros hacemos conteos y sacamos conclusiones en base a cuenta, en base a n\u00famero, en base a experiencia, que es t\u00edpicamente lo que vamos a ver en este caso de los enigramos. Estamos obteniendo soluciones a problemas, no estamos entendiendo qu\u00e9 es lo que est\u00e1 pasando. Y eso es una discusi\u00f3n catal\u00eda de hoy sigue, es decir, hay una famosa discusi\u00f3n por ah\u00ed en internet entre Chonky, esto te hablando hace dos o tres a\u00f1os, o cinco a\u00f1os, entre Chonky y Peter Norby, que discute un poco esto, es decir, si esto que estamos haciendo ahora y que ha tenido tan buenos resultados del punto de vista de reconocimiento de labla y el procedimiento de los enigramos natural es en realidad inteligencia artificial o de solamente en number crunching que no nos aporta mucho. Norby en lo que le dice, bueno, de hecho, la ciencia siempre en modo menos funcion\u00f3 as\u00ed. Bueno, entonces \u00bfcu\u00e1l es el objetivo de lo que vamos a ver ac\u00e1 son de modelos del lenguaje? El objetivo del modelo del lenguaje es calcular la probabilidad de una secuencia palabra, es decir, \u00bfqu\u00e9 tan probable es en mi lenguaje que una secuencia se es? \u00bfDe acuerdo? \u00bfPara qu\u00e9 no puede servir eso? Bueno, imag\u00ednense que ustedes, y acabamos a recordarlo otra vez el modelo del canal ruidozo, del otra vez, imag\u00ednense que tengo este texto escrito, \u00bfs\u00ed? Y por medio de un m\u00e9todo que no s\u00e9 cu\u00e1l es, tengo dos oraciones candidatas, bueno, dos textos candidatos, uno que es preneva para el curso de PLN y prueba para el curso de PLN. \u00bfDe acuerdo? Y adem\u00e1s supongamos que el m\u00e9todo que utilic\u00e9 para reconocer la escritura me dice que este es m\u00e1s probable que este. Nosotros \u00bfqu\u00e9 vamos a elegir? Vamos a elegirle abajo. \u00bfPor qu\u00e9? Porque esto no es una palabra v\u00e1lida, pero aun siendo una palabra v\u00e1lida, o aun suponiendo que fuera una palabra v\u00e1lida, podr\u00eda darse un caso donde yo identifico una palabra v\u00e1lida, se ponen los correcciones, a\u00fan as\u00ed yo pod\u00eda decir bueno, pero en este lugar, en este lugar, esa palabra no calza, digamos, \u00bfs\u00ed alguna forma yo s\u00e9? Es decir, si yo logro detectar que esta oraci\u00f3n es m\u00e1s probable que esta de alguna forma, eso me va a ayudar en la tarea de reconocimiento. Lo mismo pasa con el reconocimiento de la habla de lo que hablamos y lo otro d\u00eda con el esp\u00edritu de reconocimiento y cuando yo hablo y digo una palabra, ustedes me escuchan. Entonces, los modelos de nevoje sirven para ayudar en este tipo de tarea, t\u00edpicamente los modelos de nevoje ayudan y no tratar\u00edan. Nos abregan mucha informaci\u00f3n. Entonces, cuando nosotros hacemos reconocimiento de escritura, luego lo que decimos es, \u00bfcu\u00e1l es la probabilidad de la oraci\u00f3n origen, dada la observaci\u00f3n que tengo? Yo tengo una observaci\u00f3n, \u00bfs\u00ed? \u00bfCu\u00e1l es la probabilidad de una oraci\u00f3n origen? Es proporcionar a la probabilidad de la observaci\u00f3n, dada la oraci\u00f3n por la probabilidad de la oraci\u00f3n. \u00bfY esto qu\u00e9 es? Eso es valles, en la rir de valles. Entonces, nosotros por valles sabemos eso. Y como ven, ac\u00e1 aparece la noci\u00f3n de probabilidad de la oraci\u00f3n. Por eso es que nos interesa conocer la probabilidad de las variaciones. Ahora, \u00bfc\u00f3mo calculamos la probabilidad de la oraci\u00f3n? Bueno, hay un ejemplo m\u00e1s, \u00bfno? Por ejemplo, en la traducion autom\u00e1tica, si tenemos estas tres candidatos, nuevamente a m\u00ed me va a ayudar con conocer el orden o saber cu\u00e1l es la m\u00e1s probable en mi linguaje. En las correcci\u00f3n de errores, como vimos la vez pasada, hordas de botero es una secuencia muy de poca probabilidad. Y pensemos un poquito. \u00bfPreguntemos, no? \u00bfPor qu\u00e9? Esta oraci\u00f3n no les parece que sea muy probable. \u00bfQu\u00e9 nos podr\u00eda determinar que esta oraci\u00f3n no es muy probable? O esta, implementaci\u00f3n a la educaci\u00f3n ley. \u00bfPor qu\u00e9 podemos suponer que esa no es probable? Bueno, a m\u00ed me ocurre en dos razones, principales o dos, pero s\u00ed mansiones. \u00bfUna es por las sintaxis, \u00bfno? La sintaxis del d\u00eda de mape\u00f1\u00f3n no es as\u00ed. No decimos educaci\u00f3n ley, educaci\u00f3n... \u00bfPor qu\u00e9 no? \u00bfPor qu\u00e9 no? \u00bfPor qu\u00e9 no? La secci\u00f3n es su y de botero, como publican la verdad. Ah, bueno, \u00bfPrecio pudiera ser un suh de un tercero, \u00bfno? Ac\u00e1 seguramente lo que hay es lo que hay es un error autogr\u00e1fico de sus gordas de botero. O sea, ac\u00e1, ac\u00e1 tenemos un tema de sintaxis, ac\u00e1 no tenemos un tema de sintaxis. Deber\u00edamos conocer un poco de sem\u00e1ntica para asociar botero que pintaba mujeres gordas. Entonces, una aproximaci\u00f3n un poco m\u00e1s humilde, es la segunda, es la alguna aproximaci\u00f3n m\u00e1s \u00e9tad\u00edstica, porque si nosotros, y que juega con el hecho de que tenemos grandes vol\u00famenes de texto y ah\u00ed el cambio de los modelos probabil\u00edsticos, es que sus gordas de botero seguramente apareci\u00f3 antes en mis cuerpos de texto y hordas de botero, eso es una aproximaci\u00f3n mucho m\u00e1s \u00e9tad\u00edstica, eso es lo que vamos a hacer en los modelos de negra m\u00e1s justamente. A partir de grande vol\u00famenes de texto, detectar, calcular la probabilidad. Es una aproximaci\u00f3n puramente \u00e9tad\u00edstica, es bien salvoaje, yo no s\u00e9 qu\u00e9 estructura tiene esto, pero s\u00e9 que esto no se dio nunca y que gordas de botero s\u00ed, muchas veces. Entonces, les m\u00e1s probar\u00e9 que m\u00e1s equivocado. A ver, relacionado con esto, ahora vamos a ver por qu\u00e9 est\u00e1 relacionado, est\u00e1 el tema de la predicci\u00f3n de la siguiente palabra. \u00bfCu\u00e1les se imaginan que es la siguiente palabra a la primera relaci\u00f3n? \u00bfCu\u00e1l puede ser la siguiente palabra? \u00bfQui\u00e9n? Para y no meten mi t\u00edo pron\u00f3stico para, qu\u00e9 otra cosa puede ser? Para es una preposici\u00f3n \u00bfno? \u00bfQu\u00e9 m\u00e1s? \u00bfQu\u00e9 otra cosa puede ser ah\u00ed? \u00bfCu\u00e1l por ejemplo? \u00bfUn pron\u00f3stico alentador? O puede decir un pron\u00f3stico terrible o un pron\u00f3stico... \u00bfQu\u00e9 otra cosa m\u00e1s? Hay un m\u00e1s com\u00fan para m\u00ed. El mit\u00edo pron\u00f3stico con meteorol\u00f3gico \u00bfno? A ra\u00edz de este fen\u00f3meno se suceder\u00e1n tormentas, fuertes, importantes, muy, no creo que ah\u00ed diga tormentas gatito \u00bfno? gatito no es muy probable que sea la palabra siguiente. Nuevamente, \u00bfpor qu\u00e9 sabemos esto? Y porque es muy raro que hay en diga tormentas gatitos \u00bfno? Entonces, esto que tenemos ac\u00e1 es la posibilidad de que hay de siguiente palabra. \u00bfDada todas las anteriores? Si yo tengo todo el contexto lo que se llama contexto, dado el contexto de la palabra que sigue ac\u00e1. \u00bfS\u00ed? Una de las, lo que nosotros vamos a querer hacer en un modelo de lenguaje como camino para calcular la protecci\u00f3n de honoraci\u00f3n es dado el contexto calcular la palabra. Siguiente. \u00bfS\u00ed? \u00bfRachas de viento fuerte de componente? Veremos que. Bueno, no resulta hacer que de los ejemplos que yo tom\u00e9 a Buenos Aires, puse viento fuerte de componente, perd\u00f3n. El lino me demiti\u00f3 pron\u00f3stico especial, o sea que le ramos, se suceder\u00e1n tormentas fuertes, viento fuerte, componente subo este. Por ejemplo, perdici\u00f3n. Vamos a poner un poquito de notaci\u00f3n antes de seguir, porque vamos a ver c\u00f3mo enfrentamos este problema, es decir, c\u00f3mo calculamos esa protecci\u00f3n. Un poco de notaci\u00f3n para seguir eso. Yo lo que estoy diciendo es la probabilidad de que una variable aleatoria ah\u00ed valga, tome el valor con ocimiento, en este caso tendr\u00eda una variable aleatoria por cada posici\u00f3n del texto, \u00bfverdad? Tengo una X1 que la primera palabra ha equid\u00f3 que es la segunda X3, son variables aleatoria que lo variable aleatoria esencialmente un mapeo, es una funci\u00f3n que me apega, de un evento un n\u00famero entre cero y un. La probabilidad, perd\u00f3n. Perd\u00f3n, perd\u00f3n. Bueno, no, mientras defin\u00ed, me apega con un real y la probabilidad me devuelve un n\u00famero entre cero y un. Es decir, yo defino la probabilidad de una variable aleatoria como la distribuci\u00f3n de probabilidad de una variable aleatoria es la dado de los diferentes valores que puede tomar, cu\u00e1l es el valor de cada uno de ellos, \u00bfs\u00ed? Y esto cu\u00e1l es el rango, \u00bfqu\u00e9 valores probable tiene cada una variable aleatoria que refira palabras? El todo el vocabulario, todas las palabras diferentes que yo puedo tener. Entonces nosotros vamos a poner estos notaciones probabilidades con ocimiento, de que la palabra sea conocimiento. Vamos a denotar W1 a la N1N a la secuencia de palabras W1, W2, WN, por ejemplo en una naci\u00f3n y vamos a decir que la vamos a hablar de la probabilidad de la secuencia de palabras queriendo decir, bueno, la probabilidad de la que la primera sea W1, que la segunda sea W2, etc\u00e9tera. \u00bfDe acuerdo? O sea que esta distribuci\u00f3n de probabilidad tiene como rango todas las secuencias posibles de palabras. O sea que si mi vocabulario es V, tengo N y a la V, V a la N, V a la N. O sea que es enorme, especialmente, si todas las posibles secuencias y vamos a recordar la chain rule o la regla de multiplicaci\u00f3n de las probabilidades que es, si yo tengo la probabilidad de una secuencia de palabras W1, WN, esto es la probabilidad de la primera palabra, que de alguna forma la calculo, por la probabilidad de la segunda da la primera, da que la primera, da que la primera fue W1, observen ac\u00e1 que no son independientes, es decir, la palabra por definici\u00f3n ac\u00e1, no son eventos independientes, es decir, tengo una cierta probabilidad de que empiece con W1, la multiplicaci\u00f3n por la probabilidad de que la segunda sea W2, da que la primera fue W1, por la probabilidad que la tercera sea W3, da que las dos primeras fueron uno de ah\u00ed as\u00ed. \u00bfDe acuerdo? de esa forma con esta regla yo y al final WN la \u00faltima da toda la santer\u00eda, esto se llama regla de la cadena, yo con la regla de la cadena puedo calcular la probabilidad de una secuencia o de una oraci\u00f3n, da la secuencia, si logro calcular estas probabilidades, o sea si logro calcular predecir las palabras correctamente, voy a poder predecir la secuencia, esa forma paso de la predicci\u00f3n al c\u00e1lculo de toda la probabilidad de la oraci\u00f3n. \u00bfEntienden? Bien, entonces vamos a quedarnos con esa notaci\u00f3n, entonces yo digo bueno, un ejemplo \u00bfno? Si yo quiero saber la probabilidad de viento fuerte, de componente sudeste como el que est\u00e1 soplando, no s\u00e9 si es componente sudeste, pero fuerte, es la probabilidad de viento por la probabilidad de fuerte, dado viento por la probabilidad de dado viento fuerte etc\u00e9tera, \u00bfno? Nada menos que la regla de la cadena. Entonces yo quiero saber la \u00faltima P de sudeste, dado viento fuerte, de componente y vos con Google por ejemplo digo bueno, fuerte, de componente aparece 9.230 veces, viento fuerte, componente sudeste aparece 347 veces, y yo entonces voy a estimar la probabilidad de esa por medio de conteos, entonces la cantidad de veces que ha aparecido viento fuerte, componente sudeste, dividido la cantidad de veces que aparece fuerte, componente, 347 veces dividido, no, 9.230. Aguardo, y esta es la probabilidad de que la siguiente palabra sea sudeste, en mi estimaci\u00f3n. Si ustedes desfijan, esto es una probabilidad porque contando todas las palabras posibles que pueden seguir ac\u00e1, si yo logro determinar cu\u00e1les son, yo s\u00e9 que van a ver 9.230, van a sumar 9.230, \u00bfno? En todo lo caso posible, mira todos los casos, junto a lo que son la siguiente palabra, eso hace que como esto me va a dar 9.230, la suma de todas las cuantidades, esto va a dar uno, entonces esto s\u00ed es una distribuci\u00f3n de probabilidad, entonces que estamos bien, efectivamente que yo des una probabilidad. Aguardo, esto es lo que me dices, bueno, el 3,76% de las veces es sudeste, la siguiente palabra. Eso que acabamos de hacer es estimar la probabilidad a partir de la frecuencia de ocurrencia en un corpo grande, eso Google es un corpo grande, muy grande. Y eso se llama principio m\u00e1ximo, pero similitud que lo vimos la de pasada, es, trato de hacer, calcular la probabilidad en base a lo mejor posible a los datos que tengo, es decir, considero, yo estoy considerando que los datos que tengo, es decir, el corpo de Google es una buena aproximaci\u00f3n del mundo real, del lenguaje en realidad, yo no s\u00e9 si en realidad efectivamente cuando los seres humanos hablamos, hay un 3,76% de probabilidad de que, despu\u00e9s decir bien tofuerte componente, viene sud\u00f3 este, pero el corpo de Google es que es lo mejor que tengo como aproximaci\u00f3n, me dices eso, y eso es lo que yo utilizo, como un estimador de m\u00e1xima de la similitud, lo mejor que puedo acercarme con el corpo que tengo, eso es lo que vamos a hacer todo el tiempo ac\u00e1, calcular componentes de m\u00e1xima de la similitud. Pero tenemos alg\u00fan problema, \u00bfno? Y es, en el otro caso, dice, a ra\u00edz estos fen\u00f3menos se producir\u00e1n tormentas fuertes, la pr\u00f3egue fuertes, y a ra\u00edz estos fen\u00f3menos se producir\u00e1n tormentas, tiene un problema, ah\u00ed es que, nunca apareci\u00f3 en mi corpus, a ra\u00edz estos fen\u00f3menos se producir\u00e1n tormentas, y nunca apareci\u00f3 en mi corpus, a ra\u00edz estos fen\u00f3menos se producir\u00e1n tormentas fuertes, \u00bfs\u00ed? Y eso nos da una horrible edici\u00f3n por cero, que queremos evitar, o sea que no est\u00e1 probabilidad, ah, infinito, no s\u00e9, no est\u00e1 definida, esto, una pregunta, esto les parece que es un fen\u00f3meno com\u00fan o no, que nos puede pasar cuando estemos estimando, todo el tiempo, porque por m\u00e1s grande que sea el corpus, el lenguaje es muy creativo, entonces tenemos que buscar forma y adem\u00e1s, porque estamos haciendo un conteo de palabras, de relaci\u00f3n muy largas, o sea que la rila de la cadena no resuelve en mi problema, porque yo, una aproximaci\u00f3n bien na\u00eff para que el culo de la probabilidad de calcular toda la secuencia posible, \u00bfcu\u00e1nta vez se aparece la secuencia que quiero calcular en la elaboraci\u00f3n del total de raciones, lo cual es un disparate, pues no tengo corpus, evidentemente grande, pero esta aproximaci\u00f3n tampoco nos ayuda mucho, porque sigo teniendo contexto muy largo, porque si ustedes se fijan, en la rila de la cadena, bueno, en lo que acabamos de hacer, la \u00faltima probabilidad es casi la misma que la primera, con menos una palabra, tengo que con una forma a chicar eso. Entonces, una de las ideas fuerza para computar esta probabilidad es el lugar de tomar todas las palabras, tomar sobre las \u00faltimas, es decir, yo me quedo con las \u00faltimas N menos un palabras, N menos N, bueno, \u00bfs\u00ed? N, N, esto es en gran, \u00bfno? Y las otras no las considero, digo bueno, la con, mi, mi, mi, mi humilde aproximaci\u00f3n para que esto se pueda volver manejable, es decir, bueno, yo en realidad solamente me importan las, solo las \u00faltimas palabras afectan en la que voy a predecir, solo la \u00faltima idea. Y de eso se tratan los modelos en grama, que utilizan lo que se llama, eso que acabo de decir, yo llamo hipote sigue marco, hipote sigue marcoviana, solamente las \u00faltimas palabras afectan la siguiente, hay un l\u00edmite, \u00bft\u00e1? Y f\u00edjense que en la hipote se divide grama, yo digo, cada palabra la aproximo por la anterior, simplemente, es decir, estoy diciendo una cosa tan sencilla como la \u00faltima palabra es la \u00fanica, cada palabra condici\u00f3n en la siguiente, pero en la anterior, \u00bfno? Es muy fuerte, \u00bfno? Y de trigramas son dos y con N en grama son N, \u00bfno? S\u00ed, con la hipote sigue divide grama, mi proviezo mucho m\u00e1s sencilla que antes, porque es como, cada palabra, solo depende, vamos a mire, uno bueno, uno no est\u00e1 m\u00e1s, pero cada palabra depende del anterior, simplemente me queda que la probabilidad de una secuencia, es la probabilidad de la primera, por la probabilidad de la segunda de la primera, por la probabilidad de la tercera de la segunda, etc\u00e9tera, y aguard\u00f3, ac\u00e1 nos falta este PW1 en esa f\u00f3rmula, pero no nos preocupa demasiado porque eso lo resolvemos poniendo una marca al comienzo de la secuencia que siempre vale uno su probabilidad, es decir que todas las variaciones empiezan con una marca, y si no, multiplico ac\u00e1, \u00bfno? Si no, si lo quiere hacer de otra forma, agrega un PW, es su cero, ac\u00e1 y lo mismo, pero esencialmente lo importante ac\u00e1 es que esto se transforma en una simple multiplicaci\u00f3n de probabilidades de una palabra a la anterior, y c\u00f3mo hago para calcular esto, c\u00f3mo puedo calcular esto ac\u00e1, c\u00f3mo calcular la probabilidad de una palabra, da en anterior, contando, pero solamente den cuenta a dos, lo cual lo vuelvo poniendo mucho m\u00e1s manejable, y eso es justo lo que vamos a hacer, un modelo de lenguaje intenta predecir la pr\u00f3xima palabra de una oraci\u00f3n a partir de las n menos una anterior, y por supuesto que importa el orden en ese c\u00e1lculo, \u00bfno? Tambi\u00e9n tenemos que plantearnos cuando hagamos los enegramos, cuando calculemos la probabilidad en general, bueno, cosas que ya hemos conversado, \u00bfqu\u00e9 elemento vamos a contar? S\u00ed, por ejemplo, tengo un tema de tokenizaci\u00f3n, esta coma, la tengo que considerar un diagrama o no la tengo que considerar un diagrama, \u00bfs\u00ed? La tengo que considerar un token o no la tengo considerar un token, me interesa, bueno eso seguramente va a depender un poco de la aplicaci\u00f3n en la que les aplican a los que les utilizan, o tengo un cuerpo oral donde tengo de fluencia, de fluencia, creo que ya me ha estado. \u00bfQu\u00e9 tengo que hacer con las may\u00fasculas? \u00bfQu\u00e9 hago con la forma flexionada? Todo lo problema de la tokenizaci\u00f3n me parece en el diagrama, es decir, esto son cascadas y amo, \u00bfno? Yo acab\u00e9 a tener la tokenizaci\u00f3n realizada, lo que ya no hay respuesto universal depende de la tarea que estamos haciendo, por ejemplo, t\u00edpicamente los cuerpos orales est\u00e1n todos pasados a may\u00fasculas, como son m\u00e1s continuos, no hay la identificaci\u00f3n de raciones, no es tan importante. Si yo voy a hacer an\u00e1lisis, si estoy haciendo un an\u00e1lisis de c\u00f3mo se usan los signos de puntuaci\u00f3n en mi lenguaje, obviamente la coma la tengo que identificar, sino que para que no me interese, o me puede interesar, todo esto es mapearlos a una cosa sola que se llama signos de puntuaci\u00f3n y juntar los puntos con las coma. Bueno, tiene que hacer eso en el laboratorio, ya se van a escolar. Bueno, nada, se necesita un pretetamiento, disponible al menos palabras, yo ni el modelo, no hay modelos generales. Tambi\u00e9n va a depender un poco, nuestros n\u00fameros van a depender de la cantidad de palabras. El diccionario, el Oxford English Dictionary tiene 290.000 entradas, el trezor de la sangre franc\u00e9s tiene 54.000 y el diccionario de la radio es 88.000. \u00bfPor qu\u00e9 les parece que tienen tantas m\u00e1s acacagadas? Porque el diccionario no parece en la forma flexionada y el espa\u00f1ol est\u00e1 mucho m\u00e1s flexionado que el n\u00famero. O sea, el inmune se va a tener que arreglar m\u00e1s solito. Bueno, y despu\u00e9s tenemos corpos, esto ya hablamos un poco, y aquellos distinguyen entre el n\u00famero de toques en que son la cantidad de ocurrencias que hay en el texto y el n\u00famero de palabras distintas, el vocabular. Ac\u00e1 est\u00e1 la respuesta a la pregunta de que hac\u00edamos antes, \u00bfc\u00f3mo estimamos lo vigilan m\u00e1s? Utilizando otra vez lo que se llama un estimador de m\u00e1ximo a ver el similitud, lo que se llama m\u00e9todos de frecuencias relativas, que es cuento, la cantidad de veces que apareci\u00f3 una palabra con, por ejemplo, la probabilidad de fuerte, dado viento, se aproxima como la cantidad de veces que aparece bien tofuerte, por la dividida de la cantidad de veces que apareci\u00f3, dividido todas las posibles continuaciones, \u00bfde acuerdo? Viento fuerte, viento calmo, viento, viento diles, viento, no s\u00e9, lo que quieras. Y sumo todas las posibles, estoy haciendo normalizando como hablamos al principio de como hablamos ac\u00e1, estoy normalizando. Ahora, esto aqu\u00ed es equivalente, \u00bfc\u00f3mo puedo simplificar esto? Si yo tengo todas las disica, parece viento fuerte, viento calmo, no s\u00e9, \u00bfqu\u00e9 es la suma de todo eso? Y la cantidad es de la peseamiento, estoy igual a la cantidad de veces que aparece bien tof, en el corp. \u00bfC\u00f3mo guard\u00f3? \u00bfC\u00f3mo son todas las posibles ocurrencias? Ah\u00ed tenemos la simplificaci\u00f3n y adem\u00e1s para tener en cuenta la primera y \u00faltima palabra de honoraci\u00f3n, le vamos a agregar siempre los s\u00edmbolos de comienzo y de fin, eso para asegurarnos de que para no tener que calcularse parada la probabilidad de la primera palabra. Yo s\u00e9 que la primera palabra siempre es ese y calculo la probabilidad de la primera en el texto, digamos, ponerle \u00e9l dado que la anterior era ese, \u00bfde acuerdo? Y as\u00ed lo dejo en una sola forma. Por ejemplo, si supongamos que yo tengo ese corp, \u00bfno? Oan, abri\u00f3 la puerta, el viento abri\u00f3 la puerta, el negro abri\u00f3 limones en tus mejillas nuevas, Juan recoge limones. Y quiero saber la probabilidad de estas oraciones. Evidentemente, no las tengo en el corp, ya que no es poco tan directamente, pero quiero utilizar un modelo de diagramas para calcular. Y con lo que sabemos es bastante sencillo. Primero que nada, decimos bueno, la probabilidad de Juan abri\u00f3 limones es probabilidad de Juan dado el comienzo, probabilidad de abri\u00f3 dado Juan, probabilidad de limones de abri\u00f3, etc\u00e9tera, \u00bfno? F\u00edjense que la probabilidad Juan dado el comienzo de la cantidad de veces que apareci\u00f3 Juan en la marca del comienzo, dividido en la cantidad de marca del comienzo que es uno. Entonces, he tomado... 2 de 4. Ah, \u00bfpor qu\u00e9 hay cuatro oraciones? Claro, claro, porque yo estoy haciendo contegos directamente, no estoy haciendo probabilidad. 2 de 4 veces arranc\u00f3 con Juan, \u00bfs\u00ed? Juan abri\u00f3 es una de 2, ya hab\u00eda parecido Juan abri\u00f3 en el corpus y Juan apareci\u00f3 2 veces. O sea, de 2 veces la pareci\u00f3 Juan en la siguiente apareci\u00f3 una vez abri\u00f3. Y as\u00ed sigo multiplicando y como ve, multiplica la fracci\u00f3n y me da, bueno, 0,042, esa es la probabilidad de Juan abri\u00f3 limones. Enero abri\u00f3 la puerta, 0,17, tambi\u00e9n tiene mucho sentido, \u00bfno? A ver, justamente el hecho de que sigo un ejemplo de jubete le hace perder la gracia todo esto, porque esto funciona porque tengo grandes vol\u00famenes, sino no es una paba. Y ac\u00e1 que nos pas\u00f3, \u00bfqu\u00e9 puede haber pasado ac\u00e1? La palabra come nunca est\u00e1. Y en la puerta, en la puerta est\u00e1. La primera se explica porque come nunca est\u00e1. Creo que est\u00e1 as\u00ed, perd\u00f3n, la si, la puerta, \u00bfpor qu\u00e9 es la 0? Porque lo que no est\u00e1 es en la, en la, no aparece nunca, si ustedes miren ac\u00e1 la probabilidad de, perd\u00f3n, la cantidad de, la probabilidad de esto es la probabilidad de que empiece con \u00e9l, ya tenemos un problema con el comienzo con \u00e9l, porque creo que no hay ninguna. Ning\u00fan empieza con \u00e9l, y t\u00fa ya tienes un problema y adem\u00e1s en la tampoco est\u00e1, o sea que el conteo me da 0, si el vigrama no aparece en el cuerpo de entrenamiento, siempre mi problema me da 0, y m\u00e1s interesante a\u00fan, si cualquier vigrama de todos los que aparecen en la oraci\u00f3n, da 0, la probabilidad de la oraci\u00f3n es 0, eso es un gran problema. Resolver el problema de eso y lo que se llama el suavizado de negra m\u00e1s que vamos a ver c\u00f3mo, tenemos que ir una forma de resolver eso que nos va a pasar siempre, es decir, como nuestro cuerpo, nunca puede ser tan, aunque solo sean dos palabras, igual puede aparecer mi pareja de palabras que no aparecieron y yo no me puedo transcar con eso, \u00bfde acuerdo? Bueno, nos queda ese pendiente del cielo que lo vamos a ver despu\u00e9s porque ya te quiero comentar y con una cosa, pero vamos a acordarnos de eso, y t\u00fa y ten\u00e9is un buen problema pendiente. Bien, en general ustedes eran, bueno, pero \u00bfcu\u00e1l es el mejor ene? \u00bfNo? \u00bfPor qu\u00e9? \u00bfCu\u00e1l es el tema? Es cu\u00e1nto, cu\u00e1nto, m\u00e1s largo sea el tirama que yo utilizo, m\u00e1s informaci\u00f3n tengo de contexto, es decir, intuitivamente mejor estimar con 5 palabras que con una. Vamos a guardar con eso. \u00bfCu\u00e1l es el problema de los 3 m\u00e1s largos? \u00bfPor qu\u00e9 no puedo usar el 15? Porque tenemos mi problema, porque llegamos ac\u00e1, que con 15 no tengo corpos f\u00edjendemente grande como para que aparecan esa ocurrencia. Entonces, ese balance entre cantidad de ocurrencia, porque yo no tengo una buena estimaci\u00f3n de la cantidad de ocurrencia, no voy a poder estimar bien la probabilidad. Con eso bien que yo estoy atimada la probabilidad de partir en contegos, si yo tengo una, dos, tres ocurrencias seguramente esa probabilidad artificial, pues si hubo una ocurrencia en un corpo de miles de millones de palabras, no me est\u00e1 diciendo mucho. Realmente en igual 3 se obtienen buenos resultados, por lo menos para aproximarse de la cantidad de cada muy bien, Google hace unos a\u00f1os atr\u00e1s sac\u00f3 un corpo de negra, un s\u00ed, una lista de negra m\u00e1s de hasta 5, no recordo que no est\u00e1 ah\u00ed porque ven\u00edan en 7. O sea que determinar\u00e9, \u00bfne va a depender un poco la tarea y ese se me dio a ojos? Digamos, pues yo me estar\u00eda un poco b\u00f3blica. Ahora vamos a ver un poco de evaluaci\u00f3n, y tal y lo que dec\u00edamos, \u00bfno se agregan? Cuando son 3g m\u00e1s tengo que agregar 2 s\u00edmbolos, el comienzo de la oraci\u00f3n. Te voy a poner enero abri\u00f3, porque yo necesito 2 de contexto para calcular el triunfo en detalle. \u00bfCu\u00e1ndo? Ah\u00ed no te caas, as\u00ed que no. Y bueno, y la pregunta es \u00bfc\u00f3mo calculamos? De ver punto de vista metodol\u00f3gico, \u00bfc\u00f3mo hacemos para calcular buenas probabilidades? Ya vimos c\u00f3mo se hace el conteo. Ahora quiero ver c\u00f3mo organizo el corpo, y me parece que es interesante ver esto porque nos va a pasar en muchas cosas, en este tema, el procedimiento de lengua natural, y que muchas veces induce el mal uso metodol\u00f3gico de estas cosas, lleva error. Entonces me parece que va de la pena comentarlo esto. Yo dije que iba a ser conteo para calcular las probabilidades, \u00bfno? Entonces yo por ac\u00e1 tengo un corpus, un corpus de texto, \u00bfsi? Entonces, sencillamente lo que tengo son muchos textos, \u00bfno? Obviamente, sencillamente no, tengo muchos textos, esa es la definici\u00f3n de corno. Y yo voy a crear un modelo de un modelo de un lenguaje, es decir, yo lo que quiero construir con esto de las probabilidades de las eleaciones es un modelo del idioma pa\u00f1ol. Yo tengo un corpus de texto en espa\u00f1ol, y quiero hacer un modelo del idioma pa\u00f1ol. Supongo que yo entreno un modelo, entrenar el modelo en este caso que es decir calcular todas esas probabilidades. \u00bfC\u00f3mo hago para saber qu\u00e9 tan bueno es? \u00bfS\u00ed? \u00bfC\u00f3mo lo evalu\u00f3? Supongo que yo ahora voy a modular el cual es la medida, pero supongo que yo tengo una medida de performance que me dice bueno, aplicale tu modelo a este texto, s\u00ed, supongamos que la medida es el que le asigne, ahora vamos a ver por qu\u00e9, pero el que le asigne mayor probabilidad a todo el texto a las oraciones del texto es el mejor, el mejor modelo es que la asigna probabilidad mayor a la oraci\u00f3n en que tengo el texto. Si yo aplico mi m\u00e9todo, mi modelo, o sea, el lugo, mi modelo, sobre este mismo corpus, \u00bfqu\u00e9 problema tengo? Que me va a dar barro, porque los calcul\u00e9 ah\u00ed, es decir, yo nunca puedo, nunca, pero nunca nunca, he valuado un modelo en el mismo corpus en el que entrenes. Esto aplica siempre, cada vez que es un dif\u00edcil m\u00e9todotad\u00edstico, pensado autom\u00e1tico, lo m\u00e1s importante es saber en la pensado autom\u00e1tico, nunca, el lugo es tu modelo en un corpus, en el mismo corpus que entrenaste, porque por definici\u00f3n est\u00e1s haciendo trampa, eso lo llama sobre ajustes, sobre ajustas a tu corpus de entrenamiento. Entonces yo lo que voy a hacer es dividir mi corpus en dos, y voy a decir, este es el corpus de entrenamiento, voy a poner en ingl\u00e9s y el corpus de evaluaci\u00f3n. Entonces lo que yo voy a hacer es entrenar y cu\u00e1nto se par\u00f3 ac\u00e1. Bueno, la regla m\u00e1s o menos es 80 20. Pregunto, \u00bfpor qu\u00e9 me interesar\u00eda que esto fuera lo m\u00e1s grande posible? Para que tener m\u00e1s informaci\u00f3n, \u00bfy por qu\u00e9 no uso 90 10 o 95 o 97 3? \u00bfC\u00f3mo? Tengo que solucionar ese balance, no entretener una cantidad razonable de datos, porque si yo le val\u00fa sobre una oraci\u00f3n, la variance es muy grande, es decir, la posibilidad de equivocarme es muy grande. Entonces, una regla es m\u00e1s o menos 80 20, \u00bfs\u00ed? Y bueno, ah\u00ed habla de 90 10, yo tengo la regla de 80 20. Va a solucir un problema adicional ac\u00e1 y es que ahora lo voy a ver es, por ejemplo, si yo quiero saber cu\u00e1ntos elegir el N, \u00bfno? Yo quiero elegir el N, yo necesito lo que va a hacer es prevo con un N ac\u00e1, modelo 1, en igual 2 y a\u00fan modelo 2, en igual 3. Y esto es un poco m\u00e1s \u00fatil, y lo val\u00faa ac\u00e1 y digo M1 y M2, y me qued\u00f3 con el que me da mejor. Y esos m\u00e9todologicamente no est\u00e1n bien, \u00bfpor qu\u00e9? Y esto es una de las cosas que es m\u00e1s dif\u00edcil entender a veces, es, si yo prevo los dos modelos ac\u00e1, de alguna forma tambi\u00e9n estoy haciendo trampa, porque supongan que yo tengo no dos par\u00e1metros, porque ac\u00e1 tengo o un par\u00e1metro que tiene dos valores. Supongamos que yo quiero ajustar otro par\u00e1metro de mi m\u00e9todo, que puede tomar 500 valores posible. Si yo hago 500 en realidad, y 500 pruebas, s\u00ed, muy probablemente tambi\u00e9n estoy ajustando ac\u00e1, estoy ajustando ac\u00e1, porque estoy elegiendo de los 500 y a veces puede ser miles o 300 de miles, el que mejora anda en este corpo de evaluaci\u00f3n, o sea que estoy sobre ajustando el corpo de evaluaci\u00f3n. Entonces, para la ajuste de par\u00e1metro yo usualmente lo que tengo que hacer es definir dividir este corpus, sacar un pedacito del corpo en trainamiento, que lo llamo corpus, gel dauto, corpus de desarrollo, y lo que hago es entrenos sobre esta parte y evaluos sobre el gel dauto, y me reservo este de evaluaci\u00f3n, solamente para cuando tengo mi modelo definitivo, y quiero saber su performance, con su medio de evaluaci\u00f3n. \u00bfAguardo? Esto lo van a tener que presentar en el laboratorio, es decir, c\u00f3mo evaluar\u00edan el m\u00e9todo, un m\u00e9todo. Hay otras posibilidades que no implican un cuerpo gel dauto, por ejemplo, hacer lo que se llama coros validation, que es separo este pedacito, entrenos sobre esto y evaluos sobre este, despu\u00e9s separo otra franjita y entrenos sobre el resto y evaluos sobre la franjita, y as\u00ed con cas franjas y saco el promedio. Eso me sirve para no desperdiciar, digamos, esta parte del corpus, para poder utilizar todo el corpus entrenadito. Esa m\u00e1s, cros validation. Vamos a volver a hablar un poquito cros validation cuando le hemos clasificaci\u00f3n, pero lo que me interesa es que le quede claro la diferencia entre estos corpus, y cuando tengo el modelo final, uso esto solamente para evaluar la performance, es una medida que determinar\u00e9 seg\u00fan mi tarea. \u00bfC\u00f3mo evaluamos un modelo bueno? La manera correcta de evaluar un modelo deber\u00eda ser\u00eda emp\u00edricamente, es decir, yo quiero evaluar un modelo del lenguaje y lo estoy usando para el reconocimiento de la habla, deber\u00eda ser una evaluaci\u00f3n de que tambi\u00e9n reconozco el habla, o que tambi\u00e9n reconozco la escritura, pero eso puede ser muy costoso a veces. Yo puedo estar haciendo un modelo lenguaje, no s\u00e9 para qu\u00e9 se va a usar. Entonces, me interesa mucho, me puede interesar tener una medida intr\u00ednseca de la performance de mi modelo. Entonces, vamos a ver una forma de evaluar. A mi esta parte, de esta parte, en el libro est\u00e1 apuesta como un tema avanzado, pero a m\u00ed me parece interesante mostrarlo, porque la entrop\u00eda es un concepto que aparece muchas veces en el procedimiento de lenguaje natural de otras cosas, y me pese que le va a ir la pena por lo menos aproximarse. Supongo que yo tengo una variabilidad aleatoria y todo esto voy a llegar a una forma de evaluar un modelo, no hay que empezar a hablar de todo esto. Supongo que s\u00ed que yo tengo una variabilidad aleatoria que tiene varios eventos posibles, en otro caso dijimos que eran las palabras posibles. La entrop\u00eda, la entrop\u00eda es una variabilidad aleatoria que es un concepto que viene de la teor\u00eda de informaci\u00f3n, de CloudXanon, la teleinformaci\u00f3n lo que hablaba era, bueno, algunos capacicieron, lo vieron a alg\u00fan curso, pero la teleinformaci\u00f3n lo que trataba era de medir cu\u00e1nto me cuesta a m\u00ed transmitir un mensaje. \u00bfC\u00f3mo puedo transmitir un mensaje de forma \u00f3ptima? Digamos un poco la idea, o que hay atr\u00e1s de una comunicaci\u00f3n. La noci\u00f3n de entrop\u00eda, estas funciones, tengo el evento que quiero hacer, la probabilidad del evento, por el hogarismo de esa probabilidad. La entrop\u00eda tiene como caracter\u00edstica fundamental que es una medida que si hay un evento que tiene toda la masa de probabilidad, la entrop\u00eda es m\u00ednima, es decir, si yo tengo un dado que est\u00e1 tan carregado y una forma en algo que valentemente se puede decir que la entrop\u00eda a m\u00ed es mirado de incertidumbre sobre un evento. Si yo tengo un dado que est\u00e1 tan carregado, que cabe que lo tiro, s\u00e9 que siempre vas a salir seis, no tengo incertidumbre, mi entrop\u00eda es cero. En cambio, si el dado est\u00e1 perfectamente calibrado, equilibrado, mi entrop\u00eda es m\u00e1xima. Es decir, \u00bfc\u00f3mo est\u00e1 definida la entrop\u00eda? No puedo tener etrop\u00eda m\u00e1s alta que cuando los eventos est\u00e1n equipos lo hablan. Entonces justamente la entrop\u00eda es generalmente lo que uno mide con la entrop\u00eda de eso, \u00bfqu\u00e9 est\u00e1n parecidos? Son los resultados que est\u00e1n balanceados, est\u00e1n de alguna forma. Cuanto m\u00e1s incertidumbre tengo, porque est\u00e1n m\u00e1s balanceados. Si yo no tengo ni la menor idea de la palabra que sigue, mi entrop\u00eda es m\u00e1xima. Y adem\u00e1s tiene otra caracter\u00edstica que es que si lo har\u00edamos es en base dos. Este n\u00famero, la entrop\u00eda me mide la cantidad de bits que yo necesito m\u00ednimos para transmitir los eventos. Esto es lo mejor forma de hacerlo con un ejemplo. Supongamos, y es el ejemplo que aparece en el libro. Supongamos que yo tengo ocho caballos. Tengo ocho caballos que quiero transmitirlas las apuestas que se est\u00e1n haciendo por un cable. Entonces digo, bueno, una forma cantada de transmitir lo directa de transmitir, llamar al primer caballo 0-1, 0-10, 0-11, 101, 110, 111. De acuerdo, ac\u00e1 yo uso ocho bits. Cada vez que se apuesta por el caballo 1, yo poco 0-0, 0-1, blabla. Entonces en total yo utilizo tres bits para transmitirlas por un cable, tres bits por cada apuesta, \u00bfno? Ahora, cuando nosotros vemos las apuestas, descubrimos que la mitad de las veces se apuesta por el caballo 1. Un cuarto del caballo 2, un tercio blabla, un octavo del caballo 3, un disiseo del caballo 4, y todos estos se apuesta mucho menos. Teniendo en cuenta eso, yo lo que trato de hacer ahora es decir, bueno, quiero proponer una codificaci\u00f3n mejor que hace que yo, los caballos que se apuesta m\u00e1s, o sea que tengo que transmitir m\u00e1s seguido, los codificos con menos bits. De acuerdo, la mitad de los bits, el primer bit, lo utilizo solo para el caballo 1, es decir, que si es un 0 es que transmitir el caballo 1, necesita un solo bit. Si es un 1, si es un 1 y un 0 despu\u00e9s es el caballo 2. Si son 2, 1 y un 0 despu\u00e9s es el caballo 3. Si son 3, 1 y un 0, f\u00edjense que yo para transmitir esto caballo utilizo 1, 2, 3, 4, 5, 6 bits. Utilizo m\u00e1s bits, pero como son mucho menos probable, mi entrop\u00eda me da 2 bits, o sea, el promedio de bits que yo utilizo seg\u00fan la distribuci\u00f3n es 2 bits, que es m\u00e1s baja que los 3 bits originales. \u00bfCentiende? Incorporando la informaci\u00f3n de la distribuci\u00f3n bajo. Podemos mejorar eso, no podemos mejorar eso. Nunca vamos a hacer el etrop\u00eda, lo que lo dice es eso, nunca vas a encontrar una, porque justamente la etrop\u00eda 2, como la etrop\u00eda 2, la etrop\u00eda me da una cota inferior sobre cu\u00e1nto puedo llegar, con menos de 2 bits no puedo. \u00bfTambi\u00e9n te acuerdo? Te dice preguntar\u00e1n para qu\u00e9 sirve esto. De hecho no, la etrop\u00eda es una cota, lo que dec\u00eda, una cota m\u00ednima para el n\u00famero de bits necesaria. A partir de la etrop\u00eda yo puedo calcular la etrop\u00eda de una secuencia, la etrop\u00eda de una secuencia es de todas las combinaciones posibles de una secuencia, la probabilidad de esa combinaci\u00f3n es lo mismo para aplicar la secuencia, entonces si lo ven es un n\u00famero muy complicado, porque es la sumatoria de una cantidad impresionante de n\u00famero, porque son todas las combinaciones posibles de secuencia. Eso es lo que me mide la etrop\u00eda de la secuencia, \u00bfqu\u00e9 tanta incertidumbre hay en una secuencia? Y la tasa de etrop\u00eda ser\u00eda eso debido a N, es decir el promedio, porque si no la secuencia malarga o no tiene entrop\u00eda m\u00e1s alto, el promedio por palabra de la etrop\u00eda. Entonces la etrop\u00eda de un lenguaje, que ser\u00eda como la medida de qu\u00e9 tanta incertidumbre hay en un lenguaje, \u00bfqu\u00e9 tan, digamos, qu\u00e9 tanto pollo llegar a predecir lo que va a seguir diciendo el lenguaje? Esa l\u00edmite, pero como valoso, no en un contexto general en el lenguaje, es una medida para el lenguaje. Esa l\u00edmite cuando la secuencia tiene infinito de la tasa de etrop\u00eda, \u00bfs\u00ed? Y que es que ac\u00e1 es la suma, como dec\u00edamos, es la suma de todas las secuencias posibles, o sea que es una cosa imposible calcular, pero hay un teorema que es el de llano, como a mi la embraiman que dice que es el lenguaje, \u00e9l es estacionario y erg\u00f3lico. Estacionario y erg\u00f3lico quiere decir que no importa d\u00f3nde yo est\u00e9 parado en una secuencia, todas las posiciones van en las probabilidades o en las mismas de la limidad, lo cual no es as\u00ed en un lenguaje, porque lo que yo digo ahora y s\u00ed dentro de lo que estoy diciendo entre un minuto m\u00e1s, no, no hay aleatorio de lo m\u00e1s, pero suponiendo eso es una simplificaci\u00f3n, lo que me permite es simplemente para calcular la entrop\u00eda, la tasa de entrop\u00eda, el lenguaje es simplemente unos sobre nes divididos logarimos, f\u00edjense que perd\u00ed la probabilidad de cada una de las secuencias, es como que si yo tomo una secuencia suficientemente larga del lenguaje, voy a incluir a todas las secuencias, o sea que si yo una secuencia suficientemente larga puede ser el corpo de evaluaci\u00f3n, yo puedo calcular la entrop\u00eda sobre el corpo de evaluaci\u00f3n, y entonces, esto es un n\u00famero, ahora lo que dije ac\u00e1 es un n\u00famero, no sabemos por qu\u00e9 tengo esto, \u00bfno? Pero f\u00edjense que si yo puedo calcular lo que se llama la entrop\u00eda cruzada, porque yo que tengo, yo tengo un lenguaje que genera las palabras con una cierta distribuci\u00f3n de probabilidad, que es lo que queremos averiguar, que es lo que es lo que es lo que es nuestra problema original, es como da las palabras anteriores y se genera la siguiente, eso es algo que he desconocido, no sabemos como es, porque es el del lenguaje espa\u00f1ol, que yo quiero calcular, pero yo tengo un modelo M, que es el modelo de negramas, est\u00e1, la entrop\u00eda cruzada, lo que dice, bueno, calculamos esta hache utilizando la probabilidad original por el lovarismo del, de la probabilidad sin nada por el modelo, la probabilidad de la secuencia es la que ten\u00eda los movilidades, no la conozco, y la probabilidad, y en lovarimos s\u00ed, o sea, esa distancia es a largo emb\u00edtesis del modelo, seguramente tenemos otra vez, ya lo manmelan, yo puedo sacar esta probabilidad simplificando la suponiendo que es el gode y que lo la, y digo bueno, la entrop\u00eda cruzada es, depende s\u00f3lo lovarismo de, de la probabilidad sin nada por lenguaje, por el modelo, y esto es interesante, cualquier, cualquier entrop\u00eda cruzada que yo tenga, que yo calcule con un modelo, va a ser mayor necesariamente que la entrop\u00eda es del lenguaje, cualquier modelo va a ensinarme una entrop\u00eda mayor a la del lenguaje, entonces la, la, la, la, la cota inferior, entonces f\u00edjense que como son todas mayores, cuanto m\u00e1s parecido sea mi modelo, al modelo, al modelo de lenguaje, al, al, al, cuanto m\u00e1s modelo, m\u00e1s parecido, as\u00ed que mi probabilidad es m\u00e1s parecida de las de ac\u00e1, por c\u00f3mo est\u00e1 definido, va a ser mejor, de acuerdo, entonces, cuanto menor sea la entrop\u00eda cruzada de mi modelo, evaluado sobre una secuencia suficientemente larga, decir sobre el corpo de evaluaci\u00f3n, mejor va a ser mi aproximaci\u00f3n, y justamente la medida de esa intr\u00ednsega que est\u00e1 buscando era es esto, que es dos, porque dos no lo s\u00e9, porque lo mismo, es dos, es para sacarlo lovarimos nada m\u00e1s, es dos a la entrop\u00eda cruzada a este valor, y esto se llama perplejida, la perplejida es lo que mide el, el, lo que mide que tan bueno es interisidamente mi modelo sobre, sobre mi cuerpo de entrenamiento, sobre mi cuerpo de evaluaci\u00f3n, es decir, si yo tengo dos modelos, el que as\u00ed me mayor probabilidad, menor propiedad, mayor probabilidad, al corp\u00f3n de evaluaci\u00f3n es mejor desde ese punto de vista, lo consideramos mejor, porque porque tiene menos dudas de c\u00f3mo se comporta, porque la perplejida es, es como la incertidumbre que yo tengo ante, dada una palabra, cuando sume para una palabra, cu\u00e1l es mi incertidumbre, mi branching factor, en cu\u00e1ntas se puede abrir la siguiente palabra en promedio, un poco eso es lo que captura la perplejida, mi lenguaje va a tener un branching factor, es decir, no es que es cero, pero mi modelo siempre va a calcular algo mayor igual a ese branching factor, cuanto m\u00e1s bajo, es que si yo me estoy acercando mal a la perplejida posta, por eso la perplejida es la medida de que tambi\u00e9n hace la cosa, acuerdo, bueno, no, eso es su cuenta, por ejemplo, si nosotros entrenamos un \u00edgrama, m\u00e1s \u00edgrama, m\u00e1s \u00edgrama, en un cuerpo de art\u00edculo de Wall Street Journal, de 38 millones de palabras, probaron el cuerpo sobre un modelo, ni un cuerpo de prueba de 1,5 millones de palabras, y calcularon la perplejida, y f\u00edjense que la perplejida con los unigramos desde 962, no sabemos cu\u00e1l es el m\u00ednimo esto, no sabemos cu\u00e1nto puede bajar, pero sabemos que con v\u00edgrama lleg\u00f3 a 170 y contr\u00edgrama a 109, es decir, si yo tengo dos palabras antes, puedo predecir con mejor, porque ac\u00e1 es con un \u00edgrama, es la probabilidad de la palabra, no dice mucho, si yo tengo el anterior, r\u00e1pidamente baja, y si se fija cuando abre un tercero baja, pero no tanto, ni de cerca tanto, no, bueno, lo \u00faltimo que nos queda hablar, no dice, no pas\u00f3 con las probabilidades nudes, se acuerdan que nos quedaban las probabilidades nudes cuando no hab\u00eda contigo, bueno, uno de los problemas es las palabras que no existen, las palabras que no existen, lo \u00fanico que podemos hacer, o lo que t\u00edpicamente se hace es crear un vocabulario fijo y sustituyo las palabras desconocidas por un especial, esto es t\u00edpicamente lo que se hace, es decir, todas las palabras desconocidas las considero una sola palabra que no se equivale, y cuando aparecen enigradas m\u00e1s que no ocurren, tiene el caso de comer, que no aparecidas, pero puede ser que la enigrama no ocurra lo que voy a hacer, son t\u00e9cnicas de suavizado, yo tengo, se acuerdan, tengo el contador de, por ejemplo, ac\u00e1 es un migra a mano, contador de la palabra, de cantidad de veces la palabra dividido el total de token que hay, y as\u00ed calculo las probabilidades, la t\u00e9cnica de la plaza, lo que dice es bueno, le agrego uno a cada contador, o sea que nunca me va a dar cero, lo hago a los bestia, digamos, no, para que no me decero le sumo uno, y le sumo ve y se acuerdan el nuevo poquito de una clase pasada, le sumo ve para que esto me siga dando una distribuci\u00f3n de probabilidad, esto es simplemente lo que hace es calcular un contador ajustado, me explica por t\u00e9 y divide por temas, si me explica por el juvenil y divide por esto, por el PWI, por ejemplo, si yo digo, si este es mi corpo entrenamiento, esta es la historia de un hombre y la ciudad que cre\u00f3, f\u00edjense que mi conteo da uno, la habla y quiso me da cero, perd\u00f3n, este es el conteo, ah\u00ed va, conteo de este es uno, de la es dos y de quiso es cero, la probabilidad de este es uno y divide 13, total de palabras, una es esta y es 0 0 8, la es 2 divide 13 y quiso me da cero en la probabilidad que nos queremos que nos da cero, si nosotros aplicamos la plaza, lo que me da es sumo 25, son 12 palabras en el vocabulario, porque la unidad est\u00e1 repetida es la s\u00ed, o sea que tengo 12 en el vocabulario no 13, 13 es T y 12 es B, entonces ya hago 2 divide 25 y as\u00ed me da las nuevas probabilidades y ac\u00e1 quiso dejar de ser cero, el contador ajustado de lo que nos permite es comparar lo que ten\u00edamos antes con lo que ten\u00edamos ahora, por ejemplo, esta val\u00eda 1 y baja a 0 96, perd\u00f3n, la val\u00eda 2 y baja a 1 44 y quiso va a de 0 a 0 48, si se fijan ac\u00e1 el descuento, lo que se llama descuento que es la divisi\u00f3n entre los dos valores, lo permite ver que le estoy sacando m\u00e1s masa de probabilidad a la que hay que quedar casi igual, es decir, la meta le la tiene a la plaza el problema, por qu\u00e9 es lo que est\u00e1 pasando ac\u00e1, esto es lo que me muestra es que yo le tengo que sacar masa de probabilidad a los que aparecen, porque todo me tiene que sumar 1, toda la probabilidad me tiene que sumar 1, si yo ya agregar 5 gramas que antes estaban en cero, tengo que sacarle probabilidad a los que est\u00e1, pues no me es un mam\u00e1 que 1, entonces esto es lo que tiene que castiga mucho a los m\u00e1s frecuentes, le sacan mucho probabilidad a los m\u00e1s frecuentes y como que premia demasiado a los que no aparecen, hay otras t\u00e9cnicas no, no, vamos a entrar en eso, que tratan de ajustarlo un poco mejor, pero ahora vamos a mover alguna muy demasiada probabilidad, otra posibilidad es usar un delta en lugar de 1 y ese delta te va a calcularlo, se acuerdan lo que hablamos del cuerpo, siempre que yo tengo esos par\u00e1metros para calcular los calculos sobre el cuerpo de desarrollo, finalmente hay otro, esa es una aproximaci\u00f3n, es decir, con t\u00e9cnicas sobre el contencio, hay otra posibilidad que son un poco m\u00e1s evolucios avanzadas, digamos que es, cuando yo quiero estimar, por ejemplo en t\u00e9cnicas de trigrama, una palabra, a partir de las dos anteriores y no existen casos de las dos anteriores en el texto, de las dos anteriores seguida doble, \u00bfno? Ac\u00e1 es doble, perd\u00f3n, lo que hago es hacer lo que se llama BACOF, hacia calcularlo a trav\u00e9s de la probabilidad de la anterior, si no tengo la anterior prueba con la anterior, eso llamas BACOF, el BACOF, ten\u00e9s que resolver tambi\u00e9n que ahora otra vez est\u00e1 introduciendo en nuevas, luego caso que no ten\u00edas antes, estas probabilidades que calcularle y darle masa de probabilidad, otra vez tengo que mover probabilidad, cuando los corpos son muy muy grandes, una forma alternativa y es un m\u00e9todo muy nuevo, se llama Stupid BACOF, que es como mi corpos muy grande, t\u00edpicamente el corpos de Google, es no normalizo nada de las probabilidades, este conteo, no m\u00e1s como me fue, si una no me da prueba con la anterior, si igual tengo un mont\u00f3n de edad, o tambi\u00e9n se puede hacer interpolaci\u00f3n, es decir, la probabilidad de una palabra daba las dos anteriores, es la probabilidad de la palabra, la probabilidad nueva, es la probabilidad original de la palabra daba las dos anteriores por un cierto lambda, un cierto lambda 2 por la probabilidad de la palabra daba el sol en el vigrama, m\u00e1s la probabilidad de un vigrama, y convino las tres a la vez, es como convino las tres t\u00ednias a la vez, es decir, le doy un cierto peso a las probabilidades que yo quiero, de esta forma, porque ac\u00e1 podr\u00eda ser que existiera el vigrama anterior, pero existiera una vez sola, entonces yo no le tengo mucha confianza a esa, puede sucederme y no le tengo mucha confianza, entonces le doy un cierto peso a este tambi\u00e9n, y capa que le doy un peso un poquito m\u00e1s alto a este, o sea, si este existe, est\u00e1 todo bien, pero este es siempre una ayuda, y de esa forma balanceo, como calculo esto es lambda y con el corpos de valo, tengo que, de alguna forma calcularlo sobre el cuerpo de desarrollo, o el cuerpo gelado, tambi\u00e9n hay interpolaci\u00f3n condicionada por el contexto, o sea, hay un lambda, ac\u00e1 ya lo que pasa es un poco m\u00e1s raro, y un poco m\u00e1s moderno, digamos que es que m\u00e1s de estas \u00e9pocas, digamos, donde a m\u00ed ya no me preocupa tanto tener muchos par\u00e1metros, ac\u00e1 estoy definiendo un par\u00e1metro para cada combinaci\u00f3n de palabras, y hasta aqu\u00ed llegamos hoy, esto es este cap\u00edtulo que tengo ac\u00e1, cap\u00edtulo 4 del libro Yurazki, tiene algunas cositas m\u00e1s, presencialmente es eso, y es lo que vamos a hablar de en este curso de Nigrama, la clase que viene, presentamos la baratocha.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 26.22, "text": " En la clase de hoy vamos a ver un tema nuevo que es el de los modelos del", "tokens": [50364, 2193, 635, 44578, 368, 13775, 5295, 257, 1306, 517, 15854, 18591, 631, 785, 806, 368, 1750, 2316, 329, 1103, 51675], "temperature": 0.0, "avg_logprob": -0.2591801933620287, "compression_ratio": 1.0895522388059702, "no_speech_prob": 0.1761016696691513}, {"id": 1, "seek": 2622, "start": 26.22, "end": 35.86, "text": " lenguaje. Si ya fueran en la clase pasada, vimos que era bastante diferente, el de los", "tokens": [50364, 35044, 84, 11153, 13, 4909, 2478, 17669, 282, 465, 635, 44578, 1736, 1538, 11, 49266, 631, 4249, 14651, 20973, 11, 806, 368, 1750, 50846], "temperature": 0.0, "avg_logprob": -0.4138824939727783, "compression_ratio": 1.502824858757062, "no_speech_prob": 0.39924126863479614}, {"id": 2, "seek": 2622, "start": 35.86, "end": 44.86, "text": " transductores para resolver el tema de la morfolog\u00eda de Taufinito, unos artefactos de", "tokens": [50846, 1145, 769, 349, 2706, 1690, 34480, 806, 15854, 368, 635, 1896, 69, 29987, 368, 314, 1459, 5194, 3528, 11, 17780, 29159, 44919, 329, 368, 51296], "temperature": 0.0, "avg_logprob": -0.4138824939727783, "compression_ratio": 1.502824858757062, "no_speech_prob": 0.39924126863479614}, {"id": 3, "seek": 2622, "start": 44.86, "end": 55.46, "text": " Taufinito que permiten resolver temas a trav\u00e9s de un m\u00e9todo de reglas. Yo defino reglas de", "tokens": [51296, 314, 1459, 5194, 3528, 631, 13423, 268, 34480, 40284, 257, 24463, 368, 517, 20275, 17423, 368, 1121, 7743, 13, 7616, 1561, 78, 1121, 7743, 368, 51826], "temperature": 0.0, "avg_logprob": -0.4138824939727783, "compression_ratio": 1.502824858757062, "no_speech_prob": 0.39924126863479614}, {"id": 4, "seek": 5546, "start": 55.46, "end": 64.34, "text": " como se conforman las palabras, las combino de cierta forma y de esa forma resuelvo el", "tokens": [50364, 2617, 369, 18975, 282, 2439, 35240, 11, 2439, 2512, 2982, 368, 39769, 1328, 8366, 288, 368, 11342, 8366, 725, 3483, 3080, 806, 50808], "temperature": 0.0, "avg_logprob": -0.27136805250838, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.011463050730526447}, {"id": 5, "seek": 5546, "start": 64.34, "end": 73.22, "text": " tema de convertir de la palabra a su an\u00e1lisis y viceversa. Y despu\u00e9s vimos la segunda parte", "tokens": [50808, 15854, 368, 7620, 347, 368, 635, 31702, 257, 459, 44113, 28436, 288, 11964, 840, 64, 13, 398, 15283, 49266, 635, 21978, 6975, 51252], "temperature": 0.0, "avg_logprob": -0.27136805250838, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.011463050730526447}, {"id": 6, "seek": 5546, "start": 73.22, "end": 78.42, "text": " de un m\u00e9todo que era bastante diferente, su concepci\u00f3n, que es un m\u00e9todo estad\u00edstico,", "tokens": [51252, 368, 517, 20275, 17423, 631, 4249, 14651, 20973, 11, 459, 10413, 39859, 11, 631, 785, 517, 20275, 17423, 39160, 19512, 2789, 11, 51512], "temperature": 0.0, "avg_logprob": -0.27136805250838, "compression_ratio": 1.5254237288135593, "no_speech_prob": 0.011463050730526447}, {"id": 7, "seek": 7842, "start": 78.42, "end": 86.26, "text": " que lo que hac\u00eda era aplicando el modelo del canal ruidoso, aproximarse al problema de", "tokens": [50364, 631, 450, 631, 324, 31298, 4249, 18221, 1806, 806, 27825, 1103, 9911, 5420, 7895, 78, 11, 31270, 11668, 419, 12395, 368, 50756], "temperature": 0.0, "avg_logprob": -0.27139515346950954, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.033671118319034576}, {"id": 8, "seek": 7842, "start": 86.26, "end": 91.54, "text": " corregir el rojo de ortogr\u00e1fico. Cuando yo hablo un modelo probabilista, lo que estoy diciendo", "tokens": [50756, 1181, 3375, 347, 806, 744, 5134, 368, 23564, 47810, 23858, 78, 13, 21907, 5290, 3025, 752, 517, 27825, 31959, 5236, 11, 450, 631, 15796, 42797, 51020], "temperature": 0.0, "avg_logprob": -0.27139515346950954, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.033671118319034576}, {"id": 9, "seek": 7842, "start": 91.54, "end": 101.1, "text": " es que adem\u00e1s de, por ejemplo, clasificar o sugerir una soluci\u00f3n, lo que haces es asignarle", "tokens": [51020, 785, 631, 21251, 368, 11, 1515, 13358, 11, 596, 296, 25625, 277, 459, 1321, 347, 2002, 24807, 5687, 11, 450, 631, 324, 887, 785, 382, 788, 36153, 51498], "temperature": 0.0, "avg_logprob": -0.27139515346950954, "compression_ratio": 1.4734042553191489, "no_speech_prob": 0.033671118319034576}, {"id": 10, "seek": 10110, "start": 101.1, "end": 108.17999999999999, "text": " probabilidades a las posibles respuestas. Un m\u00e9todo probabilista, t\u00edpicamente no da una", "tokens": [50364, 31959, 10284, 257, 2439, 1366, 14428, 1597, 47794, 13, 1156, 20275, 17423, 31959, 5236, 11, 256, 28236, 23653, 572, 1120, 2002, 50718], "temperature": 0.0, "avg_logprob": -0.20108945846557616, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.14733898639678955}, {"id": 11, "seek": 10110, "start": 108.17999999999999, "end": 113.38, "text": " respuesta, sino que devuelve una distribuci\u00f3n de probabilidad. Si yo tengo varios eventos", "tokens": [50718, 40585, 11, 18108, 631, 1905, 3483, 303, 2002, 4400, 30813, 368, 31959, 4580, 13, 4909, 5290, 13989, 33665, 2280, 329, 50978], "temperature": 0.0, "avg_logprob": -0.20108945846557616, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.14733898639678955}, {"id": 12, "seek": 10110, "start": 113.38, "end": 124.17999999999999, "text": " posibles, una distribuci\u00f3n de probabilidad es un n\u00famero, entre 0 y 1, que yo asigno a cada", "tokens": [50978, 1366, 14428, 11, 2002, 4400, 30813, 368, 31959, 4580, 785, 517, 14959, 11, 3962, 1958, 288, 502, 11, 631, 5290, 382, 788, 78, 257, 8411, 51518], "temperature": 0.0, "avg_logprob": -0.20108945846557616, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.14733898639678955}, {"id": 13, "seek": 10110, "start": 124.17999999999999, "end": 129.82, "text": " evento posible, de forma que la suma de todos los eventos de en 1, eso es lo que llamamos", "tokens": [51518, 40655, 26644, 11, 368, 8366, 631, 635, 2408, 64, 368, 6321, 1750, 2280, 329, 368, 465, 502, 11, 7287, 785, 450, 631, 16848, 2151, 51800], "temperature": 0.0, "avg_logprob": -0.20108945846557616, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.14733898639678955}, {"id": 14, "seek": 12982, "start": 129.82, "end": 133.9, "text": " una distribuci\u00f3n de probabilidad. Entre 0 y 1 son todos, son todos mayores o iguales", "tokens": [50364, 2002, 4400, 30813, 368, 31959, 4580, 13, 27979, 1958, 288, 502, 1872, 6321, 11, 1872, 6321, 815, 2706, 277, 10953, 279, 50568], "temperature": 0.0, "avg_logprob": -0.21613749904908997, "compression_ratio": 2.3045685279187818, "no_speech_prob": 0.02401358261704445}, {"id": 15, "seek": 12982, "start": 133.9, "end": 138.06, "text": " que 0, menores iguales que 1 y adem\u00e1s su suma da 1, eso es una distribuci\u00f3n de probabilidad.", "tokens": [50568, 631, 1958, 11, 1706, 2706, 10953, 279, 631, 502, 288, 21251, 459, 2408, 64, 1120, 502, 11, 7287, 785, 2002, 4400, 30813, 368, 31959, 4580, 13, 50776], "temperature": 0.0, "avg_logprob": -0.21613749904908997, "compression_ratio": 2.3045685279187818, "no_speech_prob": 0.02401358261704445}, {"id": 16, "seek": 12982, "start": 138.06, "end": 142.78, "text": " 0, 5, 0, 25, 0, 25 es una distribuci\u00f3n de probabilidad. Si el evento 1 tiene probabilidad", "tokens": [50776, 1958, 11, 1025, 11, 1958, 11, 3552, 11, 1958, 11, 3552, 785, 2002, 4400, 30813, 368, 31959, 4580, 13, 4909, 806, 40655, 502, 7066, 31959, 4580, 51012], "temperature": 0.0, "avg_logprob": -0.21613749904908997, "compression_ratio": 2.3045685279187818, "no_speech_prob": 0.02401358261704445}, {"id": 17, "seek": 12982, "start": 142.78, "end": 147.74, "text": " 0, 5, el otro es 0, 25 y el otro es 0, 25, eso es una distribuci\u00f3n de probabilidad. Si no", "tokens": [51012, 1958, 11, 1025, 11, 806, 11921, 785, 1958, 11, 3552, 288, 806, 11921, 785, 1958, 11, 3552, 11, 7287, 785, 2002, 4400, 30813, 368, 31959, 4580, 13, 4909, 572, 51260], "temperature": 0.0, "avg_logprob": -0.21613749904908997, "compression_ratio": 2.3045685279187818, "no_speech_prob": 0.02401358261704445}, {"id": 18, "seek": 12982, "start": 147.74, "end": 155.34, "text": " suma 1, no son una distribuci\u00f3n de probabilidad. Y si yo, por ejemplo, tengo un evento que", "tokens": [51260, 2408, 64, 502, 11, 572, 1872, 2002, 4400, 30813, 368, 31959, 4580, 13, 398, 1511, 5290, 11, 1515, 13358, 11, 13989, 517, 40655, 631, 51640], "temperature": 0.0, "avg_logprob": -0.21613749904908997, "compression_ratio": 2.3045685279187818, "no_speech_prob": 0.02401358261704445}, {"id": 19, "seek": 15534, "start": 155.34, "end": 159.34, "text": " ocurre 10 veces, si por ejemplo hago conteo de frecuencia, por ejemplo no digo hay un evento", "tokens": [50364, 26430, 265, 1266, 17054, 11, 1511, 1515, 13358, 38721, 34444, 78, 368, 2130, 66, 47377, 11, 1515, 13358, 572, 22990, 4842, 517, 40655, 50564], "temperature": 0.0, "avg_logprob": -0.21686971187591553, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.006558598019182682}, {"id": 20, "seek": 15534, "start": 159.34, "end": 167.34, "text": " 1, que ocurre 10 veces, hay un evento 2, que ocurre 5 y hay un evento 3, que ocurre", "tokens": [50564, 502, 11, 631, 26430, 265, 1266, 17054, 11, 4842, 517, 40655, 568, 11, 631, 26430, 265, 1025, 288, 4842, 517, 40655, 805, 11, 631, 26430, 265, 50964], "temperature": 0.0, "avg_logprob": -0.21686971187591553, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.006558598019182682}, {"id": 21, "seek": 15534, "start": 167.34, "end": 177.66, "text": " 5, eso no es una distribuci\u00f3n de probabilidad, porque esto no est\u00e1 entre 0 y 1, porque no", "tokens": [50964, 1025, 11, 7287, 572, 785, 2002, 4400, 30813, 368, 31959, 4580, 11, 4021, 7433, 572, 3192, 3962, 1958, 288, 502, 11, 4021, 572, 51480], "temperature": 0.0, "avg_logprob": -0.21686971187591553, "compression_ratio": 1.7866666666666666, "no_speech_prob": 0.006558598019182682}, {"id": 22, "seek": 17766, "start": 177.66, "end": 186.98, "text": " suman 1. \u00bfC\u00f3mo hago yo para convertir esto en una distribuci\u00f3n de probabilidad? Lo que", "tokens": [50364, 2408, 282, 502, 13, 3841, 28342, 38721, 5290, 1690, 7620, 347, 7433, 465, 2002, 4400, 30813, 368, 31959, 4580, 30, 6130, 631, 50830], "temperature": 0.0, "avg_logprob": -0.27547078450520834, "compression_ratio": 1.5314285714285714, "no_speech_prob": 0.044851016253232956}, {"id": 23, "seek": 17766, "start": 186.98, "end": 199.57999999999998, "text": " hago es dividir por el total de ocurrencia, \u00bfverdad? Que en este caso es 20 y eso me da la", "tokens": [50830, 38721, 785, 4996, 347, 1515, 806, 3217, 368, 26430, 1095, 2755, 11, 3841, 331, 20034, 30, 4493, 465, 4065, 9666, 785, 945, 288, 7287, 385, 1120, 635, 51460], "temperature": 0.0, "avg_logprob": -0.27547078450520834, "compression_ratio": 1.5314285714285714, "no_speech_prob": 0.044851016253232956}, {"id": 24, "seek": 17766, "start": 199.57999999999998, "end": 205.38, "text": " proporci\u00f3n respecto a 1 y eso es siempre una distribuci\u00f3n de probabilidad. Entonces,", "tokens": [51460, 41516, 5687, 35694, 257, 502, 288, 7287, 785, 12758, 2002, 4400, 30813, 368, 31959, 4580, 13, 15097, 11, 51750], "temperature": 0.0, "avg_logprob": -0.27547078450520834, "compression_ratio": 1.5314285714285714, "no_speech_prob": 0.044851016253232956}, {"id": 25, "seek": 20538, "start": 205.38, "end": 212.46, "text": " se llama normalizar para obtener una probabilidad. Y ustedes lo van a ver que lo vamos a ver", "tokens": [50364, 369, 23272, 2710, 9736, 1690, 28326, 260, 2002, 31959, 4580, 13, 398, 17110, 450, 3161, 257, 1306, 631, 450, 5295, 257, 1306, 50718], "temperature": 0.0, "avg_logprob": -0.36898237286192, "compression_ratio": 1.5103734439834025, "no_speech_prob": 0.04097248241305351}, {"id": 26, "seek": 20538, "start": 212.46, "end": 220.01999999999998, "text": " en varias veces. El m\u00e9todo de este de correcci\u00f3n utilizaba fuertemente la regla de valles", "tokens": [50718, 465, 37496, 17054, 13, 2699, 20275, 17423, 368, 4065, 368, 1181, 13867, 5687, 19906, 5509, 8536, 911, 16288, 635, 1121, 875, 368, 371, 37927, 51096], "temperature": 0.0, "avg_logprob": -0.36898237286192, "compression_ratio": 1.5103734439834025, "no_speech_prob": 0.04097248241305351}, {"id": 27, "seek": 20538, "start": 220.01999999999998, "end": 228.98, "text": " para modelar la situaci\u00f3n. Hasta ahora hemos hablado en todas las cosas que hemos tratado", "tokens": [51096, 1690, 2316, 289, 635, 29343, 13, 45027, 9923, 15396, 26280, 1573, 465, 10906, 2439, 12218, 631, 15396, 21507, 1573, 51544], "temperature": 0.0, "avg_logprob": -0.36898237286192, "compression_ratio": 1.5103734439834025, "no_speech_prob": 0.04097248241305351}, {"id": 28, "seek": 20538, "start": 228.98, "end": 233.54, "text": " de palabras aisladas, \u00bfno? La morfolog\u00eda estudia, en primero hablamos de c\u00f3mo separar", "tokens": [51544, 368, 35240, 257, 5788, 6872, 11, 3841, 1771, 30, 2369, 1896, 69, 29987, 13542, 654, 11, 465, 21289, 26280, 2151, 368, 12826, 3128, 289, 51772], "temperature": 0.0, "avg_logprob": -0.36898237286192, "compression_ratio": 1.5103734439834025, "no_speech_prob": 0.04097248241305351}, {"id": 29, "seek": 23354, "start": 233.54, "end": 237.85999999999999, "text": " las palabras y despu\u00e9s vimos c\u00f3mo analizaba la intamimente, pero siempre habl\u00e1bamos de palabras", "tokens": [50364, 2439, 35240, 288, 15283, 49266, 12826, 2624, 590, 5509, 635, 560, 335, 332, 1576, 11, 4768, 12758, 26280, 27879, 2151, 368, 35240, 50580], "temperature": 0.0, "avg_logprob": -0.3086734940023983, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.1771499514579773}, {"id": 30, "seek": 23354, "start": 237.85999999999999, "end": 244.38, "text": " aisladas. Ac\u00e1 lo que vamos a empezar a mirar es \u00bfqu\u00e9 pasa cuando las palabras aparecen", "tokens": [50580, 257, 5788, 6872, 13, 5097, 842, 450, 631, 5295, 257, 31168, 257, 3149, 289, 785, 3841, 16412, 20260, 7767, 2439, 35240, 15004, 13037, 50906], "temperature": 0.0, "avg_logprob": -0.3086734940023983, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.1771499514579773}, {"id": 31, "seek": 23354, "start": 244.38, "end": 259.86, "text": " juntas? Es decir, nosotros lo que vamos a hablar es de la", "tokens": [50906, 22739, 296, 30, 2313, 10235, 11, 13863, 450, 631, 5295, 257, 21014, 785, 368, 635, 51680], "temperature": 0.0, "avg_logprob": -0.3086734940023983, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.1771499514579773}, {"id": 32, "seek": 25986, "start": 259.86, "end": 278.18, "text": " probabilidad de una secuencia de palabras. \u00bfPor qu\u00e9 esto importa? Porque como ustedes bien", "tokens": [50364, 31959, 4580, 368, 2002, 907, 47377, 368, 35240, 13, 3841, 24907, 8057, 7433, 33218, 30, 11287, 2617, 17110, 3610, 51280], "temperature": 0.0, "avg_logprob": -0.22367120296397108, "compression_ratio": 1.3111111111111111, "no_speech_prob": 0.030697837471961975}, {"id": 33, "seek": 25986, "start": 278.18, "end": 283.08000000000004, "text": " sabr\u00e1n, las palabras en el idioma pa\u00f1\u00f3n nos aparecen solas y no cualquier palabra", "tokens": [51280, 5560, 81, 7200, 11, 2439, 35240, 465, 806, 18014, 6440, 2502, 2791, 1801, 3269, 15004, 13037, 1404, 296, 288, 572, 21004, 31702, 51525], "temperature": 0.0, "avg_logprob": -0.22367120296397108, "compression_ratio": 1.3111111111111111, "no_speech_prob": 0.030697837471961975}, {"id": 34, "seek": 28308, "start": 283.08, "end": 290.08, "text": " as\u00ed o otra palabra. Nosotros tenemos una cantidad de reglas para expresar en el idioma", "tokens": [50364, 8582, 277, 13623, 31702, 13, 18749, 11792, 9914, 2002, 33757, 368, 1121, 7743, 1690, 33397, 289, 465, 806, 18014, 6440, 50714], "temperature": 0.0, "avg_logprob": -0.382926641191755, "compression_ratio": 1.4692737430167597, "no_speech_prob": 0.1827656477689743}, {"id": 35, "seek": 28308, "start": 290.08, "end": 302.2, "text": " que hace que el orden importe. Y de lo que se trata es ver c\u00f3mo se orden, c\u00f3mo tener", "tokens": [50714, 631, 10032, 631, 806, 28615, 974, 68, 13, 398, 368, 450, 631, 369, 31920, 785, 1306, 12826, 369, 28615, 11, 12826, 11640, 51320], "temperature": 0.0, "avg_logprob": -0.382926641191755, "compression_ratio": 1.4692737430167597, "no_speech_prob": 0.1827656477689743}, {"id": 36, "seek": 28308, "start": 302.2, "end": 305.59999999999997, "text": " en cuenta se orden, no puede ayudar a otra estaria. Creo que con alg\u00fan ejemplo lo vamos", "tokens": [51320, 465, 17868, 369, 28615, 11, 572, 8919, 38759, 257, 13623, 871, 9831, 13, 40640, 631, 416, 26300, 13358, 450, 5295, 51490], "temperature": 0.0, "avg_logprob": -0.382926641191755, "compression_ratio": 1.4692737430167597, "no_speech_prob": 0.1827656477689743}, {"id": 37, "seek": 30560, "start": 305.6, "end": 313.08000000000004, "text": " a ver m\u00e1s claro. Primero que nada vamos a recordar a Chonky, que esto yo lo comentaba", "tokens": [50364, 257, 1306, 3573, 16742, 13, 19671, 2032, 631, 8096, 5295, 257, 2136, 289, 257, 761, 266, 4133, 11, 631, 7433, 5290, 450, 14541, 5509, 50738], "temperature": 0.0, "avg_logprob": -0.33901186993247584, "compression_ratio": 1.4300518134715026, "no_speech_prob": 0.3523324429988861}, {"id": 38, "seek": 30560, "start": 313.08000000000004, "end": 320.40000000000003, "text": " en la primera clase, aquello de que Chonky dijo la noci\u00f3n de probabilidad de una oraci\u00f3n", "tokens": [50738, 465, 635, 17382, 44578, 11, 2373, 11216, 368, 631, 761, 266, 4133, 27024, 635, 572, 5687, 368, 31959, 4580, 368, 2002, 420, 3482, 51104], "temperature": 0.0, "avg_logprob": -0.33901186993247584, "compression_ratio": 1.4300518134715026, "no_speech_prob": 0.3523324429988861}, {"id": 39, "seek": 30560, "start": 320.40000000000003, "end": 328.18, "text": " es completamente in\u00fatil bajo cualquier interpretaci\u00f3n de este t\u00e9rmino y tranc\u00f3 por 20 a\u00f1os la", "tokens": [51104, 785, 28381, 294, 2481, 20007, 30139, 21004, 7302, 3482, 368, 4065, 45198, 78, 288, 504, 4463, 812, 1515, 945, 11424, 635, 51493], "temperature": 0.0, "avg_logprob": -0.33901186993247584, "compression_ratio": 1.4300518134715026, "no_speech_prob": 0.3523324429988861}, {"id": 40, "seek": 32818, "start": 328.18, "end": 334.3, "text": " investigaci\u00f3n hasta que apareci\u00f3, Shellinet que volvi\u00f3 a revivir el tema de los m\u00e9todos", "tokens": [50364, 48919, 10764, 631, 15004, 19609, 11, 22863, 259, 302, 631, 1996, 4917, 812, 257, 3698, 592, 347, 806, 15854, 368, 1750, 20275, 378, 329, 50670], "temperature": 0.0, "avg_logprob": -0.33325173602840763, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.040858689695596695}, {"id": 41, "seek": 32818, "start": 334.3, "end": 341.14, "text": " probabilistas o basados en conteos para aproxim\u00e1rselo el problema de procedimiento en el", "tokens": [50670, 31959, 14858, 277, 987, 4181, 465, 34444, 329, 1690, 31270, 20335, 790, 78, 806, 12395, 368, 6682, 14007, 465, 806, 51012], "temperature": 0.0, "avg_logprob": -0.33325173602840763, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.040858689695596695}, {"id": 42, "seek": 32818, "start": 341.14, "end": 347.02, "text": " lenguaje natural. Chonky lo que dec\u00eda esencialmente es cuando nosotros hacemos conteos y sacamos", "tokens": [51012, 35044, 84, 11153, 3303, 13, 761, 266, 4133, 450, 631, 37599, 785, 26567, 4082, 785, 7767, 13863, 33839, 34444, 329, 288, 4899, 2151, 51306], "temperature": 0.0, "avg_logprob": -0.33325173602840763, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.040858689695596695}, {"id": 43, "seek": 32818, "start": 347.02, "end": 350.94, "text": " conclusiones en base a cuenta, en base a n\u00famero, en base a experiencia, que es t\u00edpicamente", "tokens": [51306, 18646, 5411, 465, 3096, 257, 17868, 11, 465, 3096, 257, 14959, 11, 465, 3096, 257, 36489, 11, 631, 785, 256, 28236, 23653, 51502], "temperature": 0.0, "avg_logprob": -0.33325173602840763, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.040858689695596695}, {"id": 44, "seek": 32818, "start": 350.94, "end": 356.62, "text": " lo que vamos a ver en este caso de los enigramos. Estamos obteniendo soluciones a problemas,", "tokens": [51502, 450, 631, 5295, 257, 1306, 465, 4065, 9666, 368, 1750, 465, 328, 30227, 13, 34563, 28326, 7304, 1404, 46649, 257, 20720, 11, 51786], "temperature": 0.0, "avg_logprob": -0.33325173602840763, "compression_ratio": 1.6787003610108304, "no_speech_prob": 0.040858689695596695}, {"id": 45, "seek": 35662, "start": 356.62, "end": 359.86, "text": " no estamos entendiendo qu\u00e9 es lo que est\u00e1 pasando. Y eso es una discusi\u00f3n catal\u00eda de", "tokens": [50364, 572, 10382, 16612, 7304, 8057, 785, 450, 631, 3192, 45412, 13, 398, 7287, 785, 2002, 717, 1149, 2560, 13192, 2686, 368, 50526], "temperature": 0.0, "avg_logprob": -0.2949401115884586, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.03465292602777481}, {"id": 46, "seek": 35662, "start": 359.86, "end": 367.94, "text": " hoy sigue, es decir, hay una famosa discusi\u00f3n por ah\u00ed en internet entre Chonky, esto te", "tokens": [50526, 13775, 34532, 11, 785, 10235, 11, 4842, 2002, 1087, 6447, 717, 1149, 2560, 1515, 12571, 465, 4705, 3962, 761, 266, 4133, 11, 7433, 535, 50930], "temperature": 0.0, "avg_logprob": -0.2949401115884586, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.03465292602777481}, {"id": 47, "seek": 35662, "start": 367.94, "end": 377.34000000000003, "text": " hablando hace dos o tres a\u00f1os, o cinco a\u00f1os, entre Chonky y Peter Norby, que discute un", "tokens": [50930, 29369, 10032, 4491, 277, 15890, 11424, 11, 277, 21350, 11424, 11, 3962, 761, 266, 4133, 288, 6508, 6966, 2322, 11, 631, 2983, 1169, 517, 51400], "temperature": 0.0, "avg_logprob": -0.2949401115884586, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.03465292602777481}, {"id": 48, "seek": 35662, "start": 377.34000000000003, "end": 382.58, "text": " poco esto, es decir, si esto que estamos haciendo ahora y que ha tenido tan buenos resultados", "tokens": [51400, 10639, 7433, 11, 785, 10235, 11, 1511, 7433, 631, 10382, 20509, 9923, 288, 631, 324, 33104, 7603, 49617, 36796, 51662], "temperature": 0.0, "avg_logprob": -0.2949401115884586, "compression_ratio": 1.6233183856502242, "no_speech_prob": 0.03465292602777481}, {"id": 49, "seek": 38258, "start": 382.58, "end": 387.09999999999997, "text": " del punto de vista de reconocimiento de labla y el procedimiento de los enigramos natural", "tokens": [50364, 1103, 14326, 368, 22553, 368, 43838, 14007, 368, 2715, 875, 288, 806, 6682, 14007, 368, 1750, 465, 328, 30227, 3303, 50590], "temperature": 0.0, "avg_logprob": -0.35200296822240795, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.032740429043769836}, {"id": 50, "seek": 38258, "start": 387.09999999999997, "end": 391.34, "text": " es en realidad inteligencia artificial o de solamente en number crunching que no nos aporta", "tokens": [50590, 785, 465, 25635, 24777, 3213, 2755, 11677, 277, 368, 27814, 465, 1230, 13386, 278, 631, 572, 3269, 1882, 36403, 50802], "temperature": 0.0, "avg_logprob": -0.35200296822240795, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.032740429043769836}, {"id": 51, "seek": 38258, "start": 391.34, "end": 396.94, "text": " mucho. Norby en lo que le dice, bueno, de hecho, la ciencia siempre en modo menos funcion\u00f3", "tokens": [50802, 9824, 13, 6966, 2322, 465, 450, 631, 476, 10313, 11, 11974, 11, 368, 13064, 11, 635, 269, 30592, 12758, 465, 16664, 8902, 14186, 812, 51082], "temperature": 0.0, "avg_logprob": -0.35200296822240795, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.032740429043769836}, {"id": 52, "seek": 38258, "start": 396.94, "end": 403.53999999999996, "text": " as\u00ed. Bueno, entonces \u00bfcu\u00e1l es el objetivo de lo que vamos a ver ac\u00e1 son de modelos", "tokens": [51082, 8582, 13, 16046, 11, 13003, 3841, 12032, 11447, 785, 806, 29809, 368, 450, 631, 5295, 257, 1306, 23496, 1872, 368, 2316, 329, 51412], "temperature": 0.0, "avg_logprob": -0.35200296822240795, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.032740429043769836}, {"id": 53, "seek": 38258, "start": 403.53999999999996, "end": 407.7, "text": " del lenguaje? El objetivo del modelo del lenguaje es calcular la probabilidad de una", "tokens": [51412, 1103, 35044, 84, 11153, 30, 2699, 29809, 1103, 27825, 1103, 35044, 84, 11153, 785, 2104, 17792, 635, 31959, 4580, 368, 2002, 51620], "temperature": 0.0, "avg_logprob": -0.35200296822240795, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.032740429043769836}, {"id": 54, "seek": 40770, "start": 407.7, "end": 415.26, "text": " secuencia palabra, es decir, \u00bfqu\u00e9 tan probable es en mi lenguaje que una secuencia se", "tokens": [50364, 907, 47377, 31702, 11, 785, 10235, 11, 3841, 16412, 7603, 21759, 785, 465, 2752, 35044, 84, 11153, 631, 2002, 907, 47377, 369, 50742], "temperature": 0.0, "avg_logprob": -0.362268423732323, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.09408628195524216}, {"id": 55, "seek": 40770, "start": 415.26, "end": 425.65999999999997, "text": " es? \u00bfDe acuerdo? \u00bfPara qu\u00e9 no puede servir eso? Bueno, imag\u00ednense que ustedes, y acabamos", "tokens": [50742, 785, 30, 3841, 11089, 28113, 30, 3841, 47, 2419, 8057, 572, 8919, 29463, 7287, 30, 16046, 11, 2576, 10973, 1288, 631, 17110, 11, 288, 13281, 2151, 51262], "temperature": 0.0, "avg_logprob": -0.362268423732323, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.09408628195524216}, {"id": 56, "seek": 40770, "start": 425.65999999999997, "end": 432.26, "text": " a recordarlo otra vez el modelo del canal ruidozo, del otra vez, imag\u00ednense que tengo este", "tokens": [51262, 257, 2136, 19457, 13623, 5715, 806, 27825, 1103, 9911, 5420, 2925, 4765, 11, 1103, 13623, 5715, 11, 2576, 10973, 1288, 631, 13989, 4065, 51592], "temperature": 0.0, "avg_logprob": -0.362268423732323, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.09408628195524216}, {"id": 57, "seek": 43226, "start": 432.26, "end": 442.14, "text": " texto escrito, \u00bfs\u00ed? Y por medio de un m\u00e9todo que no s\u00e9 cu\u00e1l es, tengo dos oraciones", "tokens": [50364, 35503, 49451, 11, 3841, 82, 870, 30, 398, 1515, 22123, 368, 517, 20275, 17423, 631, 572, 7910, 44318, 785, 11, 13989, 4491, 420, 9188, 50858], "temperature": 0.0, "avg_logprob": -0.3036110741751535, "compression_ratio": 1.2695035460992907, "no_speech_prob": 0.021504011005163193}, {"id": 58, "seek": 43226, "start": 442.14, "end": 453.82, "text": " candidatas, bueno, dos textos candidatos, uno que es preneva para el curso de PLN y prueba", "tokens": [50858, 6268, 37892, 11, 11974, 11, 4491, 2487, 329, 6268, 26818, 11, 8526, 631, 785, 659, 716, 2757, 1690, 806, 31085, 368, 430, 43, 45, 288, 48241, 51442], "temperature": 0.0, "avg_logprob": -0.3036110741751535, "compression_ratio": 1.2695035460992907, "no_speech_prob": 0.021504011005163193}, {"id": 59, "seek": 45382, "start": 453.82, "end": 462.74, "text": " para el curso de PLN. \u00bfDe acuerdo? Y adem\u00e1s supongamos que el m\u00e9todo que utilic\u00e9 para", "tokens": [50364, 1690, 806, 31085, 368, 430, 43, 45, 13, 3841, 11089, 28113, 30, 398, 21251, 9331, 556, 2151, 631, 806, 20275, 17423, 631, 4976, 299, 526, 1690, 50810], "temperature": 0.0, "avg_logprob": -0.3153775062090085, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.08060824871063232}, {"id": 60, "seek": 45382, "start": 462.74, "end": 470.94, "text": " reconocer la escritura me dice que este es m\u00e1s probable que este. Nosotros \u00bfqu\u00e9 vamos", "tokens": [50810, 43838, 260, 635, 4721, 3210, 2991, 385, 10313, 631, 4065, 785, 3573, 21759, 631, 4065, 13, 18749, 11792, 3841, 16412, 5295, 51220], "temperature": 0.0, "avg_logprob": -0.3153775062090085, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.08060824871063232}, {"id": 61, "seek": 45382, "start": 470.94, "end": 481.86, "text": " a elegir? Vamos a elegirle abajo. \u00bfPor qu\u00e9? Porque esto no es una palabra v\u00e1lida, pero", "tokens": [51220, 257, 14459, 347, 30, 10894, 257, 14459, 347, 306, 30613, 13, 3841, 24907, 8057, 30, 11287, 7433, 572, 785, 2002, 31702, 371, 11447, 2887, 11, 4768, 51766], "temperature": 0.0, "avg_logprob": -0.3153775062090085, "compression_ratio": 1.4331550802139037, "no_speech_prob": 0.08060824871063232}, {"id": 62, "seek": 48186, "start": 481.86, "end": 490.22, "text": " aun siendo una palabra v\u00e1lida, o aun suponiendo que fuera una palabra v\u00e1lida, podr\u00eda darse", "tokens": [50364, 15879, 31423, 2002, 31702, 371, 11447, 2887, 11, 277, 15879, 9331, 266, 7304, 631, 24818, 2002, 31702, 371, 11447, 2887, 11, 27246, 4072, 405, 50782], "temperature": 0.0, "avg_logprob": -0.3737901296371069, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.08769425004720688}, {"id": 63, "seek": 48186, "start": 490.22, "end": 494.62, "text": " un caso donde yo identifico una palabra v\u00e1lida, se ponen los correcciones, a\u00fan as\u00ed yo", "tokens": [50782, 517, 9666, 10488, 5290, 49456, 78, 2002, 31702, 371, 11447, 2887, 11, 369, 9224, 268, 1750, 29731, 35560, 11, 31676, 8582, 5290, 51002], "temperature": 0.0, "avg_logprob": -0.3737901296371069, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.08769425004720688}, {"id": 64, "seek": 48186, "start": 494.62, "end": 506.02000000000004, "text": " pod\u00eda decir bueno, pero en este lugar, en este lugar, esa palabra no calza, digamos, \u00bfs\u00ed", "tokens": [51002, 45588, 10235, 11974, 11, 4768, 465, 4065, 11467, 11, 465, 4065, 11467, 11, 11342, 31702, 572, 2104, 2394, 11, 36430, 11, 3841, 82, 870, 51572], "temperature": 0.0, "avg_logprob": -0.3737901296371069, "compression_ratio": 1.691358024691358, "no_speech_prob": 0.08769425004720688}, {"id": 65, "seek": 50602, "start": 506.02, "end": 511.34, "text": " alguna forma yo s\u00e9? Es decir, si yo logro detectar que esta oraci\u00f3n es m\u00e1s probable que", "tokens": [50364, 20651, 8366, 5290, 7910, 30, 2313, 10235, 11, 1511, 5290, 3565, 340, 5531, 289, 631, 5283, 420, 3482, 785, 3573, 21759, 631, 50630], "temperature": 0.0, "avg_logprob": -0.32798194885253906, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.26339128613471985}, {"id": 66, "seek": 50602, "start": 511.34, "end": 517.9399999999999, "text": " esta de alguna forma, eso me va a ayudar en la tarea de reconocimiento. Lo mismo pasa con el", "tokens": [50630, 5283, 368, 20651, 8366, 11, 7287, 385, 2773, 257, 38759, 465, 635, 256, 35425, 368, 43838, 14007, 13, 6130, 12461, 20260, 416, 806, 50960], "temperature": 0.0, "avg_logprob": -0.32798194885253906, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.26339128613471985}, {"id": 67, "seek": 50602, "start": 517.9399999999999, "end": 521.98, "text": " reconocimiento de la habla de lo que hablamos y lo otro d\u00eda con el esp\u00edritu de reconocimiento", "tokens": [50960, 43838, 14007, 368, 635, 42135, 368, 450, 631, 26280, 2151, 288, 450, 11921, 12271, 416, 806, 48987, 3210, 84, 368, 43838, 14007, 51162], "temperature": 0.0, "avg_logprob": -0.32798194885253906, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.26339128613471985}, {"id": 68, "seek": 50602, "start": 521.98, "end": 527.14, "text": " y cuando yo hablo y digo una palabra, ustedes me escuchan. Entonces, los modelos de", "tokens": [51162, 288, 7767, 5290, 3025, 752, 288, 22990, 2002, 31702, 11, 17110, 385, 22483, 282, 13, 15097, 11, 1750, 2316, 329, 368, 51420], "temperature": 0.0, "avg_logprob": -0.32798194885253906, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.26339128613471985}, {"id": 69, "seek": 50602, "start": 527.14, "end": 532.02, "text": " nevoje sirven para ayudar en este tipo de tarea, t\u00edpicamente los modelos de nevoje", "tokens": [51420, 408, 3080, 2884, 4735, 553, 1690, 38759, 465, 4065, 9746, 368, 256, 35425, 11, 256, 28236, 23653, 1750, 2316, 329, 368, 408, 3080, 2884, 51664], "temperature": 0.0, "avg_logprob": -0.32798194885253906, "compression_ratio": 1.7598425196850394, "no_speech_prob": 0.26339128613471985}, {"id": 70, "seek": 53202, "start": 532.02, "end": 541.9399999999999, "text": " ayudan y no tratar\u00edan. Nos abregan mucha informaci\u00f3n. Entonces, cuando nosotros hacemos", "tokens": [50364, 20333, 282, 288, 572, 21507, 289, 11084, 13, 18749, 410, 3375, 282, 25248, 21660, 13, 15097, 11, 7767, 13863, 33839, 50860], "temperature": 0.0, "avg_logprob": -0.2719259382803229, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.02399694174528122}, {"id": 71, "seek": 53202, "start": 541.9399999999999, "end": 550.9399999999999, "text": " reconocimiento de escritura, luego lo que decimos es, \u00bfcu\u00e1l es la probabilidad de la oraci\u00f3n", "tokens": [50860, 43838, 14007, 368, 4721, 3210, 2991, 11, 17222, 450, 631, 979, 8372, 785, 11, 3841, 12032, 11447, 785, 635, 31959, 4580, 368, 635, 420, 3482, 51310], "temperature": 0.0, "avg_logprob": -0.2719259382803229, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.02399694174528122}, {"id": 72, "seek": 53202, "start": 550.9399999999999, "end": 557.42, "text": " origen, dada la observaci\u00f3n que tengo? Yo tengo una observaci\u00f3n, \u00bfs\u00ed? \u00bfCu\u00e1l es la", "tokens": [51310, 2349, 268, 11, 274, 1538, 635, 9951, 3482, 631, 13989, 30, 7616, 13989, 2002, 9951, 3482, 11, 3841, 82, 870, 30, 3841, 35222, 11447, 785, 635, 51634], "temperature": 0.0, "avg_logprob": -0.2719259382803229, "compression_ratio": 1.4444444444444444, "no_speech_prob": 0.02399694174528122}, {"id": 73, "seek": 55742, "start": 557.42, "end": 563.3, "text": " probabilidad de una oraci\u00f3n origen? Es proporcionar a la probabilidad de la observaci\u00f3n,", "tokens": [50364, 31959, 4580, 368, 2002, 420, 3482, 2349, 268, 30, 2313, 41516, 10015, 289, 257, 635, 31959, 4580, 368, 635, 9951, 3482, 11, 50658], "temperature": 0.0, "avg_logprob": -0.2691444656223927, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.027731038630008698}, {"id": 74, "seek": 55742, "start": 563.3, "end": 572.42, "text": " dada la oraci\u00f3n por la probabilidad de la oraci\u00f3n. \u00bfY esto qu\u00e9 es? Eso es valles, en la rir", "tokens": [50658, 274, 1538, 635, 420, 3482, 1515, 635, 31959, 4580, 368, 635, 420, 3482, 13, 3841, 56, 7433, 8057, 785, 30, 27795, 785, 371, 37927, 11, 465, 635, 367, 347, 51114], "temperature": 0.0, "avg_logprob": -0.2691444656223927, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.027731038630008698}, {"id": 75, "seek": 55742, "start": 572.42, "end": 579.5, "text": " de valles. Entonces, nosotros por valles sabemos eso. Y como ven, ac\u00e1 aparece la noci\u00f3n", "tokens": [51114, 368, 371, 37927, 13, 15097, 11, 13863, 1515, 371, 37927, 27200, 7287, 13, 398, 2617, 6138, 11, 23496, 37863, 635, 572, 5687, 51468], "temperature": 0.0, "avg_logprob": -0.2691444656223927, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.027731038630008698}, {"id": 76, "seek": 55742, "start": 579.5, "end": 583.74, "text": " de probabilidad de la oraci\u00f3n. Por eso es que nos interesa conocer la probabilidad de", "tokens": [51468, 368, 31959, 4580, 368, 635, 420, 3482, 13, 5269, 7287, 785, 631, 3269, 728, 13708, 35241, 635, 31959, 4580, 368, 51680], "temperature": 0.0, "avg_logprob": -0.2691444656223927, "compression_ratio": 1.8615384615384616, "no_speech_prob": 0.027731038630008698}, {"id": 77, "seek": 58374, "start": 583.74, "end": 593.0600000000001, "text": " las variaciones. Ahora, \u00bfc\u00f3mo calculamos la probabilidad de la oraci\u00f3n? Bueno, hay un ejemplo", "tokens": [50364, 2439, 3034, 9188, 13, 18840, 11, 3841, 46614, 4322, 2151, 635, 31959, 4580, 368, 635, 420, 3482, 30, 16046, 11, 4842, 517, 13358, 50830], "temperature": 0.0, "avg_logprob": -0.32903778858673877, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.03309871256351471}, {"id": 78, "seek": 58374, "start": 593.0600000000001, "end": 604.94, "text": " m\u00e1s, \u00bfno? Por ejemplo, en la traducion autom\u00e1tica, si tenemos estas tres candidatos, nuevamente", "tokens": [50830, 3573, 11, 3841, 1771, 30, 5269, 13358, 11, 465, 635, 2479, 1311, 313, 3553, 23432, 11, 1511, 9914, 13897, 15890, 6268, 26818, 11, 10412, 85, 3439, 51424], "temperature": 0.0, "avg_logprob": -0.32903778858673877, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.03309871256351471}, {"id": 79, "seek": 58374, "start": 604.94, "end": 610.3, "text": " a m\u00ed me va a ayudar con conocer el orden o saber cu\u00e1l es la m\u00e1s probable en mi linguaje.", "tokens": [51424, 257, 14692, 385, 2773, 257, 38759, 416, 35241, 806, 28615, 277, 12489, 44318, 785, 635, 3573, 21759, 465, 2752, 21766, 11153, 13, 51692], "temperature": 0.0, "avg_logprob": -0.32903778858673877, "compression_ratio": 1.4137931034482758, "no_speech_prob": 0.03309871256351471}, {"id": 80, "seek": 61374, "start": 613.74, "end": 622.7, "text": " En las correcci\u00f3n de errores, como vimos la vez pasada, hordas de botero es una secuencia", "tokens": [50364, 2193, 2439, 29731, 14735, 368, 45935, 495, 11, 2617, 49266, 635, 5715, 1736, 1538, 11, 276, 765, 296, 368, 10592, 2032, 785, 2002, 907, 47377, 50812], "temperature": 0.0, "avg_logprob": -0.37291188825640764, "compression_ratio": 1.232394366197183, "no_speech_prob": 0.034491874277591705}, {"id": 81, "seek": 61374, "start": 622.7, "end": 641.98, "text": " muy de poca probabilidad. Y pensemos un poquito. \u00bfPreguntemos, no? \u00bfPor qu\u00e9? Esta", "tokens": [50812, 5323, 368, 714, 496, 31959, 4580, 13, 398, 11209, 3415, 517, 28229, 13, 3841, 47, 3375, 2760, 4485, 11, 572, 30, 3841, 24907, 8057, 30, 20547, 51776], "temperature": 0.0, "avg_logprob": -0.37291188825640764, "compression_ratio": 1.232394366197183, "no_speech_prob": 0.034491874277591705}, {"id": 82, "seek": 64198, "start": 641.98, "end": 648.62, "text": " oraci\u00f3n no les parece que sea muy probable. \u00bfQu\u00e9 nos podr\u00eda determinar que esta oraci\u00f3n", "tokens": [50364, 420, 3482, 572, 1512, 14120, 631, 4158, 5323, 21759, 13, 3841, 15137, 3269, 27246, 3618, 6470, 631, 5283, 420, 3482, 50696], "temperature": 0.0, "avg_logprob": -0.19991752949166805, "compression_ratio": 1.3834586466165413, "no_speech_prob": 0.11262855678796768}, {"id": 83, "seek": 64198, "start": 648.62, "end": 666.46, "text": " no es muy probable? O esta, implementaci\u00f3n a la educaci\u00f3n ley. \u00bfPor qu\u00e9 podemos suponer", "tokens": [50696, 572, 785, 5323, 21759, 30, 422, 5283, 11, 4445, 3482, 257, 635, 48861, 27786, 13, 3841, 24907, 8057, 12234, 9331, 32949, 51588], "temperature": 0.0, "avg_logprob": -0.19991752949166805, "compression_ratio": 1.3834586466165413, "no_speech_prob": 0.11262855678796768}, {"id": 84, "seek": 66646, "start": 666.46, "end": 678.26, "text": " que esa no es probable? Bueno, a m\u00ed me ocurre en dos razones, principales o dos, pero", "tokens": [50364, 631, 11342, 572, 785, 21759, 30, 16046, 11, 257, 14692, 385, 26430, 265, 465, 4491, 9639, 2213, 11, 6959, 4229, 277, 4491, 11, 4768, 50954], "temperature": 0.0, "avg_logprob": -0.5011269513298483, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.03517741337418556}, {"id": 85, "seek": 66646, "start": 678.26, "end": 685.94, "text": " s\u00ed mansiones. \u00bfUna es por las sintaxis, \u00bfno? La sintaxis del d\u00eda de mape\u00f1\u00f3n no es as\u00ed. No", "tokens": [50954, 8600, 18868, 5411, 13, 3841, 52, 629, 785, 1515, 2439, 41259, 24633, 11, 3841, 1771, 30, 2369, 41259, 24633, 1103, 12271, 368, 463, 494, 2791, 1801, 572, 785, 8582, 13, 883, 51338], "temperature": 0.0, "avg_logprob": -0.5011269513298483, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.03517741337418556}, {"id": 86, "seek": 66646, "start": 685.94, "end": 693.58, "text": " decimos educaci\u00f3n ley, educaci\u00f3n... \u00bfPor qu\u00e9 no? \u00bfPor qu\u00e9 no? \u00bfPor qu\u00e9 no?", "tokens": [51338, 979, 8372, 48861, 27786, 11, 48861, 485, 3841, 24907, 8057, 572, 30, 3841, 24907, 8057, 572, 30, 3841, 24907, 8057, 572, 30, 51720], "temperature": 0.0, "avg_logprob": -0.5011269513298483, "compression_ratio": 1.5113636363636365, "no_speech_prob": 0.03517741337418556}, {"id": 87, "seek": 69358, "start": 693.58, "end": 703.46, "text": " La secci\u00f3n es su y de botero, como publican la verdad. Ah, bueno, \u00bfPrecio pudiera ser un", "tokens": [50364, 2369, 907, 5687, 785, 459, 288, 368, 10592, 2032, 11, 2617, 1908, 282, 635, 13692, 13, 2438, 11, 11974, 11, 3841, 47, 3326, 78, 14166, 10609, 816, 517, 50858], "temperature": 0.0, "avg_logprob": -0.4329357147216797, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.07746987789869308}, {"id": 88, "seek": 69358, "start": 703.46, "end": 708.4200000000001, "text": " suh de un tercero, \u00bfno? Ac\u00e1 seguramente lo que hay es lo que hay es un error autogr\u00e1fico", "tokens": [50858, 459, 71, 368, 517, 38103, 78, 11, 3841, 1771, 30, 5097, 842, 22179, 3439, 450, 631, 4842, 785, 450, 631, 4842, 785, 517, 6713, 1476, 47810, 23858, 78, 51106], "temperature": 0.0, "avg_logprob": -0.4329357147216797, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.07746987789869308}, {"id": 89, "seek": 69358, "start": 708.4200000000001, "end": 714.14, "text": " de sus gordas de botero. O sea, ac\u00e1, ac\u00e1 tenemos un tema de sintaxis, ac\u00e1 no tenemos un tema", "tokens": [51106, 368, 3291, 42443, 296, 368, 10592, 2032, 13, 422, 4158, 11, 23496, 11, 23496, 9914, 517, 15854, 368, 41259, 24633, 11, 23496, 572, 9914, 517, 15854, 51392], "temperature": 0.0, "avg_logprob": -0.4329357147216797, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.07746987789869308}, {"id": 90, "seek": 69358, "start": 714.14, "end": 721.4200000000001, "text": " de sintaxis. Deber\u00edamos conocer un poco de sem\u00e1ntica para asociar botero que pintaba", "tokens": [51392, 368, 41259, 24633, 13, 1346, 607, 16275, 35241, 517, 10639, 368, 4361, 27525, 2262, 1690, 382, 78, 537, 289, 10592, 2032, 631, 23924, 5509, 51756], "temperature": 0.0, "avg_logprob": -0.4329357147216797, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.07746987789869308}, {"id": 91, "seek": 72142, "start": 721.42, "end": 727.2199999999999, "text": " mujeres gordas. Entonces, una aproximaci\u00f3n un poco m\u00e1s humilde, es la segunda, es la", "tokens": [50364, 31683, 42443, 296, 13, 15097, 11, 2002, 31270, 3482, 517, 10639, 3573, 1484, 15956, 11, 785, 635, 21978, 11, 785, 635, 50654], "temperature": 0.0, "avg_logprob": -0.3387579345703125, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.0348721481859684}, {"id": 92, "seek": 72142, "start": 727.2199999999999, "end": 732.9, "text": " alguna aproximaci\u00f3n m\u00e1s \u00e9tad\u00edstica, porque si nosotros, y que juega con el hecho de que", "tokens": [50654, 20651, 31270, 3482, 3573, 4823, 345, 19512, 2262, 11, 4021, 1511, 13863, 11, 288, 631, 27833, 3680, 416, 806, 13064, 368, 631, 50938], "temperature": 0.0, "avg_logprob": -0.3387579345703125, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.0348721481859684}, {"id": 93, "seek": 72142, "start": 732.9, "end": 736.9, "text": " tenemos grandes vol\u00famenes de texto y ah\u00ed el cambio de los modelos probabil\u00edsticos, es", "tokens": [50938, 9914, 16640, 1996, 2481, 2558, 279, 368, 35503, 288, 12571, 806, 28731, 368, 1750, 2316, 329, 31959, 19512, 9940, 11, 785, 51138], "temperature": 0.0, "avg_logprob": -0.3387579345703125, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.0348721481859684}, {"id": 94, "seek": 72142, "start": 736.9, "end": 745.5, "text": " que sus gordas de botero seguramente apareci\u00f3 antes en mis cuerpos de texto y hordas de botero,", "tokens": [51138, 631, 3291, 42443, 296, 368, 10592, 2032, 22179, 3439, 15004, 19609, 11014, 465, 3346, 18363, 30010, 368, 35503, 288, 276, 765, 296, 368, 10592, 2032, 11, 51568], "temperature": 0.0, "avg_logprob": -0.3387579345703125, "compression_ratio": 1.6396396396396395, "no_speech_prob": 0.0348721481859684}, {"id": 95, "seek": 74550, "start": 745.5, "end": 750.74, "text": " eso es una aproximaci\u00f3n mucho m\u00e1s \u00e9tad\u00edstica, eso es lo que vamos a hacer en los modelos", "tokens": [50364, 7287, 785, 2002, 31270, 3482, 9824, 3573, 4823, 345, 19512, 2262, 11, 7287, 785, 450, 631, 5295, 257, 6720, 465, 1750, 2316, 329, 50626], "temperature": 0.0, "avg_logprob": -0.2503993025103819, "compression_ratio": 1.625, "no_speech_prob": 0.0040418654680252075}, {"id": 96, "seek": 74550, "start": 750.74, "end": 755.86, "text": " de negra m\u00e1s justamente. A partir de grande vol\u00famenes de texto, detectar, calcular la", "tokens": [50626, 368, 2485, 424, 3573, 41056, 13, 316, 13906, 368, 8883, 1996, 2481, 2558, 279, 368, 35503, 11, 5531, 289, 11, 2104, 17792, 635, 50882], "temperature": 0.0, "avg_logprob": -0.2503993025103819, "compression_ratio": 1.625, "no_speech_prob": 0.0040418654680252075}, {"id": 97, "seek": 74550, "start": 755.86, "end": 762.34, "text": " probabilidad. Es una aproximaci\u00f3n puramente \u00e9tad\u00edstica, es bien salvoaje, yo no s\u00e9 qu\u00e9", "tokens": [50882, 31959, 4580, 13, 2313, 2002, 31270, 3482, 1864, 3439, 4823, 345, 19512, 2262, 11, 785, 3610, 1845, 3080, 11153, 11, 5290, 572, 7910, 8057, 51206], "temperature": 0.0, "avg_logprob": -0.2503993025103819, "compression_ratio": 1.625, "no_speech_prob": 0.0040418654680252075}, {"id": 98, "seek": 74550, "start": 762.34, "end": 766.66, "text": " estructura tiene esto, pero s\u00e9 que esto no se dio nunca y que gordas de botero s\u00ed, muchas", "tokens": [51206, 43935, 2991, 7066, 7433, 11, 4768, 7910, 631, 7433, 572, 369, 31965, 13768, 288, 631, 42443, 296, 368, 10592, 2032, 8600, 11, 16072, 51422], "temperature": 0.0, "avg_logprob": -0.2503993025103819, "compression_ratio": 1.625, "no_speech_prob": 0.0040418654680252075}, {"id": 99, "seek": 76666, "start": 766.66, "end": 779.62, "text": " veces. Entonces, les m\u00e1s probar\u00e9 que m\u00e1s equivocado. A ver, relacionado con esto, ahora", "tokens": [50364, 17054, 13, 15097, 11, 1512, 3573, 1239, 36832, 631, 3573, 48726, 21636, 13, 316, 1306, 11, 27189, 1573, 416, 7433, 11, 9923, 51012], "temperature": 0.0, "avg_logprob": -0.2942631412559832, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.02096443437039852}, {"id": 100, "seek": 76666, "start": 779.62, "end": 784.06, "text": " vamos a ver por qu\u00e9 est\u00e1 relacionado, est\u00e1 el tema de la predicci\u00f3n de la siguiente", "tokens": [51012, 5295, 257, 1306, 1515, 8057, 3192, 27189, 1573, 11, 3192, 806, 15854, 368, 635, 47336, 5687, 368, 635, 25666, 51234], "temperature": 0.0, "avg_logprob": -0.2942631412559832, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.02096443437039852}, {"id": 101, "seek": 76666, "start": 784.06, "end": 795.3399999999999, "text": " palabra. \u00bfCu\u00e1les se imaginan que es la siguiente palabra a la primera relaci\u00f3n? \u00bfCu\u00e1l", "tokens": [51234, 31702, 13, 3841, 35222, 842, 904, 369, 23427, 282, 631, 785, 635, 25666, 31702, 257, 635, 17382, 37247, 30, 3841, 35222, 11447, 51798], "temperature": 0.0, "avg_logprob": -0.2942631412559832, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.02096443437039852}, {"id": 102, "seek": 79534, "start": 795.34, "end": 803.58, "text": " puede ser la siguiente palabra? \u00bfQui\u00e9n? Para y no meten mi t\u00edo pron\u00f3stico para, qu\u00e9", "tokens": [50364, 8919, 816, 635, 25666, 31702, 30, 3841, 48, 3077, 3516, 30, 11107, 288, 572, 1131, 268, 2752, 256, 20492, 7569, 45052, 2789, 1690, 11, 8057, 50776], "temperature": 0.0, "avg_logprob": -0.3891491006921839, "compression_ratio": 1.3435114503816794, "no_speech_prob": 0.06510437279939651}, {"id": 103, "seek": 79534, "start": 803.58, "end": 814.34, "text": " otra cosa puede ser? Para es una preposici\u00f3n \u00bfno? \u00bfQu\u00e9 m\u00e1s? \u00bfQu\u00e9 otra cosa puede", "tokens": [50776, 13623, 10163, 8919, 816, 30, 11107, 785, 2002, 2666, 329, 15534, 3841, 1771, 30, 3841, 15137, 3573, 30, 3841, 15137, 13623, 10163, 8919, 51314], "temperature": 0.0, "avg_logprob": -0.3891491006921839, "compression_ratio": 1.3435114503816794, "no_speech_prob": 0.06510437279939651}, {"id": 104, "seek": 81434, "start": 814.34, "end": 826.02, "text": " ser ah\u00ed? \u00bfCu\u00e1l por ejemplo? \u00bfUn pron\u00f3stico alentador? O puede decir un pron\u00f3stico terrible", "tokens": [50364, 816, 12571, 30, 3841, 35222, 11447, 1515, 13358, 30, 3841, 12405, 7569, 45052, 2789, 419, 317, 5409, 30, 422, 8919, 10235, 517, 7569, 45052, 2789, 6237, 50948], "temperature": 0.0, "avg_logprob": -0.41569562543902483, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.20931707322597504}, {"id": 105, "seek": 81434, "start": 826.02, "end": 832.7800000000001, "text": " o un pron\u00f3stico... \u00bfQu\u00e9 otra cosa m\u00e1s? Hay un m\u00e1s com\u00fan para m\u00ed. El mit\u00edo pron\u00f3stico", "tokens": [50948, 277, 517, 7569, 45052, 2789, 485, 3841, 15137, 13623, 10163, 3573, 30, 8721, 517, 3573, 45448, 1690, 14692, 13, 2699, 2194, 20492, 7569, 45052, 2789, 51286], "temperature": 0.0, "avg_logprob": -0.41569562543902483, "compression_ratio": 1.3380281690140845, "no_speech_prob": 0.20931707322597504}, {"id": 106, "seek": 83278, "start": 832.78, "end": 842.78, "text": " con meteorol\u00f3gico \u00bfno? A ra\u00edz de este fen\u00f3meno se suceder\u00e1n tormentas, fuertes, importantes,", "tokens": [50364, 416, 25313, 27629, 2789, 3841, 1771, 30, 316, 3342, 44551, 368, 4065, 26830, 812, 43232, 369, 41928, 260, 7200, 36662, 296, 11, 8536, 911, 279, 11, 27963, 11, 50864], "temperature": 0.0, "avg_logprob": -0.35048236403354377, "compression_ratio": 1.471794871794872, "no_speech_prob": 0.3402399718761444}, {"id": 107, "seek": 83278, "start": 842.78, "end": 850.26, "text": " muy, no creo que ah\u00ed diga tormentas gatito \u00bfno? gatito no es muy probable que sea la palabra", "tokens": [50864, 5323, 11, 572, 14336, 631, 12571, 2528, 64, 36662, 296, 44092, 3528, 3841, 1771, 30, 44092, 3528, 572, 785, 5323, 21759, 631, 4158, 635, 31702, 51238], "temperature": 0.0, "avg_logprob": -0.35048236403354377, "compression_ratio": 1.471794871794872, "no_speech_prob": 0.3402399718761444}, {"id": 108, "seek": 83278, "start": 850.26, "end": 855.38, "text": " siguiente. Nuevamente, \u00bfpor qu\u00e9 sabemos esto? Y porque es muy raro que hay en diga tormentas", "tokens": [51238, 25666, 13, 47970, 85, 3439, 11, 3841, 2816, 8057, 27200, 7433, 30, 398, 4021, 785, 5323, 367, 9708, 631, 4842, 465, 2528, 64, 36662, 296, 51494], "temperature": 0.0, "avg_logprob": -0.35048236403354377, "compression_ratio": 1.471794871794872, "no_speech_prob": 0.3402399718761444}, {"id": 109, "seek": 85538, "start": 855.38, "end": 865.9, "text": " gatitos \u00bfno? Entonces, esto que tenemos ac\u00e1 es la posibilidad de que hay de siguiente", "tokens": [50364, 44092, 11343, 3841, 1771, 30, 15097, 11, 7433, 631, 9914, 23496, 785, 635, 1366, 33989, 368, 631, 4842, 368, 25666, 50890], "temperature": 0.0, "avg_logprob": -0.3230410866115404, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.1358393430709839}, {"id": 110, "seek": 85538, "start": 865.9, "end": 871.58, "text": " palabra. \u00bfDada todas las anteriores? Si yo tengo todo el contexto lo que se llama", "tokens": [50890, 31702, 13, 3841, 35, 1538, 10906, 2439, 364, 34345, 2706, 30, 4909, 5290, 13989, 5149, 806, 47685, 450, 631, 369, 23272, 51174], "temperature": 0.0, "avg_logprob": -0.3230410866115404, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.1358393430709839}, {"id": 111, "seek": 85538, "start": 871.58, "end": 879.46, "text": " contexto, dado el contexto de la palabra que sigue ac\u00e1. \u00bfS\u00ed? Una de las, lo que nosotros", "tokens": [51174, 47685, 11, 29568, 806, 47685, 368, 635, 31702, 631, 34532, 23496, 13, 3841, 30463, 30, 15491, 368, 2439, 11, 450, 631, 13863, 51568], "temperature": 0.0, "avg_logprob": -0.3230410866115404, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.1358393430709839}, {"id": 112, "seek": 85538, "start": 879.46, "end": 883.22, "text": " vamos a querer hacer en un modelo de lenguaje como camino para calcular la protecci\u00f3n", "tokens": [51568, 5295, 257, 39318, 6720, 465, 517, 27825, 368, 35044, 84, 11153, 2617, 34124, 1690, 2104, 17792, 635, 5631, 14735, 51756], "temperature": 0.0, "avg_logprob": -0.3230410866115404, "compression_ratio": 1.5580357142857142, "no_speech_prob": 0.1358393430709839}, {"id": 113, "seek": 88322, "start": 883.22, "end": 891.22, "text": " de honoraci\u00f3n es dado el contexto calcular la palabra. Siguiente. \u00bfS\u00ed?", "tokens": [50364, 368, 5968, 3482, 785, 29568, 806, 47685, 2104, 17792, 635, 31702, 13, 318, 16397, 8413, 13, 3841, 30463, 30, 50764], "temperature": 0.0, "avg_logprob": -0.44849151035524765, "compression_ratio": 1.515695067264574, "no_speech_prob": 0.09666013717651367}, {"id": 114, "seek": 88322, "start": 893.22, "end": 903.02, "text": " \u00bfRachas de viento fuerte de componente? Veremos que. Bueno, no resulta hacer que de", "tokens": [50864, 3841, 49, 608, 296, 368, 371, 7814, 37129, 368, 4026, 1576, 30, 691, 19065, 631, 13, 16046, 11, 572, 1874, 64, 6720, 631, 368, 51354], "temperature": 0.0, "avg_logprob": -0.44849151035524765, "compression_ratio": 1.515695067264574, "no_speech_prob": 0.09666013717651367}, {"id": 115, "seek": 88322, "start": 903.02, "end": 907.58, "text": " los ejemplos que yo tom\u00e9 a Buenos Aires, puse viento fuerte de componente, perd\u00f3n. El", "tokens": [51354, 1750, 10012, 5895, 329, 631, 5290, 2916, 526, 257, 38058, 47058, 11, 280, 438, 371, 7814, 37129, 368, 4026, 1576, 11, 12611, 1801, 13, 2699, 51582], "temperature": 0.0, "avg_logprob": -0.44849151035524765, "compression_ratio": 1.515695067264574, "no_speech_prob": 0.09666013717651367}, {"id": 116, "seek": 88322, "start": 907.58, "end": 911.6600000000001, "text": " lino me demiti\u00f3 pron\u00f3stico especial, o sea que le ramos, se suceder\u00e1n tormentas fuertes,", "tokens": [51582, 287, 2982, 385, 1371, 270, 7138, 7569, 45052, 2789, 15342, 11, 277, 4158, 631, 476, 367, 2151, 11, 369, 41928, 260, 7200, 36662, 296, 8536, 911, 279, 11, 51786], "temperature": 0.0, "avg_logprob": -0.44849151035524765, "compression_ratio": 1.515695067264574, "no_speech_prob": 0.09666013717651367}, {"id": 117, "seek": 91166, "start": 911.66, "end": 915.42, "text": " viento fuerte, componente subo este. Por ejemplo, perdici\u00f3n.", "tokens": [50364, 371, 7814, 37129, 11, 4026, 1576, 1422, 78, 4065, 13, 5269, 13358, 11, 12611, 15534, 13, 50552], "temperature": 0.0, "avg_logprob": -0.36836376786231995, "compression_ratio": 1.5605095541401275, "no_speech_prob": 0.0020394246093928814}, {"id": 118, "seek": 91166, "start": 919.5, "end": 926.02, "text": " Vamos a poner un poquito de notaci\u00f3n antes de seguir, porque vamos a ver c\u00f3mo enfrentamos", "tokens": [50756, 10894, 257, 19149, 517, 28229, 368, 406, 3482, 11014, 368, 18584, 11, 4021, 5295, 257, 1306, 12826, 33771, 2151, 51082], "temperature": 0.0, "avg_logprob": -0.36836376786231995, "compression_ratio": 1.5605095541401275, "no_speech_prob": 0.0020394246093928814}, {"id": 119, "seek": 91166, "start": 926.02, "end": 930.8199999999999, "text": " este problema, es decir, c\u00f3mo calculamos esa protecci\u00f3n. Un poco de notaci\u00f3n para seguir", "tokens": [51082, 4065, 12395, 11, 785, 10235, 11, 12826, 2104, 2444, 2151, 11342, 5631, 14735, 13, 1156, 10639, 368, 406, 3482, 1690, 18584, 51322], "temperature": 0.0, "avg_logprob": -0.36836376786231995, "compression_ratio": 1.5605095541401275, "no_speech_prob": 0.0020394246093928814}, {"id": 120, "seek": 93082, "start": 930.82, "end": 940.9000000000001, "text": " eso. Yo lo que estoy diciendo es la probabilidad de que una variable aleatoria ah\u00ed valga,", "tokens": [50364, 7287, 13, 7616, 450, 631, 15796, 42797, 785, 635, 31959, 4580, 368, 631, 2002, 7006, 6775, 1639, 654, 12571, 1323, 3680, 11, 50868], "temperature": 0.0, "avg_logprob": -0.4443814146752451, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.3388288915157318}, {"id": 121, "seek": 93082, "start": 940.9000000000001, "end": 945.82, "text": " tome el valor con ocimiento, en este caso tendr\u00eda una variable aleatoria por cada posici\u00f3n", "tokens": [50868, 281, 1398, 806, 15367, 416, 277, 66, 14007, 11, 465, 4065, 9666, 3928, 37183, 2002, 7006, 6775, 1639, 654, 1515, 8411, 46595, 51114], "temperature": 0.0, "avg_logprob": -0.4443814146752451, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.3388288915157318}, {"id": 122, "seek": 93082, "start": 945.82, "end": 950.86, "text": " del texto, \u00bfverdad? Tengo una X1 que la primera palabra ha equid\u00f3 que es la segunda", "tokens": [51114, 1103, 35503, 11, 3841, 331, 20034, 30, 314, 30362, 2002, 1783, 16, 631, 635, 17382, 31702, 324, 1267, 327, 812, 631, 785, 635, 21978, 51366], "temperature": 0.0, "avg_logprob": -0.4443814146752451, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.3388288915157318}, {"id": 123, "seek": 93082, "start": 950.86, "end": 955.86, "text": " X3, son variables aleatoria que lo variable aleatoria esencialmente un mapeo, es una", "tokens": [51366, 1783, 18, 11, 1872, 9102, 6775, 1639, 654, 631, 450, 7006, 6775, 1639, 654, 785, 26567, 4082, 517, 463, 494, 78, 11, 785, 2002, 51616], "temperature": 0.0, "avg_logprob": -0.4443814146752451, "compression_ratio": 1.6388888888888888, "no_speech_prob": 0.3388288915157318}, {"id": 124, "seek": 95586, "start": 955.86, "end": 966.74, "text": " funci\u00f3n que me apega, de un evento un n\u00famero entre cero y un. La probabilidad, perd\u00f3n.", "tokens": [50364, 43735, 631, 385, 44315, 3680, 11, 368, 517, 40655, 517, 14959, 3962, 269, 2032, 288, 517, 13, 2369, 31959, 4580, 11, 12611, 1801, 13, 50908], "temperature": 0.0, "avg_logprob": -0.3745892598078801, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.01915879175066948}, {"id": 125, "seek": 95586, "start": 966.74, "end": 973.1800000000001, "text": " Perd\u00f3n, perd\u00f3n. Bueno, no, mientras defin\u00ed, me apega con un real y la probabilidad me", "tokens": [50908, 47633, 1801, 11, 12611, 1801, 13, 16046, 11, 572, 11, 26010, 1561, 870, 11, 385, 44315, 3680, 416, 517, 957, 288, 635, 31959, 4580, 385, 51230], "temperature": 0.0, "avg_logprob": -0.3745892598078801, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.01915879175066948}, {"id": 126, "seek": 95586, "start": 973.1800000000001, "end": 978.42, "text": " devuelve un n\u00famero entre cero y un. Es decir, yo defino la probabilidad de una variable aleatoria", "tokens": [51230, 1905, 3483, 303, 517, 14959, 3962, 269, 2032, 288, 517, 13, 2313, 10235, 11, 5290, 1561, 78, 635, 31959, 4580, 368, 2002, 7006, 6775, 1639, 654, 51492], "temperature": 0.0, "avg_logprob": -0.3745892598078801, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.01915879175066948}, {"id": 127, "seek": 95586, "start": 978.42, "end": 985.3000000000001, "text": " como la distribuci\u00f3n de probabilidad de una variable aleatoria es la dado de los diferentes", "tokens": [51492, 2617, 635, 4400, 30813, 368, 31959, 4580, 368, 2002, 7006, 6775, 1639, 654, 785, 635, 29568, 368, 1750, 17686, 51836], "temperature": 0.0, "avg_logprob": -0.3745892598078801, "compression_ratio": 1.8877551020408163, "no_speech_prob": 0.01915879175066948}, {"id": 128, "seek": 98530, "start": 985.3, "end": 992.62, "text": " valores que puede tomar, cu\u00e1l es el valor de cada uno de ellos, \u00bfs\u00ed? Y esto cu\u00e1l es", "tokens": [50364, 38790, 631, 8919, 22048, 11, 44318, 785, 806, 15367, 368, 8411, 8526, 368, 16353, 11, 3841, 82, 870, 30, 398, 7433, 44318, 785, 50730], "temperature": 0.0, "avg_logprob": -0.3537453121609158, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.044115763157606125}, {"id": 129, "seek": 98530, "start": 992.62, "end": 999.42, "text": " el rango, \u00bfqu\u00e9 valores probable tiene cada una variable aleatoria que refira palabras?", "tokens": [50730, 806, 367, 17150, 11, 3841, 16412, 38790, 21759, 7066, 8411, 2002, 7006, 6775, 1639, 654, 631, 1895, 4271, 35240, 30, 51070], "temperature": 0.0, "avg_logprob": -0.3537453121609158, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.044115763157606125}, {"id": 130, "seek": 98530, "start": 999.42, "end": 1008.54, "text": " El todo el vocabulario, todas las palabras diferentes que yo puedo tener. Entonces nosotros", "tokens": [51070, 2699, 5149, 806, 2329, 455, 1040, 1004, 11, 10906, 2439, 35240, 17686, 631, 5290, 21612, 11640, 13, 15097, 13863, 51526], "temperature": 0.0, "avg_logprob": -0.3537453121609158, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.044115763157606125}, {"id": 131, "seek": 98530, "start": 1008.54, "end": 1014.0999999999999, "text": " vamos a poner estos notaciones probabilidades con ocimiento, de que la palabra sea", "tokens": [51526, 5295, 257, 19149, 12585, 406, 9188, 31959, 10284, 416, 277, 66, 14007, 11, 368, 631, 635, 31702, 4158, 51804], "temperature": 0.0, "avg_logprob": -0.3537453121609158, "compression_ratio": 1.610091743119266, "no_speech_prob": 0.044115763157606125}, {"id": 132, "seek": 101410, "start": 1014.1, "end": 1027.78, "text": " conocimiento. Vamos a denotar W1 a la N1N a la secuencia de palabras W1, W2, WN, por ejemplo", "tokens": [50364, 15871, 14007, 13, 10894, 257, 1441, 310, 289, 343, 16, 257, 635, 426, 16, 45, 257, 635, 907, 47377, 368, 35240, 343, 16, 11, 343, 17, 11, 343, 45, 11, 1515, 13358, 51048], "temperature": 0.0, "avg_logprob": -0.2745586803981236, "compression_ratio": 1.6932515337423313, "no_speech_prob": 0.033502813428640366}, {"id": 133, "seek": 101410, "start": 1027.78, "end": 1034.8600000000001, "text": " en una naci\u00f3n y vamos a decir que la vamos a hablar de la probabilidad de la secuencia", "tokens": [51048, 465, 2002, 297, 3482, 288, 5295, 257, 10235, 631, 635, 5295, 257, 21014, 368, 635, 31959, 4580, 368, 635, 907, 47377, 51402], "temperature": 0.0, "avg_logprob": -0.2745586803981236, "compression_ratio": 1.6932515337423313, "no_speech_prob": 0.033502813428640366}, {"id": 134, "seek": 101410, "start": 1034.8600000000001, "end": 1039.34, "text": " de palabras queriendo decir, bueno, la probabilidad de la que la primera sea W1, que la segunda", "tokens": [51402, 368, 35240, 7083, 7304, 10235, 11, 11974, 11, 635, 31959, 4580, 368, 635, 631, 635, 17382, 4158, 343, 16, 11, 631, 635, 21978, 51626], "temperature": 0.0, "avg_logprob": -0.2745586803981236, "compression_ratio": 1.6932515337423313, "no_speech_prob": 0.033502813428640366}, {"id": 135, "seek": 103934, "start": 1039.34, "end": 1047.54, "text": " sea W2, etc\u00e9tera. \u00bfDe acuerdo? O sea que esta distribuci\u00f3n de probabilidad tiene como", "tokens": [50364, 4158, 343, 17, 11, 5183, 526, 23833, 13, 3841, 11089, 28113, 30, 422, 4158, 631, 5283, 4400, 30813, 368, 31959, 4580, 7066, 2617, 50774], "temperature": 0.0, "avg_logprob": -0.3195301055908203, "compression_ratio": 1.2517482517482517, "no_speech_prob": 0.0708221048116684}, {"id": 136, "seek": 103934, "start": 1047.54, "end": 1057.6599999999999, "text": " rango todas las secuencias posibles de palabras. O sea que si mi vocabulario es V, tengo N", "tokens": [50774, 367, 17150, 10906, 2439, 907, 7801, 12046, 1366, 14428, 368, 35240, 13, 422, 4158, 631, 1511, 2752, 2329, 455, 1040, 1004, 785, 691, 11, 13989, 426, 51280], "temperature": 0.0, "avg_logprob": -0.3195301055908203, "compression_ratio": 1.2517482517482517, "no_speech_prob": 0.0708221048116684}, {"id": 137, "seek": 105766, "start": 1057.66, "end": 1071.6200000000001, "text": " y a la V, V a la N, V a la N. O sea que es enorme, especialmente, si todas las posibles", "tokens": [50364, 288, 257, 635, 691, 11, 691, 257, 635, 426, 11, 691, 257, 635, 426, 13, 422, 4158, 631, 785, 33648, 11, 41546, 11, 1511, 10906, 2439, 1366, 14428, 51062], "temperature": 0.0, "avg_logprob": -0.37581195150102886, "compression_ratio": 1.3555555555555556, "no_speech_prob": 0.11939508467912674}, {"id": 138, "seek": 105766, "start": 1071.6200000000001, "end": 1082.9, "text": " secuencias y vamos a recordar la chain rule o la regla de multiplicaci\u00f3n de las probabilidades", "tokens": [51062, 907, 7801, 12046, 288, 5295, 257, 2136, 289, 635, 5021, 4978, 277, 635, 1121, 875, 368, 17596, 3482, 368, 2439, 31959, 10284, 51626], "temperature": 0.0, "avg_logprob": -0.37581195150102886, "compression_ratio": 1.3555555555555556, "no_speech_prob": 0.11939508467912674}, {"id": 139, "seek": 108290, "start": 1082.9, "end": 1092.0600000000002, "text": " que es, si yo tengo la probabilidad de una secuencia de palabras W1, WN, esto es la probabilidad", "tokens": [50364, 631, 785, 11, 1511, 5290, 13989, 635, 31959, 4580, 368, 2002, 907, 47377, 368, 35240, 343, 16, 11, 343, 45, 11, 7433, 785, 635, 31959, 4580, 50822], "temperature": 0.0, "avg_logprob": -0.2545635724308515, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0389644131064415}, {"id": 140, "seek": 108290, "start": 1092.0600000000002, "end": 1098.02, "text": " de la primera palabra, que de alguna forma la calculo, por la probabilidad de la segunda", "tokens": [50822, 368, 635, 17382, 31702, 11, 631, 368, 20651, 8366, 635, 4322, 78, 11, 1515, 635, 31959, 4580, 368, 635, 21978, 51120], "temperature": 0.0, "avg_logprob": -0.2545635724308515, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0389644131064415}, {"id": 141, "seek": 108290, "start": 1098.02, "end": 1106.1200000000001, "text": " da la primera, da que la primera, da que la primera fue W1, observen ac\u00e1 que no son", "tokens": [51120, 1120, 635, 17382, 11, 1120, 631, 635, 17382, 11, 1120, 631, 635, 17382, 9248, 343, 16, 11, 9951, 268, 23496, 631, 572, 1872, 51525], "temperature": 0.0, "avg_logprob": -0.2545635724308515, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0389644131064415}, {"id": 142, "seek": 108290, "start": 1106.1200000000001, "end": 1111.96, "text": " independientes, es decir, la palabra por definici\u00f3n ac\u00e1, no son eventos independientes,", "tokens": [51525, 4819, 20135, 11, 785, 10235, 11, 635, 31702, 1515, 1561, 15534, 23496, 11, 572, 1872, 2280, 329, 4819, 20135, 11, 51817], "temperature": 0.0, "avg_logprob": -0.2545635724308515, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.0389644131064415}, {"id": 143, "seek": 111196, "start": 1111.96, "end": 1117.88, "text": " es decir, tengo una cierta probabilidad de que empiece con W1, la multiplicaci\u00f3n por", "tokens": [50364, 785, 10235, 11, 13989, 2002, 39769, 1328, 31959, 4580, 368, 631, 4012, 46566, 416, 343, 16, 11, 635, 17596, 3482, 1515, 50660], "temperature": 0.0, "avg_logprob": -0.25960056454527614, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.004401043523102999}, {"id": 144, "seek": 111196, "start": 1117.88, "end": 1122.8, "text": " la probabilidad de que la segunda sea W2, da que la primera fue W1, por la probabilidad", "tokens": [50660, 635, 31959, 4580, 368, 631, 635, 21978, 4158, 343, 17, 11, 1120, 631, 635, 17382, 9248, 343, 16, 11, 1515, 635, 31959, 4580, 50906], "temperature": 0.0, "avg_logprob": -0.25960056454527614, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.004401043523102999}, {"id": 145, "seek": 111196, "start": 1122.8, "end": 1130.44, "text": " que la tercera sea W3, da que las dos primeras fueron uno de ah\u00ed as\u00ed. \u00bfDe acuerdo?", "tokens": [50906, 631, 635, 1796, 41034, 4158, 343, 18, 11, 1120, 631, 2439, 4491, 2886, 6985, 28739, 8526, 368, 12571, 8582, 13, 3841, 11089, 28113, 30, 51288], "temperature": 0.0, "avg_logprob": -0.25960056454527614, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.004401043523102999}, {"id": 146, "seek": 111196, "start": 1130.44, "end": 1138.16, "text": " de esa forma con esta regla yo y al final WN la \u00faltima da toda la santer\u00eda, esto se", "tokens": [51288, 368, 11342, 8366, 416, 5283, 1121, 875, 5290, 288, 419, 2572, 343, 45, 635, 28118, 1120, 11687, 635, 262, 39918, 2686, 11, 7433, 369, 51674], "temperature": 0.0, "avg_logprob": -0.25960056454527614, "compression_ratio": 1.6273584905660377, "no_speech_prob": 0.004401043523102999}, {"id": 147, "seek": 113816, "start": 1138.16, "end": 1146.8400000000001, "text": " llama regla de la cadena, yo con la regla de la cadena puedo calcular la probabilidad", "tokens": [50364, 23272, 1121, 875, 368, 635, 12209, 4118, 11, 5290, 416, 635, 1121, 875, 368, 635, 12209, 4118, 21612, 2104, 17792, 635, 31959, 4580, 50798], "temperature": 0.0, "avg_logprob": -0.1775663571479993, "compression_ratio": 1.8767123287671232, "no_speech_prob": 0.023654989898204803}, {"id": 148, "seek": 113816, "start": 1146.8400000000001, "end": 1154.6000000000001, "text": " de una secuencia o de una oraci\u00f3n, da la secuencia, si logro calcular estas probabilidades, o sea", "tokens": [50798, 368, 2002, 907, 47377, 277, 368, 2002, 420, 3482, 11, 1120, 635, 907, 47377, 11, 1511, 3565, 340, 2104, 17792, 13897, 31959, 10284, 11, 277, 4158, 51186], "temperature": 0.0, "avg_logprob": -0.1775663571479993, "compression_ratio": 1.8767123287671232, "no_speech_prob": 0.023654989898204803}, {"id": 149, "seek": 113816, "start": 1154.6000000000001, "end": 1164.0400000000002, "text": " si logro calcular predecir las palabras correctamente, voy a poder predecir la secuencia,", "tokens": [51186, 1511, 3565, 340, 2104, 17792, 24874, 23568, 2439, 35240, 3006, 3439, 11, 7552, 257, 8152, 24874, 23568, 635, 907, 47377, 11, 51658], "temperature": 0.0, "avg_logprob": -0.1775663571479993, "compression_ratio": 1.8767123287671232, "no_speech_prob": 0.023654989898204803}, {"id": 150, "seek": 116404, "start": 1164.04, "end": 1171.04, "text": " esa forma paso de la predicci\u00f3n al c\u00e1lculo de toda la probabilidad de la oraci\u00f3n. \u00bfEntienden?", "tokens": [50364, 11342, 8366, 29212, 368, 635, 47336, 5687, 419, 6476, 75, 25436, 368, 11687, 635, 31959, 4580, 368, 635, 420, 3482, 13, 3841, 42837, 1174, 268, 30, 50714], "temperature": 0.0, "avg_logprob": -0.3474002456665039, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.06073695793747902}, {"id": 151, "seek": 116404, "start": 1171.04, "end": 1182.04, "text": " Bien, entonces vamos a quedarnos con esa notaci\u00f3n, entonces yo digo bueno, un ejemplo", "tokens": [50714, 16956, 11, 13003, 5295, 257, 13617, 24979, 416, 11342, 406, 3482, 11, 13003, 5290, 22990, 11974, 11, 517, 13358, 51264], "temperature": 0.0, "avg_logprob": -0.3474002456665039, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.06073695793747902}, {"id": 152, "seek": 116404, "start": 1182.04, "end": 1186.72, "text": " \u00bfno? Si yo quiero saber la probabilidad de viento fuerte, de componente sudeste como", "tokens": [51264, 3841, 1771, 30, 4909, 5290, 16811, 12489, 635, 31959, 4580, 368, 371, 7814, 37129, 11, 368, 4026, 1576, 3707, 8887, 2617, 51498], "temperature": 0.0, "avg_logprob": -0.3474002456665039, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.06073695793747902}, {"id": 153, "seek": 116404, "start": 1186.72, "end": 1192.48, "text": " el que est\u00e1 soplando, no s\u00e9 si es componente sudeste, pero fuerte, es la probabilidad de", "tokens": [51498, 806, 631, 3192, 370, 564, 1806, 11, 572, 7910, 1511, 785, 4026, 1576, 3707, 8887, 11, 4768, 37129, 11, 785, 635, 31959, 4580, 368, 51786], "temperature": 0.0, "avg_logprob": -0.3474002456665039, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.06073695793747902}, {"id": 154, "seek": 119248, "start": 1192.48, "end": 1197.78, "text": " viento por la probabilidad de fuerte, dado viento por la probabilidad de dado viento fuerte", "tokens": [50364, 371, 7814, 1515, 635, 31959, 4580, 368, 37129, 11, 29568, 371, 7814, 1515, 635, 31959, 4580, 368, 29568, 371, 7814, 37129, 50629], "temperature": 0.0, "avg_logprob": -0.33919242858886717, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.008136573247611523}, {"id": 155, "seek": 119248, "start": 1197.78, "end": 1211.4, "text": " etc\u00e9tera, \u00bfno? Nada menos que la regla de la cadena. Entonces yo quiero saber la \u00faltima", "tokens": [50629, 5183, 526, 23833, 11, 3841, 1771, 30, 40992, 8902, 631, 635, 1121, 875, 368, 635, 12209, 4118, 13, 15097, 5290, 16811, 12489, 635, 28118, 51310], "temperature": 0.0, "avg_logprob": -0.33919242858886717, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.008136573247611523}, {"id": 156, "seek": 119248, "start": 1211.4, "end": 1217.04, "text": " P de sudeste, dado viento fuerte, de componente y vos con Google por ejemplo digo bueno,", "tokens": [51310, 430, 368, 3707, 8887, 11, 29568, 371, 7814, 37129, 11, 368, 4026, 1576, 288, 13845, 416, 3329, 1515, 13358, 22990, 11974, 11, 51592], "temperature": 0.0, "avg_logprob": -0.33919242858886717, "compression_ratio": 1.6035502958579881, "no_speech_prob": 0.008136573247611523}, {"id": 157, "seek": 121704, "start": 1217.04, "end": 1224.8799999999999, "text": " fuerte, de componente aparece 9.230 veces, viento fuerte, componente sudeste aparece", "tokens": [50364, 37129, 11, 368, 4026, 1576, 37863, 1722, 13, 17, 3446, 17054, 11, 371, 7814, 37129, 11, 4026, 1576, 3707, 8887, 37863, 50756], "temperature": 0.0, "avg_logprob": -0.3038877983615823, "compression_ratio": 1.8310810810810811, "no_speech_prob": 0.06298390030860901}, {"id": 158, "seek": 121704, "start": 1224.8799999999999, "end": 1234.04, "text": " 347 veces, y yo entonces voy a estimar la probabilidad de esa por medio de conteos, entonces", "tokens": [50756, 805, 14060, 17054, 11, 288, 5290, 13003, 7552, 257, 8017, 289, 635, 31959, 4580, 368, 11342, 1515, 22123, 368, 34444, 329, 11, 13003, 51214], "temperature": 0.0, "avg_logprob": -0.3038877983615823, "compression_ratio": 1.8310810810810811, "no_speech_prob": 0.06298390030860901}, {"id": 159, "seek": 121704, "start": 1234.04, "end": 1237.84, "text": " la cantidad de veces que ha aparecido viento fuerte, componente sudeste, dividido la cantidad", "tokens": [51214, 635, 33757, 368, 17054, 631, 324, 15004, 17994, 371, 7814, 37129, 11, 4026, 1576, 3707, 8887, 11, 4996, 2925, 635, 33757, 51404], "temperature": 0.0, "avg_logprob": -0.3038877983615823, "compression_ratio": 1.8310810810810811, "no_speech_prob": 0.06298390030860901}, {"id": 160, "seek": 123784, "start": 1237.84, "end": 1246.56, "text": " de veces que aparece fuerte, componente, 347 veces dividido, no, 9.230. Aguardo, y esta", "tokens": [50364, 368, 17054, 631, 37863, 37129, 11, 4026, 1576, 11, 805, 14060, 17054, 4996, 2925, 11, 572, 11, 1722, 13, 17, 3446, 13, 2725, 84, 12850, 11, 288, 5283, 50800], "temperature": 0.0, "avg_logprob": -0.30821929787689784, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.03661186248064041}, {"id": 161, "seek": 123784, "start": 1246.56, "end": 1251.9599999999998, "text": " es la probabilidad de que la siguiente palabra sea sudeste, en mi estimaci\u00f3n. Si ustedes", "tokens": [50800, 785, 635, 31959, 4580, 368, 631, 635, 25666, 31702, 4158, 3707, 8887, 11, 465, 2752, 8017, 3482, 13, 4909, 17110, 51070], "temperature": 0.0, "avg_logprob": -0.30821929787689784, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.03661186248064041}, {"id": 162, "seek": 123784, "start": 1251.9599999999998, "end": 1260.36, "text": " desfijan, esto es una probabilidad porque contando todas las palabras posibles que pueden", "tokens": [51070, 730, 69, 1718, 282, 11, 7433, 785, 2002, 31959, 4580, 4021, 660, 1806, 10906, 2439, 35240, 1366, 14428, 631, 14714, 51490], "temperature": 0.0, "avg_logprob": -0.30821929787689784, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.03661186248064041}, {"id": 163, "seek": 123784, "start": 1260.36, "end": 1267.72, "text": " seguir ac\u00e1, si yo logro determinar cu\u00e1les son, yo s\u00e9 que van a ver 9.230, van a sumar", "tokens": [51490, 18584, 23496, 11, 1511, 5290, 3565, 340, 3618, 6470, 44318, 279, 1872, 11, 5290, 7910, 631, 3161, 257, 1306, 1722, 13, 17, 3446, 11, 3161, 257, 2408, 289, 51858], "temperature": 0.0, "avg_logprob": -0.30821929787689784, "compression_ratio": 1.5278969957081545, "no_speech_prob": 0.03661186248064041}, {"id": 164, "seek": 126772, "start": 1267.72, "end": 1274.16, "text": " 9.230, \u00bfno? En todo lo caso posible, mira todos los casos, junto a lo que son", "tokens": [50364, 1722, 13, 17, 3446, 11, 3841, 1771, 30, 2193, 5149, 450, 9666, 26644, 11, 30286, 6321, 1750, 25135, 11, 24663, 257, 450, 631, 1872, 50686], "temperature": 0.0, "avg_logprob": -0.34781130395754417, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.0011909330496564507}, {"id": 165, "seek": 126772, "start": 1274.16, "end": 1280.68, "text": " la siguiente palabra, eso hace que como esto me va a dar 9.230, la suma de todas las", "tokens": [50686, 635, 25666, 31702, 11, 7287, 10032, 631, 2617, 7433, 385, 2773, 257, 4072, 1722, 13, 17, 3446, 11, 635, 2408, 64, 368, 10906, 2439, 51012], "temperature": 0.0, "avg_logprob": -0.34781130395754417, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.0011909330496564507}, {"id": 166, "seek": 126772, "start": 1280.68, "end": 1286.3600000000001, "text": " cuantidades, esto va a dar uno, entonces esto s\u00ed es una distribuci\u00f3n de probabilidad,", "tokens": [51012, 2702, 394, 10284, 11, 7433, 2773, 257, 4072, 8526, 11, 13003, 7433, 8600, 785, 2002, 4400, 30813, 368, 31959, 4580, 11, 51296], "temperature": 0.0, "avg_logprob": -0.34781130395754417, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.0011909330496564507}, {"id": 167, "seek": 126772, "start": 1286.3600000000001, "end": 1291.48, "text": " entonces que estamos bien, efectivamente que yo des una probabilidad. Aguardo, esto es", "tokens": [51296, 13003, 631, 10382, 3610, 11, 22565, 23957, 631, 5290, 730, 2002, 31959, 4580, 13, 2725, 84, 12850, 11, 7433, 785, 51552], "temperature": 0.0, "avg_logprob": -0.34781130395754417, "compression_ratio": 1.572093023255814, "no_speech_prob": 0.0011909330496564507}, {"id": 168, "seek": 129148, "start": 1291.48, "end": 1308.28, "text": " lo que me dices, bueno, el 3,76% de las veces es sudeste, la siguiente palabra. Eso", "tokens": [50364, 450, 631, 385, 274, 1473, 11, 11974, 11, 806, 805, 11, 25026, 4, 368, 2439, 17054, 785, 3707, 8887, 11, 635, 25666, 31702, 13, 27795, 51204], "temperature": 0.0, "avg_logprob": -0.2165024150501598, "compression_ratio": 1.3692307692307693, "no_speech_prob": 0.027228258550167084}, {"id": 169, "seek": 129148, "start": 1308.28, "end": 1315.84, "text": " que acabamos de hacer es estimar la probabilidad a partir de la frecuencia de ocurrencia en un", "tokens": [51204, 631, 13281, 2151, 368, 6720, 785, 8017, 289, 635, 31959, 4580, 257, 13906, 368, 635, 2130, 66, 47377, 368, 26430, 1095, 2755, 465, 517, 51582], "temperature": 0.0, "avg_logprob": -0.2165024150501598, "compression_ratio": 1.3692307692307693, "no_speech_prob": 0.027228258550167084}, {"id": 170, "seek": 131584, "start": 1315.84, "end": 1322.72, "text": " corpo grande, eso Google es un corpo grande, muy grande. Y eso se llama principio m\u00e1ximo,", "tokens": [50364, 23257, 8883, 11, 7287, 3329, 785, 517, 23257, 8883, 11, 5323, 8883, 13, 398, 7287, 369, 23272, 34308, 38876, 11, 50708], "temperature": 0.0, "avg_logprob": -0.27758896350860596, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.34732586145401}, {"id": 171, "seek": 131584, "start": 1322.72, "end": 1328.36, "text": " pero similitud que lo vimos la de pasada, es, trato de hacer, calcular la probabilidad", "tokens": [50708, 4768, 1034, 388, 21875, 631, 450, 49266, 635, 368, 1736, 1538, 11, 785, 11, 504, 2513, 368, 6720, 11, 2104, 17792, 635, 31959, 4580, 50990], "temperature": 0.0, "avg_logprob": -0.27758896350860596, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.34732586145401}, {"id": 172, "seek": 131584, "start": 1328.36, "end": 1336.12, "text": " en base a lo mejor posible a los datos que tengo, es decir, considero, yo estoy considerando", "tokens": [50990, 465, 3096, 257, 450, 11479, 26644, 257, 1750, 27721, 631, 13989, 11, 785, 10235, 11, 1949, 78, 11, 5290, 15796, 1949, 1806, 51378], "temperature": 0.0, "avg_logprob": -0.27758896350860596, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.34732586145401}, {"id": 173, "seek": 131584, "start": 1336.12, "end": 1341.1599999999999, "text": " que los datos que tengo, es decir, el corpo de Google es una buena aproximaci\u00f3n del mundo", "tokens": [51378, 631, 1750, 27721, 631, 13989, 11, 785, 10235, 11, 806, 23257, 368, 3329, 785, 2002, 25710, 31270, 3482, 1103, 7968, 51630], "temperature": 0.0, "avg_logprob": -0.27758896350860596, "compression_ratio": 1.7190476190476192, "no_speech_prob": 0.34732586145401}, {"id": 174, "seek": 134116, "start": 1341.16, "end": 1347.64, "text": " real, del lenguaje en realidad, yo no s\u00e9 si en realidad efectivamente cuando los seres", "tokens": [50364, 957, 11, 1103, 35044, 84, 11153, 465, 25635, 11, 5290, 572, 7910, 1511, 465, 25635, 22565, 23957, 7767, 1750, 44721, 50688], "temperature": 0.0, "avg_logprob": -0.2754074243398813, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.27377259731292725}, {"id": 175, "seek": 134116, "start": 1347.64, "end": 1357.5600000000002, "text": " humanos hablamos, hay un 3,76% de probabilidad de que, despu\u00e9s decir bien tofuerte componente,", "tokens": [50688, 34555, 26280, 2151, 11, 4842, 517, 805, 11, 25026, 4, 368, 31959, 4580, 368, 631, 11, 15283, 10235, 3610, 281, 69, 5486, 975, 4026, 1576, 11, 51184], "temperature": 0.0, "avg_logprob": -0.2754074243398813, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.27377259731292725}, {"id": 176, "seek": 134116, "start": 1357.5600000000002, "end": 1362.44, "text": " viene sud\u00f3 este, pero el corpo de Google es que es lo mejor que tengo como aproximaci\u00f3n,", "tokens": [51184, 19561, 3707, 812, 4065, 11, 4768, 806, 23257, 368, 3329, 785, 631, 785, 450, 11479, 631, 13989, 2617, 31270, 3482, 11, 51428], "temperature": 0.0, "avg_logprob": -0.2754074243398813, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.27377259731292725}, {"id": 177, "seek": 134116, "start": 1362.44, "end": 1367.5600000000002, "text": " me dices eso, y eso es lo que yo utilizo, como un estimador de m\u00e1xima de la similitud,", "tokens": [51428, 385, 274, 1473, 7287, 11, 288, 7287, 785, 450, 631, 5290, 4976, 19055, 11, 2617, 517, 8017, 5409, 368, 31031, 64, 368, 635, 1034, 388, 21875, 11, 51684], "temperature": 0.0, "avg_logprob": -0.2754074243398813, "compression_ratio": 1.5338983050847457, "no_speech_prob": 0.27377259731292725}, {"id": 178, "seek": 136756, "start": 1367.56, "end": 1372.12, "text": " lo mejor que puedo acercarme con el corpo que tengo, eso es lo que vamos a hacer todo el", "tokens": [50364, 450, 11479, 631, 21612, 696, 2869, 35890, 416, 806, 23257, 631, 13989, 11, 7287, 785, 450, 631, 5295, 257, 6720, 5149, 806, 50592], "temperature": 0.0, "avg_logprob": -0.2995349679674421, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.03481819108128548}, {"id": 179, "seek": 136756, "start": 1372.12, "end": 1381.1599999999999, "text": " tiempo ac\u00e1, calcular componentes de m\u00e1xima de la similitud. Pero tenemos alg\u00fan problema,", "tokens": [50592, 11772, 23496, 11, 2104, 17792, 6542, 279, 368, 31031, 64, 368, 635, 1034, 388, 21875, 13, 9377, 9914, 26300, 12395, 11, 51044], "temperature": 0.0, "avg_logprob": -0.2995349679674421, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.03481819108128548}, {"id": 180, "seek": 136756, "start": 1381.1599999999999, "end": 1387.8799999999999, "text": " \u00bfno? Y es, en el otro caso, dice, a ra\u00edz estos fen\u00f3menos se producir\u00e1n tormentas fuertes,", "tokens": [51044, 3841, 1771, 30, 398, 785, 11, 465, 806, 11921, 9666, 11, 10313, 11, 257, 3342, 44551, 12585, 26830, 812, 2558, 329, 369, 1082, 23568, 7200, 36662, 296, 8536, 911, 279, 11, 51380], "temperature": 0.0, "avg_logprob": -0.2995349679674421, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.03481819108128548}, {"id": 181, "seek": 136756, "start": 1387.8799999999999, "end": 1395.8, "text": " la pr\u00f3egue fuertes, y a ra\u00edz estos fen\u00f3menos se producir\u00e1n tormentas, tiene un problema,", "tokens": [51380, 635, 8565, 68, 11929, 8536, 911, 279, 11, 288, 257, 3342, 44551, 12585, 26830, 812, 2558, 329, 369, 1082, 23568, 7200, 36662, 296, 11, 7066, 517, 12395, 11, 51776], "temperature": 0.0, "avg_logprob": -0.2995349679674421, "compression_ratio": 1.6681818181818182, "no_speech_prob": 0.03481819108128548}, {"id": 182, "seek": 139580, "start": 1395.8, "end": 1401.8799999999999, "text": " ah\u00ed es que, nunca apareci\u00f3 en mi corpus, a ra\u00edz estos fen\u00f3menos se producir\u00e1n tormentas,", "tokens": [50364, 12571, 785, 631, 11, 13768, 15004, 19609, 465, 2752, 1181, 31624, 11, 257, 3342, 44551, 12585, 26830, 812, 2558, 329, 369, 1082, 23568, 7200, 36662, 296, 11, 50668], "temperature": 0.0, "avg_logprob": -0.2723888295941648, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.006045451387763023}, {"id": 183, "seek": 139580, "start": 1401.8799999999999, "end": 1406.52, "text": " y nunca apareci\u00f3 en mi corpus, a ra\u00edz estos fen\u00f3menos se producir\u00e1n tormentas fuertes,", "tokens": [50668, 288, 13768, 15004, 19609, 465, 2752, 1181, 31624, 11, 257, 3342, 44551, 12585, 26830, 812, 2558, 329, 369, 1082, 23568, 7200, 36662, 296, 8536, 911, 279, 11, 50900], "temperature": 0.0, "avg_logprob": -0.2723888295941648, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.006045451387763023}, {"id": 184, "seek": 139580, "start": 1406.52, "end": 1415.72, "text": " \u00bfs\u00ed? Y eso nos da una horrible edici\u00f3n por cero, que queremos evitar, o sea que no", "tokens": [50900, 3841, 82, 870, 30, 398, 7287, 3269, 1120, 2002, 9263, 1257, 15534, 1515, 269, 2032, 11, 631, 26813, 31326, 11, 277, 4158, 631, 572, 51360], "temperature": 0.0, "avg_logprob": -0.2723888295941648, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.006045451387763023}, {"id": 185, "seek": 139580, "start": 1415.72, "end": 1424.8, "text": " est\u00e1 probabilidad, ah, infinito, no s\u00e9, no est\u00e1 definida, esto, una pregunta, esto les parece", "tokens": [51360, 3192, 31959, 4580, 11, 3716, 11, 7193, 3528, 11, 572, 7910, 11, 572, 3192, 1561, 2887, 11, 7433, 11, 2002, 24252, 11, 7433, 1512, 14120, 51814], "temperature": 0.0, "avg_logprob": -0.2723888295941648, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.006045451387763023}, {"id": 186, "seek": 142480, "start": 1424.8, "end": 1431.56, "text": " que es un fen\u00f3meno com\u00fan o no, que nos puede pasar cuando estemos estimando, todo el tiempo,", "tokens": [50364, 631, 785, 517, 26830, 812, 43232, 45448, 277, 572, 11, 631, 3269, 8919, 25344, 7767, 871, 4485, 8017, 1806, 11, 5149, 806, 11772, 11, 50702], "temperature": 0.0, "avg_logprob": -0.26582659353124033, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.002797405468299985}, {"id": 187, "seek": 142480, "start": 1431.56, "end": 1440.52, "text": " porque por m\u00e1s grande que sea el corpus, el lenguaje es muy creativo, entonces tenemos que buscar", "tokens": [50702, 4021, 1515, 3573, 8883, 631, 4158, 806, 1181, 31624, 11, 806, 35044, 84, 11153, 785, 5323, 1428, 6340, 11, 13003, 9914, 631, 26170, 51150], "temperature": 0.0, "avg_logprob": -0.26582659353124033, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.002797405468299985}, {"id": 188, "seek": 142480, "start": 1440.52, "end": 1446.04, "text": " forma y adem\u00e1s, porque estamos haciendo un conteo de palabras, de relaci\u00f3n muy largas,", "tokens": [51150, 8366, 288, 21251, 11, 4021, 10382, 20509, 517, 34444, 78, 368, 35240, 11, 368, 37247, 5323, 1613, 10549, 11, 51426], "temperature": 0.0, "avg_logprob": -0.26582659353124033, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.002797405468299985}, {"id": 189, "seek": 142480, "start": 1446.04, "end": 1452.44, "text": " o sea que la rila de la cadena no resuelve en mi problema, porque yo, una aproximaci\u00f3n bien", "tokens": [51426, 277, 4158, 631, 635, 367, 7371, 368, 635, 12209, 4118, 572, 725, 3483, 303, 465, 2752, 12395, 11, 4021, 5290, 11, 2002, 31270, 3482, 3610, 51746], "temperature": 0.0, "avg_logprob": -0.26582659353124033, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.002797405468299985}, {"id": 190, "seek": 145244, "start": 1452.44, "end": 1457.68, "text": " na\u00eff para que el culo de la probabilidad de calcular toda la secuencia posible, \u00bfcu\u00e1nta", "tokens": [50364, 1667, 15487, 69, 1690, 631, 806, 11021, 78, 368, 635, 31959, 4580, 368, 2104, 17792, 11687, 635, 907, 47377, 26644, 11, 3841, 12032, 27525, 64, 50626], "temperature": 0.0, "avg_logprob": -0.27660680586291897, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0325714536011219}, {"id": 191, "seek": 145244, "start": 1457.68, "end": 1461.3200000000002, "text": " vez se aparece la secuencia que quiero calcular en la elaboraci\u00f3n del total de raciones,", "tokens": [50626, 5715, 369, 37863, 635, 907, 47377, 631, 16811, 2104, 17792, 465, 635, 16298, 3482, 1103, 3217, 368, 4129, 5411, 11, 50808], "temperature": 0.0, "avg_logprob": -0.27660680586291897, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0325714536011219}, {"id": 192, "seek": 145244, "start": 1461.3200000000002, "end": 1464.92, "text": " lo cual es un disparate, pues no tengo corpus, evidentemente grande, pero esta aproximaci\u00f3n", "tokens": [50808, 450, 10911, 785, 517, 14548, 473, 11, 11059, 572, 13989, 1181, 31624, 11, 16371, 16288, 8883, 11, 4768, 5283, 31270, 3482, 50988], "temperature": 0.0, "avg_logprob": -0.27660680586291897, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0325714536011219}, {"id": 193, "seek": 145244, "start": 1464.92, "end": 1471.48, "text": " tampoco nos ayuda mucho, porque sigo teniendo contexto muy largo, porque si ustedes se fijan,", "tokens": [50988, 36838, 3269, 30737, 9824, 11, 4021, 4556, 78, 2064, 7304, 47685, 5323, 31245, 11, 4021, 1511, 17110, 369, 42001, 282, 11, 51316], "temperature": 0.0, "avg_logprob": -0.27660680586291897, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0325714536011219}, {"id": 194, "seek": 145244, "start": 1471.48, "end": 1477.2, "text": " en la rila de la cadena, bueno, en lo que acabamos de hacer, la \u00faltima probabilidad es casi", "tokens": [51316, 465, 635, 367, 7371, 368, 635, 12209, 4118, 11, 11974, 11, 465, 450, 631, 13281, 2151, 368, 6720, 11, 635, 28118, 31959, 4580, 785, 22567, 51602], "temperature": 0.0, "avg_logprob": -0.27660680586291897, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.0325714536011219}, {"id": 195, "seek": 147720, "start": 1477.2, "end": 1491.04, "text": " la misma que la primera, con menos una palabra, tengo que con una forma a chicar eso. Entonces,", "tokens": [50364, 635, 24946, 631, 635, 17382, 11, 416, 8902, 2002, 31702, 11, 13989, 631, 416, 2002, 8366, 257, 417, 7953, 7287, 13, 15097, 11, 51056], "temperature": 0.0, "avg_logprob": -0.3313805716378348, "compression_ratio": 1.443609022556391, "no_speech_prob": 0.09239763021469116}, {"id": 196, "seek": 147720, "start": 1491.04, "end": 1501.56, "text": " una de las ideas fuerza para computar esta probabilidad es el lugar de tomar todas las palabras,", "tokens": [51056, 2002, 368, 2439, 3487, 39730, 1690, 2807, 289, 5283, 31959, 4580, 785, 806, 11467, 368, 22048, 10906, 2439, 35240, 11, 51582], "temperature": 0.0, "avg_logprob": -0.3313805716378348, "compression_ratio": 1.443609022556391, "no_speech_prob": 0.09239763021469116}, {"id": 197, "seek": 150156, "start": 1501.56, "end": 1512.08, "text": " tomar sobre las \u00faltimas, es decir, yo me quedo con las \u00faltimas N menos un palabras, N menos", "tokens": [50364, 22048, 5473, 2439, 11499, 17957, 11, 785, 10235, 11, 5290, 385, 13617, 78, 416, 2439, 11499, 17957, 426, 8902, 517, 35240, 11, 426, 8902, 50890], "temperature": 0.0, "avg_logprob": -0.33553204006618925, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.06271937489509583}, {"id": 198, "seek": 150156, "start": 1512.08, "end": 1521.84, "text": " N, bueno, \u00bfs\u00ed? N, N, esto es en gran, \u00bfno? Y las otras no las considero, digo bueno,", "tokens": [50890, 426, 11, 11974, 11, 3841, 82, 870, 30, 426, 11, 426, 11, 7433, 785, 465, 9370, 11, 3841, 1771, 30, 398, 2439, 20244, 572, 2439, 1949, 78, 11, 22990, 11974, 11, 51378], "temperature": 0.0, "avg_logprob": -0.33553204006618925, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.06271937489509583}, {"id": 199, "seek": 150156, "start": 1521.84, "end": 1529.32, "text": " la con, mi, mi, mi, mi humilde aproximaci\u00f3n para que esto se pueda volver manejable, es decir,", "tokens": [51378, 635, 416, 11, 2752, 11, 2752, 11, 2752, 11, 2752, 1484, 15956, 31270, 3482, 1690, 631, 7433, 369, 31907, 33998, 12743, 73, 712, 11, 785, 10235, 11, 51752], "temperature": 0.0, "avg_logprob": -0.33553204006618925, "compression_ratio": 1.538888888888889, "no_speech_prob": 0.06271937489509583}, {"id": 200, "seek": 152932, "start": 1529.32, "end": 1533.08, "text": " bueno, yo en realidad solamente me importan las, solo las \u00faltimas palabras afectan en la que", "tokens": [50364, 11974, 11, 5290, 465, 25635, 27814, 385, 974, 282, 2439, 11, 6944, 2439, 11499, 17957, 35240, 30626, 282, 465, 635, 631, 50552], "temperature": 0.0, "avg_logprob": -0.29837506392906454, "compression_ratio": 1.75, "no_speech_prob": 0.045102477073669434}, {"id": 201, "seek": 152932, "start": 1533.08, "end": 1542.72, "text": " voy a predecir, solo la \u00faltima idea. Y de eso se tratan los modelos en grama, que utilizan", "tokens": [50552, 7552, 257, 24874, 23568, 11, 6944, 635, 28118, 1558, 13, 398, 368, 7287, 369, 21507, 282, 1750, 2316, 329, 465, 677, 2404, 11, 631, 19906, 282, 51034], "temperature": 0.0, "avg_logprob": -0.29837506392906454, "compression_ratio": 1.75, "no_speech_prob": 0.045102477073669434}, {"id": 202, "seek": 152932, "start": 1542.72, "end": 1546.48, "text": " lo que se llama, eso que acabo de decir, yo llamo hipote sigue marco, hipote sigue marcoviana,", "tokens": [51034, 450, 631, 369, 23272, 11, 7287, 631, 13281, 78, 368, 10235, 11, 5290, 4849, 10502, 8103, 1370, 34532, 1849, 1291, 11, 8103, 1370, 34532, 1849, 1291, 85, 8497, 11, 51222], "temperature": 0.0, "avg_logprob": -0.29837506392906454, "compression_ratio": 1.75, "no_speech_prob": 0.045102477073669434}, {"id": 203, "seek": 152932, "start": 1546.48, "end": 1558.48, "text": " solamente las \u00faltimas palabras afectan la siguiente, hay un l\u00edmite, \u00bft\u00e1? Y f\u00edjense que en la hipote", "tokens": [51222, 27814, 2439, 11499, 17957, 35240, 30626, 282, 635, 25666, 11, 4842, 517, 287, 14569, 642, 11, 3841, 83, 842, 30, 398, 283, 870, 73, 1288, 631, 465, 635, 8103, 1370, 51822], "temperature": 0.0, "avg_logprob": -0.29837506392906454, "compression_ratio": 1.75, "no_speech_prob": 0.045102477073669434}, {"id": 204, "seek": 155848, "start": 1558.48, "end": 1565.16, "text": " se divide grama, yo digo, cada palabra la aproximo por la anterior, simplemente, es decir, estoy", "tokens": [50364, 369, 9845, 677, 2404, 11, 5290, 22990, 11, 8411, 31702, 635, 31270, 78, 1515, 635, 22272, 11, 33190, 11, 785, 10235, 11, 15796, 50698], "temperature": 0.0, "avg_logprob": -0.34448252882912894, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.005482816603034735}, {"id": 205, "seek": 155848, "start": 1565.16, "end": 1571.1200000000001, "text": " diciendo una cosa tan sencilla como la \u00faltima palabra es la \u00fanica, cada palabra condici\u00f3n", "tokens": [50698, 42797, 2002, 10163, 7603, 3151, 66, 5291, 2617, 635, 28118, 31702, 785, 635, 30104, 11, 8411, 31702, 2224, 15534, 50996], "temperature": 0.0, "avg_logprob": -0.34448252882912894, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.005482816603034735}, {"id": 206, "seek": 155848, "start": 1571.1200000000001, "end": 1576.72, "text": " en la siguiente, pero en la anterior, \u00bfno? Es muy fuerte, \u00bfno? Y de trigramas son dos y con", "tokens": [50996, 465, 635, 25666, 11, 4768, 465, 635, 22272, 11, 3841, 1771, 30, 2313, 5323, 37129, 11, 3841, 1771, 30, 398, 368, 35386, 2356, 296, 1872, 4491, 288, 416, 51276], "temperature": 0.0, "avg_logprob": -0.34448252882912894, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.005482816603034735}, {"id": 207, "seek": 155848, "start": 1576.72, "end": 1585.0, "text": " N en grama son N, \u00bfno? S\u00ed, con la hipote sigue divide grama, mi proviezo mucho m\u00e1s", "tokens": [51276, 426, 465, 677, 2404, 1872, 426, 11, 3841, 1771, 30, 12375, 11, 416, 635, 8103, 1370, 34532, 9845, 677, 2404, 11, 2752, 1439, 414, 4765, 9824, 3573, 51690], "temperature": 0.0, "avg_logprob": -0.34448252882912894, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.005482816603034735}, {"id": 208, "seek": 158500, "start": 1585.0, "end": 1591.72, "text": " sencilla que antes, porque es como, cada palabra, solo depende, vamos a mire, uno bueno, uno", "tokens": [50364, 3151, 66, 5291, 631, 11014, 11, 4021, 785, 2617, 11, 8411, 31702, 11, 6944, 47091, 11, 5295, 257, 275, 621, 11, 8526, 11974, 11, 8526, 50700], "temperature": 0.0, "avg_logprob": -0.30615116173112894, "compression_ratio": 1.7452229299363058, "no_speech_prob": 0.22480657696723938}, {"id": 209, "seek": 158500, "start": 1591.72, "end": 1601.04, "text": " no est\u00e1 m\u00e1s, pero cada palabra depende del anterior, simplemente me queda que la probabilidad", "tokens": [50700, 572, 3192, 3573, 11, 4768, 8411, 31702, 47091, 1103, 22272, 11, 33190, 385, 23314, 631, 635, 31959, 4580, 51166], "temperature": 0.0, "avg_logprob": -0.30615116173112894, "compression_ratio": 1.7452229299363058, "no_speech_prob": 0.22480657696723938}, {"id": 210, "seek": 158500, "start": 1601.04, "end": 1612.96, "text": " de una secuencia, es la probabilidad de la primera, por la probabilidad de la segunda", "tokens": [51166, 368, 2002, 907, 47377, 11, 785, 635, 31959, 4580, 368, 635, 17382, 11, 1515, 635, 31959, 4580, 368, 635, 21978, 51762], "temperature": 0.0, "avg_logprob": -0.30615116173112894, "compression_ratio": 1.7452229299363058, "no_speech_prob": 0.22480657696723938}, {"id": 211, "seek": 161296, "start": 1612.96, "end": 1634.28, "text": " de la primera, por la probabilidad de la tercera de la segunda, etc\u00e9tera, y aguard\u00f3, ac\u00e1", "tokens": [50364, 368, 635, 17382, 11, 1515, 635, 31959, 4580, 368, 635, 1796, 41034, 368, 635, 21978, 11, 5183, 526, 23833, 11, 288, 623, 16981, 812, 11, 23496, 51430], "temperature": 0.0, "avg_logprob": -0.29452317101614817, "compression_ratio": 1.316546762589928, "no_speech_prob": 0.03808817267417908}, {"id": 212, "seek": 161296, "start": 1634.28, "end": 1640.52, "text": " nos falta este PW1 en esa f\u00f3rmula, pero no nos preocupa demasiado porque eso lo resolvemos", "tokens": [51430, 3269, 22111, 4065, 46375, 16, 465, 11342, 283, 15614, 76, 3780, 11, 4768, 572, 3269, 23080, 64, 39820, 4021, 7287, 450, 7923, 85, 4485, 51742], "temperature": 0.0, "avg_logprob": -0.29452317101614817, "compression_ratio": 1.316546762589928, "no_speech_prob": 0.03808817267417908}, {"id": 213, "seek": 164052, "start": 1640.52, "end": 1645.04, "text": " poniendo una marca al comienzo de la secuencia que siempre vale uno su probabilidad, es decir", "tokens": [50364, 9224, 7304, 2002, 30582, 419, 395, 1053, 4765, 368, 635, 907, 47377, 631, 12758, 15474, 8526, 459, 31959, 4580, 11, 785, 10235, 50590], "temperature": 0.0, "avg_logprob": -0.33314714153993474, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.19100958108901978}, {"id": 214, "seek": 164052, "start": 1645.04, "end": 1651.6, "text": " que todas las variaciones empiezan con una marca, y si no, multiplico ac\u00e1, \u00bfno? Si no, si", "tokens": [50590, 631, 10906, 2439, 3034, 9188, 4012, 18812, 282, 416, 2002, 30582, 11, 288, 1511, 572, 11, 12788, 2789, 23496, 11, 3841, 1771, 30, 4909, 572, 11, 1511, 50918], "temperature": 0.0, "avg_logprob": -0.33314714153993474, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.19100958108901978}, {"id": 215, "seek": 164052, "start": 1651.6, "end": 1656.96, "text": " lo quiere hacer de otra forma, agrega un PW, es su cero, ac\u00e1 y lo mismo, pero esencialmente", "tokens": [50918, 450, 23877, 6720, 368, 13623, 8366, 11, 623, 3375, 64, 517, 46375, 11, 785, 459, 269, 2032, 11, 23496, 288, 450, 12461, 11, 4768, 785, 26567, 4082, 51186], "temperature": 0.0, "avg_logprob": -0.33314714153993474, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.19100958108901978}, {"id": 216, "seek": 164052, "start": 1656.96, "end": 1661.24, "text": " lo importante ac\u00e1 es que esto se transforma en una simple multiplicaci\u00f3n de probabilidades", "tokens": [51186, 450, 9416, 23496, 785, 631, 7433, 369, 4088, 64, 465, 2002, 2199, 17596, 3482, 368, 31959, 10284, 51400], "temperature": 0.0, "avg_logprob": -0.33314714153993474, "compression_ratio": 1.592274678111588, "no_speech_prob": 0.19100958108901978}, {"id": 217, "seek": 166124, "start": 1661.24, "end": 1675.16, "text": " de una palabra a la anterior, y c\u00f3mo hago para calcular esto, c\u00f3mo puedo calcular esto ac\u00e1,", "tokens": [50364, 368, 2002, 31702, 257, 635, 22272, 11, 288, 12826, 38721, 1690, 2104, 17792, 7433, 11, 12826, 21612, 2104, 17792, 7433, 23496, 11, 51060], "temperature": 0.0, "avg_logprob": -0.33362880912986964, "compression_ratio": 1.6294117647058823, "no_speech_prob": 0.2674483358860016}, {"id": 218, "seek": 166124, "start": 1675.16, "end": 1680.92, "text": " c\u00f3mo calcular la probabilidad de una palabra, da en anterior, contando, pero solamente", "tokens": [51060, 12826, 2104, 17792, 635, 31959, 4580, 368, 2002, 31702, 11, 1120, 465, 22272, 11, 660, 1806, 11, 4768, 27814, 51348], "temperature": 0.0, "avg_logprob": -0.33362880912986964, "compression_ratio": 1.6294117647058823, "no_speech_prob": 0.2674483358860016}, {"id": 219, "seek": 166124, "start": 1680.92, "end": 1688.32, "text": " den cuenta a dos, lo cual lo vuelvo poniendo mucho m\u00e1s manejable, y eso es justo lo que vamos", "tokens": [51348, 1441, 17868, 257, 4491, 11, 450, 10911, 450, 20126, 3080, 9224, 7304, 9824, 3573, 12743, 73, 712, 11, 288, 7287, 785, 40534, 450, 631, 5295, 51718], "temperature": 0.0, "avg_logprob": -0.33362880912986964, "compression_ratio": 1.6294117647058823, "no_speech_prob": 0.2674483358860016}, {"id": 220, "seek": 168832, "start": 1688.32, "end": 1694.28, "text": " a hacer, un modelo de lenguaje intenta predecir la pr\u00f3xima palabra de una oraci\u00f3n a partir", "tokens": [50364, 257, 6720, 11, 517, 27825, 368, 35044, 84, 11153, 8446, 64, 24874, 23568, 635, 24096, 31702, 368, 2002, 420, 3482, 257, 13906, 50662], "temperature": 0.0, "avg_logprob": -0.32785596074284734, "compression_ratio": 1.450261780104712, "no_speech_prob": 0.02095099911093712}, {"id": 221, "seek": 168832, "start": 1694.28, "end": 1700.9199999999998, "text": " de las n menos una anterior, y por supuesto que importa el orden en ese c\u00e1lculo, \u00bfno?", "tokens": [50662, 368, 2439, 297, 8902, 2002, 22272, 11, 288, 1515, 34177, 631, 33218, 806, 28615, 465, 10167, 6476, 75, 25436, 11, 3841, 1771, 30, 50994], "temperature": 0.0, "avg_logprob": -0.32785596074284734, "compression_ratio": 1.450261780104712, "no_speech_prob": 0.02095099911093712}, {"id": 222, "seek": 168832, "start": 1706.52, "end": 1712.6, "text": " Tambi\u00e9n tenemos que plantearnos cuando hagamos los enegramos, cuando calculemos la probabilidad", "tokens": [51274, 25682, 9914, 631, 36829, 24979, 7767, 42386, 2151, 1750, 465, 1146, 30227, 11, 7767, 2104, 66, 2271, 3415, 635, 31959, 4580, 51578], "temperature": 0.0, "avg_logprob": -0.32785596074284734, "compression_ratio": 1.450261780104712, "no_speech_prob": 0.02095099911093712}, {"id": 223, "seek": 171260, "start": 1712.6, "end": 1718.1599999999999, "text": " en general, bueno, cosas que ya hemos conversado, \u00bfqu\u00e9 elemento vamos a contar? S\u00ed, por", "tokens": [50364, 465, 2674, 11, 11974, 11, 12218, 631, 2478, 15396, 2615, 1573, 11, 3841, 16412, 47961, 5295, 257, 27045, 30, 12375, 11, 1515, 50642], "temperature": 0.0, "avg_logprob": -0.3043865731092003, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.06483849138021469}, {"id": 224, "seek": 171260, "start": 1718.1599999999999, "end": 1725.8, "text": " ejemplo, tengo un tema de tokenizaci\u00f3n, esta coma, la tengo que considerar un diagrama", "tokens": [50642, 13358, 11, 13989, 517, 15854, 368, 14862, 27603, 11, 5283, 35106, 11, 635, 13989, 631, 1949, 289, 517, 10686, 64, 51024], "temperature": 0.0, "avg_logprob": -0.3043865731092003, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.06483849138021469}, {"id": 225, "seek": 171260, "start": 1725.8, "end": 1731.08, "text": " o no la tengo que considerar un diagrama, \u00bfs\u00ed? La tengo que considerar un token o no la tengo", "tokens": [51024, 277, 572, 635, 13989, 631, 1949, 289, 517, 10686, 64, 11, 3841, 82, 870, 30, 2369, 13989, 631, 1949, 289, 517, 14862, 277, 572, 635, 13989, 51288], "temperature": 0.0, "avg_logprob": -0.3043865731092003, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.06483849138021469}, {"id": 226, "seek": 171260, "start": 1731.08, "end": 1736.0, "text": " considerar un token, me interesa, bueno eso seguramente va a depender un poco de la aplicaci\u00f3n", "tokens": [51288, 1949, 289, 517, 14862, 11, 385, 728, 13708, 11, 11974, 7287, 22179, 3439, 2773, 257, 1367, 3216, 517, 10639, 368, 635, 18221, 3482, 51534], "temperature": 0.0, "avg_logprob": -0.3043865731092003, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.06483849138021469}, {"id": 227, "seek": 171260, "start": 1736.0, "end": 1742.56, "text": " en la que les aplican a los que les utilizan, o tengo un cuerpo oral donde tengo", "tokens": [51534, 465, 635, 631, 1512, 18221, 282, 257, 1750, 631, 1512, 19906, 282, 11, 277, 13989, 517, 20264, 19338, 10488, 13989, 51862], "temperature": 0.0, "avg_logprob": -0.3043865731092003, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.06483849138021469}, {"id": 228, "seek": 174256, "start": 1742.56, "end": 1748.08, "text": " de fluencia, de fluencia, creo que ya me ha estado. \u00bfQu\u00e9 tengo que hacer con las", "tokens": [50364, 368, 5029, 10974, 11, 368, 5029, 10974, 11, 14336, 631, 2478, 385, 324, 18372, 13, 3841, 15137, 13989, 631, 6720, 416, 2439, 50640], "temperature": 0.0, "avg_logprob": -0.40483835339546204, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.003317601513117552}, {"id": 229, "seek": 174256, "start": 1748.08, "end": 1753.24, "text": " may\u00fasculas? \u00bfQu\u00e9 hago con la forma flexionada? Todo lo problema de la tokenizaci\u00f3n me", "tokens": [50640, 815, 10227, 2444, 296, 30, 3841, 15137, 38721, 416, 635, 8366, 5896, 313, 1538, 30, 26466, 450, 12395, 368, 635, 14862, 27603, 385, 50898], "temperature": 0.0, "avg_logprob": -0.40483835339546204, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.003317601513117552}, {"id": 230, "seek": 174256, "start": 1753.24, "end": 1758.76, "text": " parece en el diagrama, es decir, esto son cascadas y amo, \u00bfno? Yo acab\u00e9 a tener la tokenizaci\u00f3n", "tokens": [50898, 14120, 465, 806, 10686, 64, 11, 785, 10235, 11, 7433, 1872, 3058, 66, 6872, 288, 43155, 11, 3841, 1771, 30, 7616, 13281, 526, 257, 11640, 635, 14862, 27603, 51174], "temperature": 0.0, "avg_logprob": -0.40483835339546204, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.003317601513117552}, {"id": 231, "seek": 174256, "start": 1758.76, "end": 1762.24, "text": " realizada, lo que ya no hay respuesto universal depende de la tarea que estamos haciendo, por", "tokens": [51174, 22828, 1538, 11, 450, 631, 2478, 572, 4842, 1597, 22057, 11455, 47091, 368, 635, 256, 35425, 631, 10382, 20509, 11, 1515, 51348], "temperature": 0.0, "avg_logprob": -0.40483835339546204, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.003317601513117552}, {"id": 232, "seek": 174256, "start": 1762.24, "end": 1770.56, "text": " ejemplo, t\u00edpicamente los cuerpos orales est\u00e1n todos pasados a may\u00fasculas, como son", "tokens": [51348, 13358, 11, 256, 28236, 23653, 1750, 18363, 30010, 19338, 279, 10368, 6321, 1736, 4181, 257, 815, 10227, 2444, 296, 11, 2617, 1872, 51764], "temperature": 0.0, "avg_logprob": -0.40483835339546204, "compression_ratio": 1.6223021582733812, "no_speech_prob": 0.003317601513117552}, {"id": 233, "seek": 177056, "start": 1770.56, "end": 1779.9199999999998, "text": " m\u00e1s continuos, no hay la identificaci\u00f3n de raciones, no es tan importante. Si yo voy", "tokens": [50364, 3573, 2993, 329, 11, 572, 4842, 635, 2473, 40802, 368, 4129, 5411, 11, 572, 785, 7603, 9416, 13, 4909, 5290, 7552, 50832], "temperature": 0.0, "avg_logprob": -0.3193180861982327, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0716971680521965}, {"id": 234, "seek": 177056, "start": 1779.9199999999998, "end": 1785.48, "text": " a hacer an\u00e1lisis, si estoy haciendo un an\u00e1lisis de c\u00f3mo se usan los signos de puntuaci\u00f3n", "tokens": [50832, 257, 6720, 44113, 28436, 11, 1511, 15796, 20509, 517, 44113, 28436, 368, 12826, 369, 505, 282, 1750, 1465, 329, 368, 18212, 84, 3482, 51110], "temperature": 0.0, "avg_logprob": -0.3193180861982327, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0716971680521965}, {"id": 235, "seek": 177056, "start": 1785.48, "end": 1791.48, "text": " en mi lenguaje, obviamente la coma la tengo que identificar, sino que para que no me interese,", "tokens": [51110, 465, 2752, 35044, 84, 11153, 11, 36325, 635, 35106, 635, 13989, 631, 2473, 25625, 11, 18108, 631, 1690, 631, 572, 385, 728, 1130, 11, 51410], "temperature": 0.0, "avg_logprob": -0.3193180861982327, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0716971680521965}, {"id": 236, "seek": 177056, "start": 1791.48, "end": 1798.48, "text": " o me puede interesar, todo esto es mapearlos a una cosa sola que se llama signos de puntuaci\u00f3n", "tokens": [51410, 277, 385, 8919, 728, 18876, 11, 5149, 7433, 785, 463, 494, 39734, 257, 2002, 10163, 34162, 631, 369, 23272, 1465, 329, 368, 18212, 84, 3482, 51760], "temperature": 0.0, "avg_logprob": -0.3193180861982327, "compression_ratio": 1.7209302325581395, "no_speech_prob": 0.0716971680521965}, {"id": 237, "seek": 179848, "start": 1798.48, "end": 1805.44, "text": " y juntar los puntos con las coma. Bueno, tiene que hacer eso en el laboratorio, ya se van a", "tokens": [50364, 288, 22739, 289, 1750, 34375, 416, 2439, 35106, 13, 16046, 11, 7066, 631, 6720, 7287, 465, 806, 5938, 48028, 11, 2478, 369, 3161, 257, 50712], "temperature": 0.0, "avg_logprob": -0.3554096611178651, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.03745797276496887}, {"id": 238, "seek": 179848, "start": 1805.44, "end": 1809.8, "text": " escolar. Bueno, nada, se necesita un pretetamiento, disponible al menos palabras, yo ni", "tokens": [50712, 4721, 15276, 13, 16046, 11, 8096, 11, 369, 45485, 517, 1162, 302, 16971, 11, 23311, 964, 419, 8902, 35240, 11, 5290, 3867, 50930], "temperature": 0.0, "avg_logprob": -0.3554096611178651, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.03745797276496887}, {"id": 239, "seek": 179848, "start": 1809.8, "end": 1818.2, "text": " el modelo, no hay modelos generales. Tambi\u00e9n va a depender un poco, nuestros n\u00fameros van", "tokens": [50930, 806, 27825, 11, 572, 4842, 2316, 329, 2674, 279, 13, 25682, 2773, 257, 1367, 3216, 517, 10639, 11, 24099, 36545, 3161, 51350], "temperature": 0.0, "avg_logprob": -0.3554096611178651, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.03745797276496887}, {"id": 240, "seek": 179848, "start": 1818.2, "end": 1826.16, "text": " a depender de la cantidad de palabras. El diccionario, el Oxford English Dictionary tiene", "tokens": [51350, 257, 1367, 3216, 368, 635, 33757, 368, 35240, 13, 2699, 14285, 10015, 4912, 11, 806, 24786, 3669, 413, 4105, 822, 7066, 51748], "temperature": 0.0, "avg_logprob": -0.3554096611178651, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.03745797276496887}, {"id": 241, "seek": 182616, "start": 1826.16, "end": 1832.96, "text": " 290.000 entradas, el trezor de la sangre franc\u00e9s tiene 54.000 y el diccionario de la radio", "tokens": [50364, 568, 7771, 13, 1360, 948, 48906, 11, 806, 2192, 89, 284, 368, 635, 45878, 30514, 2191, 7066, 20793, 13, 1360, 288, 806, 14285, 10015, 4912, 368, 635, 6477, 50704], "temperature": 0.0, "avg_logprob": -0.4635224691251429, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.03746601566672325}, {"id": 242, "seek": 182616, "start": 1832.96, "end": 1844.68, "text": " es 88.000. \u00bfPor qu\u00e9 les parece que tienen tantas m\u00e1s acacagadas? Porque el diccionario", "tokens": [50704, 785, 24587, 13, 1360, 13, 3841, 24907, 8057, 1512, 14120, 631, 12536, 12095, 296, 3573, 696, 326, 559, 6872, 30, 11287, 806, 14285, 10015, 4912, 51290], "temperature": 0.0, "avg_logprob": -0.4635224691251429, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.03746601566672325}, {"id": 243, "seek": 182616, "start": 1844.68, "end": 1849.48, "text": " no parece en la forma flexionada y el espa\u00f1ol est\u00e1 mucho m\u00e1s flexionado que el n\u00famero.", "tokens": [51290, 572, 14120, 465, 635, 8366, 5896, 313, 1538, 288, 806, 31177, 3192, 9824, 3573, 5896, 313, 1573, 631, 806, 14959, 13, 51530], "temperature": 0.0, "avg_logprob": -0.4635224691251429, "compression_ratio": 1.4782608695652173, "no_speech_prob": 0.03746601566672325}, {"id": 244, "seek": 184948, "start": 1850.48, "end": 1857.3600000000001, "text": " O sea, el inmune se va a tener que arreglar m\u00e1s solito. Bueno, y despu\u00e9s tenemos corpos,", "tokens": [50414, 422, 4158, 11, 806, 41052, 2613, 369, 2773, 257, 11640, 631, 594, 3375, 2200, 3573, 1404, 3528, 13, 16046, 11, 288, 15283, 9914, 1181, 30010, 11, 50758], "temperature": 0.0, "avg_logprob": -0.4378539520559959, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.022903598845005035}, {"id": 245, "seek": 184948, "start": 1857.3600000000001, "end": 1863.32, "text": " esto ya hablamos un poco, y aquellos distinguyen entre el n\u00famero de toques en que son la cantidad", "tokens": [50758, 7433, 2478, 26280, 2151, 517, 10639, 11, 288, 49835, 11365, 16580, 3962, 806, 14959, 368, 281, 7519, 465, 631, 1872, 635, 33757, 51056], "temperature": 0.0, "avg_logprob": -0.4378539520559959, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.022903598845005035}, {"id": 246, "seek": 184948, "start": 1863.32, "end": 1867.6, "text": " de ocurrencias que hay en el texto y el n\u00famero de palabras distintas, el vocabular.", "tokens": [51056, 368, 26430, 1095, 12046, 631, 4842, 465, 806, 35503, 288, 806, 14959, 368, 35240, 31489, 296, 11, 806, 2329, 455, 1040, 13, 51270], "temperature": 0.0, "avg_logprob": -0.4378539520559959, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.022903598845005035}, {"id": 247, "seek": 184948, "start": 1874.08, "end": 1877.76, "text": " Ac\u00e1 est\u00e1 la respuesta a la pregunta de que hac\u00edamos antes, \u00bfc\u00f3mo estimamos lo vigilan m\u00e1s?", "tokens": [51594, 5097, 842, 3192, 635, 40585, 257, 635, 24252, 368, 631, 46093, 16275, 11014, 11, 3841, 46614, 8017, 2151, 450, 15366, 21752, 3573, 30, 51778], "temperature": 0.0, "avg_logprob": -0.4378539520559959, "compression_ratio": 1.5330578512396693, "no_speech_prob": 0.022903598845005035}, {"id": 248, "seek": 187776, "start": 1877.76, "end": 1882.16, "text": " Utilizando otra vez lo que se llama un estimador de m\u00e1ximo a ver el similitud, lo que se llama", "tokens": [50364, 12555, 47043, 1806, 13623, 5715, 450, 631, 369, 23272, 517, 8017, 5409, 368, 38876, 257, 1306, 806, 1034, 388, 21875, 11, 450, 631, 369, 23272, 50584], "temperature": 0.0, "avg_logprob": -0.394517343255538, "compression_ratio": 1.55, "no_speech_prob": 0.07904897630214691}, {"id": 249, "seek": 187776, "start": 1882.16, "end": 1887.6, "text": " m\u00e9todos de frecuencias relativas, que es cuento, la cantidad de veces que apareci\u00f3 una", "tokens": [50584, 20275, 378, 329, 368, 2130, 66, 7801, 12046, 21960, 296, 11, 631, 785, 2702, 15467, 11, 635, 33757, 368, 17054, 631, 15004, 19609, 2002, 50856], "temperature": 0.0, "avg_logprob": -0.394517343255538, "compression_ratio": 1.55, "no_speech_prob": 0.07904897630214691}, {"id": 250, "seek": 187776, "start": 1887.6, "end": 1901.72, "text": " palabra con, por ejemplo, la probabilidad de fuerte, dado viento, se aproxima como la cantidad", "tokens": [50856, 31702, 416, 11, 1515, 13358, 11, 635, 31959, 4580, 368, 37129, 11, 29568, 371, 7814, 11, 369, 31270, 64, 2617, 635, 33757, 51562], "temperature": 0.0, "avg_logprob": -0.394517343255538, "compression_ratio": 1.55, "no_speech_prob": 0.07904897630214691}, {"id": 251, "seek": 190172, "start": 1901.72, "end": 1917.28, "text": " de veces que aparece bien tofuerte, por la dividida de la cantidad de veces que apareci\u00f3,", "tokens": [50364, 368, 17054, 631, 37863, 3610, 281, 69, 84, 10634, 11, 1515, 635, 4996, 2887, 368, 635, 33757, 368, 17054, 631, 15004, 19609, 11, 51142], "temperature": 0.0, "avg_logprob": -0.4106070200602214, "compression_ratio": 1.2, "no_speech_prob": 0.011575870215892792}, {"id": 252, "seek": 191728, "start": 1917.28, "end": 1934.84, "text": " dividido todas las posibles continuaciones, \u00bfde acuerdo? Viento fuerte, viento calmo, viento,", "tokens": [50364, 4996, 2925, 10906, 2439, 1366, 14428, 2993, 9188, 11, 3841, 1479, 28113, 30, 691, 7814, 37129, 11, 371, 7814, 2104, 3280, 11, 371, 7814, 11, 51242], "temperature": 0.0, "avg_logprob": -0.3607540766398112, "compression_ratio": 1.455223880597015, "no_speech_prob": 0.08410265296697617}, {"id": 253, "seek": 191728, "start": 1934.84, "end": 1944.12, "text": " viento diles, viento, no s\u00e9, lo que quieras. Y sumo todas las posibles, estoy haciendo normalizando", "tokens": [51242, 371, 7814, 274, 4680, 11, 371, 7814, 11, 572, 7910, 11, 450, 631, 23572, 296, 13, 398, 2408, 78, 10906, 2439, 1366, 14428, 11, 15796, 20509, 2710, 590, 1806, 51706], "temperature": 0.0, "avg_logprob": -0.3607540766398112, "compression_ratio": 1.455223880597015, "no_speech_prob": 0.08410265296697617}, {"id": 254, "seek": 194412, "start": 1944.12, "end": 1949.36, "text": " como hablamos al principio de como hablamos ac\u00e1, estoy normalizando. Ahora, esto aqu\u00ed es equivalente,", "tokens": [50364, 2617, 26280, 2151, 419, 34308, 368, 2617, 26280, 2151, 23496, 11, 15796, 2710, 590, 1806, 13, 18840, 11, 7433, 6661, 785, 9052, 1576, 11, 50626], "temperature": 0.0, "avg_logprob": -0.4024206797281901, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.029955992475152016}, {"id": 255, "seek": 194412, "start": 1950.52, "end": 1952.12, "text": " \u00bfc\u00f3mo puedo simplificar esto?", "tokens": [50684, 3841, 46614, 21612, 6883, 25625, 7433, 30, 50764], "temperature": 0.0, "avg_logprob": -0.4024206797281901, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.029955992475152016}, {"id": 256, "seek": 194412, "start": 1958.4799999999998, "end": 1968.0, "text": " Si yo tengo todas las disica, parece viento fuerte, viento calmo, no s\u00e9, \u00bfqu\u00e9 es la suma de todo eso?", "tokens": [51082, 4909, 5290, 13989, 10906, 2439, 717, 2262, 11, 14120, 371, 7814, 37129, 11, 371, 7814, 2104, 3280, 11, 572, 7910, 11, 3841, 16412, 785, 635, 2408, 64, 368, 5149, 7287, 30, 51558], "temperature": 0.0, "avg_logprob": -0.4024206797281901, "compression_ratio": 1.3714285714285714, "no_speech_prob": 0.029955992475152016}, {"id": 257, "seek": 197412, "start": 1974.12, "end": 1979.9599999999998, "text": " Y la cantidad es de la peseamiento, estoy igual a la cantidad de veces que aparece bien tof, en el corp.", "tokens": [50364, 398, 635, 33757, 785, 368, 635, 280, 1130, 16971, 11, 15796, 10953, 257, 635, 33757, 368, 17054, 631, 37863, 3610, 281, 69, 11, 465, 806, 1181, 79, 13, 50656], "temperature": 0.0, "avg_logprob": -0.43182577546109857, "compression_ratio": 1.5, "no_speech_prob": 0.04349987581372261}, {"id": 258, "seek": 197412, "start": 1980.7199999999998, "end": 1985.36, "text": " \u00bfC\u00f3mo guard\u00f3? \u00bfC\u00f3mo son todas las posibles ocurrencias?", "tokens": [50694, 3841, 28342, 6290, 812, 30, 3841, 28342, 1872, 10906, 2439, 1366, 14428, 26430, 1095, 12046, 30, 50926], "temperature": 0.0, "avg_logprob": -0.43182577546109857, "compression_ratio": 1.5, "no_speech_prob": 0.04349987581372261}, {"id": 259, "seek": 197412, "start": 1991.04, "end": 1996.08, "text": " Ah\u00ed tenemos la simplificaci\u00f3n y adem\u00e1s para tener en cuenta la primera y \u00faltima palabra", "tokens": [51210, 49924, 9914, 635, 6883, 40802, 288, 21251, 1690, 11640, 465, 17868, 635, 17382, 288, 28118, 31702, 51462], "temperature": 0.0, "avg_logprob": -0.43182577546109857, "compression_ratio": 1.5, "no_speech_prob": 0.04349987581372261}, {"id": 260, "seek": 197412, "start": 1996.08, "end": 2000.76, "text": " de honoraci\u00f3n, le vamos a agregar siempre los s\u00edmbolos de comienzo y de fin, eso para asegurarnos", "tokens": [51462, 368, 5968, 3482, 11, 476, 5295, 257, 4554, 2976, 12758, 1750, 8600, 5612, 329, 368, 395, 1053, 4765, 288, 368, 962, 11, 7287, 1690, 38174, 374, 24979, 51696], "temperature": 0.0, "avg_logprob": -0.43182577546109857, "compression_ratio": 1.5, "no_speech_prob": 0.04349987581372261}, {"id": 261, "seek": 200076, "start": 2001.76, "end": 2008.08, "text": " de que para no tener que calcularse parada la probabilidad de la primera palabra. Yo s\u00e9 que la primera", "tokens": [50414, 368, 631, 1690, 572, 11640, 631, 2104, 17792, 405, 971, 1538, 635, 31959, 4580, 368, 635, 17382, 31702, 13, 7616, 7910, 631, 635, 17382, 50730], "temperature": 0.0, "avg_logprob": -0.26746982183211887, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.046905506402254105}, {"id": 262, "seek": 200076, "start": 2008.08, "end": 2016.36, "text": " palabra siempre es ese y calculo la probabilidad de la primera en el texto, digamos, ponerle \u00e9l", "tokens": [50730, 31702, 12758, 785, 10167, 288, 4322, 78, 635, 31959, 4580, 368, 635, 17382, 465, 806, 35503, 11, 36430, 11, 19149, 306, 11810, 51144], "temperature": 0.0, "avg_logprob": -0.26746982183211887, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.046905506402254105}, {"id": 263, "seek": 200076, "start": 2016.36, "end": 2025.8, "text": " dado que la anterior era ese, \u00bfde acuerdo? Y as\u00ed lo dejo en una sola forma. Por ejemplo,", "tokens": [51144, 29568, 631, 635, 22272, 4249, 10167, 11, 3841, 1479, 28113, 30, 398, 8582, 450, 368, 5134, 465, 2002, 34162, 8366, 13, 5269, 13358, 11, 51616], "temperature": 0.0, "avg_logprob": -0.26746982183211887, "compression_ratio": 1.6256983240223464, "no_speech_prob": 0.046905506402254105}, {"id": 264, "seek": 202580, "start": 2025.8, "end": 2034.44, "text": " si supongamos que yo tengo ese corp, \u00bfno? Oan, abri\u00f3 la puerta, el viento abri\u00f3 la puerta,", "tokens": [50364, 1511, 9331, 556, 2151, 631, 5290, 13989, 10167, 1181, 79, 11, 3841, 1771, 30, 422, 282, 11, 410, 44802, 635, 48597, 11, 806, 371, 7814, 410, 44802, 635, 48597, 11, 50796], "temperature": 0.0, "avg_logprob": -0.3208235931396484, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.010843385010957718}, {"id": 265, "seek": 202580, "start": 2034.44, "end": 2041.2, "text": " el negro abri\u00f3 limones en tus mejillas nuevas, Juan recoge limones. Y quiero saber la probabilidad", "tokens": [50796, 806, 40008, 410, 44802, 2364, 2213, 465, 20647, 37758, 20243, 42817, 11, 17064, 7759, 432, 2364, 2213, 13, 398, 16811, 12489, 635, 31959, 4580, 51134], "temperature": 0.0, "avg_logprob": -0.3208235931396484, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.010843385010957718}, {"id": 266, "seek": 202580, "start": 2041.2, "end": 2047.3999999999999, "text": " de estas oraciones. Evidentemente, no las tengo en el corp, ya que no es poco tan directamente,", "tokens": [51134, 368, 13897, 420, 9188, 13, 5689, 1078, 16288, 11, 572, 2439, 13989, 465, 806, 1181, 79, 11, 2478, 631, 572, 785, 10639, 7603, 46230, 11, 51444], "temperature": 0.0, "avg_logprob": -0.3208235931396484, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.010843385010957718}, {"id": 267, "seek": 202580, "start": 2049.8, "end": 2053.48, "text": " pero quiero utilizar un modelo de diagramas para calcular.", "tokens": [51564, 4768, 16811, 24060, 517, 27825, 368, 10686, 296, 1690, 2104, 17792, 13, 51748], "temperature": 0.0, "avg_logprob": -0.3208235931396484, "compression_ratio": 1.6036866359447004, "no_speech_prob": 0.010843385010957718}, {"id": 268, "seek": 205580, "start": 2055.8, "end": 2064.84, "text": " Y con lo que sabemos es bastante sencillo. Primero que nada, decimos bueno, la probabilidad de", "tokens": [50364, 398, 416, 450, 631, 27200, 785, 14651, 46749, 78, 13, 19671, 2032, 631, 8096, 11, 979, 8372, 11974, 11, 635, 31959, 4580, 368, 50816], "temperature": 0.0, "avg_logprob": -0.2697228375603171, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.02313952147960663}, {"id": 269, "seek": 205580, "start": 2064.84, "end": 2076.44, "text": " Juan abri\u00f3 limones es probabilidad de Juan dado el comienzo, probabilidad de abri\u00f3 dado Juan,", "tokens": [50816, 17064, 410, 44802, 2364, 2213, 785, 31959, 4580, 368, 17064, 29568, 806, 395, 1053, 4765, 11, 31959, 4580, 368, 410, 44802, 29568, 17064, 11, 51396], "temperature": 0.0, "avg_logprob": -0.2697228375603171, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.02313952147960663}, {"id": 270, "seek": 205580, "start": 2076.44, "end": 2085.44, "text": " probabilidad de limones de abri\u00f3, etc\u00e9tera, \u00bfno? F\u00edjense que la probabilidad Juan dado el comienzo", "tokens": [51396, 31959, 4580, 368, 2364, 2213, 368, 410, 44802, 11, 5183, 526, 23833, 11, 3841, 1771, 30, 479, 870, 73, 1288, 631, 635, 31959, 4580, 17064, 29568, 806, 395, 1053, 4765, 51846], "temperature": 0.0, "avg_logprob": -0.2697228375603171, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.02313952147960663}, {"id": 271, "seek": 208544, "start": 2085.44, "end": 2089.8, "text": " de la cantidad de veces que apareci\u00f3 Juan en la marca del comienzo, dividido en la cantidad de", "tokens": [50364, 368, 635, 33757, 368, 17054, 631, 15004, 19609, 17064, 465, 635, 30582, 1103, 395, 1053, 4765, 11, 4996, 2925, 465, 635, 33757, 368, 50582], "temperature": 0.0, "avg_logprob": -0.42174138060403527, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0046104430221021175}, {"id": 272, "seek": 208544, "start": 2089.8, "end": 2092.84, "text": " marca del comienzo que es uno. Entonces, he tomado...", "tokens": [50582, 30582, 1103, 395, 1053, 4765, 631, 785, 8526, 13, 15097, 11, 415, 2916, 1573, 485, 50734], "temperature": 0.0, "avg_logprob": -0.42174138060403527, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0046104430221021175}, {"id": 273, "seek": 208544, "start": 2097.2400000000002, "end": 2100.2400000000002, "text": " 2 de 4. Ah, \u00bfpor qu\u00e9 hay cuatro oraciones?", "tokens": [50954, 568, 368, 1017, 13, 2438, 11, 3841, 2816, 8057, 4842, 28795, 420, 9188, 30, 51104], "temperature": 0.0, "avg_logprob": -0.42174138060403527, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0046104430221021175}, {"id": 274, "seek": 208544, "start": 2100.2400000000002, "end": 2104.96, "text": " Claro, claro, porque yo estoy haciendo contegos directamente, no estoy haciendo probabilidad.", "tokens": [51104, 33380, 11, 16742, 11, 4021, 5290, 15796, 20509, 660, 1146, 329, 46230, 11, 572, 15796, 20509, 31959, 4580, 13, 51340], "temperature": 0.0, "avg_logprob": -0.42174138060403527, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0046104430221021175}, {"id": 275, "seek": 208544, "start": 2104.96, "end": 2115.4, "text": " 2 de 4 veces arranc\u00f3 con Juan, \u00bfs\u00ed? Juan abri\u00f3 es una de 2, ya hab\u00eda parecido", "tokens": [51340, 568, 368, 1017, 17054, 50235, 66, 812, 416, 17064, 11, 3841, 82, 870, 30, 17064, 410, 44802, 785, 2002, 368, 568, 11, 2478, 16395, 7448, 17994, 51862], "temperature": 0.0, "avg_logprob": -0.42174138060403527, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0046104430221021175}, {"id": 276, "seek": 211540, "start": 2115.4, "end": 2123.56, "text": " Juan abri\u00f3 en el corpus y Juan apareci\u00f3 2 veces. O sea, de 2 veces la pareci\u00f3 Juan en la siguiente", "tokens": [50364, 17064, 410, 44802, 465, 806, 1181, 31624, 288, 17064, 15004, 19609, 568, 17054, 13, 422, 4158, 11, 368, 568, 17054, 635, 7448, 19609, 17064, 465, 635, 25666, 50772], "temperature": 0.0, "avg_logprob": -0.35759445337148815, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.002895941259339452}, {"id": 277, "seek": 211540, "start": 2123.56, "end": 2129.4, "text": " apareci\u00f3 una vez abri\u00f3. Y as\u00ed sigo multiplicando y como ve, multiplica la fracci\u00f3n y me da, bueno,", "tokens": [50772, 15004, 19609, 2002, 5715, 410, 44802, 13, 398, 8582, 4556, 78, 17596, 1806, 288, 2617, 1241, 11, 12788, 2262, 635, 431, 49354, 288, 385, 1120, 11, 11974, 11, 51064], "temperature": 0.0, "avg_logprob": -0.35759445337148815, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.002895941259339452}, {"id": 278, "seek": 211540, "start": 2130.4, "end": 2133.4, "text": " 0,042, esa es la probabilidad de Juan abri\u00f3 limones.", "tokens": [51114, 1958, 11, 15, 15628, 11, 11342, 785, 635, 31959, 4580, 368, 17064, 410, 44802, 2364, 2213, 13, 51264], "temperature": 0.0, "avg_logprob": -0.35759445337148815, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.002895941259339452}, {"id": 279, "seek": 211540, "start": 2137.4, "end": 2144.04, "text": " Enero abri\u00f3 la puerta, 0,17, tambi\u00e9n tiene mucho sentido, \u00bfno? A ver,", "tokens": [51464, 2193, 2032, 410, 44802, 635, 48597, 11, 1958, 11, 7773, 11, 6407, 7066, 9824, 19850, 11, 3841, 1771, 30, 316, 1306, 11, 51796], "temperature": 0.0, "avg_logprob": -0.35759445337148815, "compression_ratio": 1.5395348837209302, "no_speech_prob": 0.002895941259339452}, {"id": 280, "seek": 214404, "start": 2144.04, "end": 2148.64, "text": " justamente el hecho de que sigo un ejemplo de jubete le hace perder la gracia todo esto,", "tokens": [50364, 41056, 806, 13064, 368, 631, 4556, 78, 517, 13358, 368, 361, 836, 3498, 476, 10032, 26971, 635, 11625, 654, 5149, 7433, 11, 50594], "temperature": 0.0, "avg_logprob": -0.4157409985860189, "compression_ratio": 1.3478260869565217, "no_speech_prob": 0.009838581085205078}, {"id": 281, "seek": 214404, "start": 2148.64, "end": 2157.44, "text": " porque esto funciona porque tengo grandes vol\u00famenes, sino no es una paba. Y ac\u00e1 que nos pas\u00f3,", "tokens": [50594, 4021, 7433, 26210, 4021, 13989, 16640, 1996, 2481, 2558, 279, 11, 18108, 572, 785, 2002, 280, 5509, 13, 398, 23496, 631, 3269, 41382, 11, 51034], "temperature": 0.0, "avg_logprob": -0.4157409985860189, "compression_ratio": 1.3478260869565217, "no_speech_prob": 0.009838581085205078}, {"id": 282, "seek": 214404, "start": 2157.44, "end": 2159.84, "text": " \u00bfqu\u00e9 puede haber pasado ac\u00e1?", "tokens": [51034, 3841, 16412, 8919, 15811, 24794, 23496, 30, 51154], "temperature": 0.0, "avg_logprob": -0.4157409985860189, "compression_ratio": 1.3478260869565217, "no_speech_prob": 0.009838581085205078}, {"id": 283, "seek": 217404, "start": 2175.04, "end": 2190.2799999999997, "text": " La palabra come nunca est\u00e1. Y en la puerta, en la puerta est\u00e1. La primera se explica", "tokens": [50414, 2369, 31702, 808, 13768, 3192, 13, 398, 465, 635, 48597, 11, 465, 635, 48597, 3192, 13, 2369, 17382, 369, 1490, 2262, 51176], "temperature": 0.0, "avg_logprob": -0.3407872009277344, "compression_ratio": 1.162162162162162, "no_speech_prob": 0.05216161534190178}, {"id": 284, "seek": 219028, "start": 2190.28, "end": 2215.2400000000002, "text": " porque come nunca est\u00e1. Creo que est\u00e1 as\u00ed, perd\u00f3n, la si, la puerta, \u00bfpor qu\u00e9 es", "tokens": [50364, 4021, 808, 13768, 3192, 13, 40640, 631, 3192, 8582, 11, 12611, 1801, 11, 635, 1511, 11, 635, 48597, 11, 3841, 2816, 8057, 785, 51612], "temperature": 0.0, "avg_logprob": -0.6238620192916305, "compression_ratio": 1.0238095238095237, "no_speech_prob": 0.15651138126850128}, {"id": 285, "seek": 221524, "start": 2215.24, "end": 2224.6, "text": " la 0? Porque lo que no est\u00e1 es en la, en la, no aparece nunca, si ustedes miren ac\u00e1 la probabilidad", "tokens": [50364, 635, 1958, 30, 11287, 450, 631, 572, 3192, 785, 465, 635, 11, 465, 635, 11, 572, 37863, 13768, 11, 1511, 17110, 275, 18833, 23496, 635, 31959, 4580, 50832], "temperature": 0.0, "avg_logprob": -0.2942675260397104, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.21262098848819733}, {"id": 286, "seek": 221524, "start": 2224.6, "end": 2229.8399999999997, "text": " de, perd\u00f3n, la cantidad de, la probabilidad de esto es la probabilidad de que empiece con \u00e9l,", "tokens": [50832, 368, 11, 12611, 1801, 11, 635, 33757, 368, 11, 635, 31959, 4580, 368, 7433, 785, 635, 31959, 4580, 368, 631, 4012, 46566, 416, 11810, 11, 51094], "temperature": 0.0, "avg_logprob": -0.2942675260397104, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.21262098848819733}, {"id": 287, "seek": 221524, "start": 2229.8399999999997, "end": 2232.8799999999997, "text": " ya tenemos un problema con el comienzo con \u00e9l, porque creo que no hay ninguna.", "tokens": [51094, 2478, 9914, 517, 12395, 416, 806, 395, 1053, 4765, 416, 11810, 11, 4021, 14336, 631, 572, 4842, 36073, 13, 51246], "temperature": 0.0, "avg_logprob": -0.2942675260397104, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.21262098848819733}, {"id": 288, "seek": 221524, "start": 2238.08, "end": 2244.12, "text": " Ning\u00fan empieza con \u00e9l, y t\u00fa ya tienes un problema y adem\u00e1s en la tampoco est\u00e1, o sea que el", "tokens": [51506, 39417, 9453, 44577, 416, 11810, 11, 288, 15056, 2478, 20716, 517, 12395, 288, 21251, 465, 635, 36838, 3192, 11, 277, 4158, 631, 806, 51808], "temperature": 0.0, "avg_logprob": -0.2942675260397104, "compression_ratio": 1.7809523809523808, "no_speech_prob": 0.21262098848819733}, {"id": 289, "seek": 224412, "start": 2244.12, "end": 2253.24, "text": " conteo me da 0, si el vigrama no aparece en el cuerpo de entrenamiento, siempre mi problema", "tokens": [50364, 34444, 78, 385, 1120, 1958, 11, 1511, 806, 15366, 29762, 572, 37863, 465, 806, 20264, 368, 45069, 16971, 11, 12758, 2752, 12395, 50820], "temperature": 0.0, "avg_logprob": -0.2891382694244385, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0006591092678718269}, {"id": 290, "seek": 224412, "start": 2253.24, "end": 2261.08, "text": " me da 0, y m\u00e1s interesante a\u00fan, si cualquier vigrama de todos los que aparecen en la oraci\u00f3n,", "tokens": [50820, 385, 1120, 1958, 11, 288, 3573, 36396, 31676, 11, 1511, 21004, 15366, 29762, 368, 6321, 1750, 631, 15004, 13037, 465, 635, 420, 3482, 11, 51212], "temperature": 0.0, "avg_logprob": -0.2891382694244385, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0006591092678718269}, {"id": 291, "seek": 224412, "start": 2261.08, "end": 2271.68, "text": " da 0, la probabilidad de la oraci\u00f3n es 0, eso es un gran problema. Resolver el problema de eso", "tokens": [51212, 1120, 1958, 11, 635, 31959, 4580, 368, 635, 420, 3482, 785, 1958, 11, 7287, 785, 517, 9370, 12395, 13, 5015, 401, 331, 806, 12395, 368, 7287, 51742], "temperature": 0.0, "avg_logprob": -0.2891382694244385, "compression_ratio": 1.6136363636363635, "no_speech_prob": 0.0006591092678718269}, {"id": 292, "seek": 227168, "start": 2271.68, "end": 2275.72, "text": " y lo que se llama el suavizado de negra m\u00e1s que vamos a ver c\u00f3mo, tenemos que ir una forma de", "tokens": [50364, 288, 450, 631, 369, 23272, 806, 459, 706, 590, 1573, 368, 2485, 424, 3573, 631, 5295, 257, 1306, 12826, 11, 9914, 631, 3418, 2002, 8366, 368, 50566], "temperature": 0.0, "avg_logprob": -0.39431733025444876, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.059601571410894394}, {"id": 293, "seek": 227168, "start": 2275.72, "end": 2281.56, "text": " resolver eso que nos va a pasar siempre, es decir, como nuestro cuerpo, nunca puede ser tan,", "tokens": [50566, 34480, 7287, 631, 3269, 2773, 257, 25344, 12758, 11, 785, 10235, 11, 2617, 14726, 20264, 11, 13768, 8919, 816, 7603, 11, 50858], "temperature": 0.0, "avg_logprob": -0.39431733025444876, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.059601571410894394}, {"id": 294, "seek": 227168, "start": 2281.56, "end": 2286.44, "text": " aunque solo sean dos palabras, igual puede aparecer mi pareja de palabras que no aparecieron y yo", "tokens": [50858, 21962, 6944, 37670, 4491, 35240, 11, 10953, 8919, 43336, 2752, 7448, 2938, 368, 35240, 631, 572, 15004, 537, 16308, 288, 5290, 51102], "temperature": 0.0, "avg_logprob": -0.39431733025444876, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.059601571410894394}, {"id": 295, "seek": 227168, "start": 2286.44, "end": 2289.2, "text": " no me puedo transcar con eso, \u00bfde acuerdo?", "tokens": [51102, 572, 385, 21612, 1145, 6166, 416, 7287, 11, 3841, 1479, 28113, 30, 51240], "temperature": 0.0, "avg_logprob": -0.39431733025444876, "compression_ratio": 1.5865384615384615, "no_speech_prob": 0.059601571410894394}, {"id": 296, "seek": 228920, "start": 2289.2, "end": 2301.3199999999997, "text": " Bueno, nos queda ese pendiente del cielo que lo vamos a ver despu\u00e9s porque ya te quiero comentar", "tokens": [50364, 16046, 11, 3269, 23314, 10167, 12179, 8413, 1103, 49549, 631, 450, 5295, 257, 1306, 15283, 4021, 2478, 535, 16811, 14541, 289, 50970], "temperature": 0.0, "avg_logprob": -0.5051668978285515, "compression_ratio": 1.49, "no_speech_prob": 0.07525267452001572}, {"id": 297, "seek": 228920, "start": 2301.3199999999997, "end": 2304.3599999999997, "text": " y con una cosa, pero vamos a acordarnos de eso, y t\u00fa y ten\u00e9is un buen problema pendiente.", "tokens": [50970, 288, 416, 2002, 10163, 11, 4768, 5295, 257, 38077, 24979, 368, 7287, 11, 288, 15056, 288, 2064, 15064, 517, 30037, 12395, 12179, 8413, 13, 51122], "temperature": 0.0, "avg_logprob": -0.5051668978285515, "compression_ratio": 1.49, "no_speech_prob": 0.07525267452001572}, {"id": 298, "seek": 228920, "start": 2308.8399999999997, "end": 2315.4399999999996, "text": " Bien, en general ustedes eran, bueno, pero \u00bfcu\u00e1l es el mejor ene? \u00bfNo? \u00bfPor qu\u00e9? \u00bfCu\u00e1l es el tema? Es", "tokens": [51346, 16956, 11, 465, 2674, 17110, 32762, 11, 11974, 11, 4768, 3841, 12032, 11447, 785, 806, 11479, 465, 68, 30, 3841, 4540, 30, 3841, 24907, 8057, 30, 3841, 35222, 11447, 785, 806, 15854, 30, 2313, 51676], "temperature": 0.0, "avg_logprob": -0.5051668978285515, "compression_ratio": 1.49, "no_speech_prob": 0.07525267452001572}, {"id": 299, "seek": 231544, "start": 2316.2400000000002, "end": 2328.32, "text": " cu\u00e1nto, cu\u00e1nto, m\u00e1s largo sea el tirama que yo utilizo, m\u00e1s informaci\u00f3n tengo de contexto,", "tokens": [50404, 44256, 78, 11, 44256, 78, 11, 3573, 31245, 4158, 806, 13807, 2404, 631, 5290, 4976, 19055, 11, 3573, 21660, 13989, 368, 47685, 11, 51008], "temperature": 0.0, "avg_logprob": -0.4552393537579161, "compression_ratio": 1.3892215568862276, "no_speech_prob": 0.027093620970845222}, {"id": 300, "seek": 231544, "start": 2328.32, "end": 2333.36, "text": " es decir, intuitivamente mejor estimar con 5 palabras que con una.", "tokens": [51008, 785, 10235, 11, 16224, 23957, 11479, 8017, 289, 416, 1025, 35240, 631, 416, 2002, 13, 51260], "temperature": 0.0, "avg_logprob": -0.4552393537579161, "compression_ratio": 1.3892215568862276, "no_speech_prob": 0.027093620970845222}, {"id": 301, "seek": 231544, "start": 2337.36, "end": 2340.52, "text": " Vamos a guardar con eso. \u00bfCu\u00e1l es el problema de los 3 m\u00e1s largos?", "tokens": [51460, 10894, 257, 6290, 289, 416, 7287, 13, 3841, 35222, 11447, 785, 806, 12395, 368, 1750, 805, 3573, 11034, 329, 30, 51618], "temperature": 0.0, "avg_logprob": -0.4552393537579161, "compression_ratio": 1.3892215568862276, "no_speech_prob": 0.027093620970845222}, {"id": 302, "seek": 234052, "start": 2340.52, "end": 2346.72, "text": " \u00bfPor qu\u00e9 no puedo usar el 15?", "tokens": [50364, 3841, 24907, 8057, 572, 21612, 14745, 806, 2119, 30, 50674], "temperature": 0.0, "avg_logprob": -0.34403650620404413, "compression_ratio": 1.57, "no_speech_prob": 0.017786866053938866}, {"id": 303, "seek": 234052, "start": 2352.08, "end": 2357.0, "text": " Porque tenemos mi problema, porque llegamos ac\u00e1, que con 15 no tengo corpos f\u00edjendemente grande", "tokens": [50942, 11287, 9914, 2752, 12395, 11, 4021, 11234, 2151, 23496, 11, 631, 416, 2119, 572, 13989, 1181, 30010, 283, 870, 73, 521, 16288, 8883, 51188], "temperature": 0.0, "avg_logprob": -0.34403650620404413, "compression_ratio": 1.57, "no_speech_prob": 0.017786866053938866}, {"id": 304, "seek": 234052, "start": 2357.0, "end": 2366.04, "text": " como para que aparecan esa ocurrencia. Entonces, ese balance entre cantidad de ocurrencia,", "tokens": [51188, 2617, 1690, 631, 15004, 7035, 11342, 26430, 1095, 2755, 13, 15097, 11, 10167, 4772, 3962, 33757, 368, 26430, 1095, 2755, 11, 51640], "temperature": 0.0, "avg_logprob": -0.34403650620404413, "compression_ratio": 1.57, "no_speech_prob": 0.017786866053938866}, {"id": 305, "seek": 234052, "start": 2366.04, "end": 2369.72, "text": " porque yo no tengo una buena estimaci\u00f3n de la cantidad de ocurrencia, no voy a poder estimar", "tokens": [51640, 4021, 5290, 572, 13989, 2002, 25710, 8017, 3482, 368, 635, 33757, 368, 26430, 1095, 2755, 11, 572, 7552, 257, 8152, 8017, 289, 51824], "temperature": 0.0, "avg_logprob": -0.34403650620404413, "compression_ratio": 1.57, "no_speech_prob": 0.017786866053938866}, {"id": 306, "seek": 236972, "start": 2369.72, "end": 2373.56, "text": " bien la probabilidad. Con eso bien que yo estoy atimada la probabilidad de partir en contegos,", "tokens": [50364, 3610, 635, 31959, 4580, 13, 2656, 7287, 3610, 631, 5290, 15796, 412, 332, 1538, 635, 31959, 4580, 368, 13906, 465, 660, 1146, 329, 11, 50556], "temperature": 0.0, "avg_logprob": -0.48866918463456005, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0051283808425068855}, {"id": 307, "seek": 236972, "start": 2373.56, "end": 2379.9599999999996, "text": " si yo tengo una, dos, tres ocurrencias seguramente esa probabilidad artificial, pues si hubo", "tokens": [50556, 1511, 5290, 13989, 2002, 11, 4491, 11, 15890, 26430, 1095, 12046, 22179, 3439, 11342, 31959, 4580, 11677, 11, 11059, 1511, 11838, 78, 50876], "temperature": 0.0, "avg_logprob": -0.48866918463456005, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0051283808425068855}, {"id": 308, "seek": 236972, "start": 2379.9599999999996, "end": 2384.04, "text": " una ocurrencia en un corpo de miles de millones de palabras, no me est\u00e1 diciendo mucho.", "tokens": [50876, 2002, 26430, 1095, 2755, 465, 517, 1181, 2259, 368, 6193, 368, 22416, 368, 35240, 11, 572, 385, 3192, 42797, 9824, 13, 51080], "temperature": 0.0, "avg_logprob": -0.48866918463456005, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0051283808425068855}, {"id": 309, "seek": 236972, "start": 2387.04, "end": 2396.0, "text": " Realmente en igual 3 se obtienen buenos resultados, por lo menos para aproximarse de", "tokens": [51230, 8467, 4082, 465, 10953, 805, 369, 7464, 22461, 49617, 36796, 11, 1515, 450, 8902, 1690, 31270, 11668, 368, 51678], "temperature": 0.0, "avg_logprob": -0.48866918463456005, "compression_ratio": 1.6559633027522935, "no_speech_prob": 0.0051283808425068855}, {"id": 310, "seek": 239600, "start": 2396.0, "end": 2403.48, "text": " la cantidad de cada muy bien, Google hace unos a\u00f1os atr\u00e1s sac\u00f3 un corpo de negra, un s\u00ed,", "tokens": [50364, 635, 33757, 368, 8411, 5323, 3610, 11, 3329, 10032, 17780, 11424, 22906, 4899, 812, 517, 1181, 2259, 368, 2485, 424, 11, 517, 8600, 11, 50738], "temperature": 0.0, "avg_logprob": -0.6101981268988715, "compression_ratio": 1.4747899159663866, "no_speech_prob": 0.0022750026546418667}, {"id": 311, "seek": 239600, "start": 2403.48, "end": 2409.48, "text": " una lista de negra m\u00e1s de hasta 5, no recordo que no est\u00e1 ah\u00ed porque ven\u00edan en 7.", "tokens": [50738, 2002, 27764, 368, 2485, 424, 3573, 368, 10764, 1025, 11, 572, 2136, 78, 631, 572, 3192, 12571, 4021, 6138, 11084, 465, 1614, 13, 51038], "temperature": 0.0, "avg_logprob": -0.6101981268988715, "compression_ratio": 1.4747899159663866, "no_speech_prob": 0.0022750026546418667}, {"id": 312, "seek": 239600, "start": 2413.08, "end": 2416.36, "text": " O sea que determinar\u00e9, \u00bfne va a depender un poco la tarea y ese se me dio a ojos?", "tokens": [51218, 422, 4158, 631, 3618, 6470, 526, 11, 3841, 716, 2773, 257, 1367, 3216, 517, 10639, 635, 256, 35425, 288, 10167, 369, 385, 31965, 257, 39519, 30, 51382], "temperature": 0.0, "avg_logprob": -0.6101981268988715, "compression_ratio": 1.4747899159663866, "no_speech_prob": 0.0022750026546418667}, {"id": 313, "seek": 239600, "start": 2416.36, "end": 2420.56, "text": " Digamos, pues yo me estar\u00eda un poco b\u00f3blica. Ahora vamos a ver un poco de evaluaci\u00f3n,", "tokens": [51382, 10976, 2151, 11, 11059, 5290, 385, 8755, 2686, 517, 10639, 272, 812, 11489, 64, 13, 18840, 5295, 257, 1306, 517, 10639, 368, 6133, 3482, 11, 51592], "temperature": 0.0, "avg_logprob": -0.6101981268988715, "compression_ratio": 1.4747899159663866, "no_speech_prob": 0.0022750026546418667}, {"id": 314, "seek": 242056, "start": 2421.56, "end": 2426.08, "text": " y tal y lo que dec\u00edamos, \u00bfno se agregan? Cuando son 3g m\u00e1s tengo que agregar 2 s\u00edmbolos,", "tokens": [50414, 288, 4023, 288, 450, 631, 979, 16275, 11, 3841, 1771, 369, 623, 3375, 282, 30, 21907, 1872, 805, 70, 3573, 13989, 631, 4554, 2976, 568, 8600, 5612, 329, 11, 50640], "temperature": 0.0, "avg_logprob": -0.4554176955926614, "compression_ratio": 1.2466666666666666, "no_speech_prob": 0.03933651000261307}, {"id": 315, "seek": 242056, "start": 2426.08, "end": 2438.2, "text": " el comienzo de la oraci\u00f3n. Te voy a poner enero abri\u00f3, porque yo necesito 2 de contexto para", "tokens": [50640, 806, 395, 1053, 4765, 368, 635, 420, 3482, 13, 1989, 7552, 257, 19149, 465, 2032, 410, 44802, 11, 4021, 5290, 11909, 3528, 568, 368, 47685, 1690, 51246], "temperature": 0.0, "avg_logprob": -0.4554176955926614, "compression_ratio": 1.2466666666666666, "no_speech_prob": 0.03933651000261307}, {"id": 316, "seek": 243820, "start": 2438.2, "end": 2446.3199999999997, "text": " calcular el triunfo en detalle. \u00bfCu\u00e1ndo? Ah\u00ed no te caas, as\u00ed que no.", "tokens": [50364, 2104, 17792, 806, 1376, 409, 16931, 465, 1141, 11780, 13, 3841, 35222, 18606, 78, 30, 49924, 572, 535, 1335, 296, 11, 8582, 631, 572, 13, 50770], "temperature": 0.0, "avg_logprob": -0.5873173607720269, "compression_ratio": 1.458128078817734, "no_speech_prob": 0.016405748203396797}, {"id": 317, "seek": 243820, "start": 2451.3599999999997, "end": 2453.8799999999997, "text": " Y bueno, y la pregunta es \u00bfc\u00f3mo calculamos?", "tokens": [51022, 398, 11974, 11, 288, 635, 24252, 785, 3841, 46614, 2104, 2444, 2151, 30, 51148], "temperature": 0.0, "avg_logprob": -0.5873173607720269, "compression_ratio": 1.458128078817734, "no_speech_prob": 0.016405748203396797}, {"id": 318, "seek": 243820, "start": 2457.04, "end": 2460.96, "text": " De ver punto de vista metodol\u00f3gico, \u00bfc\u00f3mo hacemos para calcular buenas probabilidades?", "tokens": [51306, 1346, 1306, 14326, 368, 22553, 1131, 378, 27629, 2789, 11, 3841, 46614, 33839, 1690, 2104, 17792, 43852, 31959, 10284, 30, 51502], "temperature": 0.0, "avg_logprob": -0.5873173607720269, "compression_ratio": 1.458128078817734, "no_speech_prob": 0.016405748203396797}, {"id": 319, "seek": 243820, "start": 2460.96, "end": 2466.8399999999997, "text": " Ya vimos c\u00f3mo se hace el conteo. Ahora quiero ver c\u00f3mo organizo el corpo, y me parece", "tokens": [51502, 6080, 49266, 12826, 369, 10032, 806, 34444, 78, 13, 18840, 16811, 1306, 12826, 4645, 78, 806, 1181, 2259, 11, 288, 385, 14120, 51796], "temperature": 0.0, "avg_logprob": -0.5873173607720269, "compression_ratio": 1.458128078817734, "no_speech_prob": 0.016405748203396797}, {"id": 320, "seek": 246684, "start": 2466.84, "end": 2471.2400000000002, "text": " que es interesante ver esto porque nos va a pasar en muchas cosas, en este tema,", "tokens": [50364, 631, 785, 36396, 1306, 7433, 4021, 3269, 2773, 257, 25344, 465, 16072, 12218, 11, 465, 4065, 15854, 11, 50584], "temperature": 0.0, "avg_logprob": -0.3477357161672492, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.007806217297911644}, {"id": 321, "seek": 246684, "start": 2471.2400000000002, "end": 2476.88, "text": " el procedimiento de lengua natural, y que muchas veces induce el mal uso metodol\u00f3gico de estas", "tokens": [50584, 806, 6682, 14007, 368, 35044, 4398, 3303, 11, 288, 631, 16072, 17054, 13716, 384, 806, 2806, 22728, 1131, 378, 27629, 2789, 368, 13897, 50866], "temperature": 0.0, "avg_logprob": -0.3477357161672492, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.007806217297911644}, {"id": 322, "seek": 246684, "start": 2476.88, "end": 2488.6400000000003, "text": " cosas, lleva error. Entonces me parece que va de la pena comentarlo esto. Yo dije que", "tokens": [50866, 12218, 11, 37681, 6713, 13, 15097, 385, 14120, 631, 2773, 368, 635, 29222, 14541, 19457, 7433, 13, 7616, 39414, 631, 51454], "temperature": 0.0, "avg_logprob": -0.3477357161672492, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.007806217297911644}, {"id": 323, "seek": 246684, "start": 2488.6400000000003, "end": 2493.44, "text": " iba a ser conteo para calcular las probabilidades, \u00bfno? Entonces yo por ac\u00e1 tengo un corpus,", "tokens": [51454, 33423, 257, 816, 34444, 78, 1690, 2104, 17792, 2439, 31959, 10284, 11, 3841, 1771, 30, 15097, 5290, 1515, 23496, 13989, 517, 1181, 31624, 11, 51694], "temperature": 0.0, "avg_logprob": -0.3477357161672492, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.007806217297911644}, {"id": 324, "seek": 249344, "start": 2493.44, "end": 2506.08, "text": " un corpus de texto, \u00bfsi? Entonces, sencillamente lo que tengo son muchos textos, \u00bfno?", "tokens": [50364, 517, 1181, 31624, 368, 35503, 11, 3841, 7691, 30, 15097, 11, 46749, 3439, 450, 631, 13989, 1872, 17061, 2487, 329, 11, 3841, 1771, 30, 50996], "temperature": 0.0, "avg_logprob": -0.35265159606933594, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0015372016932815313}, {"id": 325, "seek": 249344, "start": 2506.08, "end": 2510.92, "text": " Obviamente, sencillamente no, tengo muchos textos, esa es la definici\u00f3n de corno.", "tokens": [50996, 4075, 23347, 11, 46749, 3439, 572, 11, 13989, 17061, 2487, 329, 11, 11342, 785, 635, 1561, 15534, 368, 1181, 1771, 13, 51238], "temperature": 0.0, "avg_logprob": -0.35265159606933594, "compression_ratio": 1.4285714285714286, "no_speech_prob": 0.0015372016932815313}, {"id": 326, "seek": 251092, "start": 2510.92, "end": 2526.28, "text": " Y yo voy a crear un modelo de un modelo de un lenguaje, es decir, yo lo que quiero", "tokens": [50364, 398, 5290, 7552, 257, 31984, 517, 27825, 368, 517, 27825, 368, 517, 35044, 84, 11153, 11, 785, 10235, 11, 5290, 450, 631, 16811, 51132], "temperature": 0.0, "avg_logprob": -0.3578263617850639, "compression_ratio": 1.7315436241610738, "no_speech_prob": 0.0463920496404171}, {"id": 327, "seek": 251092, "start": 2526.28, "end": 2532.84, "text": " construir con esto de las probabilidades de las eleaciones es un modelo del idioma pa\u00f1ol.", "tokens": [51132, 38445, 416, 7433, 368, 2439, 31959, 10284, 368, 2439, 1118, 9188, 785, 517, 27825, 1103, 18014, 6440, 2502, 2791, 401, 13, 51460], "temperature": 0.0, "avg_logprob": -0.3578263617850639, "compression_ratio": 1.7315436241610738, "no_speech_prob": 0.0463920496404171}, {"id": 328, "seek": 251092, "start": 2532.84, "end": 2536.0, "text": " Yo tengo un corpus de texto en espa\u00f1ol, y quiero hacer un modelo del idioma pa\u00f1ol.", "tokens": [51460, 7616, 13989, 517, 1181, 31624, 368, 35503, 465, 31177, 11, 288, 16811, 6720, 517, 27825, 1103, 18014, 6440, 2502, 2791, 401, 13, 51618], "temperature": 0.0, "avg_logprob": -0.3578263617850639, "compression_ratio": 1.7315436241610738, "no_speech_prob": 0.0463920496404171}, {"id": 329, "seek": 253600, "start": 2536.0, "end": 2542.2, "text": " Supongo que yo entreno un modelo, entrenar el modelo en este caso que es decir calcular", "tokens": [50364, 9141, 25729, 631, 5290, 45069, 78, 517, 27825, 11, 45069, 289, 806, 27825, 465, 4065, 9666, 631, 785, 10235, 2104, 17792, 50674], "temperature": 0.0, "avg_logprob": -0.3913850268802127, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.15183965861797333}, {"id": 330, "seek": 253600, "start": 2542.2, "end": 2551.0, "text": " todas esas probabilidades. \u00bfC\u00f3mo hago para saber qu\u00e9 tan bueno es? \u00bfS\u00ed? \u00bfC\u00f3mo lo", "tokens": [50674, 10906, 23388, 31959, 10284, 13, 3841, 28342, 38721, 1690, 12489, 8057, 7603, 11974, 785, 30, 3841, 30463, 30, 3841, 28342, 450, 51114], "temperature": 0.0, "avg_logprob": -0.3913850268802127, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.15183965861797333}, {"id": 331, "seek": 253600, "start": 2551.0, "end": 2558.16, "text": " evalu\u00f3? Supongo que yo ahora voy a modular el cual es la medida, pero supongo que yo tengo", "tokens": [51114, 6133, 812, 30, 9141, 25729, 631, 5290, 9923, 7552, 257, 1072, 1040, 806, 10911, 785, 635, 32984, 11, 4768, 9331, 25729, 631, 5290, 13989, 51472], "temperature": 0.0, "avg_logprob": -0.3913850268802127, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.15183965861797333}, {"id": 332, "seek": 255816, "start": 2558.16, "end": 2566.2799999999997, "text": " una medida de performance que me dice bueno, aplicale tu modelo a este texto, s\u00ed, supongamos", "tokens": [50364, 2002, 32984, 368, 3389, 631, 385, 10313, 11974, 11, 18221, 1220, 2604, 27825, 257, 4065, 35503, 11, 8600, 11, 9331, 556, 2151, 50770], "temperature": 0.0, "avg_logprob": -0.28394429524739584, "compression_ratio": 1.6441717791411044, "no_speech_prob": 0.16570903360843658}, {"id": 333, "seek": 255816, "start": 2566.2799999999997, "end": 2569.44, "text": " que la medida es el que le asigne, ahora vamos a ver por qu\u00e9, pero el que le asigne", "tokens": [50770, 631, 635, 32984, 785, 806, 631, 476, 382, 788, 68, 11, 9923, 5295, 257, 1306, 1515, 8057, 11, 4768, 806, 631, 476, 382, 788, 68, 50928], "temperature": 0.0, "avg_logprob": -0.28394429524739584, "compression_ratio": 1.6441717791411044, "no_speech_prob": 0.16570903360843658}, {"id": 334, "seek": 255816, "start": 2569.44, "end": 2577.56, "text": " mayor probabilidad a todo el texto a las oraciones del texto es el mejor, el mejor modelo", "tokens": [50928, 10120, 31959, 4580, 257, 5149, 806, 35503, 257, 2439, 420, 9188, 1103, 35503, 785, 806, 11479, 11, 806, 11479, 27825, 51334], "temperature": 0.0, "avg_logprob": -0.28394429524739584, "compression_ratio": 1.6441717791411044, "no_speech_prob": 0.16570903360843658}, {"id": 335, "seek": 257756, "start": 2577.56, "end": 2586.96, "text": " es que la asigna probabilidad mayor a la oraci\u00f3n en que tengo el texto. Si yo aplico mi", "tokens": [50364, 785, 631, 635, 382, 788, 64, 31959, 4580, 10120, 257, 635, 420, 3482, 465, 631, 13989, 806, 35503, 13, 4909, 5290, 25522, 2789, 2752, 50834], "temperature": 0.0, "avg_logprob": -0.37967896461486816, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.17904052138328552}, {"id": 336, "seek": 257756, "start": 2586.96, "end": 2592.6, "text": " m\u00e9todo, mi modelo, o sea, el lugo, mi modelo, sobre este mismo corpus, \u00bfqu\u00e9 problema tengo?", "tokens": [50834, 20275, 17423, 11, 2752, 27825, 11, 277, 4158, 11, 806, 287, 20746, 11, 2752, 27825, 11, 5473, 4065, 12461, 1181, 31624, 11, 3841, 16412, 12395, 13989, 30, 51116], "temperature": 0.0, "avg_logprob": -0.37967896461486816, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.17904052138328552}, {"id": 337, "seek": 257756, "start": 2594.36, "end": 2600.56, "text": " Que me va a dar barro, porque los calcul\u00e9 ah\u00ed, es decir, yo nunca puedo, nunca, pero nunca", "tokens": [51204, 4493, 385, 2773, 257, 4072, 2159, 340, 11, 4021, 1750, 4322, 526, 12571, 11, 785, 10235, 11, 5290, 13768, 21612, 11, 13768, 11, 4768, 13768, 51514], "temperature": 0.0, "avg_logprob": -0.37967896461486816, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.17904052138328552}, {"id": 338, "seek": 257756, "start": 2600.56, "end": 2606.36, "text": " nunca, he valuado un modelo en el mismo corpus en el que entrenes. Esto aplica siempre, cada vez", "tokens": [51514, 13768, 11, 415, 1323, 84, 1573, 517, 27825, 465, 806, 12461, 1181, 31624, 465, 806, 631, 45069, 279, 13, 20880, 25522, 2262, 12758, 11, 8411, 5715, 51804], "temperature": 0.0, "avg_logprob": -0.37967896461486816, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.17904052138328552}, {"id": 339, "seek": 260636, "start": 2606.36, "end": 2610.96, "text": " que es un dif\u00edcil m\u00e9todotad\u00edstico, pensado autom\u00e1tico, lo m\u00e1s importante es saber en la", "tokens": [50364, 631, 785, 517, 17258, 20275, 378, 310, 345, 19512, 2789, 11, 6099, 1573, 3553, 28234, 11, 450, 3573, 9416, 785, 12489, 465, 635, 50594], "temperature": 0.0, "avg_logprob": -0.3314746063534576, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.002517322078347206}, {"id": 340, "seek": 260636, "start": 2610.96, "end": 2617.44, "text": " pensado autom\u00e1tico, nunca, el lugo es tu modelo en un corpus, en el mismo corpus que entrenaste,", "tokens": [50594, 6099, 1573, 3553, 28234, 11, 13768, 11, 806, 287, 20746, 785, 2604, 27825, 465, 517, 1181, 31624, 11, 465, 806, 12461, 1181, 31624, 631, 45069, 9079, 11, 50918], "temperature": 0.0, "avg_logprob": -0.3314746063534576, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.002517322078347206}, {"id": 341, "seek": 260636, "start": 2617.44, "end": 2623.6, "text": " porque por definici\u00f3n est\u00e1s haciendo trampa, eso lo llama sobre ajustes, sobre ajustas", "tokens": [50918, 4021, 1515, 1561, 15534, 24389, 20509, 504, 26625, 11, 7287, 450, 23272, 5473, 41023, 279, 11, 5473, 41023, 296, 51226], "temperature": 0.0, "avg_logprob": -0.3314746063534576, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.002517322078347206}, {"id": 342, "seek": 260636, "start": 2623.6, "end": 2631.84, "text": " a tu corpus de entrenamiento. Entonces yo lo que voy a hacer es dividir mi corpus en", "tokens": [51226, 257, 2604, 1181, 31624, 368, 45069, 16971, 13, 15097, 5290, 450, 631, 7552, 257, 6720, 785, 4996, 347, 2752, 1181, 31624, 465, 51638], "temperature": 0.0, "avg_logprob": -0.3314746063534576, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.002517322078347206}, {"id": 343, "seek": 263184, "start": 2631.84, "end": 2641.96, "text": " dos, y voy a decir, este es el corpus de entrenamiento, voy a poner en ingl\u00e9s y el corpus", "tokens": [50364, 4491, 11, 288, 7552, 257, 10235, 11, 4065, 785, 806, 1181, 31624, 368, 45069, 16971, 11, 7552, 257, 19149, 465, 49766, 288, 806, 1181, 31624, 50870], "temperature": 0.0, "avg_logprob": -0.2843489470305266, "compression_ratio": 1.3609022556390977, "no_speech_prob": 0.10959447920322418}, {"id": 344, "seek": 263184, "start": 2641.96, "end": 2658.32, "text": " de evaluaci\u00f3n. Entonces lo que yo voy a hacer es entrenar y cu\u00e1nto se par\u00f3 ac\u00e1. Bueno,", "tokens": [50870, 368, 6133, 3482, 13, 15097, 450, 631, 5290, 7552, 257, 6720, 785, 45069, 289, 288, 44256, 78, 369, 971, 812, 23496, 13, 16046, 11, 51688], "temperature": 0.0, "avg_logprob": -0.2843489470305266, "compression_ratio": 1.3609022556390977, "no_speech_prob": 0.10959447920322418}, {"id": 345, "seek": 266184, "start": 2661.92, "end": 2664.6400000000003, "text": " la regla m\u00e1s o menos es 80 20.", "tokens": [50368, 635, 1121, 875, 3573, 277, 8902, 785, 4688, 945, 13, 50504], "temperature": 0.0, "avg_logprob": -0.48048851706764917, "compression_ratio": 1.2244897959183674, "no_speech_prob": 0.018431905657052994}, {"id": 346, "seek": 266184, "start": 2672.04, "end": 2676.8, "text": " Pregunto, \u00bfpor qu\u00e9 me interesar\u00eda que esto fuera lo m\u00e1s grande posible?", "tokens": [50874, 430, 3375, 24052, 11, 3841, 2816, 8057, 385, 20157, 21178, 631, 7433, 24818, 450, 3573, 8883, 26644, 30, 51112], "temperature": 0.0, "avg_logprob": -0.48048851706764917, "compression_ratio": 1.2244897959183674, "no_speech_prob": 0.018431905657052994}, {"id": 347, "seek": 266184, "start": 2683.6000000000004, "end": 2690.84, "text": " Para que tener m\u00e1s informaci\u00f3n, \u00bfy por qu\u00e9 no uso 90 10 o 95 o 97 3?", "tokens": [51452, 11107, 631, 11640, 3573, 21660, 11, 3841, 88, 1515, 8057, 572, 22728, 4289, 1266, 277, 13420, 277, 23399, 805, 30, 51814], "temperature": 0.0, "avg_logprob": -0.48048851706764917, "compression_ratio": 1.2244897959183674, "no_speech_prob": 0.018431905657052994}, {"id": 348, "seek": 269184, "start": 2692.84, "end": 2694.84, "text": " \u00bfC\u00f3mo?", "tokens": [50414, 3841, 28342, 30, 50514], "temperature": 0.0, "avg_logprob": -0.3882822751998901, "compression_ratio": 1.3626943005181347, "no_speech_prob": 0.0025829735677689314}, {"id": 349, "seek": 269184, "start": 2697.84, "end": 2702.52, "text": " Tengo que solucionar ese balance, no entretener una cantidad razonable de datos, porque si yo le", "tokens": [50664, 314, 30362, 631, 1404, 1311, 33639, 10167, 4772, 11, 572, 3962, 1147, 260, 2002, 33757, 367, 6317, 712, 368, 27721, 11, 4021, 1511, 5290, 476, 50898], "temperature": 0.0, "avg_logprob": -0.3882822751998901, "compression_ratio": 1.3626943005181347, "no_speech_prob": 0.0025829735677689314}, {"id": 350, "seek": 269184, "start": 2702.52, "end": 2709.88, "text": " val\u00fa sobre una oraci\u00f3n, la variance es muy grande, es decir, la posibilidad de equivocarme", "tokens": [50898, 1323, 2481, 5473, 2002, 420, 3482, 11, 635, 21977, 785, 5323, 8883, 11, 785, 10235, 11, 635, 1366, 33989, 368, 48726, 905, 35890, 51266], "temperature": 0.0, "avg_logprob": -0.3882822751998901, "compression_ratio": 1.3626943005181347, "no_speech_prob": 0.0025829735677689314}, {"id": 351, "seek": 269184, "start": 2709.88, "end": 2716.8, "text": " es muy grande. Entonces, una regla es m\u00e1s o menos 80 20, \u00bfs\u00ed?", "tokens": [51266, 785, 5323, 8883, 13, 15097, 11, 2002, 1121, 875, 785, 3573, 277, 8902, 4688, 945, 11, 3841, 82, 870, 30, 51612], "temperature": 0.0, "avg_logprob": -0.3882822751998901, "compression_ratio": 1.3626943005181347, "no_speech_prob": 0.0025829735677689314}, {"id": 352, "seek": 272184, "start": 2722.84, "end": 2729.84, "text": " Y bueno, ah\u00ed habla de 90 10, yo tengo la regla de 80 20.", "tokens": [50414, 398, 11974, 11, 12571, 42135, 368, 4289, 1266, 11, 5290, 13989, 635, 1121, 875, 368, 4688, 945, 13, 50764], "temperature": 0.0, "avg_logprob": -0.41459947951296544, "compression_ratio": 1.1451612903225807, "no_speech_prob": 0.003557078307494521}, {"id": 353, "seek": 272184, "start": 2732.84, "end": 2741.56, "text": " Va a solucir un problema adicional ac\u00e1 y es que ahora lo voy a ver es, por ejemplo,", "tokens": [50914, 16822, 257, 1404, 1311, 347, 517, 12395, 614, 33010, 23496, 288, 785, 631, 9923, 450, 7552, 257, 1306, 785, 11, 1515, 13358, 11, 51350], "temperature": 0.0, "avg_logprob": -0.41459947951296544, "compression_ratio": 1.1451612903225807, "no_speech_prob": 0.003557078307494521}, {"id": 354, "seek": 274156, "start": 2741.56, "end": 2755.2799999999997, "text": " si yo quiero saber cu\u00e1ntos elegir el N, \u00bfno? Yo quiero elegir el N, yo necesito lo que", "tokens": [50364, 1511, 5290, 16811, 12489, 44256, 329, 14459, 347, 806, 426, 11, 3841, 1771, 30, 7616, 16811, 14459, 347, 806, 426, 11, 5290, 11909, 3528, 450, 631, 51050], "temperature": 0.0, "avg_logprob": -0.3681326749032004, "compression_ratio": 1.3515625, "no_speech_prob": 0.010294009000062943}, {"id": 355, "seek": 274156, "start": 2755.2799999999997, "end": 2768.36, "text": " va a hacer es prevo con un N ac\u00e1, modelo 1, en igual 2 y a\u00fan modelo 2, en igual 3.", "tokens": [51050, 2773, 257, 6720, 785, 659, 3080, 416, 517, 426, 23496, 11, 27825, 502, 11, 465, 10953, 568, 288, 31676, 27825, 568, 11, 465, 10953, 805, 13, 51704], "temperature": 0.0, "avg_logprob": -0.3681326749032004, "compression_ratio": 1.3515625, "no_speech_prob": 0.010294009000062943}, {"id": 356, "seek": 277156, "start": 2772.56, "end": 2780.72, "text": " Y esto es un poco m\u00e1s \u00fatil, y lo val\u00faa ac\u00e1 y digo M1 y M2, y me qued\u00f3 con el", "tokens": [50414, 398, 7433, 785, 517, 10639, 3573, 49191, 11, 288, 450, 1323, 46362, 23496, 288, 22990, 376, 16, 288, 376, 17, 11, 288, 385, 13617, 812, 416, 806, 50822], "temperature": 0.0, "avg_logprob": -0.3870548286823311, "compression_ratio": 1.4353448275862069, "no_speech_prob": 0.011904771439731121}, {"id": 357, "seek": 277156, "start": 2780.72, "end": 2787.12, "text": " que me da mejor. Y esos m\u00e9todologicamente no est\u00e1n bien, \u00bfpor qu\u00e9?", "tokens": [50822, 631, 385, 1120, 11479, 13, 398, 22411, 20275, 378, 1132, 23653, 572, 10368, 3610, 11, 3841, 2816, 8057, 30, 51142], "temperature": 0.0, "avg_logprob": -0.3870548286823311, "compression_ratio": 1.4353448275862069, "no_speech_prob": 0.011904771439731121}, {"id": 358, "seek": 277156, "start": 2790.7599999999998, "end": 2796.7599999999998, "text": " Y esto es una de las cosas que es m\u00e1s dif\u00edcil entender a veces, es, si yo prevo los dos", "tokens": [51324, 398, 7433, 785, 2002, 368, 2439, 12218, 631, 785, 3573, 17258, 20054, 257, 17054, 11, 785, 11, 1511, 5290, 659, 3080, 1750, 4491, 51624], "temperature": 0.0, "avg_logprob": -0.3870548286823311, "compression_ratio": 1.4353448275862069, "no_speech_prob": 0.011904771439731121}, {"id": 359, "seek": 277156, "start": 2796.7599999999998, "end": 2800.92, "text": " modelos ac\u00e1, de alguna forma tambi\u00e9n estoy haciendo trampa, porque supongan que yo tengo", "tokens": [51624, 2316, 329, 23496, 11, 368, 20651, 8366, 6407, 15796, 20509, 504, 26625, 11, 4021, 9331, 556, 282, 631, 5290, 13989, 51832], "temperature": 0.0, "avg_logprob": -0.3870548286823311, "compression_ratio": 1.4353448275862069, "no_speech_prob": 0.011904771439731121}, {"id": 360, "seek": 280092, "start": 2800.92, "end": 2806.84, "text": " no dos par\u00e1metros, porque ac\u00e1 tengo o un par\u00e1metro que tiene dos valores. Supongamos", "tokens": [50364, 572, 4491, 971, 842, 29570, 11, 4021, 23496, 13989, 277, 517, 971, 842, 45400, 631, 7066, 4491, 38790, 13, 9141, 556, 2151, 50660], "temperature": 0.0, "avg_logprob": -0.2899140006617496, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.00044505629921332}, {"id": 361, "seek": 280092, "start": 2806.84, "end": 2812.6800000000003, "text": " que yo quiero ajustar otro par\u00e1metro de mi m\u00e9todo, que puede tomar 500 valores posible.", "tokens": [50660, 631, 5290, 16811, 41023, 289, 11921, 971, 842, 45400, 368, 2752, 20275, 17423, 11, 631, 8919, 22048, 5923, 38790, 26644, 13, 50952], "temperature": 0.0, "avg_logprob": -0.2899140006617496, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.00044505629921332}, {"id": 362, "seek": 280092, "start": 2812.6800000000003, "end": 2823.76, "text": " Si yo hago 500 en realidad, y 500 pruebas, s\u00ed, muy probablemente tambi\u00e9n estoy ajustando", "tokens": [50952, 4909, 5290, 38721, 5923, 465, 25635, 11, 288, 5923, 32820, 16342, 11, 8600, 11, 5323, 21759, 4082, 6407, 15796, 41023, 1806, 51506], "temperature": 0.0, "avg_logprob": -0.2899140006617496, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.00044505629921332}, {"id": 363, "seek": 280092, "start": 2823.76, "end": 2828.6, "text": " ac\u00e1, estoy ajustando ac\u00e1, porque estoy elegiendo de los 500 y a veces puede ser miles o", "tokens": [51506, 23496, 11, 15796, 41023, 1806, 23496, 11, 4021, 15796, 14459, 7304, 368, 1750, 5923, 288, 257, 17054, 8919, 816, 6193, 277, 51748], "temperature": 0.0, "avg_logprob": -0.2899140006617496, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.00044505629921332}, {"id": 364, "seek": 282860, "start": 2828.6, "end": 2833.68, "text": " 300 de miles, el que mejora anda en este corpo de evaluaci\u00f3n, o sea que estoy", "tokens": [50364, 6641, 368, 6193, 11, 806, 631, 11479, 64, 21851, 465, 4065, 23257, 368, 6133, 3482, 11, 277, 4158, 631, 15796, 50618], "temperature": 0.0, "avg_logprob": -0.3033102007879727, "compression_ratio": 1.5030674846625767, "no_speech_prob": 0.00872071273624897}, {"id": 365, "seek": 282860, "start": 2833.68, "end": 2839.24, "text": " sobre ajustando el corpo de evaluaci\u00f3n. Entonces, para la ajuste de par\u00e1metro yo", "tokens": [50618, 5473, 41023, 1806, 806, 23257, 368, 6133, 3482, 13, 15097, 11, 1690, 635, 41023, 68, 368, 971, 842, 45400, 5290, 50896], "temperature": 0.0, "avg_logprob": -0.3033102007879727, "compression_ratio": 1.5030674846625767, "no_speech_prob": 0.00872071273624897}, {"id": 366, "seek": 282860, "start": 2839.24, "end": 2849.7999999999997, "text": " usualmente lo que tengo que hacer es definir dividir este corpus, sacar un pedacito", "tokens": [50896, 7713, 4082, 450, 631, 13989, 631, 6720, 785, 1561, 347, 4996, 347, 4065, 1181, 31624, 11, 43823, 517, 5670, 326, 3528, 51424], "temperature": 0.0, "avg_logprob": -0.3033102007879727, "compression_ratio": 1.5030674846625767, "no_speech_prob": 0.00872071273624897}, {"id": 367, "seek": 284980, "start": 2849.8, "end": 2862.32, "text": " del corpo en trainamiento, que lo llamo corpus, gel dauto, corpus de desarrollo, y lo que", "tokens": [50364, 1103, 23257, 465, 3847, 16971, 11, 631, 450, 4849, 10502, 1181, 31624, 11, 4087, 274, 1375, 78, 11, 1181, 31624, 368, 38295, 11, 288, 450, 631, 50990], "temperature": 0.0, "avg_logprob": -0.3822811423958122, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.031608130782842636}, {"id": 368, "seek": 284980, "start": 2862.32, "end": 2870.04, "text": " hago es entrenos sobre esta parte y evaluos sobre el gel dauto, y me reservo este de evaluaci\u00f3n,", "tokens": [50990, 38721, 785, 45069, 329, 5473, 5283, 6975, 288, 6133, 329, 5473, 806, 4087, 274, 1375, 78, 11, 288, 385, 16454, 78, 4065, 368, 6133, 3482, 11, 51376], "temperature": 0.0, "avg_logprob": -0.3822811423958122, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.031608130782842636}, {"id": 369, "seek": 284980, "start": 2870.04, "end": 2874.7200000000003, "text": " solamente para cuando tengo mi modelo definitivo, y quiero saber su performance, con su", "tokens": [51376, 27814, 1690, 7767, 13989, 2752, 27825, 28781, 6340, 11, 288, 16811, 12489, 459, 3389, 11, 416, 459, 51610], "temperature": 0.0, "avg_logprob": -0.3822811423958122, "compression_ratio": 1.5895953757225434, "no_speech_prob": 0.031608130782842636}, {"id": 370, "seek": 287472, "start": 2874.72, "end": 2884.9599999999996, "text": " medio de evaluaci\u00f3n. \u00bfAguardo? Esto lo van a tener que presentar en el laboratorio,", "tokens": [50364, 22123, 368, 6133, 3482, 13, 3841, 32, 2794, 12850, 30, 20880, 450, 3161, 257, 11640, 631, 1974, 289, 465, 806, 5938, 48028, 11, 50876], "temperature": 0.0, "avg_logprob": -0.3673884807488857, "compression_ratio": 1.387434554973822, "no_speech_prob": 0.0167534202337265}, {"id": 371, "seek": 287472, "start": 2884.9599999999996, "end": 2892.48, "text": " es decir, c\u00f3mo evaluar\u00edan el m\u00e9todo, un m\u00e9todo. Hay otras posibilidades que no implican", "tokens": [50876, 785, 10235, 11, 12826, 6133, 289, 11084, 806, 20275, 17423, 11, 517, 20275, 17423, 13, 8721, 20244, 1366, 11607, 10284, 631, 572, 10629, 282, 51252], "temperature": 0.0, "avg_logprob": -0.3673884807488857, "compression_ratio": 1.387434554973822, "no_speech_prob": 0.0167534202337265}, {"id": 372, "seek": 287472, "start": 2892.48, "end": 2898.72, "text": " un cuerpo gel dauto, por ejemplo, hacer lo que se llama coros validation, que es separo", "tokens": [51252, 517, 20264, 4087, 274, 1375, 78, 11, 1515, 13358, 11, 6720, 450, 631, 369, 23272, 1181, 329, 24071, 11, 631, 785, 3128, 78, 51564], "temperature": 0.0, "avg_logprob": -0.3673884807488857, "compression_ratio": 1.387434554973822, "no_speech_prob": 0.0167534202337265}, {"id": 373, "seek": 289872, "start": 2898.72, "end": 2907.2799999999997, "text": " este pedacito, entrenos sobre esto y evaluos sobre este, despu\u00e9s separo otra franjita y entrenos", "tokens": [50364, 4065, 5670, 326, 3528, 11, 45069, 329, 5473, 7433, 288, 6133, 329, 5473, 4065, 11, 15283, 3128, 78, 13623, 431, 282, 73, 2786, 288, 45069, 329, 50792], "temperature": 0.0, "avg_logprob": -0.27414988349465763, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.12458572536706924}, {"id": 374, "seek": 289872, "start": 2907.2799999999997, "end": 2914.12, "text": " sobre el resto y evaluos sobre la franjita, y as\u00ed con cas franjas y saco el promedio. Eso", "tokens": [50792, 5473, 806, 28247, 288, 6133, 329, 5473, 635, 431, 282, 73, 2786, 11, 288, 8582, 416, 3058, 431, 282, 19221, 288, 4899, 78, 806, 2234, 292, 1004, 13, 27795, 51134], "temperature": 0.0, "avg_logprob": -0.27414988349465763, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.12458572536706924}, {"id": 375, "seek": 289872, "start": 2914.12, "end": 2918.3199999999997, "text": " me sirve para no desperdiciar, digamos, esta parte del corpus, para poder utilizar todo", "tokens": [51134, 385, 4735, 303, 1690, 572, 10679, 67, 8787, 289, 11, 36430, 11, 5283, 6975, 1103, 1181, 31624, 11, 1690, 8152, 24060, 5149, 51344], "temperature": 0.0, "avg_logprob": -0.27414988349465763, "compression_ratio": 1.6235294117647059, "no_speech_prob": 0.12458572536706924}, {"id": 376, "seek": 291832, "start": 2918.32, "end": 2929.7200000000003, "text": " el corpus entrenadito. Esa m\u00e1s, cros validation. Vamos a volver a hablar un poquito", "tokens": [50364, 806, 1181, 31624, 45069, 345, 3528, 13, 2313, 64, 3573, 11, 941, 329, 24071, 13, 10894, 257, 33998, 257, 21014, 517, 28229, 50934], "temperature": 0.0, "avg_logprob": -0.4254019328526088, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.05496605485677719}, {"id": 377, "seek": 291832, "start": 2929.7200000000003, "end": 2934.2000000000003, "text": " cros validation cuando le hemos clasificaci\u00f3n, pero lo que me interesa es que le quede claro", "tokens": [50934, 941, 329, 24071, 7767, 476, 15396, 596, 296, 40802, 11, 4768, 450, 631, 385, 728, 13708, 785, 631, 476, 421, 4858, 16742, 51158], "temperature": 0.0, "avg_logprob": -0.4254019328526088, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.05496605485677719}, {"id": 378, "seek": 291832, "start": 2934.2000000000003, "end": 2943.88, "text": " la diferencia entre estos corpus, y cuando tengo el modelo final, uso esto solamente para", "tokens": [51158, 635, 38844, 3962, 12585, 1181, 31624, 11, 288, 7767, 13989, 806, 27825, 2572, 11, 22728, 7433, 27814, 1690, 51642], "temperature": 0.0, "avg_logprob": -0.4254019328526088, "compression_ratio": 1.5056179775280898, "no_speech_prob": 0.05496605485677719}, {"id": 379, "seek": 294388, "start": 2943.88, "end": 2950.44, "text": " evaluar la performance, es una medida que determinar\u00e9 seg\u00fan mi tarea. \u00bfC\u00f3mo evaluamos", "tokens": [50364, 6133, 289, 635, 3389, 11, 785, 2002, 32984, 631, 3618, 6470, 526, 36570, 2752, 256, 35425, 13, 3841, 28342, 6133, 2151, 50692], "temperature": 0.0, "avg_logprob": -0.23188566192378843, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04130474478006363}, {"id": 380, "seek": 294388, "start": 2950.44, "end": 2955.76, "text": " un modelo bueno? La manera correcta de evaluar un modelo deber\u00eda ser\u00eda emp\u00edricamente,", "tokens": [50692, 517, 27825, 11974, 30, 2369, 13913, 3006, 64, 368, 6133, 289, 517, 27825, 29671, 2686, 816, 2686, 4012, 870, 1341, 3439, 11, 50958], "temperature": 0.0, "avg_logprob": -0.23188566192378843, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04130474478006363}, {"id": 381, "seek": 294388, "start": 2955.76, "end": 2960.2000000000003, "text": " es decir, yo quiero evaluar un modelo del lenguaje y lo estoy usando para el reconocimiento", "tokens": [50958, 785, 10235, 11, 5290, 16811, 6133, 289, 517, 27825, 1103, 35044, 84, 11153, 288, 450, 15796, 29798, 1690, 806, 43838, 14007, 51180], "temperature": 0.0, "avg_logprob": -0.23188566192378843, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04130474478006363}, {"id": 382, "seek": 294388, "start": 2960.2000000000003, "end": 2966.2400000000002, "text": " de la habla, deber\u00eda ser una evaluaci\u00f3n de que tambi\u00e9n reconozco el habla, o que tambi\u00e9n", "tokens": [51180, 368, 635, 42135, 11, 29671, 2686, 816, 2002, 6133, 3482, 368, 631, 6407, 850, 8957, 89, 1291, 806, 42135, 11, 277, 631, 6407, 51482], "temperature": 0.0, "avg_logprob": -0.23188566192378843, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04130474478006363}, {"id": 383, "seek": 294388, "start": 2966.2400000000002, "end": 2970.52, "text": " reconozco la escritura, pero eso puede ser muy costoso a veces. Yo puedo estar haciendo un", "tokens": [51482, 850, 8957, 89, 1291, 635, 4721, 3210, 2991, 11, 4768, 7287, 8919, 816, 5323, 2063, 9869, 257, 17054, 13, 7616, 21612, 8755, 20509, 517, 51696], "temperature": 0.0, "avg_logprob": -0.23188566192378843, "compression_ratio": 1.7196969696969697, "no_speech_prob": 0.04130474478006363}, {"id": 384, "seek": 297052, "start": 2970.52, "end": 2974.8, "text": " modelo lenguaje, no s\u00e9 para qu\u00e9 se va a usar. Entonces, me interesa mucho, me puede", "tokens": [50364, 27825, 35044, 84, 11153, 11, 572, 7910, 1690, 8057, 369, 2773, 257, 14745, 13, 15097, 11, 385, 728, 13708, 9824, 11, 385, 8919, 50578], "temperature": 0.0, "avg_logprob": -0.3464233295337574, "compression_ratio": 1.5654761904761905, "no_speech_prob": 0.07029682397842407}, {"id": 385, "seek": 297052, "start": 2974.8, "end": 2986.56, "text": " interesar tener una medida intr\u00ednseca de la performance de mi modelo. Entonces, vamos", "tokens": [50578, 728, 18876, 11640, 2002, 32984, 17467, 10973, 405, 496, 368, 635, 3389, 368, 2752, 27825, 13, 15097, 11, 5295, 51166], "temperature": 0.0, "avg_logprob": -0.3464233295337574, "compression_ratio": 1.5654761904761905, "no_speech_prob": 0.07029682397842407}, {"id": 386, "seek": 297052, "start": 2986.56, "end": 2993.04, "text": " a ver una forma de evaluar. A mi esta parte, de esta parte, en el libro est\u00e1 apuesta como", "tokens": [51166, 257, 1306, 2002, 8366, 368, 6133, 289, 13, 316, 2752, 5283, 6975, 11, 368, 5283, 6975, 11, 465, 806, 29354, 3192, 1882, 25316, 2617, 51490], "temperature": 0.0, "avg_logprob": -0.3464233295337574, "compression_ratio": 1.5654761904761905, "no_speech_prob": 0.07029682397842407}, {"id": 387, "seek": 299304, "start": 2993.04, "end": 3004.2799999999997, "text": " un tema avanzado, pero a m\u00ed me parece interesante mostrarlo, porque la entrop\u00eda es un concepto", "tokens": [50364, 517, 15854, 42444, 1573, 11, 4768, 257, 14692, 385, 14120, 36396, 21487, 752, 11, 4021, 635, 948, 1513, 2686, 785, 517, 3410, 78, 50926], "temperature": 0.0, "avg_logprob": -0.4010076328199737, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.4732590317726135}, {"id": 388, "seek": 299304, "start": 3004.2799999999997, "end": 3007.36, "text": " que aparece muchas veces en el procedimiento de lenguaje natural de otras cosas, y me", "tokens": [50926, 631, 37863, 16072, 17054, 465, 806, 6682, 14007, 368, 35044, 84, 11153, 3303, 368, 20244, 12218, 11, 288, 385, 51080], "temperature": 0.0, "avg_logprob": -0.4010076328199737, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.4732590317726135}, {"id": 389, "seek": 299304, "start": 3007.36, "end": 3012.96, "text": " pese que le va a ir la pena por lo menos aproximarse. Supongo que yo tengo una variabilidad", "tokens": [51080, 280, 1130, 631, 476, 2773, 257, 3418, 635, 29222, 1515, 450, 8902, 31270, 11668, 13, 9141, 25729, 631, 5290, 13989, 2002, 3034, 5177, 4580, 51360], "temperature": 0.0, "avg_logprob": -0.4010076328199737, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.4732590317726135}, {"id": 390, "seek": 299304, "start": 3012.96, "end": 3018.32, "text": " aleatoria y todo esto voy a llegar a una forma de evaluar un modelo, no hay que empezar", "tokens": [51360, 6775, 1639, 654, 288, 5149, 7433, 7552, 257, 24892, 257, 2002, 8366, 368, 6133, 289, 517, 27825, 11, 572, 4842, 631, 31168, 51628], "temperature": 0.0, "avg_logprob": -0.4010076328199737, "compression_ratio": 1.5603448275862069, "no_speech_prob": 0.4732590317726135}, {"id": 391, "seek": 301832, "start": 3018.32, "end": 3025.4, "text": " a hablar de todo esto. Supongo que s\u00ed que yo tengo una variabilidad aleatoria que tiene", "tokens": [50364, 257, 21014, 368, 5149, 7433, 13, 9141, 25729, 631, 8600, 631, 5290, 13989, 2002, 3034, 5177, 4580, 6775, 1639, 654, 631, 7066, 50718], "temperature": 0.0, "avg_logprob": -0.26462052997789887, "compression_ratio": 1.6625, "no_speech_prob": 0.33319929242134094}, {"id": 392, "seek": 301832, "start": 3025.4, "end": 3034.0800000000004, "text": " varios eventos posibles, en otro caso dijimos que eran las palabras posibles. La entrop\u00eda,", "tokens": [50718, 33665, 2280, 329, 1366, 14428, 11, 465, 11921, 9666, 47709, 8372, 631, 32762, 2439, 35240, 1366, 14428, 13, 2369, 948, 1513, 2686, 11, 51152], "temperature": 0.0, "avg_logprob": -0.26462052997789887, "compression_ratio": 1.6625, "no_speech_prob": 0.33319929242134094}, {"id": 393, "seek": 301832, "start": 3034.0800000000004, "end": 3039.6400000000003, "text": " la entrop\u00eda es una variabilidad aleatoria que es un concepto que viene de la teor\u00eda", "tokens": [51152, 635, 948, 1513, 2686, 785, 2002, 3034, 5177, 4580, 6775, 1639, 654, 631, 785, 517, 3410, 78, 631, 19561, 368, 635, 40238, 2686, 51430], "temperature": 0.0, "avg_logprob": -0.26462052997789887, "compression_ratio": 1.6625, "no_speech_prob": 0.33319929242134094}, {"id": 394, "seek": 303964, "start": 3039.64, "end": 3054.6, "text": " de informaci\u00f3n, de CloudXanon, la teleinformaci\u00f3n lo que hablaba era, bueno, algunos capacicieron,", "tokens": [50364, 368, 21660, 11, 368, 8061, 55, 282, 266, 11, 635, 4304, 37811, 3482, 450, 631, 26280, 5509, 4249, 11, 11974, 11, 21078, 1410, 326, 299, 14440, 11, 51112], "temperature": 0.0, "avg_logprob": -0.39055265699114117, "compression_ratio": 1.6179775280898876, "no_speech_prob": 0.5270612835884094}, {"id": 395, "seek": 303964, "start": 3054.6, "end": 3060.04, "text": " lo vieron a alg\u00fan curso, pero la teleinformaci\u00f3n lo que trataba era de medir cu\u00e1nto me cuesta", "tokens": [51112, 450, 371, 14440, 257, 26300, 31085, 11, 4768, 635, 4304, 37811, 3482, 450, 631, 21507, 5509, 4249, 368, 1205, 347, 44256, 78, 385, 2702, 7841, 51384], "temperature": 0.0, "avg_logprob": -0.39055265699114117, "compression_ratio": 1.6179775280898876, "no_speech_prob": 0.5270612835884094}, {"id": 396, "seek": 303964, "start": 3060.04, "end": 3063.4, "text": " a m\u00ed transmitir un mensaje. \u00bfC\u00f3mo puedo transmitir un mensaje de forma \u00f3ptima? Digamos", "tokens": [51384, 257, 14692, 17831, 347, 517, 10923, 11153, 13, 3841, 28342, 21612, 17831, 347, 517, 10923, 11153, 368, 8366, 11857, 662, 4775, 30, 10976, 2151, 51552], "temperature": 0.0, "avg_logprob": -0.39055265699114117, "compression_ratio": 1.6179775280898876, "no_speech_prob": 0.5270612835884094}, {"id": 397, "seek": 306340, "start": 3063.4, "end": 3072.52, "text": " un poco la idea, o que hay atr\u00e1s de una comunicaci\u00f3n. La noci\u00f3n de entrop\u00eda, estas", "tokens": [50364, 517, 10639, 635, 1558, 11, 277, 631, 4842, 22906, 368, 2002, 31710, 3482, 13, 2369, 572, 5687, 368, 948, 1513, 2686, 11, 13897, 50820], "temperature": 0.0, "avg_logprob": -0.2970136742842825, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.3915194869041443}, {"id": 398, "seek": 306340, "start": 3072.52, "end": 3078.56, "text": " funciones, tengo el evento que quiero hacer, la probabilidad del evento, por el hogarismo", "tokens": [50820, 1019, 23469, 11, 13989, 806, 40655, 631, 16811, 6720, 11, 635, 31959, 4580, 1103, 40655, 11, 1515, 806, 24855, 289, 6882, 51122], "temperature": 0.0, "avg_logprob": -0.2970136742842825, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.3915194869041443}, {"id": 399, "seek": 306340, "start": 3078.56, "end": 3084.4, "text": " de esa probabilidad. La entrop\u00eda tiene como caracter\u00edstica fundamental que es una medida", "tokens": [51122, 368, 11342, 31959, 4580, 13, 2369, 948, 1513, 2686, 7066, 2617, 34297, 2262, 8088, 631, 785, 2002, 32984, 51414], "temperature": 0.0, "avg_logprob": -0.2970136742842825, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.3915194869041443}, {"id": 400, "seek": 306340, "start": 3084.4, "end": 3092.4, "text": " que si hay un evento que tiene toda la masa de probabilidad, la entrop\u00eda es m\u00ednima, es", "tokens": [51414, 631, 1511, 4842, 517, 40655, 631, 7066, 11687, 635, 29216, 368, 31959, 4580, 11, 635, 948, 1513, 2686, 785, 33656, 4775, 11, 785, 51814], "temperature": 0.0, "avg_logprob": -0.2970136742842825, "compression_ratio": 1.7115384615384615, "no_speech_prob": 0.3915194869041443}, {"id": 401, "seek": 309240, "start": 3092.4, "end": 3097.92, "text": " decir, si yo tengo un dado que est\u00e1 tan carregado y una forma en algo que valentemente", "tokens": [50364, 10235, 11, 1511, 5290, 13989, 517, 29568, 631, 3192, 7603, 1032, 3375, 1573, 288, 2002, 8366, 465, 8655, 631, 1323, 1576, 4082, 50640], "temperature": 0.0, "avg_logprob": -0.3095691608932783, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.1936502903699875}, {"id": 402, "seek": 309240, "start": 3097.92, "end": 3103.08, "text": " se puede decir que la entrop\u00eda a m\u00ed es mirado de incertidumbre sobre un evento. Si yo", "tokens": [50640, 369, 8919, 10235, 631, 635, 948, 1513, 2686, 257, 14692, 785, 3149, 1573, 368, 834, 911, 327, 449, 2672, 5473, 517, 40655, 13, 4909, 5290, 50898], "temperature": 0.0, "avg_logprob": -0.3095691608932783, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.1936502903699875}, {"id": 403, "seek": 309240, "start": 3103.08, "end": 3107.32, "text": " tengo un dado que est\u00e1 tan carregado, que cabe que lo tiro, s\u00e9 que siempre vas a salir", "tokens": [50898, 13989, 517, 29568, 631, 3192, 7603, 1032, 3375, 1573, 11, 631, 18893, 631, 450, 44188, 11, 7910, 631, 12758, 11481, 257, 31514, 51110], "temperature": 0.0, "avg_logprob": -0.3095691608932783, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.1936502903699875}, {"id": 404, "seek": 309240, "start": 3107.32, "end": 3118.48, "text": " seis, no tengo incertidumbre, mi entrop\u00eda es cero. En cambio, si el dado est\u00e1 perfectamente", "tokens": [51110, 28233, 11, 572, 13989, 834, 911, 327, 449, 2672, 11, 2752, 948, 1513, 2686, 785, 269, 2032, 13, 2193, 28731, 11, 1511, 806, 29568, 3192, 2176, 3439, 51668], "temperature": 0.0, "avg_logprob": -0.3095691608932783, "compression_ratio": 1.7635467980295567, "no_speech_prob": 0.1936502903699875}, {"id": 405, "seek": 311848, "start": 3118.48, "end": 3129.48, "text": " calibrado, equilibrado, mi entrop\u00eda es m\u00e1xima. Es decir, \u00bfc\u00f3mo est\u00e1 definida la entrop\u00eda?", "tokens": [50364, 2104, 6414, 1573, 11, 1267, 388, 6414, 1573, 11, 2752, 948, 1513, 2686, 785, 31031, 64, 13, 2313, 10235, 11, 3841, 46614, 3192, 1561, 2887, 635, 948, 1513, 2686, 30, 50914], "temperature": 0.0, "avg_logprob": -0.3339497636004192, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.030418746173381805}, {"id": 406, "seek": 311848, "start": 3129.48, "end": 3139.8, "text": " No puedo tener etrop\u00eda m\u00e1s alta que cuando los eventos est\u00e1n equipos lo hablan. Entonces", "tokens": [50914, 883, 21612, 11640, 1030, 1513, 2686, 3573, 26495, 631, 7767, 1750, 2280, 329, 10368, 5037, 329, 450, 3025, 8658, 13, 15097, 51430], "temperature": 0.0, "avg_logprob": -0.3339497636004192, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.030418746173381805}, {"id": 407, "seek": 311848, "start": 3139.8, "end": 3144.76, "text": " justamente la entrop\u00eda es generalmente lo que uno mide con la entrop\u00eda de eso, \u00bfqu\u00e9", "tokens": [51430, 41056, 635, 948, 1513, 2686, 785, 2674, 4082, 450, 631, 8526, 275, 482, 416, 635, 948, 1513, 2686, 368, 7287, 11, 3841, 16412, 51678], "temperature": 0.0, "avg_logprob": -0.3339497636004192, "compression_ratio": 1.5536723163841808, "no_speech_prob": 0.030418746173381805}, {"id": 408, "seek": 314476, "start": 3144.76, "end": 3150.6800000000003, "text": " est\u00e1n parecidos? Son los resultados que est\u00e1n balanceados, est\u00e1n de alguna forma. Cuanto", "tokens": [50364, 10368, 7448, 46579, 30, 5185, 1750, 36796, 631, 10368, 4772, 4181, 11, 10368, 368, 20651, 8366, 13, 13205, 5857, 50660], "temperature": 0.0, "avg_logprob": -0.20137310028076172, "compression_ratio": 1.4972067039106145, "no_speech_prob": 0.08990149199962616}, {"id": 409, "seek": 314476, "start": 3150.6800000000003, "end": 3153.84, "text": " m\u00e1s incertidumbre tengo, porque est\u00e1n m\u00e1s balanceados. Si yo no tengo ni la menor", "tokens": [50660, 3573, 834, 911, 327, 449, 2672, 13989, 11, 4021, 10368, 3573, 4772, 4181, 13, 4909, 5290, 572, 13989, 3867, 635, 26343, 50818], "temperature": 0.0, "avg_logprob": -0.20137310028076172, "compression_ratio": 1.4972067039106145, "no_speech_prob": 0.08990149199962616}, {"id": 410, "seek": 314476, "start": 3153.84, "end": 3168.36, "text": " idea de la palabra que sigue, mi entrop\u00eda es m\u00e1xima. Y adem\u00e1s tiene otra caracter\u00edstica", "tokens": [50818, 1558, 368, 635, 31702, 631, 34532, 11, 2752, 948, 1513, 2686, 785, 31031, 64, 13, 398, 21251, 7066, 13623, 34297, 2262, 51544], "temperature": 0.0, "avg_logprob": -0.20137310028076172, "compression_ratio": 1.4972067039106145, "no_speech_prob": 0.08990149199962616}, {"id": 411, "seek": 316836, "start": 3168.36, "end": 3177.0, "text": " que es que si lo har\u00edamos es en base dos. Este n\u00famero, la entrop\u00eda me mide la cantidad", "tokens": [50364, 631, 785, 631, 1511, 450, 2233, 16275, 785, 465, 3096, 4491, 13, 16105, 14959, 11, 635, 948, 1513, 2686, 385, 275, 482, 635, 33757, 50796], "temperature": 0.0, "avg_logprob": -0.31574981689453124, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.29216164350509644}, {"id": 412, "seek": 316836, "start": 3177.0, "end": 3188.2400000000002, "text": " de bits que yo necesito m\u00ednimos para transmitir los eventos. Esto es lo mejor forma de", "tokens": [50796, 368, 9239, 631, 5290, 11909, 3528, 33656, 8372, 1690, 17831, 347, 1750, 2280, 329, 13, 20880, 785, 450, 11479, 8366, 368, 51358], "temperature": 0.0, "avg_logprob": -0.31574981689453124, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.29216164350509644}, {"id": 413, "seek": 316836, "start": 3188.2400000000002, "end": 3194.28, "text": " hacerlo con un ejemplo. Supongamos, y es el ejemplo que aparece en el libro. Supongamos", "tokens": [51358, 32039, 416, 517, 13358, 13, 9141, 556, 2151, 11, 288, 785, 806, 13358, 631, 37863, 465, 806, 29354, 13, 9141, 556, 2151, 51660], "temperature": 0.0, "avg_logprob": -0.31574981689453124, "compression_ratio": 1.497175141242938, "no_speech_prob": 0.29216164350509644}, {"id": 414, "seek": 319428, "start": 3194.28, "end": 3201.0, "text": " que yo tengo ocho caballos. Tengo ocho caballos que quiero transmitirlas las apuestas", "tokens": [50364, 631, 5290, 13989, 3795, 78, 5487, 336, 329, 13, 314, 30362, 3795, 78, 5487, 336, 329, 631, 16811, 17831, 1648, 296, 2439, 1882, 47794, 50700], "temperature": 0.0, "avg_logprob": -0.2769784927368164, "compression_ratio": 1.3858267716535433, "no_speech_prob": 0.38365909457206726}, {"id": 415, "seek": 319428, "start": 3201.0, "end": 3205.1600000000003, "text": " que se est\u00e1n haciendo por un cable. Entonces digo, bueno, una forma cantada de transmitir", "tokens": [50700, 631, 369, 10368, 20509, 1515, 517, 8220, 13, 15097, 22990, 11, 11974, 11, 2002, 8366, 11223, 1538, 368, 17831, 347, 50908], "temperature": 0.0, "avg_logprob": -0.2769784927368164, "compression_ratio": 1.3858267716535433, "no_speech_prob": 0.38365909457206726}, {"id": 416, "seek": 320516, "start": 3205.16, "end": 3223.56, "text": " lo directa de transmitir, llamar al primer caballo 0-1, 0-10, 0-11, 101, 110, 111.", "tokens": [50364, 450, 2047, 64, 368, 17831, 347, 11, 16848, 289, 419, 12595, 5487, 37104, 1958, 12, 16, 11, 1958, 12, 3279, 11, 1958, 12, 5348, 11, 21055, 11, 20154, 11, 2975, 16, 13, 51284], "temperature": 0.0, "avg_logprob": -0.3901730199013987, "compression_ratio": 1.2846153846153847, "no_speech_prob": 0.5379200577735901}, {"id": 417, "seek": 320516, "start": 3223.56, "end": 3234.3199999999997, "text": " De acuerdo, ac\u00e1 yo uso ocho bits. Cada vez que se apuesta por el caballo 1, yo poco", "tokens": [51284, 1346, 28113, 11, 23496, 5290, 22728, 3795, 78, 9239, 13, 38603, 5715, 631, 369, 1882, 25316, 1515, 806, 5487, 37104, 502, 11, 5290, 10639, 51822], "temperature": 0.0, "avg_logprob": -0.3901730199013987, "compression_ratio": 1.2846153846153847, "no_speech_prob": 0.5379200577735901}, {"id": 418, "seek": 323432, "start": 3234.32, "end": 3240.8, "text": " 0-0, 0-1, blabla. Entonces en total yo utilizo tres bits para transmitirlas por un cable,", "tokens": [50364, 1958, 12, 15, 11, 1958, 12, 16, 11, 888, 455, 875, 13, 15097, 465, 3217, 5290, 4976, 19055, 15890, 9239, 1690, 17831, 1648, 296, 1515, 517, 8220, 11, 50688], "temperature": 0.0, "avg_logprob": -0.2507886338507992, "compression_ratio": 1.5, "no_speech_prob": 0.054928332567214966}, {"id": 419, "seek": 323432, "start": 3240.8, "end": 3247.44, "text": " tres bits por cada apuesta, \u00bfno? Ahora, cuando nosotros vemos las apuestas, descubrimos", "tokens": [50688, 15890, 9239, 1515, 8411, 1882, 25316, 11, 3841, 1771, 30, 18840, 11, 7767, 13863, 20909, 2439, 1882, 47794, 11, 32592, 5565, 329, 51020], "temperature": 0.0, "avg_logprob": -0.2507886338507992, "compression_ratio": 1.5, "no_speech_prob": 0.054928332567214966}, {"id": 420, "seek": 323432, "start": 3247.44, "end": 3257.92, "text": " que la mitad de las veces se apuesta por el caballo 1. Un cuarto del caballo 2, un tercio blabla,", "tokens": [51020, 631, 635, 46895, 368, 2439, 17054, 369, 1882, 25316, 1515, 806, 5487, 37104, 502, 13, 1156, 48368, 1103, 5487, 37104, 568, 11, 517, 1796, 8529, 888, 455, 875, 11, 51544], "temperature": 0.0, "avg_logprob": -0.2507886338507992, "compression_ratio": 1.5, "no_speech_prob": 0.054928332567214966}, {"id": 421, "seek": 325792, "start": 3257.92, "end": 3263.16, "text": " un octavo del caballo 3, un disiseo del caballo 4, y todos estos se apuesta mucho menos.", "tokens": [50364, 517, 13350, 25713, 1103, 5487, 37104, 805, 11, 517, 717, 908, 78, 1103, 5487, 37104, 1017, 11, 288, 6321, 12585, 369, 1882, 25316, 9824, 8902, 13, 50626], "temperature": 0.0, "avg_logprob": -0.24801082422237586, "compression_ratio": 1.6367924528301887, "no_speech_prob": 0.06445622444152832}, {"id": 422, "seek": 325792, "start": 3263.16, "end": 3268.64, "text": " Teniendo en cuenta eso, yo lo que trato de hacer ahora es decir, bueno, quiero proponer", "tokens": [50626, 9380, 7304, 465, 17868, 7287, 11, 5290, 450, 631, 504, 2513, 368, 6720, 9923, 785, 10235, 11, 11974, 11, 16811, 2365, 32949, 50900], "temperature": 0.0, "avg_logprob": -0.24801082422237586, "compression_ratio": 1.6367924528301887, "no_speech_prob": 0.06445622444152832}, {"id": 423, "seek": 325792, "start": 3268.64, "end": 3277.48, "text": " una codificaci\u00f3n mejor que hace que yo, los caballos que se apuesta m\u00e1s, o sea que", "tokens": [50900, 2002, 17656, 40802, 11479, 631, 10032, 631, 5290, 11, 1750, 5487, 336, 329, 631, 369, 1882, 25316, 3573, 11, 277, 4158, 631, 51342], "temperature": 0.0, "avg_logprob": -0.24801082422237586, "compression_ratio": 1.6367924528301887, "no_speech_prob": 0.06445622444152832}, {"id": 424, "seek": 325792, "start": 3277.48, "end": 3284.88, "text": " tengo que transmitir m\u00e1s seguido, los codificos con menos bits. De acuerdo, la mitad", "tokens": [51342, 13989, 631, 17831, 347, 3573, 8878, 2925, 11, 1750, 17656, 1089, 329, 416, 8902, 9239, 13, 1346, 28113, 11, 635, 46895, 51712], "temperature": 0.0, "avg_logprob": -0.24801082422237586, "compression_ratio": 1.6367924528301887, "no_speech_prob": 0.06445622444152832}, {"id": 425, "seek": 328488, "start": 3284.88, "end": 3290.44, "text": " de los bits, el primer bit, lo utilizo solo para el caballo 1, es decir, que si es un", "tokens": [50364, 368, 1750, 9239, 11, 806, 12595, 857, 11, 450, 4976, 19055, 6944, 1690, 806, 5487, 37104, 502, 11, 785, 10235, 11, 631, 1511, 785, 517, 50642], "temperature": 0.0, "avg_logprob": -0.21066567270379316, "compression_ratio": 1.7828947368421053, "no_speech_prob": 0.03432128205895424}, {"id": 426, "seek": 328488, "start": 3290.44, "end": 3304.2400000000002, "text": " 0 es que transmitir el caballo 1, necesita un solo bit. Si es un 1, si es un 1 y un 0 despu\u00e9s", "tokens": [50642, 1958, 785, 631, 17831, 347, 806, 5487, 37104, 502, 11, 45485, 517, 6944, 857, 13, 4909, 785, 517, 502, 11, 1511, 785, 517, 502, 288, 517, 1958, 15283, 51332], "temperature": 0.0, "avg_logprob": -0.21066567270379316, "compression_ratio": 1.7828947368421053, "no_speech_prob": 0.03432128205895424}, {"id": 427, "seek": 328488, "start": 3304.2400000000002, "end": 3311.92, "text": " es el caballo 2. Si son 2, 1 y un 0 despu\u00e9s es el caballo 3. Si son 3, 1 y un 0, f\u00edjense", "tokens": [51332, 785, 806, 5487, 37104, 568, 13, 4909, 1872, 568, 11, 502, 288, 517, 1958, 15283, 785, 806, 5487, 37104, 805, 13, 4909, 1872, 805, 11, 502, 288, 517, 1958, 11, 283, 870, 73, 1288, 51716], "temperature": 0.0, "avg_logprob": -0.21066567270379316, "compression_ratio": 1.7828947368421053, "no_speech_prob": 0.03432128205895424}, {"id": 428, "seek": 331192, "start": 3311.92, "end": 3321.64, "text": " que yo para transmitir esto caballo utilizo 1, 2, 3, 4, 5, 6 bits. Utilizo m\u00e1s bits,", "tokens": [50364, 631, 5290, 1690, 17831, 347, 7433, 5487, 37104, 4976, 19055, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 9239, 13, 12555, 388, 19055, 3573, 9239, 11, 50850], "temperature": 0.0, "avg_logprob": -0.27320130292107075, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.0970364362001419}, {"id": 429, "seek": 331192, "start": 3321.64, "end": 3329.16, "text": " pero como son mucho menos probable, mi entrop\u00eda me da 2 bits, o sea, el promedio de bits", "tokens": [50850, 4768, 2617, 1872, 9824, 8902, 21759, 11, 2752, 948, 1513, 2686, 385, 1120, 568, 9239, 11, 277, 4158, 11, 806, 2234, 292, 1004, 368, 9239, 51226], "temperature": 0.0, "avg_logprob": -0.27320130292107075, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.0970364362001419}, {"id": 430, "seek": 331192, "start": 3329.16, "end": 3338.0, "text": " que yo utilizo seg\u00fan la distribuci\u00f3n es 2 bits, que es m\u00e1s baja que los 3 bits originales.", "tokens": [51226, 631, 5290, 4976, 19055, 36570, 635, 4400, 30813, 785, 568, 9239, 11, 631, 785, 3573, 49427, 631, 1750, 805, 9239, 3380, 279, 13, 51668], "temperature": 0.0, "avg_logprob": -0.27320130292107075, "compression_ratio": 1.469945355191257, "no_speech_prob": 0.0970364362001419}, {"id": 431, "seek": 333800, "start": 3338.56, "end": 3346.72, "text": " \u00bfCentiende? Incorporando la informaci\u00f3n de la distribuci\u00f3n bajo. Podemos mejorar eso, no", "tokens": [50392, 3841, 34, 317, 45816, 30, 39120, 2816, 1806, 635, 21660, 368, 635, 4400, 30813, 30139, 13, 12646, 4485, 48858, 7287, 11, 572, 50800], "temperature": 0.0, "avg_logprob": -0.39379469256534755, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.1477760523557663}, {"id": 432, "seek": 333800, "start": 3346.72, "end": 3350.72, "text": " podemos mejorar eso. Nunca vamos a hacer el etrop\u00eda, lo que lo dice es eso, nunca vas a encontrar", "tokens": [50800, 12234, 48858, 7287, 13, 23696, 496, 5295, 257, 6720, 806, 1030, 1513, 2686, 11, 450, 631, 450, 10313, 785, 7287, 11, 13768, 11481, 257, 17525, 51000], "temperature": 0.0, "avg_logprob": -0.39379469256534755, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.1477760523557663}, {"id": 433, "seek": 333800, "start": 3350.72, "end": 3356.32, "text": " una, porque justamente la etrop\u00eda 2, como la etrop\u00eda 2, la etrop\u00eda me da una cota inferior", "tokens": [51000, 2002, 11, 4021, 41056, 635, 1030, 1513, 2686, 568, 11, 2617, 635, 1030, 1513, 2686, 568, 11, 635, 1030, 1513, 2686, 385, 1120, 2002, 269, 5377, 24249, 51280], "temperature": 0.0, "avg_logprob": -0.39379469256534755, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.1477760523557663}, {"id": 434, "seek": 333800, "start": 3356.32, "end": 3365.68, "text": " sobre cu\u00e1nto puedo llegar, con menos de 2 bits no puedo. \u00bfTambi\u00e9n te acuerdo? Te dice", "tokens": [51280, 5473, 44256, 78, 21612, 24892, 11, 416, 8902, 368, 568, 9239, 572, 21612, 13, 3841, 51, 2173, 5770, 535, 28113, 30, 1989, 10313, 51748], "temperature": 0.0, "avg_logprob": -0.39379469256534755, "compression_ratio": 1.5940170940170941, "no_speech_prob": 0.1477760523557663}, {"id": 435, "seek": 336568, "start": 3365.68, "end": 3374.3199999999997, "text": " preguntar\u00e1n para qu\u00e9 sirve esto. De hecho no, la etrop\u00eda es una cota, lo que dec\u00eda, una", "tokens": [50364, 19860, 289, 7200, 1690, 8057, 4735, 303, 7433, 13, 1346, 13064, 572, 11, 635, 1030, 1513, 2686, 785, 2002, 269, 5377, 11, 450, 631, 37599, 11, 2002, 50796], "temperature": 0.0, "avg_logprob": -0.2814000112968579, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.031763285398483276}, {"id": 436, "seek": 336568, "start": 3374.3199999999997, "end": 3380.56, "text": " cota m\u00ednima para el n\u00famero de bits necesaria. A partir de la etrop\u00eda yo puedo calcular la", "tokens": [50796, 269, 5377, 33656, 4775, 1690, 806, 14959, 368, 9239, 11909, 9831, 13, 316, 13906, 368, 635, 1030, 1513, 2686, 5290, 21612, 2104, 17792, 635, 51108], "temperature": 0.0, "avg_logprob": -0.2814000112968579, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.031763285398483276}, {"id": 437, "seek": 338056, "start": 3380.56, "end": 3392.68, "text": " etrop\u00eda de una secuencia, la etrop\u00eda de una secuencia es de todas las combinaciones", "tokens": [50364, 1030, 1513, 2686, 368, 2002, 907, 47377, 11, 635, 1030, 1513, 2686, 368, 2002, 907, 47377, 785, 368, 10906, 2439, 38514, 9188, 50970], "temperature": 0.0, "avg_logprob": -0.25024213073074175, "compression_ratio": 2.039772727272727, "no_speech_prob": 0.30317798256874084}, {"id": 438, "seek": 338056, "start": 3392.68, "end": 3399.24, "text": " posibles de una secuencia, la probabilidad de esa combinaci\u00f3n es lo mismo para aplicar", "tokens": [50970, 1366, 14428, 368, 2002, 907, 47377, 11, 635, 31959, 4580, 368, 11342, 38514, 3482, 785, 450, 12461, 1690, 18221, 289, 51298], "temperature": 0.0, "avg_logprob": -0.25024213073074175, "compression_ratio": 2.039772727272727, "no_speech_prob": 0.30317798256874084}, {"id": 439, "seek": 338056, "start": 3399.24, "end": 3403.2799999999997, "text": " la secuencia, entonces si lo ven es un n\u00famero muy complicado, porque es la sumatoria de una", "tokens": [51298, 635, 907, 47377, 11, 13003, 1511, 450, 6138, 785, 517, 14959, 5323, 49850, 11, 4021, 785, 635, 2408, 1639, 654, 368, 2002, 51500], "temperature": 0.0, "avg_logprob": -0.25024213073074175, "compression_ratio": 2.039772727272727, "no_speech_prob": 0.30317798256874084}, {"id": 440, "seek": 338056, "start": 3403.2799999999997, "end": 3407.7599999999998, "text": " cantidad impresionante de n\u00famero, porque son todas las combinaciones posibles de secuencia.", "tokens": [51500, 33757, 35672, 313, 2879, 368, 14959, 11, 4021, 1872, 10906, 2439, 38514, 9188, 1366, 14428, 368, 907, 47377, 13, 51724], "temperature": 0.0, "avg_logprob": -0.25024213073074175, "compression_ratio": 2.039772727272727, "no_speech_prob": 0.30317798256874084}, {"id": 441, "seek": 340776, "start": 3407.76, "end": 3418.5600000000004, "text": " Eso es lo que me mide la etrop\u00eda de la secuencia, \u00bfqu\u00e9 tanta incertidumbre hay en una secuencia?", "tokens": [50364, 27795, 785, 450, 631, 385, 275, 482, 635, 1030, 1513, 2686, 368, 635, 907, 47377, 11, 3841, 16412, 40864, 834, 911, 327, 449, 2672, 4842, 465, 2002, 907, 47377, 30, 50904], "temperature": 0.0, "avg_logprob": -0.3706663031327097, "compression_ratio": 1.2868217054263567, "no_speech_prob": 0.06712547689676285}, {"id": 442, "seek": 340776, "start": 3427.88, "end": 3435.2400000000002, "text": " Y la tasa de etrop\u00eda ser\u00eda eso debido a N, es decir el promedio,", "tokens": [51370, 398, 635, 8023, 64, 368, 1030, 1513, 2686, 23679, 7287, 50003, 257, 426, 11, 785, 10235, 806, 2234, 292, 1004, 11, 51738], "temperature": 0.0, "avg_logprob": -0.3706663031327097, "compression_ratio": 1.2868217054263567, "no_speech_prob": 0.06712547689676285}, {"id": 443, "seek": 343524, "start": 3435.24, "end": 3442.8399999999997, "text": " porque si no la secuencia malarga o no tiene entrop\u00eda m\u00e1s alto, el promedio por palabra", "tokens": [50364, 4021, 1511, 572, 635, 907, 47377, 2806, 289, 3680, 277, 572, 7066, 948, 1513, 2686, 3573, 21275, 11, 806, 2234, 292, 1004, 1515, 31702, 50744], "temperature": 0.0, "avg_logprob": -0.41743524724786935, "compression_ratio": 1.4, "no_speech_prob": 0.036192573606967926}, {"id": 444, "seek": 343524, "start": 3442.8399999999997, "end": 3459.9599999999996, "text": " de la etrop\u00eda. Entonces la etrop\u00eda de un lenguaje, que ser\u00eda como la medida de qu\u00e9 tanta", "tokens": [50744, 368, 635, 1030, 1513, 2686, 13, 15097, 635, 1030, 1513, 2686, 368, 517, 35044, 84, 11153, 11, 631, 23679, 2617, 635, 32984, 368, 8057, 40864, 51600], "temperature": 0.0, "avg_logprob": -0.41743524724786935, "compression_ratio": 1.4, "no_speech_prob": 0.036192573606967926}, {"id": 445, "seek": 345996, "start": 3459.96, "end": 3471.52, "text": " incertidumbre hay en un lenguaje, \u00bfqu\u00e9 tan, digamos, qu\u00e9 tanto pollo llegar a predecir", "tokens": [50364, 834, 911, 327, 449, 2672, 4842, 465, 517, 35044, 84, 11153, 11, 3841, 16412, 7603, 11, 36430, 11, 8057, 10331, 714, 1913, 24892, 257, 24874, 23568, 50942], "temperature": 0.0, "avg_logprob": -0.2942756877225988, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.1107533872127533}, {"id": 446, "seek": 345996, "start": 3471.52, "end": 3477.08, "text": " lo que va a seguir diciendo el lenguaje? Esa l\u00edmite, pero como valoso, no en un contexto", "tokens": [50942, 450, 631, 2773, 257, 18584, 42797, 806, 35044, 84, 11153, 30, 2313, 64, 287, 14569, 642, 11, 4768, 2617, 1323, 9869, 11, 572, 465, 517, 47685, 51220], "temperature": 0.0, "avg_logprob": -0.2942756877225988, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.1107533872127533}, {"id": 447, "seek": 345996, "start": 3477.08, "end": 3482.2, "text": " general en el lenguaje, es una medida para el lenguaje. Esa l\u00edmite cuando la secuencia", "tokens": [51220, 2674, 465, 806, 35044, 84, 11153, 11, 785, 2002, 32984, 1690, 806, 35044, 84, 11153, 13, 2313, 64, 287, 14569, 642, 7767, 635, 907, 47377, 51476], "temperature": 0.0, "avg_logprob": -0.2942756877225988, "compression_ratio": 1.5433526011560694, "no_speech_prob": 0.1107533872127533}, {"id": 448, "seek": 348220, "start": 3482.2, "end": 3488.04, "text": " tiene infinito de la tasa de etrop\u00eda, \u00bfs\u00ed?", "tokens": [50364, 7066, 7193, 3528, 368, 635, 8023, 64, 368, 1030, 1513, 2686, 11, 3841, 82, 870, 30, 50656], "temperature": 0.0, "avg_logprob": -0.39746927897135415, "compression_ratio": 1.4533333333333334, "no_speech_prob": 0.07909557223320007}, {"id": 449, "seek": 348220, "start": 3498.04, "end": 3502.4399999999996, "text": " Y que es que ac\u00e1 es la suma, como dec\u00edamos, es la suma de todas las secuencias posibles,", "tokens": [51156, 398, 631, 785, 631, 23496, 785, 635, 2408, 64, 11, 2617, 979, 16275, 11, 785, 635, 2408, 64, 368, 10906, 2439, 907, 7801, 12046, 1366, 14428, 11, 51376], "temperature": 0.0, "avg_logprob": -0.39746927897135415, "compression_ratio": 1.4533333333333334, "no_speech_prob": 0.07909557223320007}, {"id": 450, "seek": 348220, "start": 3502.4399999999996, "end": 3507.9199999999996, "text": " o sea que es una cosa imposible calcular, pero hay un teorema que es el de llano,", "tokens": [51376, 277, 4158, 631, 785, 2002, 10163, 38396, 964, 2104, 17792, 11, 4768, 4842, 517, 535, 418, 1696, 631, 785, 806, 368, 4849, 3730, 11, 51650], "temperature": 0.0, "avg_logprob": -0.39746927897135415, "compression_ratio": 1.4533333333333334, "no_speech_prob": 0.07909557223320007}, {"id": 451, "seek": 350792, "start": 3507.92, "end": 3513.6, "text": " como a mi la embraiman que dice que es el lenguaje, \u00e9l es estacionario y erg\u00f3lico. Estacionario", "tokens": [50364, 2617, 257, 2752, 635, 4605, 424, 25504, 631, 10313, 631, 785, 806, 35044, 84, 11153, 11, 11810, 785, 871, 18803, 4912, 288, 1189, 70, 812, 1050, 78, 13, 4410, 18803, 4912, 50648], "temperature": 0.0, "avg_logprob": -0.38715624453416514, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.05192607268691063}, {"id": 452, "seek": 350792, "start": 3513.6, "end": 3519.48, "text": " y erg\u00f3lico quiere decir que no importa d\u00f3nde yo est\u00e9 parado en una secuencia, todas las", "tokens": [50648, 288, 1189, 70, 812, 1050, 78, 23877, 10235, 631, 572, 33218, 34264, 5290, 34584, 971, 1573, 465, 2002, 907, 47377, 11, 10906, 2439, 50942], "temperature": 0.0, "avg_logprob": -0.38715624453416514, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.05192607268691063}, {"id": 453, "seek": 350792, "start": 3519.48, "end": 3526.56, "text": " posiciones van en las probabilidades o en las mismas de la limidad, lo cual no es as\u00ed en", "tokens": [50942, 1366, 29719, 3161, 465, 2439, 31959, 10284, 277, 465, 2439, 23220, 296, 368, 635, 2364, 4580, 11, 450, 10911, 572, 785, 8582, 465, 51296], "temperature": 0.0, "avg_logprob": -0.38715624453416514, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.05192607268691063}, {"id": 454, "seek": 350792, "start": 3526.56, "end": 3530.4, "text": " un lenguaje, porque lo que yo digo ahora y s\u00ed dentro de lo que estoy diciendo entre un", "tokens": [51296, 517, 35044, 84, 11153, 11, 4021, 450, 631, 5290, 22990, 9923, 288, 8600, 10856, 368, 450, 631, 15796, 42797, 3962, 517, 51488], "temperature": 0.0, "avg_logprob": -0.38715624453416514, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.05192607268691063}, {"id": 455, "seek": 350792, "start": 3530.4, "end": 3536.6800000000003, "text": " minuto m\u00e1s, no, no hay aleatorio de lo m\u00e1s, pero suponiendo eso es una simplificaci\u00f3n,", "tokens": [51488, 923, 8262, 3573, 11, 572, 11, 572, 4842, 6775, 48028, 368, 450, 3573, 11, 4768, 9331, 266, 7304, 7287, 785, 2002, 6883, 40802, 11, 51802], "temperature": 0.0, "avg_logprob": -0.38715624453416514, "compression_ratio": 1.7014925373134329, "no_speech_prob": 0.05192607268691063}, {"id": 456, "seek": 353668, "start": 3536.68, "end": 3544.2799999999997, "text": " lo que me permite es simplemente para calcular la entrop\u00eda, la tasa de entrop\u00eda, el lenguaje", "tokens": [50364, 450, 631, 385, 31105, 785, 33190, 1690, 2104, 17792, 635, 948, 1513, 2686, 11, 635, 8023, 64, 368, 948, 1513, 2686, 11, 806, 35044, 84, 11153, 50744], "temperature": 0.0, "avg_logprob": -0.23667311879385888, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.008591548539698124}, {"id": 457, "seek": 353668, "start": 3544.2799999999997, "end": 3549.56, "text": " es simplemente unos sobre nes divididos logarimos, f\u00edjense que perd\u00ed la probabilidad de cada", "tokens": [50744, 785, 33190, 17780, 5473, 297, 279, 4996, 7895, 3565, 289, 8372, 11, 283, 870, 73, 1288, 631, 12611, 870, 635, 31959, 4580, 368, 8411, 51008], "temperature": 0.0, "avg_logprob": -0.23667311879385888, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.008591548539698124}, {"id": 458, "seek": 353668, "start": 3549.56, "end": 3555.44, "text": " una de las secuencias, es como que si yo tomo una secuencia suficientemente larga del lenguaje,", "tokens": [51008, 2002, 368, 2439, 907, 7801, 12046, 11, 785, 2617, 631, 1511, 5290, 2916, 78, 2002, 907, 47377, 459, 1786, 1196, 16288, 1613, 3680, 1103, 35044, 84, 11153, 11, 51302], "temperature": 0.0, "avg_logprob": -0.23667311879385888, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.008591548539698124}, {"id": 459, "seek": 353668, "start": 3555.44, "end": 3562.3599999999997, "text": " voy a incluir a todas las secuencias, o sea que si yo una secuencia suficientemente larga", "tokens": [51302, 7552, 257, 25520, 347, 257, 10906, 2439, 907, 7801, 12046, 11, 277, 4158, 631, 1511, 5290, 2002, 907, 47377, 459, 1786, 1196, 16288, 1613, 3680, 51648], "temperature": 0.0, "avg_logprob": -0.23667311879385888, "compression_ratio": 1.913265306122449, "no_speech_prob": 0.008591548539698124}, {"id": 460, "seek": 356236, "start": 3562.36, "end": 3567.1600000000003, "text": " puede ser el corpo de evaluaci\u00f3n, yo puedo calcular la entrop\u00eda sobre el corpo de evaluaci\u00f3n,", "tokens": [50364, 8919, 816, 806, 1181, 2259, 368, 6133, 3482, 11, 5290, 21612, 2104, 17792, 635, 948, 1513, 2686, 5473, 806, 1181, 2259, 368, 6133, 3482, 11, 50604], "temperature": 0.0, "avg_logprob": -0.26963397106492376, "compression_ratio": 1.6432748538011697, "no_speech_prob": 0.058290690183639526}, {"id": 461, "seek": 356236, "start": 3578.1200000000003, "end": 3584.84, "text": " y entonces, esto es un n\u00famero, ahora lo que dije ac\u00e1 es un n\u00famero, no sabemos por qu\u00e9 tengo", "tokens": [51152, 288, 13003, 11, 7433, 785, 517, 14959, 11, 9923, 450, 631, 39414, 23496, 785, 517, 14959, 11, 572, 27200, 1515, 8057, 13989, 51488], "temperature": 0.0, "avg_logprob": -0.26963397106492376, "compression_ratio": 1.6432748538011697, "no_speech_prob": 0.058290690183639526}, {"id": 462, "seek": 356236, "start": 3584.84, "end": 3591.6, "text": " esto, \u00bfno? Pero f\u00edjense que si yo puedo calcular lo que se llama la entrop\u00eda cruzada,", "tokens": [51488, 7433, 11, 3841, 1771, 30, 9377, 283, 870, 73, 1288, 631, 1511, 5290, 21612, 2104, 17792, 450, 631, 369, 23272, 635, 948, 1513, 2686, 5140, 89, 1538, 11, 51826], "temperature": 0.0, "avg_logprob": -0.26963397106492376, "compression_ratio": 1.6432748538011697, "no_speech_prob": 0.058290690183639526}, {"id": 463, "seek": 359160, "start": 3591.68, "end": 3598.2, "text": " porque yo que tengo, yo tengo un lenguaje que genera las palabras con una cierta distribuci\u00f3n", "tokens": [50368, 4021, 5290, 631, 13989, 11, 5290, 13989, 517, 35044, 84, 11153, 631, 1337, 64, 2439, 35240, 416, 2002, 39769, 1328, 4400, 30813, 50694], "temperature": 0.0, "avg_logprob": -0.2790535948330298, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.00772510701790452}, {"id": 464, "seek": 359160, "start": 3598.2, "end": 3604.0, "text": " de probabilidad, que es lo que queremos averiguar, que es lo que es lo que es lo que es nuestra", "tokens": [50694, 368, 31959, 4580, 11, 631, 785, 450, 631, 26813, 18247, 16397, 289, 11, 631, 785, 450, 631, 785, 450, 631, 785, 450, 631, 785, 16825, 50984], "temperature": 0.0, "avg_logprob": -0.2790535948330298, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.00772510701790452}, {"id": 465, "seek": 359160, "start": 3604.0, "end": 3609.2799999999997, "text": " problema original, es como da las palabras anteriores y se genera la siguiente, eso es algo que", "tokens": [50984, 12395, 3380, 11, 785, 2617, 1120, 2439, 35240, 22272, 279, 288, 369, 1337, 64, 635, 25666, 11, 7287, 785, 8655, 631, 51248], "temperature": 0.0, "avg_logprob": -0.2790535948330298, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.00772510701790452}, {"id": 466, "seek": 359160, "start": 3609.2799999999997, "end": 3614.0, "text": " he desconocido, no sabemos como es, porque es el del lenguaje espa\u00f1ol, que yo quiero calcular,", "tokens": [51248, 415, 49801, 905, 2925, 11, 572, 27200, 2617, 785, 11, 4021, 785, 806, 1103, 35044, 84, 11153, 31177, 11, 631, 5290, 16811, 2104, 17792, 11, 51484], "temperature": 0.0, "avg_logprob": -0.2790535948330298, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.00772510701790452}, {"id": 467, "seek": 359160, "start": 3614.0, "end": 3621.0, "text": " pero yo tengo un modelo M, que es el modelo de negramas, est\u00e1, la entrop\u00eda cruzada, lo que", "tokens": [51484, 4768, 5290, 13989, 517, 27825, 376, 11, 631, 785, 806, 27825, 368, 2485, 2356, 296, 11, 3192, 11, 635, 948, 1513, 2686, 5140, 89, 1538, 11, 450, 631, 51834], "temperature": 0.0, "avg_logprob": -0.2790535948330298, "compression_ratio": 1.8627450980392157, "no_speech_prob": 0.00772510701790452}, {"id": 468, "seek": 362100, "start": 3621.0, "end": 3631.56, "text": " dice, bueno, calculamos esta hache utilizando la probabilidad original por el lovarismo", "tokens": [50364, 10313, 11, 11974, 11, 4322, 2151, 5283, 324, 1876, 19906, 1806, 635, 31959, 4580, 3380, 1515, 806, 450, 8517, 6882, 50892], "temperature": 0.0, "avg_logprob": -0.42266125679016114, "compression_ratio": 1.725, "no_speech_prob": 0.0036921477876603603}, {"id": 469, "seek": 362100, "start": 3631.56, "end": 3638.36, "text": " del, de la probabilidad sin nada por el modelo, la probabilidad de la secuencia es la que ten\u00eda", "tokens": [50892, 1103, 11, 368, 635, 31959, 4580, 3343, 8096, 1515, 806, 27825, 11, 635, 31959, 4580, 368, 635, 907, 47377, 785, 635, 631, 23718, 51232], "temperature": 0.0, "avg_logprob": -0.42266125679016114, "compression_ratio": 1.725, "no_speech_prob": 0.0036921477876603603}, {"id": 470, "seek": 362100, "start": 3638.36, "end": 3644.16, "text": " los movilidades, no la conozco, y la probabilidad, y en lovarimos s\u00ed, o sea, esa distancia", "tokens": [51232, 1750, 2402, 388, 10284, 11, 572, 635, 416, 15151, 1291, 11, 288, 635, 31959, 4580, 11, 288, 465, 450, 8517, 8372, 8600, 11, 277, 4158, 11, 11342, 1483, 22862, 51522], "temperature": 0.0, "avg_logprob": -0.42266125679016114, "compression_ratio": 1.725, "no_speech_prob": 0.0036921477876603603}, {"id": 471, "seek": 364416, "start": 3644.16, "end": 3651.72, "text": " es a largo emb\u00edtesis del modelo, seguramente tenemos otra vez, ya lo manmelan, yo puedo sacar esta", "tokens": [50364, 785, 257, 31245, 4605, 6712, 9374, 1103, 27825, 11, 22179, 3439, 9914, 13623, 5715, 11, 2478, 450, 587, 10909, 282, 11, 5290, 21612, 43823, 5283, 50742], "temperature": 0.0, "avg_logprob": -0.467861811319987, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.03040984831750393}, {"id": 472, "seek": 364416, "start": 3651.72, "end": 3656.56, "text": " probabilidad simplificando la suponiendo que es el gode y que lo la, y digo bueno, la entrop\u00eda cruzada", "tokens": [50742, 31959, 4580, 6883, 1089, 1806, 635, 9331, 266, 7304, 631, 785, 806, 352, 1479, 288, 631, 450, 635, 11, 288, 22990, 11974, 11, 635, 948, 1513, 2686, 5140, 89, 1538, 50984], "temperature": 0.0, "avg_logprob": -0.467861811319987, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.03040984831750393}, {"id": 473, "seek": 364416, "start": 3658.7999999999997, "end": 3668.3599999999997, "text": " es, depende s\u00f3lo lovarismo de, de la probabilidad sin nada por lenguaje, por el modelo, y esto es", "tokens": [51096, 785, 11, 47091, 22885, 450, 8517, 6882, 368, 11, 368, 635, 31959, 4580, 3343, 8096, 1515, 35044, 84, 11153, 11, 1515, 806, 27825, 11, 288, 7433, 785, 51574], "temperature": 0.0, "avg_logprob": -0.467861811319987, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.03040984831750393}, {"id": 474, "seek": 366836, "start": 3668.36, "end": 3677.6800000000003, "text": " interesante, cualquier, cualquier entrop\u00eda cruzada que yo tenga, que yo calcule con un modelo,", "tokens": [50364, 36396, 11, 21004, 11, 21004, 948, 1513, 2686, 5140, 89, 1538, 631, 5290, 36031, 11, 631, 5290, 2104, 66, 2271, 416, 517, 27825, 11, 50830], "temperature": 0.0, "avg_logprob": -0.21538876552207797, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.0501481331884861}, {"id": 475, "seek": 366836, "start": 3677.6800000000003, "end": 3687.96, "text": " va a ser mayor necesariamente que la entrop\u00eda es del lenguaje, cualquier modelo va a", "tokens": [50830, 2773, 257, 816, 10120, 11909, 45149, 631, 635, 948, 1513, 2686, 785, 1103, 35044, 84, 11153, 11, 21004, 27825, 2773, 257, 51344], "temperature": 0.0, "avg_logprob": -0.21538876552207797, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.0501481331884861}, {"id": 476, "seek": 368796, "start": 3687.96, "end": 3693.2, "text": " ensinarme una entrop\u00eda mayor a la del lenguaje, entonces la, la, la, la, la cota inferior,", "tokens": [50364, 3489, 6470, 1398, 2002, 948, 1513, 2686, 10120, 257, 635, 1103, 35044, 84, 11153, 11, 13003, 635, 11, 635, 11, 635, 11, 635, 11, 635, 269, 5377, 24249, 11, 50626], "temperature": 0.0, "avg_logprob": -0.41776182296428277, "compression_ratio": 1.3431372549019607, "no_speech_prob": 0.028925295919179916}, {"id": 477, "seek": 368796, "start": 3709.2, "end": 3712.28, "text": " entonces f\u00edjense que como son todas mayores,", "tokens": [51426, 13003, 283, 870, 73, 1288, 631, 2617, 1872, 10906, 815, 2706, 11, 51580], "temperature": 0.0, "avg_logprob": -0.41776182296428277, "compression_ratio": 1.3431372549019607, "no_speech_prob": 0.028925295919179916}, {"id": 478, "seek": 371228, "start": 3712.28, "end": 3719.7200000000003, "text": " cuanto m\u00e1s parecido sea mi modelo, al modelo, al modelo de lenguaje, al, al, al, cuanto m\u00e1s", "tokens": [50364, 36685, 3573, 7448, 17994, 4158, 2752, 27825, 11, 419, 27825, 11, 419, 27825, 368, 35044, 84, 11153, 11, 419, 11, 419, 11, 419, 11, 36685, 3573, 50736], "temperature": 0.0, "avg_logprob": -0.30232527545679394, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.020489461719989777}, {"id": 479, "seek": 371228, "start": 3719.7200000000003, "end": 3724.2400000000002, "text": " modelo, m\u00e1s parecido, as\u00ed que mi probabilidad es m\u00e1s parecida de las de ac\u00e1, por c\u00f3mo est\u00e1 definido,", "tokens": [50736, 27825, 11, 3573, 7448, 17994, 11, 8582, 631, 2752, 31959, 4580, 785, 3573, 7448, 37200, 368, 2439, 368, 23496, 11, 1515, 12826, 3192, 1561, 2925, 11, 50962], "temperature": 0.0, "avg_logprob": -0.30232527545679394, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.020489461719989777}, {"id": 480, "seek": 371228, "start": 3725.36, "end": 3734.44, "text": " va a ser mejor, de acuerdo, entonces, cuanto menor sea la entrop\u00eda cruzada de mi modelo,", "tokens": [51018, 2773, 257, 816, 11479, 11, 368, 28113, 11, 13003, 11, 36685, 26343, 4158, 635, 948, 1513, 2686, 5140, 89, 1538, 368, 2752, 27825, 11, 51472], "temperature": 0.0, "avg_logprob": -0.30232527545679394, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.020489461719989777}, {"id": 481, "seek": 371228, "start": 3734.44, "end": 3739.0400000000004, "text": " evaluado sobre una secuencia suficientemente larga, decir sobre el corpo de evaluaci\u00f3n,", "tokens": [51472, 6133, 1573, 5473, 2002, 907, 47377, 459, 1786, 1196, 16288, 1613, 3680, 11, 10235, 5473, 806, 23257, 368, 6133, 3482, 11, 51702], "temperature": 0.0, "avg_logprob": -0.30232527545679394, "compression_ratio": 1.8221153846153846, "no_speech_prob": 0.020489461719989777}, {"id": 482, "seek": 373904, "start": 3740.0, "end": 3746.8, "text": " mejor va a ser mi aproximaci\u00f3n, y justamente la medida de esa intr\u00ednsega que est\u00e1 buscando era", "tokens": [50412, 11479, 2773, 257, 816, 2752, 31270, 3482, 11, 288, 41056, 635, 32984, 368, 11342, 17467, 10973, 405, 3680, 631, 3192, 46804, 4249, 50752], "temperature": 0.0, "avg_logprob": -0.3848974951382341, "compression_ratio": 1.4125874125874125, "no_speech_prob": 0.004095761105418205}, {"id": 483, "seek": 373904, "start": 3751.64, "end": 3762.2799999999997, "text": " es esto, que es dos, porque dos no lo s\u00e9, porque lo mismo, es dos, es para sacarlo lovarimos nada m\u00e1s,", "tokens": [50994, 785, 7433, 11, 631, 785, 4491, 11, 4021, 4491, 572, 450, 7910, 11, 4021, 450, 12461, 11, 785, 4491, 11, 785, 1690, 4899, 19457, 450, 8517, 8372, 8096, 3573, 11, 51526], "temperature": 0.0, "avg_logprob": -0.3848974951382341, "compression_ratio": 1.4125874125874125, "no_speech_prob": 0.004095761105418205}, {"id": 484, "seek": 376228, "start": 3762.84, "end": 3772.52, "text": " es dos a la entrop\u00eda cruzada a este valor, y esto se llama perplejida, la perplejida es lo que", "tokens": [50392, 785, 4491, 257, 635, 948, 1513, 2686, 5140, 89, 1538, 257, 4065, 15367, 11, 288, 7433, 369, 23272, 680, 781, 73, 2887, 11, 635, 680, 781, 73, 2887, 785, 450, 631, 50876], "temperature": 0.0, "avg_logprob": -0.2988096360237368, "compression_ratio": 1.5, "no_speech_prob": 0.0017336596501991153}, {"id": 485, "seek": 376228, "start": 3772.52, "end": 3786.84, "text": " mide el, el, lo que mide que tan bueno es interisidamente mi modelo sobre, sobre mi cuerpo de", "tokens": [50876, 275, 482, 806, 11, 806, 11, 450, 631, 275, 482, 631, 7603, 11974, 785, 728, 271, 49663, 2752, 27825, 5473, 11, 5473, 2752, 20264, 368, 51592], "temperature": 0.0, "avg_logprob": -0.2988096360237368, "compression_ratio": 1.5, "no_speech_prob": 0.0017336596501991153}, {"id": 486, "seek": 378684, "start": 3786.84, "end": 3792.04, "text": " entrenamiento, sobre mi cuerpo de evaluaci\u00f3n, es decir, si yo tengo dos modelos, el que as\u00ed me", "tokens": [50364, 45069, 16971, 11, 5473, 2752, 20264, 368, 6133, 3482, 11, 785, 10235, 11, 1511, 5290, 13989, 4491, 2316, 329, 11, 806, 631, 8582, 385, 50624], "temperature": 0.0, "avg_logprob": -0.27592756163399174, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.03863810375332832}, {"id": 487, "seek": 378684, "start": 3792.04, "end": 3799.44, "text": " mayor probabilidad, menor propiedad, mayor probabilidad, al corp\u00f3n de evaluaci\u00f3n es mejor desde", "tokens": [50624, 10120, 31959, 4580, 11, 26343, 2365, 1091, 345, 11, 10120, 31959, 4580, 11, 419, 1181, 79, 1801, 368, 6133, 3482, 785, 11479, 10188, 50994], "temperature": 0.0, "avg_logprob": -0.27592756163399174, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.03863810375332832}, {"id": 488, "seek": 378684, "start": 3799.44, "end": 3803.96, "text": " ese punto de vista, lo consideramos mejor, porque porque tiene menos dudas de c\u00f3mo se comporta,", "tokens": [50994, 10167, 14326, 368, 22553, 11, 450, 1949, 2151, 11479, 11, 4021, 4021, 7066, 8902, 38512, 296, 368, 12826, 369, 25883, 64, 11, 51220], "temperature": 0.0, "avg_logprob": -0.27592756163399174, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.03863810375332832}, {"id": 489, "seek": 378684, "start": 3803.96, "end": 3813.4, "text": " porque la perplejida es, es como la incertidumbre que yo tengo ante, dada una palabra, cuando", "tokens": [51220, 4021, 635, 680, 781, 73, 2887, 785, 11, 785, 2617, 635, 834, 911, 327, 449, 2672, 631, 5290, 13989, 23411, 11, 274, 1538, 2002, 31702, 11, 7767, 51692], "temperature": 0.0, "avg_logprob": -0.27592756163399174, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.03863810375332832}, {"id": 490, "seek": 381340, "start": 3813.56, "end": 3819.1600000000003, "text": " sume para una palabra, cu\u00e1l es mi incertidumbre, mi branching factor, en cu\u00e1ntas se puede abrir la", "tokens": [50372, 2408, 68, 1690, 2002, 31702, 11, 44318, 785, 2752, 834, 911, 327, 449, 2672, 11, 2752, 9819, 278, 5952, 11, 465, 44256, 296, 369, 8919, 27446, 635, 50652], "temperature": 0.0, "avg_logprob": -0.2643861363076756, "compression_ratio": 1.7723214285714286, "no_speech_prob": 0.008728188462555408}, {"id": 491, "seek": 381340, "start": 3819.1600000000003, "end": 3824.92, "text": " siguiente palabra en promedio, un poco eso es lo que captura la perplejida, mi lenguaje va a tener un", "tokens": [50652, 25666, 31702, 465, 2234, 292, 1004, 11, 517, 10639, 7287, 785, 450, 631, 3770, 2991, 635, 680, 781, 73, 2887, 11, 2752, 35044, 84, 11153, 2773, 257, 11640, 517, 50940], "temperature": 0.0, "avg_logprob": -0.2643861363076756, "compression_ratio": 1.7723214285714286, "no_speech_prob": 0.008728188462555408}, {"id": 492, "seek": 381340, "start": 3824.92, "end": 3830.48, "text": " branching factor, es decir, no es que es cero, pero mi modelo siempre va a calcular algo mayor", "tokens": [50940, 9819, 278, 5952, 11, 785, 10235, 11, 572, 785, 631, 785, 269, 2032, 11, 4768, 2752, 27825, 12758, 2773, 257, 2104, 17792, 8655, 10120, 51218], "temperature": 0.0, "avg_logprob": -0.2643861363076756, "compression_ratio": 1.7723214285714286, "no_speech_prob": 0.008728188462555408}, {"id": 493, "seek": 381340, "start": 3830.48, "end": 3836.36, "text": " igual a ese branching factor, cuanto m\u00e1s bajo, es que si yo me estoy acercando mal a la perplejida", "tokens": [51218, 10953, 257, 10167, 9819, 278, 5952, 11, 36685, 3573, 30139, 11, 785, 631, 1511, 5290, 385, 15796, 696, 2869, 1806, 2806, 257, 635, 680, 781, 73, 2887, 51512], "temperature": 0.0, "avg_logprob": -0.2643861363076756, "compression_ratio": 1.7723214285714286, "no_speech_prob": 0.008728188462555408}, {"id": 494, "seek": 383636, "start": 3836.36, "end": 3842.28, "text": " posta, por eso la perplejida es la medida de que tambi\u00e9n hace la cosa, acuerdo,", "tokens": [50364, 2183, 64, 11, 1515, 7287, 635, 680, 781, 73, 2887, 785, 635, 32984, 368, 631, 6407, 10032, 635, 10163, 11, 28113, 11, 50660], "temperature": 0.0, "avg_logprob": -0.4936837905492538, "compression_ratio": 1.5470588235294118, "no_speech_prob": 0.017977310344576836}, {"id": 495, "seek": 383636, "start": 3848.1200000000003, "end": 3858.52, "text": " bueno, no, eso es su cuenta, por ejemplo, si nosotros entrenamos un \u00edgrama, m\u00e1s \u00edgrama,", "tokens": [50952, 11974, 11, 572, 11, 7287, 785, 459, 17868, 11, 1515, 13358, 11, 1511, 13863, 45069, 2151, 517, 18645, 1342, 64, 11, 3573, 18645, 1342, 64, 11, 51472], "temperature": 0.0, "avg_logprob": -0.4936837905492538, "compression_ratio": 1.5470588235294118, "no_speech_prob": 0.017977310344576836}, {"id": 496, "seek": 383636, "start": 3858.52, "end": 3862.76, "text": " m\u00e1s \u00edgrama, en un cuerpo de art\u00edculo de Wall Street Journal, de 38 millones de palabras,", "tokens": [51472, 3573, 18645, 1342, 64, 11, 465, 517, 20264, 368, 1523, 34365, 368, 9551, 7638, 16936, 11, 368, 12843, 22416, 368, 35240, 11, 51684], "temperature": 0.0, "avg_logprob": -0.4936837905492538, "compression_ratio": 1.5470588235294118, "no_speech_prob": 0.017977310344576836}, {"id": 497, "seek": 386276, "start": 3862.76, "end": 3870.92, "text": " probaron el cuerpo sobre un modelo, ni un cuerpo de prueba de 1,5 millones de palabras,", "tokens": [50364, 1239, 6372, 806, 20264, 5473, 517, 27825, 11, 3867, 517, 20264, 368, 48241, 368, 502, 11, 20, 22416, 368, 35240, 11, 50772], "temperature": 0.0, "avg_logprob": -0.26519818849201443, "compression_ratio": 1.5562130177514792, "no_speech_prob": 0.04143957793712616}, {"id": 498, "seek": 386276, "start": 3870.92, "end": 3878.36, "text": " y calcularon la perplejida, y f\u00edjense que la perplejida con los unigramos desde 962,", "tokens": [50772, 288, 2104, 17792, 266, 635, 680, 781, 73, 2887, 11, 288, 283, 870, 73, 1288, 631, 635, 680, 781, 73, 2887, 416, 1750, 517, 328, 30227, 10188, 1722, 28052, 11, 51144], "temperature": 0.0, "avg_logprob": -0.26519818849201443, "compression_ratio": 1.5562130177514792, "no_speech_prob": 0.04143957793712616}, {"id": 499, "seek": 386276, "start": 3880.5200000000004, "end": 3885.84, "text": " no sabemos cu\u00e1l es el m\u00ednimo esto, no sabemos cu\u00e1nto puede bajar, pero sabemos que con", "tokens": [51252, 572, 27200, 44318, 785, 806, 47393, 7433, 11, 572, 27200, 44256, 78, 8919, 23589, 289, 11, 4768, 27200, 631, 416, 51518], "temperature": 0.0, "avg_logprob": -0.26519818849201443, "compression_ratio": 1.5562130177514792, "no_speech_prob": 0.04143957793712616}, {"id": 500, "seek": 388584, "start": 3885.84, "end": 3891.52, "text": " v\u00edgrama lleg\u00f3 a 170 y contr\u00edgrama a 109, es decir, si yo tengo dos palabras antes, puedo", "tokens": [50364, 6153, 1342, 64, 46182, 257, 27228, 288, 660, 81, 870, 1342, 64, 257, 1266, 24, 11, 785, 10235, 11, 1511, 5290, 13989, 4491, 35240, 11014, 11, 21612, 50648], "temperature": 0.0, "avg_logprob": -0.4100251564612755, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.05536773428320885}, {"id": 501, "seek": 388584, "start": 3891.52, "end": 3897.28, "text": " predecir con mejor, porque ac\u00e1 es con un \u00edgrama, es la probabilidad de la palabra, no dice mucho,", "tokens": [50648, 24874, 23568, 416, 11479, 11, 4021, 23496, 785, 416, 517, 18645, 1342, 64, 11, 785, 635, 31959, 4580, 368, 635, 31702, 11, 572, 10313, 9824, 11, 50936], "temperature": 0.0, "avg_logprob": -0.4100251564612755, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.05536773428320885}, {"id": 502, "seek": 388584, "start": 3897.28, "end": 3903.76, "text": " si yo tengo el anterior, r\u00e1pidamente baja, y si se fija cuando abre un tercero baja, pero no tanto,", "tokens": [50936, 1511, 5290, 13989, 806, 22272, 11, 18213, 49663, 49427, 11, 288, 1511, 369, 283, 20642, 7767, 41594, 517, 38103, 78, 49427, 11, 4768, 572, 10331, 11, 51260], "temperature": 0.0, "avg_logprob": -0.4100251564612755, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.05536773428320885}, {"id": 503, "seek": 388584, "start": 3903.76, "end": 3914.56, "text": " ni de cerca tanto, no, bueno, lo \u00faltimo que nos queda hablar,", "tokens": [51260, 3867, 368, 26770, 10331, 11, 572, 11, 11974, 11, 450, 21013, 631, 3269, 23314, 21014, 11, 51800], "temperature": 0.0, "avg_logprob": -0.4100251564612755, "compression_ratio": 1.5848214285714286, "no_speech_prob": 0.05536773428320885}, {"id": 504, "seek": 391584, "start": 3916.8, "end": 3924.4, "text": " no dice, no pas\u00f3 con las probabilidades nudes, se acuerdan que nos quedaban las probabilidades nudes", "tokens": [50412, 572, 10313, 11, 572, 41382, 416, 2439, 31959, 10284, 297, 10131, 11, 369, 696, 5486, 10312, 631, 3269, 13617, 18165, 2439, 31959, 10284, 297, 10131, 50792], "temperature": 0.0, "avg_logprob": -0.32183927419234293, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.018585560843348503}, {"id": 505, "seek": 391584, "start": 3924.4, "end": 3929.36, "text": " cuando no hab\u00eda contigo, bueno, uno de los problemas es las palabras que no existen,", "tokens": [50792, 7767, 572, 16395, 660, 7483, 11, 11974, 11, 8526, 368, 1750, 20720, 785, 2439, 35240, 631, 572, 2514, 268, 11, 51040], "temperature": 0.0, "avg_logprob": -0.32183927419234293, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.018585560843348503}, {"id": 506, "seek": 391584, "start": 3930.88, "end": 3934.96, "text": " las palabras que no existen, lo \u00fanico que podemos hacer, o lo que t\u00edpicamente se hace es", "tokens": [51116, 2439, 35240, 631, 572, 2514, 268, 11, 450, 26113, 631, 12234, 6720, 11, 277, 450, 631, 256, 28236, 23653, 369, 10032, 785, 51320], "temperature": 0.0, "avg_logprob": -0.32183927419234293, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.018585560843348503}, {"id": 507, "seek": 391584, "start": 3936.1600000000003, "end": 3941.84, "text": " crear un vocabulario fijo y sustituyo las palabras desconocidas por un especial,", "tokens": [51380, 31984, 517, 2329, 455, 1040, 1004, 283, 24510, 288, 5402, 6380, 8308, 2439, 35240, 49801, 905, 11382, 1515, 517, 15342, 11, 51664], "temperature": 0.0, "avg_logprob": -0.32183927419234293, "compression_ratio": 1.7860696517412935, "no_speech_prob": 0.018585560843348503}, {"id": 508, "seek": 394184, "start": 3941.84, "end": 3946.6400000000003, "text": " esto es t\u00edpicamente lo que se hace, es decir, todas las palabras desconocidas las considero una", "tokens": [50364, 7433, 785, 256, 28236, 23653, 450, 631, 369, 10032, 11, 785, 10235, 11, 10906, 2439, 35240, 49801, 905, 11382, 2439, 1949, 78, 2002, 50604], "temperature": 0.0, "avg_logprob": -0.34527974314503856, "compression_ratio": 1.6204819277108433, "no_speech_prob": 0.00649511581286788}, {"id": 509, "seek": 394184, "start": 3946.6400000000003, "end": 3953.52, "text": " sola palabra que no se equivale, y cuando aparecen enigradas m\u00e1s que no ocurren,", "tokens": [50604, 34162, 31702, 631, 572, 369, 48726, 1220, 11, 288, 7767, 15004, 13037, 465, 328, 48906, 3573, 631, 572, 26430, 1095, 11, 50948], "temperature": 0.0, "avg_logprob": -0.34527974314503856, "compression_ratio": 1.6204819277108433, "no_speech_prob": 0.00649511581286788}, {"id": 510, "seek": 394184, "start": 3953.52, "end": 3959.44, "text": " tiene el caso de comer, que no aparecidas, pero puede ser que la enigrama no ocurra lo que", "tokens": [50948, 7066, 806, 9666, 368, 16510, 11, 631, 572, 15004, 66, 11382, 11, 4768, 8919, 816, 631, 635, 465, 328, 29762, 572, 26430, 424, 450, 631, 51244], "temperature": 0.0, "avg_logprob": -0.34527974314503856, "compression_ratio": 1.6204819277108433, "no_speech_prob": 0.00649511581286788}, {"id": 511, "seek": 395944, "start": 3959.44, "end": 3973.76, "text": " voy a hacer, son t\u00e9cnicas de suavizado, yo tengo, se acuerdan, tengo el contador de,", "tokens": [50364, 7552, 257, 6720, 11, 1872, 25564, 40672, 368, 459, 706, 590, 1573, 11, 5290, 13989, 11, 369, 696, 5486, 10312, 11, 13989, 806, 660, 5409, 368, 11, 51080], "temperature": 0.0, "avg_logprob": -0.3231729439326695, "compression_ratio": 1.4094488188976377, "no_speech_prob": 0.060690827667713165}, {"id": 512, "seek": 395944, "start": 3976.32, "end": 3982.16, "text": " por ejemplo, ac\u00e1 es un migra a mano, contador de la palabra, de cantidad de veces la palabra", "tokens": [51208, 1515, 13358, 11, 23496, 785, 517, 6186, 424, 257, 18384, 11, 660, 5409, 368, 635, 31702, 11, 368, 33757, 368, 17054, 635, 31702, 51500], "temperature": 0.0, "avg_logprob": -0.3231729439326695, "compression_ratio": 1.4094488188976377, "no_speech_prob": 0.060690827667713165}, {"id": 513, "seek": 398216, "start": 3982.16, "end": 3993.2799999999997, "text": " dividido el total de token que hay, y as\u00ed calculo las probabilidades, la t\u00e9cnica de la plaza,", "tokens": [50364, 4996, 2925, 806, 3217, 368, 14862, 631, 4842, 11, 288, 8582, 4322, 78, 2439, 31959, 10284, 11, 635, 45411, 368, 635, 499, 12257, 11, 50920], "temperature": 0.0, "avg_logprob": -0.34386026757395166, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.07677313685417175}, {"id": 514, "seek": 398216, "start": 3995.04, "end": 3999.2799999999997, "text": " lo que dice es bueno, le agrego uno a cada contador, o sea que nunca me va a dar cero,", "tokens": [51008, 450, 631, 10313, 785, 11974, 11, 476, 623, 3375, 78, 8526, 257, 8411, 660, 5409, 11, 277, 4158, 631, 13768, 385, 2773, 257, 4072, 269, 2032, 11, 51220], "temperature": 0.0, "avg_logprob": -0.34386026757395166, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.07677313685417175}, {"id": 515, "seek": 398216, "start": 3999.2799999999997, "end": 4004.64, "text": " lo hago a los bestia, digamos, no, para que no me decero le sumo uno, y le sumo ve y se acuerdan", "tokens": [51220, 450, 38721, 257, 1750, 1151, 654, 11, 36430, 11, 572, 11, 1690, 631, 572, 385, 979, 2032, 476, 2408, 78, 8526, 11, 288, 476, 2408, 78, 1241, 288, 369, 696, 5486, 10312, 51488], "temperature": 0.0, "avg_logprob": -0.34386026757395166, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.07677313685417175}, {"id": 516, "seek": 398216, "start": 4004.64, "end": 4008.12, "text": " el nuevo poquito de una clase pasada, le sumo ve para que esto me siga dando una distribuci\u00f3n de", "tokens": [51488, 806, 18591, 28229, 368, 2002, 44578, 1736, 1538, 11, 476, 2408, 78, 1241, 1690, 631, 7433, 385, 4556, 64, 29854, 2002, 4400, 30813, 368, 51662], "temperature": 0.0, "avg_logprob": -0.34386026757395166, "compression_ratio": 1.6391304347826088, "no_speech_prob": 0.07677313685417175}, {"id": 517, "seek": 400812, "start": 4008.12, "end": 4024.52, "text": " probabilidad, esto es simplemente lo que hace es calcular un contador ajustado,", "tokens": [50364, 31959, 4580, 11, 7433, 785, 33190, 450, 631, 10032, 785, 2104, 17792, 517, 660, 5409, 41023, 1573, 11, 51184], "temperature": 0.0, "avg_logprob": -0.5588770847694546, "compression_ratio": 1.4126984126984128, "no_speech_prob": 0.01316373236477375}, {"id": 518, "seek": 400812, "start": 4026.2799999999997, "end": 4034.2, "text": " me explica por t\u00e9 y divide por temas, si me explica por el juvenil y divide por esto, por el PWI,", "tokens": [51272, 385, 1490, 2262, 1515, 19809, 288, 9845, 1515, 40284, 11, 1511, 385, 1490, 2262, 1515, 806, 32641, 388, 288, 9845, 1515, 7433, 11, 1515, 806, 46375, 40, 11, 51668], "temperature": 0.0, "avg_logprob": -0.5588770847694546, "compression_ratio": 1.4126984126984128, "no_speech_prob": 0.01316373236477375}, {"id": 519, "seek": 403812, "start": 4038.6, "end": 4048.04, "text": " por ejemplo, si yo digo, si este es mi corpo entrenamiento, esta es la historia de un hombre y la", "tokens": [50388, 1515, 13358, 11, 1511, 5290, 22990, 11, 1511, 4065, 785, 2752, 23257, 45069, 16971, 11, 5283, 785, 635, 18385, 368, 517, 26102, 288, 635, 50860], "temperature": 0.0, "avg_logprob": -0.3440713882446289, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.010427700355648994}, {"id": 520, "seek": 403812, "start": 4048.04, "end": 4061.88, "text": " ciudad que cre\u00f3, f\u00edjense que mi conteo da uno, la habla y quiso me da cero, perd\u00f3n, este es el", "tokens": [50860, 24329, 631, 1197, 812, 11, 283, 870, 73, 1288, 631, 2752, 34444, 78, 1120, 8526, 11, 635, 42135, 288, 421, 19227, 385, 1120, 269, 2032, 11, 12611, 1801, 11, 4065, 785, 806, 51552], "temperature": 0.0, "avg_logprob": -0.3440713882446289, "compression_ratio": 1.4130434782608696, "no_speech_prob": 0.010427700355648994}, {"id": 521, "seek": 406188, "start": 4061.88, "end": 4068.2400000000002, "text": " conteo, ah\u00ed va, conteo de este es uno, de la es dos y de quiso es cero, la probabilidad de este es", "tokens": [50364, 34444, 78, 11, 12571, 2773, 11, 34444, 78, 368, 4065, 785, 8526, 11, 368, 635, 785, 4491, 288, 368, 421, 19227, 785, 269, 2032, 11, 635, 31959, 4580, 368, 4065, 785, 50682], "temperature": 0.0, "avg_logprob": -0.24577855663139278, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.004054162185639143}, {"id": 522, "seek": 406188, "start": 4068.2400000000002, "end": 4076.6400000000003, "text": " uno y divide 13, total de palabras, una es esta y es 0 0 8, la es 2 divide 13 y quiso me da cero en la", "tokens": [50682, 8526, 288, 9845, 3705, 11, 3217, 368, 35240, 11, 2002, 785, 5283, 288, 785, 1958, 1958, 1649, 11, 635, 785, 568, 9845, 3705, 288, 421, 19227, 385, 1120, 269, 2032, 465, 635, 51102], "temperature": 0.0, "avg_logprob": -0.24577855663139278, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.004054162185639143}, {"id": 523, "seek": 406188, "start": 4076.6400000000003, "end": 4082.96, "text": " probabilidad que nos queremos que nos da cero, si nosotros aplicamos la plaza, lo que me da es", "tokens": [51102, 31959, 4580, 631, 3269, 26813, 631, 3269, 1120, 269, 2032, 11, 1511, 13863, 18221, 2151, 635, 499, 12257, 11, 450, 631, 385, 1120, 785, 51418], "temperature": 0.0, "avg_logprob": -0.24577855663139278, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.004054162185639143}, {"id": 524, "seek": 406188, "start": 4082.96, "end": 4090.1600000000003, "text": " sumo 25, son 12 palabras en el vocabulario, porque la unidad est\u00e1 repetida es la", "tokens": [51418, 2408, 78, 3552, 11, 1872, 2272, 35240, 465, 806, 2329, 455, 1040, 1004, 11, 4021, 635, 517, 4580, 3192, 13645, 2887, 785, 635, 51778], "temperature": 0.0, "avg_logprob": -0.24577855663139278, "compression_ratio": 1.7546296296296295, "no_speech_prob": 0.004054162185639143}, {"id": 525, "seek": 409188, "start": 4091.96, "end": 4102.88, "text": " s\u00ed, o sea que tengo 12 en el vocabulario no 13, 13 es T y 12 es B, entonces ya hago 2 divide 25 y as\u00ed me da", "tokens": [50368, 8600, 11, 277, 4158, 631, 13989, 2272, 465, 806, 2329, 455, 1040, 1004, 572, 3705, 11, 3705, 785, 314, 288, 2272, 785, 363, 11, 13003, 2478, 38721, 568, 9845, 3552, 288, 8582, 385, 1120, 50914], "temperature": 0.0, "avg_logprob": -0.2903498531727309, "compression_ratio": 1.495049504950495, "no_speech_prob": 0.0014287037774920464}, {"id": 526, "seek": 409188, "start": 4102.88, "end": 4110.24, "text": " las nuevas probabilidades y ac\u00e1 quiso dejar de ser cero, el contador ajustado de lo que nos permite", "tokens": [50914, 2439, 42817, 31959, 10284, 288, 23496, 421, 19227, 24391, 368, 816, 269, 2032, 11, 806, 660, 5409, 41023, 1573, 368, 450, 631, 3269, 31105, 51282], "temperature": 0.0, "avg_logprob": -0.2903498531727309, "compression_ratio": 1.495049504950495, "no_speech_prob": 0.0014287037774920464}, {"id": 527, "seek": 409188, "start": 4110.24, "end": 4115.76, "text": " es comparar lo que ten\u00edamos antes con lo que ten\u00edamos ahora, por ejemplo, esta val\u00eda 1 y", "tokens": [51282, 785, 6311, 289, 450, 631, 2064, 16275, 11014, 416, 450, 631, 2064, 16275, 9923, 11, 1515, 13358, 11, 5283, 1323, 2686, 502, 288, 51558], "temperature": 0.0, "avg_logprob": -0.2903498531727309, "compression_ratio": 1.495049504950495, "no_speech_prob": 0.0014287037774920464}, {"id": 528, "seek": 411576, "start": 4115.76, "end": 4135.320000000001, "text": " baja a 0 96, perd\u00f3n, la val\u00eda 2 y baja a 1 44 y quiso va a de 0 a 0 48, si se fijan ac\u00e1 el", "tokens": [50364, 49427, 257, 1958, 24124, 11, 12611, 1801, 11, 635, 1323, 2686, 568, 288, 49427, 257, 502, 16408, 288, 421, 19227, 2773, 257, 368, 1958, 257, 1958, 11174, 11, 1511, 369, 42001, 282, 23496, 806, 51342], "temperature": 0.0, "avg_logprob": -0.3640979528427124, "compression_ratio": 1.3146853146853146, "no_speech_prob": 0.0018281358061358333}, {"id": 529, "seek": 411576, "start": 4135.320000000001, "end": 4141.400000000001, "text": " descuento, lo que se llama descuento que es la divisi\u00f3n entre los dos valores, lo permite ver", "tokens": [51342, 7471, 84, 15467, 11, 450, 631, 369, 23272, 7471, 84, 15467, 631, 785, 635, 25974, 2560, 3962, 1750, 4491, 38790, 11, 450, 31105, 1306, 51646], "temperature": 0.0, "avg_logprob": -0.3640979528427124, "compression_ratio": 1.3146853146853146, "no_speech_prob": 0.0018281358061358333}, {"id": 530, "seek": 414140, "start": 4141.4, "end": 4152.96, "text": " que le estoy sacando m\u00e1s masa de probabilidad a la que hay que quedar casi igual, es decir, la", "tokens": [50364, 631, 476, 15796, 4899, 1806, 3573, 29216, 368, 31959, 4580, 257, 635, 631, 4842, 631, 39244, 22567, 10953, 11, 785, 10235, 11, 635, 50942], "temperature": 0.0, "avg_logprob": -0.4246725634993794, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.014671919867396355}, {"id": 531, "seek": 414140, "start": 4152.96, "end": 4157.879999999999, "text": " meta le la tiene a la plaza el problema, por qu\u00e9 es lo que est\u00e1 pasando ac\u00e1, esto es lo que me", "tokens": [50942, 19616, 476, 635, 7066, 257, 635, 499, 12257, 806, 12395, 11, 1515, 8057, 785, 450, 631, 3192, 45412, 23496, 11, 7433, 785, 450, 631, 385, 51188], "temperature": 0.0, "avg_logprob": -0.4246725634993794, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.014671919867396355}, {"id": 532, "seek": 414140, "start": 4157.879999999999, "end": 4165.96, "text": " muestra es que yo le tengo que sacar masa de probabilidad a los que aparecen, porque todo me", "tokens": [51188, 2992, 32036, 785, 631, 5290, 476, 13989, 631, 43823, 29216, 368, 31959, 4580, 257, 1750, 631, 15004, 13037, 11, 4021, 5149, 385, 51592], "temperature": 0.0, "avg_logprob": -0.4246725634993794, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.014671919867396355}, {"id": 533, "seek": 414140, "start": 4165.96, "end": 4170.599999999999, "text": " tiene que sumar 1, toda la probabilidad me tiene que sumar 1, si yo ya agregar 5 gramas que antes", "tokens": [51592, 7066, 631, 2408, 289, 502, 11, 11687, 635, 31959, 4580, 385, 7066, 631, 2408, 289, 502, 11, 1511, 5290, 2478, 4554, 2976, 1025, 677, 19473, 631, 11014, 51824], "temperature": 0.0, "avg_logprob": -0.4246725634993794, "compression_ratio": 1.8461538461538463, "no_speech_prob": 0.014671919867396355}, {"id": 534, "seek": 417060, "start": 4170.6, "end": 4177.120000000001, "text": " estaban en cero, tengo que sacarle probabilidad a los que est\u00e1, pues no me es un mam\u00e1 que 1, entonces", "tokens": [50364, 36713, 465, 269, 2032, 11, 13989, 631, 4899, 36153, 31959, 4580, 257, 1750, 631, 3192, 11, 11059, 572, 385, 785, 517, 13524, 842, 631, 502, 11, 13003, 50690], "temperature": 0.0, "avg_logprob": -0.3193146160670689, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.0034658650401979685}, {"id": 535, "seek": 417060, "start": 4177.120000000001, "end": 4184.6, "text": " esto es lo que tiene que castiga mucho a los m\u00e1s frecuentes, le sacan mucho probabilidad a los", "tokens": [50690, 7433, 785, 450, 631, 7066, 631, 4193, 9900, 9824, 257, 1750, 3573, 2130, 12032, 9240, 11, 476, 4899, 282, 9824, 31959, 4580, 257, 1750, 51064], "temperature": 0.0, "avg_logprob": -0.3193146160670689, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.0034658650401979685}, {"id": 536, "seek": 417060, "start": 4184.6, "end": 4190.88, "text": " m\u00e1s frecuentes y como que premia demasiado a los que no aparecen, hay otras t\u00e9cnicas no, no,", "tokens": [51064, 3573, 2130, 12032, 9240, 288, 2617, 631, 5624, 654, 39820, 257, 1750, 631, 572, 15004, 13037, 11, 4842, 20244, 25564, 40672, 572, 11, 572, 11, 51378], "temperature": 0.0, "avg_logprob": -0.3193146160670689, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.0034658650401979685}, {"id": 537, "seek": 419088, "start": 4190.88, "end": 4197.32, "text": " vamos a entrar en eso, que tratan de ajustarlo un poco mejor, pero ahora vamos a mover alguna", "tokens": [50364, 5295, 257, 20913, 465, 7287, 11, 631, 21507, 282, 368, 41023, 19457, 517, 10639, 11479, 11, 4768, 9923, 5295, 257, 39945, 20651, 50686], "temperature": 0.0, "avg_logprob": -0.40906958830984014, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.007228686474263668}, {"id": 538, "seek": 419088, "start": 4201.96, "end": 4213.08, "text": " muy demasiada probabilidad, otra posibilidad es usar un delta en lugar de 1 y ese delta", "tokens": [50918, 5323, 35259, 1538, 31959, 4580, 11, 13623, 1366, 33989, 785, 14745, 517, 8289, 465, 11467, 368, 502, 288, 10167, 8289, 51474], "temperature": 0.0, "avg_logprob": -0.40906958830984014, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.007228686474263668}, {"id": 539, "seek": 419088, "start": 4213.08, "end": 4218.04, "text": " te va a calcularlo, se acuerdan lo que hablamos del cuerpo, siempre que yo tengo esos par\u00e1metros", "tokens": [51474, 535, 2773, 257, 2104, 17792, 752, 11, 369, 696, 5486, 10312, 450, 631, 26280, 2151, 1103, 20264, 11, 12758, 631, 5290, 13989, 22411, 971, 842, 29570, 51722], "temperature": 0.0, "avg_logprob": -0.40906958830984014, "compression_ratio": 1.532967032967033, "no_speech_prob": 0.007228686474263668}, {"id": 540, "seek": 421804, "start": 4218.04, "end": 4230.8, "text": " para calcular los calculos sobre el cuerpo de desarrollo, finalmente hay otro, esa es una", "tokens": [50364, 1690, 2104, 17792, 1750, 4322, 329, 5473, 806, 20264, 368, 38295, 11, 35577, 4842, 11921, 11, 11342, 785, 2002, 51002], "temperature": 0.0, "avg_logprob": -0.30099309815300834, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0014602674636989832}, {"id": 541, "seek": 421804, "start": 4230.8, "end": 4236.56, "text": " aproximaci\u00f3n, es decir, con t\u00e9cnicas sobre el contencio, hay otra posibilidad que son un poco", "tokens": [51002, 31270, 3482, 11, 785, 10235, 11, 416, 25564, 40672, 5473, 806, 21795, 8529, 11, 4842, 13623, 1366, 33989, 631, 1872, 517, 10639, 51290], "temperature": 0.0, "avg_logprob": -0.30099309815300834, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0014602674636989832}, {"id": 542, "seek": 421804, "start": 4236.56, "end": 4245.2, "text": " m\u00e1s evolucios avanzadas, digamos que es, cuando yo quiero estimar, por ejemplo en t\u00e9cnicas de", "tokens": [51290, 3573, 7117, 84, 23132, 42444, 6872, 11, 36430, 631, 785, 11, 7767, 5290, 16811, 8017, 289, 11, 1515, 13358, 465, 25564, 40672, 368, 51722], "temperature": 0.0, "avg_logprob": -0.30099309815300834, "compression_ratio": 1.543956043956044, "no_speech_prob": 0.0014602674636989832}, {"id": 543, "seek": 424520, "start": 4245.2, "end": 4255.44, "text": " trigrama, una palabra, a partir de las dos anteriores y no existen casos de las dos anteriores", "tokens": [50364, 35386, 29762, 11, 2002, 31702, 11, 257, 13906, 368, 2439, 4491, 364, 34345, 2706, 288, 572, 2514, 268, 25135, 368, 2439, 4491, 364, 34345, 2706, 50876], "temperature": 0.0, "avg_logprob": -0.3663661790930707, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.005423149559646845}, {"id": 544, "seek": 424520, "start": 4255.44, "end": 4267.96, "text": " en el texto, de las dos anteriores seguida doble, \u00bfno? Ac\u00e1 es doble, perd\u00f3n, lo que hago es hacer lo", "tokens": [50876, 465, 806, 35503, 11, 368, 2439, 4491, 364, 34345, 2706, 8878, 2887, 360, 638, 11, 3841, 1771, 30, 5097, 842, 785, 360, 638, 11, 12611, 1801, 11, 450, 631, 38721, 785, 6720, 450, 51502], "temperature": 0.0, "avg_logprob": -0.3663661790930707, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.005423149559646845}, {"id": 545, "seek": 424520, "start": 4267.96, "end": 4273.5599999999995, "text": " que se llama BACOF, hacia calcularlo a trav\u00e9s de la probabilidad de la anterior, si no tengo la", "tokens": [51502, 631, 369, 23272, 363, 4378, 46, 37, 11, 21365, 2104, 17792, 752, 257, 24463, 368, 635, 31959, 4580, 368, 635, 22272, 11, 1511, 572, 13989, 635, 51782], "temperature": 0.0, "avg_logprob": -0.3663661790930707, "compression_ratio": 1.6032608695652173, "no_speech_prob": 0.005423149559646845}, {"id": 546, "seek": 427356, "start": 4273.56, "end": 4284.080000000001, "text": " anterior prueba con la anterior, eso llamas BACOF, el BACOF, ten\u00e9s que resolver tambi\u00e9n que ahora", "tokens": [50364, 22272, 48241, 416, 635, 22272, 11, 7287, 16848, 296, 363, 4378, 46, 37, 11, 806, 363, 4378, 46, 37, 11, 2064, 2191, 631, 34480, 6407, 631, 9923, 50890], "temperature": 0.0, "avg_logprob": -0.3510265350341797, "compression_ratio": 1.5567567567567568, "no_speech_prob": 0.0005910666659474373}, {"id": 547, "seek": 427356, "start": 4284.080000000001, "end": 4289.400000000001, "text": " otra vez est\u00e1 introduciendo en nuevas, luego caso que no ten\u00edas antes, estas probabilidades", "tokens": [50890, 13623, 5715, 3192, 2814, 16830, 465, 42817, 11, 17222, 9666, 631, 572, 2064, 10025, 11014, 11, 13897, 31959, 10284, 51156], "temperature": 0.0, "avg_logprob": -0.3510265350341797, "compression_ratio": 1.5567567567567568, "no_speech_prob": 0.0005910666659474373}, {"id": 548, "seek": 427356, "start": 4289.400000000001, "end": 4297.92, "text": " que calcularle y darle masa de probabilidad, otra vez tengo que mover probabilidad, cuando los", "tokens": [51156, 631, 2104, 17792, 306, 288, 37666, 29216, 368, 31959, 4580, 11, 13623, 5715, 13989, 631, 39945, 31959, 4580, 11, 7767, 1750, 51582], "temperature": 0.0, "avg_logprob": -0.3510265350341797, "compression_ratio": 1.5567567567567568, "no_speech_prob": 0.0005910666659474373}, {"id": 549, "seek": 429792, "start": 4297.92, "end": 4306.4400000000005, "text": " corpos son muy muy grandes, una forma alternativa y es un m\u00e9todo muy nuevo, se llama Stupid BACOF,", "tokens": [50364, 1181, 30010, 1872, 5323, 5323, 16640, 11, 2002, 8366, 5400, 18740, 288, 785, 517, 20275, 17423, 5323, 18591, 11, 369, 23272, 37659, 363, 4378, 46, 37, 11, 50790], "temperature": 0.0, "avg_logprob": -0.387271432315602, "compression_ratio": 1.4773869346733668, "no_speech_prob": 0.045330628752708435}, {"id": 550, "seek": 429792, "start": 4306.4400000000005, "end": 4313.56, "text": " que es como mi corpos muy grande, t\u00edpicamente el corpos de Google, es no normalizo nada de las", "tokens": [50790, 631, 785, 2617, 2752, 1181, 30010, 5323, 8883, 11, 256, 28236, 23653, 806, 1181, 30010, 368, 3329, 11, 785, 572, 2710, 19055, 8096, 368, 2439, 51146], "temperature": 0.0, "avg_logprob": -0.387271432315602, "compression_ratio": 1.4773869346733668, "no_speech_prob": 0.045330628752708435}, {"id": 551, "seek": 429792, "start": 4313.56, "end": 4319.28, "text": " probabilidades, este conteo, no m\u00e1s como me fue, si una no me da prueba con la anterior, si igual", "tokens": [51146, 31959, 10284, 11, 4065, 34444, 78, 11, 572, 3573, 2617, 385, 9248, 11, 1511, 2002, 572, 385, 1120, 48241, 416, 635, 22272, 11, 1511, 10953, 51432], "temperature": 0.0, "avg_logprob": -0.387271432315602, "compression_ratio": 1.4773869346733668, "no_speech_prob": 0.045330628752708435}, {"id": 552, "seek": 431928, "start": 4319.28, "end": 4330.12, "text": " tengo un mont\u00f3n de edad, o tambi\u00e9n se puede hacer interpolaci\u00f3n, es decir, la probabilidad", "tokens": [50364, 13989, 517, 45259, 368, 1257, 345, 11, 277, 6407, 369, 8919, 6720, 44902, 3482, 11, 785, 10235, 11, 635, 31959, 4580, 50906], "temperature": 0.0, "avg_logprob": -0.24171864146917638, "compression_ratio": 1.9285714285714286, "no_speech_prob": 0.020114410668611526}, {"id": 553, "seek": 431928, "start": 4330.12, "end": 4340.4, "text": " de una palabra daba las dos anteriores, es la probabilidad de la palabra, la probabilidad", "tokens": [50906, 368, 2002, 31702, 274, 5509, 2439, 4491, 364, 34345, 2706, 11, 785, 635, 31959, 4580, 368, 635, 31702, 11, 635, 31959, 4580, 51420], "temperature": 0.0, "avg_logprob": -0.24171864146917638, "compression_ratio": 1.9285714285714286, "no_speech_prob": 0.020114410668611526}, {"id": 554, "seek": 431928, "start": 4340.4, "end": 4345.24, "text": " nueva, es la probabilidad original de la palabra daba las dos anteriores por un cierto", "tokens": [51420, 28963, 11, 785, 635, 31959, 4580, 3380, 368, 635, 31702, 274, 5509, 2439, 4491, 364, 34345, 2706, 1515, 517, 28558, 51662], "temperature": 0.0, "avg_logprob": -0.24171864146917638, "compression_ratio": 1.9285714285714286, "no_speech_prob": 0.020114410668611526}, {"id": 555, "seek": 434524, "start": 4345.28, "end": 4351.28, "text": " lambda, un cierto lambda 2 por la probabilidad de la palabra daba el sol en el vigrama, m\u00e1s la", "tokens": [50366, 13607, 11, 517, 28558, 13607, 568, 1515, 635, 31959, 4580, 368, 635, 31702, 274, 5509, 806, 1404, 465, 806, 15366, 29762, 11, 3573, 635, 50666], "temperature": 0.0, "avg_logprob": -0.2933434540370725, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.014697718434035778}, {"id": 556, "seek": 434524, "start": 4351.28, "end": 4357.639999999999, "text": " probabilidad de un vigrama, y convino las tres a la vez, es como convino las tres t\u00ednias a la", "tokens": [50666, 31959, 4580, 368, 517, 15366, 29762, 11, 288, 3754, 2982, 2439, 15890, 257, 635, 5715, 11, 785, 2617, 3754, 2982, 2439, 15890, 256, 870, 3722, 296, 257, 635, 50984], "temperature": 0.0, "avg_logprob": -0.2933434540370725, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.014697718434035778}, {"id": 557, "seek": 434524, "start": 4357.639999999999, "end": 4365.719999999999, "text": " vez, es decir, le doy un cierto peso a las probabilidades que yo quiero, de esta forma,", "tokens": [50984, 5715, 11, 785, 10235, 11, 476, 360, 88, 517, 28558, 28149, 257, 2439, 31959, 10284, 631, 5290, 16811, 11, 368, 5283, 8366, 11, 51388], "temperature": 0.0, "avg_logprob": -0.2933434540370725, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.014697718434035778}, {"id": 558, "seek": 434524, "start": 4365.719999999999, "end": 4370.5599999999995, "text": " porque ac\u00e1 podr\u00eda ser que existiera el vigrama anterior, pero existiera una vez sola, entonces", "tokens": [51388, 4021, 23496, 27246, 816, 631, 2514, 10609, 806, 15366, 29762, 22272, 11, 4768, 2514, 10609, 2002, 5715, 1404, 64, 11, 13003, 51630], "temperature": 0.0, "avg_logprob": -0.2933434540370725, "compression_ratio": 1.8564356435643565, "no_speech_prob": 0.014697718434035778}, {"id": 559, "seek": 437056, "start": 4370.56, "end": 4376.04, "text": " yo no le tengo mucha confianza a esa, puede sucederme y no le tengo mucha confianza, entonces", "tokens": [50364, 5290, 572, 476, 13989, 25248, 49081, 2394, 257, 11342, 11, 8919, 41928, 31081, 288, 572, 476, 13989, 25248, 49081, 2394, 11, 13003, 50638], "temperature": 0.0, "avg_logprob": -0.36868865673358625, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.1114836186170578}, {"id": 560, "seek": 437056, "start": 4376.04, "end": 4379.8, "text": " le doy un cierto peso a este tambi\u00e9n, y capa que le doy un peso un poquito m\u00e1s alto a este,", "tokens": [50638, 476, 360, 88, 517, 28558, 28149, 257, 4065, 6407, 11, 288, 1410, 64, 631, 476, 360, 88, 517, 28149, 517, 28229, 3573, 21275, 257, 4065, 11, 50826], "temperature": 0.0, "avg_logprob": -0.36868865673358625, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.1114836186170578}, {"id": 561, "seek": 437056, "start": 4379.8, "end": 4384.6, "text": " o sea, si este existe, est\u00e1 todo bien, pero este es siempre una ayuda, y de esa forma", "tokens": [50826, 277, 4158, 11, 1511, 4065, 16304, 11, 3192, 5149, 3610, 11, 4768, 4065, 785, 12758, 2002, 30737, 11, 288, 368, 11342, 8366, 51066], "temperature": 0.0, "avg_logprob": -0.36868865673358625, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.1114836186170578}, {"id": 562, "seek": 437056, "start": 4384.6, "end": 4394.080000000001, "text": " balanceo, como calculo esto es lambda y con el corpos de valo, tengo que, de alguna forma", "tokens": [51066, 4772, 78, 11, 2617, 4322, 78, 7433, 785, 13607, 288, 416, 806, 1181, 30010, 368, 1323, 78, 11, 13989, 631, 11, 368, 20651, 8366, 51540], "temperature": 0.0, "avg_logprob": -0.36868865673358625, "compression_ratio": 1.7251184834123223, "no_speech_prob": 0.1114836186170578}, {"id": 563, "seek": 439408, "start": 4394.08, "end": 4403.76, "text": " calcularlo sobre el cuerpo de desarrollo, o el cuerpo gelado, tambi\u00e9n hay interpolaci\u00f3n", "tokens": [50364, 2104, 17792, 752, 5473, 806, 20264, 368, 38295, 11, 277, 806, 20264, 4087, 1573, 11, 6407, 4842, 44902, 3482, 50848], "temperature": 0.0, "avg_logprob": -0.25755732267805675, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.013470038771629333}, {"id": 564, "seek": 439408, "start": 4405.76, "end": 4411.36, "text": " condicionada por el contexto, o sea, hay un lambda, ac\u00e1 ya lo que pasa es un poco m\u00e1s raro,", "tokens": [50948, 2224, 18899, 1538, 1515, 806, 47685, 11, 277, 4158, 11, 4842, 517, 13607, 11, 23496, 2478, 450, 631, 20260, 785, 517, 10639, 3573, 367, 9708, 11, 51228], "temperature": 0.0, "avg_logprob": -0.25755732267805675, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.013470038771629333}, {"id": 565, "seek": 439408, "start": 4411.36, "end": 4416.64, "text": " y un poco m\u00e1s moderno, digamos que es que m\u00e1s de estas \u00e9pocas, digamos, donde a m\u00ed ya no me", "tokens": [51228, 288, 517, 10639, 3573, 4363, 78, 11, 36430, 631, 785, 631, 3573, 368, 13897, 21018, 905, 296, 11, 36430, 11, 10488, 257, 14692, 2478, 572, 385, 51492], "temperature": 0.0, "avg_logprob": -0.25755732267805675, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.013470038771629333}, {"id": 566, "seek": 439408, "start": 4416.64, "end": 4421.5199999999995, "text": " preocupa tanto tener muchos par\u00e1metros, ac\u00e1 estoy definiendo un par\u00e1metro para cada combinaci\u00f3n de", "tokens": [51492, 23080, 64, 10331, 11640, 17061, 971, 842, 29570, 11, 23496, 15796, 1561, 7304, 517, 971, 842, 45400, 1690, 8411, 38514, 3482, 368, 51736], "temperature": 0.0, "avg_logprob": -0.25755732267805675, "compression_ratio": 1.6828193832599119, "no_speech_prob": 0.013470038771629333}, {"id": 567, "seek": 442152, "start": 4421.52, "end": 4437.320000000001, "text": " palabras, y hasta aqu\u00ed llegamos hoy, esto es este cap\u00edtulo que tengo ac\u00e1, cap\u00edtulo 4 del libro", "tokens": [50364, 35240, 11, 288, 10764, 6661, 11234, 2151, 13775, 11, 7433, 785, 4065, 1410, 30389, 631, 13989, 23496, 11, 1410, 30389, 1017, 1103, 29354, 51154], "temperature": 0.0, "avg_logprob": -0.27403568803218376, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.0046990495175123215}, {"id": 568, "seek": 442152, "start": 4437.320000000001, "end": 4446.0, "text": " Yurazki, tiene algunas cositas m\u00e1s, presencialmente es eso, y es lo que vamos a hablar de en este curso", "tokens": [51154, 398, 374, 921, 2984, 11, 7066, 27316, 3792, 14182, 3573, 11, 1183, 26567, 4082, 785, 7287, 11, 288, 785, 450, 631, 5295, 257, 21014, 368, 465, 4065, 31085, 51588], "temperature": 0.0, "avg_logprob": -0.27403568803218376, "compression_ratio": 1.380952380952381, "no_speech_prob": 0.0046990495175123215}, {"id": 569, "seek": 444600, "start": 4446.0, "end": 4450.76, "text": " de Nigrama, la clase que viene, presentamos la baratocha.", "tokens": [50364, 368, 426, 328, 29762, 11, 635, 44578, 631, 19561, 11, 1974, 2151, 635, 2159, 2513, 4413, 13, 50602], "temperature": 0.0, "avg_logprob": -0.673343276977539, "compression_ratio": 0.9047619047619048, "no_speech_prob": 0.22439910471439362}], "language": "es"}