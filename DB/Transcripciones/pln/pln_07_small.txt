En la clase de hoy, vamos a ver un tema nuevo que es el de los modelos de lenguaje.
Si se acuerdan a la clase pasada, vimos dos temas que eran bastante diferentes.
El de los traductores para resolver el tema de la morfología de estado finito, unos artefactos
de estado finito que permiten resolver temas a través de un método de reglas.
Y de esa forma resuelvo el tema de convertir de la palabra a su análisis y viceversa.
En la segunda parte vimos un método que era bastante diferente de su concepción,
que es su método estadístico, que lo que hace era aplicando el modelo del canal ruidoso,
aproximarse al problema de corregir errores ortográficos.
Cuando yo hablo de un modelo probabilista, lo que estoy diciendo es que además de, por ejemplo,
clasificar o sugerir una solución, lo que hace es asignarle probabilidades a las posibles respuestas.
Un método probabilista, típicamente no da una respuesta, sino que devuelve una distribución de probabilidad.
Es decir, si yo tengo varios eventos posibles, una distribución de probabilidad es un número
entre 0 y 1 que yo asigno a cada evento posible, de forma que la suma de todos los eventos dan 1,
eso es lo que llamamos una distribución de probabilidad.
Entre 0 y 1 son todos, son todos mayores o iguales que 0, menores y iguales que 1,
y además su suma da 1, eso es una distribución de probabilidad.
0, 5, 0, 25, 0, 25 es una distribución de probabilidad.
Si el evento 1 tiene probabilidad 0, 5, el otro 0, 25, y el otro 0, 25, eso es una distribución de probabilidad.
Si no suma a un 1, no son una distribución de probabilidad.
Y si yo, por ejemplo, tengo un evento que ocurre 10 veces, si por ejemplo hago conteo de frecuencia,
por ejemplo, no digo hay un evento 1 que ocurre 10 veces,
hay un evento 2 que ocurre 5 y hay un evento 3 que ocurre 5.
Eso no es una distribución de probabilidad, porque esto no está entre 0 y 1, porque no suman 1.
¿Cómo hago yo para convertir esto en una distribución de probabilidad?
Lo que hago es dividir por el total de ocurrencias, ¿verdad?
Que en este caso es 20 y eso me da la proporción respecto a 1,
y eso es siempre una distribución de probabilidad.
Esto se llama normalizar para obtener una probabilidad.
Esto ustedes lo van a ver que lo vamos a ver en varias veces.
El método de este corrección utilizaba fuertemente la regla de Valles para modelar la situación.
Hasta ahora hemos hablado en todas las cosas que hemos tratado de palabras aisladas.
La morfología estudia, primero hablamos de cómo separar las palabras
y después vimos cómo analizarla internamente, pero siempre hablábamos de palabras aisladas.
Acá lo que vamos a empezar a mirar es qué pasa cuando las palabras aparecen juntas.
Es decir, nosotros lo que vamos a hablar es de la probabilidad de una secuencia de palabras.
¿Por qué esto importa?
Porque, como ustedes bien sabrán, las palabras en el idioma pañón las aparecen solas.
Y no cualquier palabra sigue a otra palabra.
Nosotros tenemos una cantidad de reglas para expresar en el idioma que hace que el orden importe.
Es decir, lo que se trata es de ver cómo tener en cuenta ese orden nos puede ayudar a otra estaria.
Creo que con algún ejemplo lo vamos a ver más claro.
Primero que nada, vamos a recordar a Chonky, que esto yo lo comentaba en la primera clase,
aquello de que Chonky dijo la noción de probabilidad de una oración es completamente inútil bajo cualquier interpretación de este término.
Y trancó por 20 años, la investigación hasta acá apareció.
Chelline, que volvió a revivir el tema de los métodos probabilistas o basados en conteos para aproximarse a los problemas de procesamiento en el lenguaje natural.
Chonky lo que decía esencialmente es cuando nosotros lo hacemos con teo y sacamos conclusión en base a cuenta, en base a números, en base a experiencia,
que es típicamente lo que vamos a ver en este caso de los enegramas,
estamos obteniendo soluciones a problemas, pero no estamos entendiendo qué es lo que está pasando.
Y eso es una discusión catalida de hoy sí.
Es decir, hay una famosa discusión por ahí en internet entre Chonky,
esto te hablando hace dos o tres años, o cinco años, entre Chonky y Peter Norby,
que discute un poco esto.
Es decir, si esto que estamos haciendo ahora y que ha tenido tan buenos resultados desde el punto de vista de reconocimiento del habla y el procesamiento del lenguaje natural
es en realidad inteligencia artificial o es solamente number crunching que no nos aporta mucho.
Norby lo que le dice es bueno, de hecho la ciencia es siempre más o menos funcionada así.
Bueno, entonces ¿cuál es el objetivo de lo que vamos a hablar acá?
Son de modelos del lenguaje.
El objetivo del modelo del lenguaje es calcular la probabilidad de una secuencia de palabra.
Es decir, qué tan probable es en mil lenguajes que una secuencia se dé.
¿De acuerdo?
¿Para qué nos puede servir eso?
Bueno, imagínense ustedes que, y acá vamos a recordar otra vez el modelo del canal ruidoso, de la otra vez,
imagínense ustedes que tengo este texto escrito y por medio de un método que no sé cuál es.
Tengo dos oraciones candidatas, ¿de acuerdo?
Dos textos candidatos.
Uno que es PRNEVA para el curso de PLN y PREVA para el curso de PLN.
¿De acuerdo?
Y además supongamos que el método que utilicé para reconocer la escritura
me dice que este es más probable que este.
¿Cuál vamos a elegir?
¿Cuál vamos a elegir?
Vamos a elegir el de abajo.
¿Por qué?
¿Por qué esto no es una palabra válida?
Pero aún siendo una palabra válida, o aún suponiendo que fuera una palabra válida,
podría darse un caso donde yo identifico una palabra válida, ¿se acuerdan lo corrección?
Aún así, yo podría decir, bueno, pero en este lugar,
esa palabra no calza, digan.
Si de alguna forma yo sé.
Es decir, si yo logro detectar que esta oración es más probable que esta,
de alguna forma, eso me va a ayudar en la tarea de reconocimiento.
Lo mismo pasa con el reconocimiento del habla,
de lo que hablamos el otro día con el speed recognition,
y cuando yo hablo y digo una palabra, ustedes me escuchan.
Entonces, los modelos de lenguaje sirven para ayudar en este tipo de tarea.
Típicamente los modelos de lenguaje ayudan en otra tarea.
Nos agregan mucha información.
Entonces, cuando nosotros hacemos reconocimiento de escritura,
un poco lo que decimos es,
¿Cuál es la probabilidad de la oración origen dada la observación que tengo?
Yo tengo una observación, ¿sí?
¿Cuál es la probabilidad de una oración origen?
Es proporcionar a la probabilidad de la observación dada la oración
por la probabilidad de la oración.
¿Y esto qué es?
Eso es valles, es la regla de valles.
Entonces, nosotros por valles sabemos eso,
y como ven, acá aparece la noción de probabilidad de la oración.
Por eso es que nos interesa conocer la probabilidad de la oración.
Ahora, ¿Cómo calculamos la probabilidad de la oración?
Bueno, hay algún ejemplo más, ¿no?
Por ejemplo, en la traducción autonática,
en la traducción autonática, si tenemos estos tres candidatos,
nuevamente a mí me va a ayudar conocer el orden
o saber cuál es la más probable en mi lenguaje.
En la corrección de errores, como vimos en la vez pasada,
hordas de botero es una secuencia muy de poca probabilidad,
y pensemos un poquito,
¿Preguntemos no?
¿Por qué esta oración no les parece que sea muy probable?
¿Qué nos podría determinar que esta oración no es muy probable?
¿O esta? ¿Implementación a la educación ley?
¿Por qué podemos suponer que esa no es probable?
Bueno, a mí se me ocurren dos razones principales,
dos aproximaciones, una es por la sintaxis, ¿no?
La sintaxis del idioma pañón no es así.
¿Nos decimos educación ley, educación que...?
En la segunda, porque no publicamos la procesión.
¿Por qué no qué?
En la procesión, porque si tenemos sus y de botero,
estamos publicando, ¿verdad?
Ah, bueno, pero ese podría ser un sus de un tercero, ¿no?
Acá seguramente lo que hay es un error toráfico de sus hordas de botero.
O sea, acá tenemos un tema de sintaxis.
Acá no tenemos un tema de sintaxis.
Deberíamos conocer un poco de semántica para asociar botero,
que pintaba mujeres gordas y entonces...
O una aproximación un poco más humilde,
que es la segunda, es la una aproximación más detalística,
porque si nosotros...
y que juega con el hecho de que tenemos grandes volúmenes de texto
y de ahí el cambio de los modelos probabilísticos,
es que sus gordas de botero seguramente apareció
antes en mis corpus de texto
y hordas de botero, no.
Es una aproximación mucho más detalística.
Eso es lo que vamos a hacer en los modelos de diagrama, justamente.
A partir de grandes volúmenes de texto,
detectar e calcular las probabilidades.
Es una aproximación puramente estadística,
es bien salvaje, es.
Yo no sé qué estructura tiene esto,
pero sé que esto no se dio nunca
de botero, sí, muchas veces.
Entonces, es más probable que me haya equivocado.
A ver, relacionado con esto,
ahora vamos a ver por qué está relacionado.
Está el tema de la predicción de la siguiente palabra.
¿Cuáles se imaginan que es la siguiente palabra
a la primera oración?
¿Cuál puede ser la siguiente palabra?
¿Para?
¿Para?
Para.
¿Para?
¿Para?
¿Para?
¿Para?
¿Para es una preposición, no?
¿Qué más?
¿Qué otra cosa puede decir ahí?
¿Cuál, por ejemplo?
¿Un pronóstico alentador?
¿Un pronóstico alentador?
¿O puede decir un pronóstico terrible?
¿Un pronóstico...
¿O qué otra cosa más?
Hay uno más común para mí.
Elmitió un pronóstico meteorológico, no?
¿A raíz de este fenómeno se sucederán tormentas?
Fuertes,
importantes,
muy.
No creo que hay diga tormentas
gatito, ¿no?
Esto no es muy probable que sea la palabra siguiente.
Nuevamente, ¿por qué sabemos esto?
Porque es muy raro que hay un día tormentas gatito, digamos, ¿no?
Entonces,
esto que tenemos acá
es
las posibilidades que hay de siguiente palabra.
Dadas todas las anteriores.
Es decir, yo tengo todo el contexto, lo que se llama contexto,
dado el contexto de la palabra que sigue acá.
¿Sí?
Una de las, lo que nosotros vamos a querer hacer
en un modelo de lenguaje,
como camino para calcular la probabilidad de una oración,
es dado el contexto
calcular la palabra.
Siguiente.
¿Sí?
Rachas de viento fuerte de componente.
Veremos que.
Bueno, resulta ser que de los ejemplos que yo tomé,
ah bueno, puse viento fuerte de componente,
el linómede emitió pronótico especial,
o sea que le ramos,
se sonan tormentas fuertes,
viento fuerte de componente sudo este,
ejemplo, predicción.
Vamos a poner un poquito de notación
antes de que,
antes de seguir,
porque vamos a ver cómo enfrentamos este problema,
es decir, ¿cómo calculamos esa probabilidad?
Un poco de notación para seguir,
yo lo que estoy diciendo es
la probabilidad de que una variable aleatoria
ahí
valga,
tome el valor conocimiento,
en este caso tendría una variable aleatoria
por cada posición en el texto,
¿verdad?
Tengo una X1, que es la primera palabra, aquí dos,
que es la segunda, aquí tres.
Son variables aleatorias, que la variable aleatoria
es un mapeo, es una función que mapea
de un evento, un número entre 0 y 1.
¿La probabilidad de una?
La probabilidad de una.
Perdón.
Perdón.
Bueno, no vamos a entrar en definiciones,
mapea con un real y la probabilidad
me devuelve un número entre 0 y 1,
es decir, yo defino la probabilidad
de una variable aleatoria,
como
la distribución
de probabilidad de una variable aleatoria,
dado los diferentes valores que puede tomar
¿Cuál es el valor de cada uno de ellos?
¿Sí?
Y esto, ¿cuál es el rango?
¿Qué valor es probable que tiene acá
una variable aleatoria
que refiera palabras?
Todo el vocabulario, ¿no?
Todas las palabras diferentes que yo puedo tener.
¿De acuerdo?
Entonces nosotros vamos a poner
en notación probabilidad de conocimiento,
de que la palabra sea conocimiento.
Vamos a denotar W1n
1n
a la secuencia
de palabras W1
W2
Wn, por ejemplo, en una eración
y vamos a decir
vamos a decir que vamos a hablar
de la probabilidad de
la secuencia de palabras queriendo decir, bueno,
la probabilidad de la que la primera sea W1
que la segunda sea W2, etc.
¿De acuerdo?
O sea que esta distribución de probabilidad
tiene como rango
todas las secuencias posibles de palabras.
¿Sí?
O sea que si mi vocabulario es B
tengo
N a la B
V a la N
V a la N
Posita V a la N
O sea que es enorme
esencialmente, ¿no?
Todas las posibles secuencias.
Y
vamos a recordar
la chain rule
la regla de multiplicación
de las probabilidades
que es, si yo tengo la probabilidad de una
secuencia de palabras
W1, Wn
esto es
la probabilidad de la primera
palabra, de alguna forma
la calculo
por la probabilidad de la segunda dada
la primera, dado que la primera
fue W1
observen acá que
no son independientes
es decir, las palabras por definición
acá, no son eventos independientes
es decir
tengo una cierta probabilidad de que
empiece con W1
la multiplico por la probabilidad de que
la segunda sea W2, dado que la primera
fue W1
por la probabilidad que
la tercera sea W3, dado que las dos primeras
fueron uno de hoy así
de acuerdo
de esa forma con esta regla yo
y al final Wn
la última dada todas las anteriores
esto se llama
regla de la cadena
yo con la regla de la cadena
puedo calcular la probabilidad
de una secuencia o de una oración
dada la secuencia
si logro calcular estas
probabilidades, o sea
si logro calcular
predecir las palabras
correctamente
voy a
poder predecir la secuencia
de esa forma paso de la predicción al cálculo
de toda la probabilidad de la oración ¿se entienden?
bien
entonces vamos a quedarnos con esa notación
entonces yo digo bueno
un ejemplo no, si yo quiero saber
la probabilidad de
viento fuerte de componente sudoeste
como el que está soplando
no sé si de componente sudoeste pero es fuerte
es la probabilidad de viento
por la probabilidad de fuerte
dado viento por la probabilidad de
dado viento fuerte etc.
nada menos que la regla de la cadena
entonces
yo quiero saber la última
p de sudoeste dado viento fuerte
de componente
y vos con google por ejemplo y digo
bueno viento fuerte de componente
aparece 9.230 veces
viento fuerte de componente sudoeste
aparece 347 veces
y yo entonces
voy a estimar la probabilidad de esa
por medio de conteos
la cantidad de veces que apareció
viento fuerte de componente sudoeste
dividido la cantidad de veces que aparece
fuerte de componente
dividido 9.230
¿Aguardo?
y esta es la probabilidad
de que la siguiente palabra
sea sudoeste en mi estimación
si ustedes se fijan
esto es una probabilidad
porque
contando
todas las palabras posibles que pueden seguir
acá si yo logro determinar cuáles son
yo sé que van a ver 9.230
van a sumar 9.230
¿no?
es decir todos los casos posibles
miro todos los casos junto a lo que es la siguiente palabra
eso hace que como esto me va a dar
9.230 a la suma de todas las cantidades
esto va a dar uno
el total
entonces esto sí es una distribución de probabilidad
entonces que estamos bien
efectivamente aquello es una probabilidad
¿de acuerdo? esto lo que me dice es
bueno
el 3,76% de las veces
es sudoeste la siguiente palabra
eso que acabamos de hacer
es estimar la probabilidad
a partir de la frecuencia
de ocurrencia en un cuerpo grande
eso Google es un cuerpo grande
muy grande
y eso se llama principio máximo
pero similitud que lo vimos en la de pasada
es trato de hacer
calcular la probabilidad en base
a lo mejor posible
a los datos que tengo
es decir considero
yo estoy considerando que los datos que tengo
es decir el corpo de Google
es una buena aproximación
del mundo de lenguaje
en realidad
yo no sé si en realidad
efectivamente cuando los seres humanos hablamos
hay un 3,76%
de probabilidad
de que después de decir
viento fuerte componente
viene sudoeste
pero el corpo de Google
que es lo mejor que tengo como aproximación
me dice eso y eso es lo que yo utilizo
como un estimador de máxima verosimilitud
es lo mejor que puedo acercarme
con el cuerpo que tengo
eso es lo que vamos a hacer todo el tiempo acá
calcular
componentes de máxima verosimilitud
pero tenemos algún problema
y es
en el otro casos
dice, a raíz de estos fenómenos se producirán
tormentas fuertes
la probabilidad de fuertes
y a raíz de estos fenómenos se producirán tormentas
tienen un problema y es que
nunca apareció en mi corpus
a raíz de estos fenómenos
se producirán tormentas
y nunca apareció en mi corpus
a raíz de estos fenómenos se producirán tormentas fuertes
y
Y eso nos da una horrible visión por cero, que queremos evitar, o sea que nuestra probabilidad
da infinito, no sé, no está definida.
Esto, una pregunta, ¿esto les parece que es un fenómeno común o no, que nos puede
pasar cuando estemos estimando todo el tiempo, porque por más grande que sea el corpus, el
lenguaje es muy creativo?
Entonces tenemos que buscar forma y además porque estamos haciendo un conteo de palabras
de oraciones muy largas.
O sea que la regla de la cadena no resuelve mi problema, porque yo, una aproximación
bien naif para calcular la probabilidad de calcular toda la secuencia posible, ¿sí?
¿cuántas veces aparece la secuencia que quiero calcular, la oración del total de
oraciones?
Bueno, tengo un corpus evidentemente grande, pero esta aproximación tampoco nos ayuda
mucho porque sigo teniendo contestos muy largos, porque si ustedes se fijan en la regla de
la cadena, bueno, en lo que acabamos de hacer, la última probabilidad es casi la misma
que la primera, menos una palabra, tengo que buscar una forma de achicar eso.
Entonces, una de las ideas fuerzas para computar esta probabilidad es en lugar de tomar todas
las palabras, tomar sobre las últimas, es decir, yo me quedo con las últimas n menos
un palabras, ¿sí? n menos n, bueno. ¿sí? En esto es enigrante, ¿no? Y las otras no
las considero, digo, bueno, mi humilde aproximación para que esto se pueda volver manejable es
decir, bueno, yo en realidad solamente me importan las últimas palabras afectan a la
que voy a predecir, son las últimas. Y de eso se tratan los modelos enigramas que
utilizan lo que se llama, eso que acabo de decir, yo llamo hipótesis de marcovo, hipótesis
marcoviana. Solamente las últimas palabras afectan a siguiente, hay un límite. Y fíjense
que en la hipótesis de bigrama, yo digo, cada palabra es la próxima por la anterior,
simplemente, estoy diciendo una cosa tan sencilla como la última, la última palabra es la
única, cada palabra es la siguiente, pero las anteriores no. Es muy fuerte, ¿no? Y
de trigramas son dos y con n, con n son n. ¿sí? Con la hipótesis de bigrama, mi probabilidad
es mucho más sencilla que antes, porque es como cada palabra solo depende, vamos a mirar,
uno bueno uno no está más, pero cada palabra depende de la anterior, simplemente me queda
que la probabilidad de una secuencia es la probabilidad de la primera, por la probabilidad
de la segunda a la primera, por la probabilidad de la tercera a la segunda, etcétera. ¿Le
guardo? Acá nos falta este PW1 en esa fórmula, pero no nos preocupa demasiado porque eso
lo resolvemos poniendo una marca al comienzo de la secuencia que siempre vale uno, su probabilidad,
es decir, que toda la gración empieza con una marca. Y si no, multiplico acá, ¿no? Si
no, si lo quiero hacer de otra forma, agregue un PW0 acá y lo mismo. Pero esencialmente
lo importante acá es que esto se transforma en una simple multiplicación de probabilidades
de una palabra dada en anterior. ¿Y cómo hago para calcular esto? ¿Cómo puedo calcular
esto acá? ¿Cómo calcula la probabilidad de una palabra dada en anterior? Contando,
pero solamente tienen cuenta dos, lo cual lo vuelve un problema mucho más manejable.
Y eso es justo lo que vamos a hacer. Un modelo de lenguaje intenta predecir la próxima palabra
de una oración a partir de las n menos una anteriores y, por supuesto, que importa el
orden en ese cálculo, ¿no? También tenemos que plantearnos cuando hagamos los engramas,
cuando calculemos la probabilidad de una palabra, bueno, cosas que ya hemos conversado. ¿Qué
elemento vamos a contar? Por ejemplo, tengo un tema de toquenización, esta coma, ¿la tengo
que considerar un engrama o no la tengo que considerar un engrama? ¿La tengo que considerar
un token o no la tengo que considerar un token? ¿Me interesa? Bueno, eso seguramente va a
depender un poco de la aplicación en la que lo estoy aplicando, a lo que lo estoy utilizando.
O tengo un cuerpo oral donde tengo disfluencias, disfluencias, creo que se llama esto. ¿Qué tengo
que hacer con las mayúsculas? ¿Qué hago con la forma flexionada? Todo los problemas de la
toquenización me parecen en los engramas, es decir, estos son cascadas, digamos, ¿no? Yo acabo
de tener la toquenización realizada. En realidad no hay respuesta universal, depende de la tarea
que estamos haciendo. Por ejemplo, típicamente los corporeales están todos pasados a mayúsculas,
porque como son más continuos, la identificación de oraciones no es tan importante. Si yo voy
a hacer análisis, si estoy haciendo un análisis de cómo se usan los signos de puntuación
en mi lenguaje, obviamente la coma la tengo que identificar, sino capaz que no me interese.
O me puede interesar todo estos, mapearlos a una cosa sola que se llama signo de puntuación
y juntar los puntos con las comas. Van a tener que hacer eso en el laboratorio.
Ya se van a colar. Bueno, nada, se necesita un pretratamiento disponible al menos para
las oraciones y el modelo no hay modelos generales. También va a depender un poco,
nuestros números van a depender de la cantidad de palabras. El dictionary,
el Oxford English dictionary tiene 290.000 entradas, el Tresor de la langue francés tiene 54.000
y el dicionario de la radio 88.000. ¿Por qué les parece que hay tantas más acá que acá?
Porque el dicionario no parece en la forma flexionada y el español está mucho más flexionado.
O sea que el inglés la tiene que arreglar más solito. Bueno, y después tenemos corpus.
Esto ya hablamos un poco y aquello distinguir entre el número de token que son la cantidad de
ocurrencias que hay en el texto y el número de palabras distintas, el vocabular.
Acá está la respuesta a la pregunta que hacíamos antes. ¿Cómo estimamos los
bigramas utilizando otra vez lo que se llama un estimador de máxima verosimilituos,
lo que se llama métodos de frecuencias relativas? Que es, cuento las cantidades de
la cantidad de veces que apareció una palabra con, por ejemplo, la probabilidad de fuerte,
dado viento, se aproxima como la cantidad de veces que aparece viento fuerte.
Por la dividida de la cantidad de veces que apareció
dividido todas las posibles continuaciones. ¿De acuerdo? Viento fuerte, viento calmo,
viento, viento dile, viento, no sé, lo que quieras. Y sumo todas las posibles,
lo que estoy haciendo es normalizando, como hablamos al principio de, como hablamos acá,
¿no? Estoy normalizando. Ahora, esto aquí es equivalente. ¿Cómo puedo simplificar esto?
Si yo tengo todas las veces que aparecen viento fuerte,
viento calmo, no sé, ¿cuál es la suma de todo eso?
Es la cantidad de veces que apareció viento. Esto es igual a la cantidad de
secas que aparecen vientos en el cuerpo. ¿Cómo puedo recordar?
Como son todas las posibles ocurrencias.
Ahí tenemos la simplificación y, además, para tener en cuenta la primera y última
palabra en oración, le vamos a agregar siempre los símbolos de comienzo y de fin. Eso para
asegurarnos de que, para no tener que calcular separada la probabilidad de la primera palabra.
Yo sé que la primera palabra siempre es ese y calculo la probabilidad de la primera en el texto,
digamos, ponerle el dado que la anterior era ese. De acuerdo? Y así lo dejo en una sola forma.
Por ejemplo, si supongamos que yo tengo ese cuerpo, ¿no? Juan abrió la puerta, el viento abrió
la puerta, enero abrió limones en tus mejillas nuevas, Juan recoge limones. Y quiero saber
la probabilidad de estas oraciones. Evidentemente no las tengo en el cuerpo, o sea, que no puedo
contar directamente. Pero quiero utilizar un modelo de diagrama para calcular. Y con lo que
sabemos es bastante sencillo. Primero que nada decimos, bueno, la probabilidad de Juan abrió
limones es probabilidad de Juan dado el comienzo. Probabilidad de comienzo siempre es uno. Probabilidad
de abrió dado Juan, probabilidad de limones dado abrió, etcétera, ¿no? Fíjense que la probabilidad
Juan dado el comienzo de la cantidad de veces que apareció Juan en la marca de comienzo dividido
de la cantidad de marcas de comienzo que es uno. Entonces esto me da 2 de 4. Ah, porque hay cuatro
oraciones. Perdón. Claro, porque yo estoy haciendo contegos directamente. No, no estoy haciendo
2 de 4 veces arrancó con Juan, ¿sí? Juan abrió es una de dos. Ya había aparecido Juan abrió
en el corpus y Juan aparece dos veces. O sea, de dos veces le apareció Juan y la siguiente apareció
una vez abrió. Y así sigo multiplicando y como me multiplico la fracción y me da, bueno, 0-0-42.
Esa es la probabilidad de Juan abrió el limón. Enero abrió la puerta 0-17. Esto no tiene mucho
sentido, ¿no? A ver, justamente el hecho de que sea un ejemplo de juguete le hace perder la gracia
todo esto porque esto funciona porque tengo grandes volúmenes, sino es una pasada. ¿Y acá que nos
pasó? ¿Qué puede haber pasado acá? La palabra come nunca está. ¿Y en la puerta?
En la puerta está. La primera se explica porque come nunca está, ¿no?
Creo que está así. Perdón. La así, la puerta.
¿Por qué da 0? Porque lo que no está es en la. En la no aparece nunca. Si ustedes miran acá la
probabilidad de, perdón, la cantidad de, la probabilidad de esto es la probabilidad de que
empiece con él, ya tenemos un problema con el comienzo con él porque creo que no hay ninguna.
Ninguna empieza con él y eso ya tiene un problema. Y además en la tampoco está. O sea que el
conteo me va a dar 0. Si el bigrama no aparece en el cuerpo de entrenamiento, siempre mi
problema da 0. Y más interesante aún, si cualquier bigrama de todos los que aparecen en la oración
da 0, la probabilidad de la oración es 0. Eso es un gran problema. Resolver el problema de eso,
de lo que se llama el suavizado de engramas que vamos a ver cómo. Tenemos que buscar alguna forma
de resolver eso que nos va a pasar siempre. Es decir, como nuestro cuerpo nunca puede ser tan,
aunque solo sean dos palabras, igual puede parecerme parezca de palabras que no aparecieron y yo no me
puedo trascar con eso. ¿De acuerdo? Bueno, nos queda ese pendiente de 0 que lo vamos a ver
después porque ya te quiero comentar alguna cosa. Pero vamos a acordarnos de eso, que tuvimos este
problema pendiente. Bien, en general, ustedes dirán, bueno, pero ¿cuál es el mejor N? ¿Por qué? ¿Cuál es el tema?
¿Es? ¿Cuanto? ¿Cuanto más largo sea el tigrama que yo utilizo? Más información tengo de contexto. Es decir,
intuitivamente es mejor estimar con cinco palabras que con una.
Estamos guardados con eso. ¿Cuál es el problema de los tigramas largos?
¿Por qué no puedo usar 15?
Porque tenemos el mismo problema por el que llegamos acá, que con 15 no tengo
cuerpo suficientemente grande como para que aparezcan esa ocurrencia.
Entonces, ese balance entre cantidad de ocurrencia, porque si yo no tengo una buena estimación de la
cantidad de ocurrencia, no voy a poder estimar bien la probabilidad. Con lo que yo estoy estimando la
probabilidad partido en conteos. Si yo tengo una, dos, tres ocurrencias, seguramente esa probabilidad
sea artificial. Porque si hubo una ocurrencia en un cuerpo de miles de millones de palabras,
no me está diciendo mucho. Generalmente en igualtré se obtienen buenos resultados.
Por lo menos para aproximarse da muy bien. Google hace unos años atrás sacó un cuerpo de negra,
un sí, una lista de negramas de hasta cinco. Me acuerdo en esa época bien en ese.
O sea que determinar n va a depender un poco de la tarea y ese se me dio a ojo, digamos, pues una
tarea un poco complica. Ahora vamos a ver un poco de evaluación. Ita y lo que decíamos, ¿no? ¿Se
agregan? Como cuando son trigramas, tengo que agregar dos símbolos al comienzo de la oración.
Tengo a poner.
Enero, abrió. Porque yo necesito dos de contexto para calcular el trigramo en detalle.
Y bueno, y la pregunta es cómo calculamos
desde el punto de vista metodológico, cómo hacemos para calcular buenas probabilidades.
Ya vimos cómo se hace el conteo. Ya ahora quiero ver cómo organiza el corpus y me parece
que es interesante ver esto porque nos va a pasar en muchas cosas, en este tema de
preservamiento del lenguaje natural y que muchas veces induce el mal uso metodológico de estas cosas
lleva error. Entonces me parece que vale la pena comentarlo esto.
Yo. Yo dije que iba a hacer conteo para calcular las probabilidades, ¿no? Entonces yo por acá tengo
un corpus, un corpus de texto.
Sí. Entonces esencialmente lo que tengo son muchos textos, ¿no? Obviamente, esencialmente no,
tengo muchos textos. Esa es la definición de corpus.
Y yo voy a establecer, voy a crear un modelo de una, de un, un modelo de un lenguaje. Es decir,
yo lo que quiero construir con esto de las probabilidades de las olaciones es un modelo
del idioma español. Yo tengo un corpus de texto en español y quiero hacer un modelo del idioma
español. Supongo que yo entren un modelo, entrenar el modelo en este caso quiere decir
calcular todas esas probabilidades. ¿Cómo hago para saber qué tan bueno es?
¿Sí? ¿Cómo lo evalúo?
Supongo que yo, ahora vamos a hablar de cuál es la medida, pero supongo que yo tengo una
medida de performa que me dice, bueno, aplicarle tu modelo a este texto.
Sí. Supongamos que la medida es el que le asigne, ahora vamos a ver por qué, pero el que le asigne
mayor probabilidad a todo el texto, a las oraciones del texto, es el mejor. El mejor modelo es el que
la asigna probabilidad mayor a las oraciones que tengo en el texto. Si yo aplico mi método,
mi modelo, o sea, evalúo mi modelo. Sobre este mismo corpus, ¿qué problema tengo?
Que me va a dar bárbaro porque lo calculé ahí. Es decir, yo nunca puedo, nunca, pero nunca,
nunca, evaluar un modelo en el mismo corpus en el que entrené. Esto aplica siempre. Cabe que
yo utilizo un método estadístico, aprendizaje automático. Lo más importante a saber en el
aprendizaje automático es nunca evalúes tu modelo en un corpus, en el mismo corpus que entrenaste,
porque por definiciones estás haciendo trampa, eso lo que se llama
sobreajuste. Vos sobreajustas a tu corpus de entrenamiento. Entonces yo lo que voy a hacer es
dividir mi corpus en dos y voy a decir, este es el corpus de entrenamiento,
voy a poner en inglés,
y el corpus de evaluación.
Entonces lo que yo voy a hacer es entrenar y ¿cuánto se para acá? Bueno,
la regla más o menos es 80-20.
Pregunto, ¿por qué me interesaría que esto fuera lo más grande posible?
Para que tener más información. ¿Y por qué no abuso 90-10 o 95-5 o 97-3?
¿Cómo?
¿Quieres evaluarlo con una cantidad de datos?
Tengo que solucionar ese balance, entretener una cantidad razonable de datos para hablar,
porque si yo le evaluo sobre una oración, la varianza es muy grande, es decir, la posibilidad
de equivocarme es muy grande. Entonces una regla es más o menos 80-20.
¿Sí?
Y bueno, ahí habla de 90-10, yo tengo la regla de 80-20.
Va a surgir un problema adicional acá, y es que ahora lo que voy a mover es,
por ejemplo, si yo quiero saber cuántos elegí el n, ¿no?
Yo quiero elegir el n, yo necesito, lo que puedo hacer es pruebo con un n acá,
modelo 1, n igual 2, y hago modelo 2, n igual 3.
Esto es un poco más útil de ver. Y lo evaluo acá y digo m1 y m2, y me quedo con el que me da mejor.
Eso metodologicamente no está bien. ¿Por qué?
Y esto es una de las cosas que es más difícil de entender a veces. Si yo pruebo los dos modelos
acá, de alguna forma también estoy haciendo trampa, porque supónganse que yo tengo no dos
parámetros, porque acá tengo un parámetro que tiene dos valores. Supongamos que yo quiero
ajustar otro parámetro de mi método que puede tomar 500 valores posibles. Si yo hago 500
entrenamientos y 500 pruebas, muy probablemente también esté ajustando acá, esté sobreajustando
acá, porque estoy eligiendo de los 500, y a veces pueden ser miles o cientos de miles,
el que mejor anda en este cuerpo de evaluación, o sea que estoy sobreajustando el cuerpo de evaluación.
Entonces, para el ajuste de parámetros, yo usualmente lo que tengo que hacer es definir
dividir este corpus, sacar un pedacito del cuerpo de entrenamiento,
que lo llamo corpus held auto, corpus de desarrollo.
Y lo que hago es entreno sobre esta parte y evaluo sobre el held auto, y me reservo este
de evaluación, solamente para cuando tengo mi modelo definitivo y quiero saber su
performance, con su medida de evaluación, ¿de acuerdo?
Esto lo van a, algo como esto van a tener que presentar en el laboratorio,
decir, cómo evaluarían el método, un método.
Hay otras posibilidades que no implican un corpus held auto, por ejemplo, hacer lo que se
llama cross validation, que es separo este pedacito, entreno sobre esto y evaluo sobre este,
si, después separo otra franjita, entreno sobre el resto y evaluo sobre la franjita y así con
en cada franca, franjas y saco el promedio, eso me sirve para no desperdiciar, digamos,
esta parte del corpus, para poder utilizar todo el cuerpo de entrenamiento, se llama cross
validation. Vamos a volver a hablar un poquito de cross validation, cuando hablemos de
clasificación, pero lo que me interesa es que le quede claro la diferencia entre estos corpus
y cuando, como decía, cuando tengo el modelo final, uso esto solamente para evaluar las
performas, en una medida que determinaría ese unitaria. ¿Cómo evaluamos un modelo bueno?
La manera correcta de evaluar un modelo debería, sería empíricamente, es decir, si yo quiero
valorar un modelo de lenguaje y lo estoy usando para el reconocimiento del habla, debería ser una
evaluación de qué también reconozco el habla o qué también reconozco la escritura, pero eso puede
ser muy costoso a veces, o yo puedo estar haciendo un modelo en lenguaje y no sé para qué se va a
usar, entonces me interesa mucho o me puede interesar tener una media intrínseca de la
performa de mi modelo. Entonces, vamos a ver una forma de evaluar. A mí esta parte de este
parte en el libro está puesta como un tema avanzado, pero a mí me parece interesante mostrarlo porque
porque la entropía es un concepto que aparece muchas veces en el profesoramiento del lenguaje
natural y en otras cosas me parece que le vale la pena por lo menos aproximarse. Supongan que
yo tengo una variada de la aleatoria y todo esto voy a llegar a una forma de evaluar un modelo,
¿no? No hay que empezar a hablar de esto porque sí. Supongan que yo tengo una variada de la
aleatoria que tiene varios eventos posibles, en nuestro caso dijimos que eran las palabras
posibles. La entropía, la entropía es una variada de la aleatoria que es un concepto que viene de la
teoría de la información, de Claude Shannon. La teoría de información lo que hablaba era, bueno,
alguno capaz que hicieron, lo vieron en un curso, pero la teoría de información lo que trataba
era de medir cuánto me cuesta a mí transmitir un mensaje. ¿Cómo puedo transmitir un mensaje
de forma óptima? Digamos, es un poco la idea, o qué hay atrás de una comunicación. La noción
de entropía, esta función es, tengo el evento, quiero decir, la probabilidad del evento por el
valorismo de esa probabilidad, ¿sí? La entropía tiene como característica fundamental que es una
medida que, si hay un evento que tiene toda la masa de probabilidad, la entropía es mínima. Es decir,
si yo tengo un dado que está tan cargado y una forma, algo que, equivalentemente se puede decir
que la entropía mide migrado disertidumbre sobre un evento. Si yo tengo un dado que está tan cargado,
que cabe que lo tiro, sé que siempre va a salir seis, no tengo disertidumbre. Mi entropía es cero.
En cambio, si el dado está perfectamente calibrado, equilibrado, ¿sí? Mi entropía es máxima.
Es decir, ¿por cómo está definida la entropía? No puedo tener
entropía más alta que cuando los eventos están equipobables. Entonces, justamente la entropía,
generalmente lo que uno mide con la entropía es eso. ¿Qué tan parecido son los resultados? ¿Qué
tan balanceados están de alguna forma? Cuanto más incertidumbre tengo, ¿por qué tan más balanceados?
Si yo no tengo ni la menor idea de la palabra que sigue, mi entropía es máxima.
Y además tiene otra característica que es que si el logaritmo es en base 2, este número,
la entropía me mide la cantidad de bits que yo necesito,
mínimo para transmitir los eventos. Esto es lo mejor forma de verlo con un ejemplo.
Supongamos, y es el ejemplo que aparece en el libro, supongamos que yo tengo ocho caballos.
Sí, tengo ocho caballos y quiero transmitir las apuestas que se están haciendo por un cable.
Entonces digo, bueno, una forma cantada de transmitirlo o directa de transmitir llamar al
primer caballo 001, 010, 011, 100, 101, 110, 111. ¿De acuerdo? Acá yo uso ocho bits.
Cada vez que se apuesta por el caballo 01, yo pongo 001, blablabla. Entonces en total yo
utilizo tres bits para transmitirlo por un cable. Tres bits por cada apuesta, ¿no? Ahora,
cuando nosotros vemos las apuestas descubrimos que la mitad de las veces se apuesta por el caballo 1.
Un cuarto del caballo 02, un tercio, blablabla. Un octavo del caballo 03, un 16ado del caballo 04,
y todos estos se apuestan mucho menos. Teniendo en cuenta eso, yo lo que trato de hacer ahora es decir,
bueno, quiero proponer una codificación mejor que hace que yo, los caballos que se apuestan más,
o sea que tengo que transmitir más seguido, los codifico con menos bits. ¿De acuerdo? La
mitad de los bits, el primer bit, lo utilizo solo para el caballo 01. Es decir, que si es un 0,
es que transmitir el caballo 01 necesita un solo bit. Si es un 01, si es un 01 y un 0 después,
es el caballo 02. Si son 01 y un 0, después es el caballo 03. Si son 01 y un 0, fíjense que yo
para transmitir estos caballos utilizo 1, 2, 3, 4, 5, 6 bits. Utilizo más bits. Pero como son
mucho menos probables, mi entropía me da 2 bits. O sea, el promedio de bits que yo utilizo según
la distribución es 2 bits, que es más baja que los 3 bit originales. ¿Se entiende? Incorporando
la información de la distribución bajo. Podemos mejorar eso. No, no podemos mejorar eso. Nunca
vamos a, la entropía lo que nos dice es eso. Nunca vas a encontrar una, porque justamente la
entropía es 2. Como la entropía es 2, la entropía me da una cota inferior sobre cuánto puedo llegar.
Con menos de 2 bits no puedo. ¿De acuerdo?
Entonces se preguntarán para qué sirve esto.
De hecho no, la entropía es una cota de lo que decía, una cota mínima para el número de bits
necesarios. A partir de la entropía yo puedo calcular la entropía de una secuencia.
La entropía de una secuencia es de todas las combinaciones posibles,
de una secuencia la probabilidad de esa combinación es lo mismo para aplicado a secuencia.
Este si lo ven es un número muy complicado porque es la sumatoria de una cantidad impresionante del
número, porque son todas las combinaciones posibles de secuencia. Eso es lo que me
dice es la entropía de la secuencia. ¿Qué tanta incertidumbre hay en una secuencia?
Y la tasa entropía sería eso dividido de n, es decir el promedio, porque si no la secuencia
más larga o no la entropíamos antes. El promedio por palabra de la entropía.
Entonces, la entropía de un lenguaje que sería como la medida de qué tanta incertidumbre hay en un
lenguaje. ¿Qué tanto puedo yo llegar a predecir lo que va a seguir diciendo el lenguaje?
Ese al límite, pero como valoró, no en un contexto en general en el lenguaje,
es una medida para el lenguaje. Ese al límite cuando la secuencia tiene infinito de la tasa
entropía.
Y que sé que acá es la suma, como decíamos, es la suma de todas las secuencias posibles.
Es decir, que es una cosa imposible, calcular. Pero hay un teorema que es el de Llano Muamí
Lambrayman que dice que el lenguaje es estacionario y ergódico. Estacionario y ergódico quiere
decir que no importa dónde yo esté parado en una secuencia, todas las posiciones, las probabilidades
son las mismas de continuidad. Lo cual no es así en el lenguaje, porque lo que yo digo ahora
incide dentro de lo que estoy diciendo entre un minuto más. No, no es aleatorio, digamos. Pero
suponiendo eso es una simplificación, lo que me permite es simplemente para calcular la entropía,
la tasa de entropía del lenguaje es simplemente uno sobre n dividido en logaritmo. Fíjense que
perdí las probabilidades de cada una de las de la secuencia. Es como que si yo tomo una secuencia
suficientemente larga del lenguaje, voy a incluir a todas las subsecuencias. O sea que si yo una
secuencia suficientemente larga, puede ser el cuerpo de evaluación. Yo puedo calcular la entropía
sobre el cuerpo de evaluación. Entonces esto es un número, hasta ahora lo que dije acá es un
número, no sabemos por qué tengo esto. Pero fíjense que si yo puedo calcular lo que se llama la
entropía cruzada, porque yo que tengo, yo tengo un lenguaje que genera las palabras con una cierta
distribución de probabilidad, que es lo que queremos averiguar, que es tan lo que es lo que es
nuestro problema original, cómo da las palabras anteriores y genera la siguiente. Eso es algo
que he desconocido, no sabemos cómo es, porque es el del lenguaje español el que yo quiero
pero yo tengo un modelo M, que es el modelo de negramas. La entropía cruzada lo que dice es bueno
calculamos esta H utilizando la probabilidad original por el logaritmo de la probabilidad
asignada por el modelo. La probabilidad de la secuencia es la que tenía el lenguaje general,
que no la conozco, y el logaritmo sí, o sea esa distancia, esa largo envícese del modelo.
Según el teorema otra vez, ya no manmilan, yo puedo sacar esta probabilidad simplificándola,
suponiendo que es ergodico, y digo bueno, la entropía cruzada depende solo del logaritmo
de la probabilidad asignada por el modelo. Y esto es lo interesante, cualquier entropía cruzada
que yo obtenga, que yo calcule con un modelo, va a ser mayor necesariamente que la entropía
dé lenguaje. Cualquier modelo va a asignarme una entropía mayor a la de lenguaje, esto es la cota inferior.
Entonces fíjense que como son todas mayores, cuanto más parecido sea mi modelo, al modelo
del lenguaje, cuanto más aparecido, asigne probabilidad más parecida de las de acá,
por como está definido, va a ser mejor. Entonces, cuanto menor sea la entropía cruzada de mi modelo,
evaluado sobre una secuencia suficientemente larga, es decir, sobre el corpo de evaluación,
mejor va a ser mi aproximación. Y justamente, la medida de esa intrínseca que estábamos buscando
era esto, que es dos, ¿por qué es dos? No lo sé, porque es lo mismo, es para sacarlos
logaritmos nada más, es dos a la entropía cruzada, a este valor, y esto se llama perplejidad. La
perplejidad es lo que mide qué tan bueno es intrínseamente mi modelo sobre mi cuerpo de
entrenamiento, sobre mi cuerpo de evaluación. Es decir, si yo tengo dos modelos, el que asigne
mayor probabilidad, menor perplejidad, mayor probabilidad al cuerpo de evaluación, es mejor
desde ese punto de vista, lo consideramos mejor. ¿Por qué? Porque tiene menos dudas de cómo se
comporta, porque la perplejidad es como la incertidumbre que yo tengo ante... Dada una palabra,
cuando yo me paro una palabra, ¿cuál es mi incertidumbre? Mi branching factor,
en cuanto se puede abrir la siguiente palabra en promedio? Un poco eso es lo que captura la
perplejidad. Mi lenguaje va a tener un branching factor, es decir, no es que es cero, pero mi modelo
siempre va a calcular algo mayor o igual a ese branching factor. Cuanto más bajo sea,
es que quiere decir que yo no estoy acercando más a la perplejidad posta, por eso la perplejidad
es la medida de que también hace las cosas. ¿De acuerdo? Bueno, no, eso es cuentas.
Por ejemplo, si nosotros entrenamos unigramas, bigramas y triramas en un corpo de artículo
de Wall Street Journal de 38 millones de palabras, probaron el cuerpo sobre un modelo de un
cuerpo de prueba de 1,5 millones de palabras y calcularon la perplejidad. Y fíjense que la
perplejidad con los unigramas es de 962. ¿No sabemos cuál es el mínimo de esto? No sabemos cuánto
puede bajar, pero sabemos que con bigrama llega a 170 y con triramas a 109. Es decir, si yo tengo
dos palabras antes, puedo predecir con mejor, porque acá es con unigrama, es la probabilidad
que a palabra no dice mucho. Si yo tengo el anterior, lo rápidamente baja. Y si se fija,
cuando agrega un tercero baja, pero no tanto, ni cerca tanto. Bueno, lo último que nos queda
hablar es muy bien. ¿Qué pasó con las probabilidades nuladas? ¿Se acuerdan que nos quedaban las
probabilidades nuladas cuando no había conteo? Bueno, uno de los problemas es la palabra que
no existen. La palabra que no existen lo único que podemos hacer o lo que típicamente se hace es
crear un vocabulario fijo y sustituyo las palabras de conocida por una especial. Esto es típicamente
lo que se hace. Es decir, todas las palabras de conocida las considero una sola palabra que nos
equivalece. Y cuando aparecen enigramas que no ocurren, este es el caso de come que no aparecía,
pero puede ser que el enigrama no ocurra, lo que voy a hacer son técnicas de suavizado.
Yo tengo, ¿se acuerdan? Tengo el contador de, por ejemplo, acá es un enigrama, ¿no?
Contador de la palabra, el cantidad de veces de la palabra, dividido el total de token que hay.
Y así calculo las probabilidades. La técnica de la plaz, lo que dice es, bueno, le agrego uno
a cada contador, o sea que nunca me va a dar cero, lo hago a lo bestia, digamos, ¿no? Compare
que no me dé cero, le sumo uno. Y le sumo B, ¿se acuerdan? Que lo he vivido en la clase pasada. Le sumo
B para que esto me siga dando una distribución de probabilidad. Esto simplemente lo que hace
es calcular un contador ajustado, multiplica por T y divide por T más B, es decir, multiplica
por el junial y divide por esto, ¿no? Por el PWBI. Por ejemplo, si yo digo, si este es mi corpo
entrenamiento, esta es la historia de un hombre de la ciudad que creo, fíjense que me conté
o da uno, y quiso me da cero. Perdón, este es el conteo. Ahí va, el conteo de esta es uno,
de la es dos y de quiso es cero. La probabilidad de esta es uno dividido trece. En total de palabras,
una es esta y es cero, cero, ocho. La es dos dividido trece y quiso me da cero en la probabilidad
de que no queremos que no de cero. Si nosotros aplicamos la plaza, lo que me da es sumo veinticinco,
¿no? Son doce palabras en el vocabulario porque la única que está repetida es la. O sea que tengo
doce en el vocabulario, no trece, trece es T y doce es B. Entonces, se hago dos dividido veinticinco y
así me da las nuevas probabilidades. Y acá quiso deja de ser cero. El contador ajustado
de lo que nos permite es comparar lo que teníamos antes con lo que teníamos ahora. Por ejemplo,
esta valía uno y baja a cero noventa y seis. ¿De acuerdo? La valía dos y baja a uno cuarenta y cuatro.
Y quiso va de cero a cero cuarenta y ocho. Si se fijan acá, lo que se llama descuento,
que es el cociente entre los dos valores, me permite ver que le estoy sacando más masa de
probabilidad a la que a esta, que queda casi igual. Es decir, le tenía a la plaza el problema,
porque ¿qué es lo que está pasando acá? Esto es lo que me muestra, es que yo le tengo que sacar
masa de probabilidad a los que aparecen, porque todo me tiene que sumar uno, todas las probabilidades
me tienen que sumar uno. Si yo iba a agregar diagramas que antes estaban en cero, tengo que
sacarle probabilidad a los que están, pues no me suma más que uno. Entonces, esto es lo que tiene
que castiga mucho a los más frecuentes. Les sacan mucho probabilidad a los más frecuentes y como
que premia demasiado a los que no aparecen. Hay otras técnicas, no vamos a entrar en eso que tratan
de ajustarlo un poco mejor, pues ahora vamos a mover algunas, perdón. Mueve demasiada probabilidad.
Otra posibilidad es usar un delta en lugar de uno. Ese delta tengo que calcularlo, se acuerdan lo
calamos del cuerpo de, siempre que yo tengo esos parámetros para calcular, los calculo sobre el
cuerpo de desarrollo. Finalmente, hay otra, esa es una aproximación, es decir, con técnicas sobre
el contenci. Hay otra posibilidad que son un poco más avanzadas, digamos que es cuando yo quiero
estimar, por ejemplo, en técnicas de trigrama, una palabra a partir de las dos anteriores y
no existen casos de las dos anteriores en el texto, de las dos anteriores seguidas a W,
acá es WN, perdón. Sí, lo que hago es hacer lo que se llama BACOV, calcularlo a través de la
probabilidad de la anterior. Bueno, si no tengo la anterior, pruebo con la anterior, se entiende,
eso se llama hacer BACOV. El BACOV, tenés que resolver también que ahora otra vez se
está introduciendo nuevas, luego caso que no tenías antes. Estas probabilidades tengo que
calcularlo y darle más a la probabilidad. O sea, otra vez tengo que mover probabilidad.
Cuando los corpus son muy, muy, muy grandes, una forma alternativa y es un método muy nuevo,
se llama STUPID BACOV, que es, como mi cuerpo es muy grande, típicamente el cuerpo de Google,
no normalizo nada las probabilidades, conteo nomás como me fué y ya está. Si una no me da
pruebo con la anterior, si es igual tengo un montón de edad. O también se puede hacer
interpolación, es decir, la probabilidad de una palabra dada a las dos anteriores
es la probabilidad de la palabra, la probabilidad nueva, es la probabilidad original de la palabra
dada a las dos anteriores por un cierto lambda, más un cierto lambda 2 por la probabilidad de la
palabra dada solo en el bigrama, más la probabilidad del unigrama. Y combino las tres a la vez,
es como combino las tres técnicas a la vez, ¿de acuerdo? Es decir, le doy un cierto peso a las
probabilidades que yo quiero. De esta forma, porque acá podría ser que existiera el bigrama anterior,
pero existiera una vez sola, entonces yo no le tengo mucha confianza a esa. Puedes usarme y no
le tenga mucha confianza, entonces le doy un cierto peso a este también, capaz que le doy un
poquito más alto a este. O sea, el 7 existe, está todo bien, pero este siempre me ayuda. Y de esa
forma va balanceo. ¿Cómo calculo estos lambda y con el cuerpo de... tengo que
de alguna forma calcularlo sobre el cuerpo de desarrollo o el cuerpo gel dado?
También hay interpolación
condicionada por el contexto, o sea, hay un lambda, acá ya lo que pasa es un poco más raro,
y un poco más moderno. Digamos que es que más de estas épocas, digamos, donde a mí ya no me
preocupa tanto tener muchos parámetros. Acá estoy definiendo un parámetro para cada combinación de palabras.
Y hasta aquí llegamos hoy. Esto es el... es este capítulo que tengo acá, capítulo 4 del
libro de Juraski. Tiene algunas cositas más, pero esencialmente es eso.
Y es lo que vamos a hablar de en este curso de Enigrama. La clase que viene presentamos laboratorio.
