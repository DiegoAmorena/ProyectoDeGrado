start	end	text
0	23680	Una vez que elegí en mi, con el paso 1, elegí cuántas palabras en español y bolsar en el
23680	27800	paso 2, es lo que voy a elegir es una lineación, una función de lineación que me dice
27800	31000	cada palabra, con cual se va a corresponder, cada palabra, el lado de español, con que
31000	37260	palabra en inglés se va a corresponder. Este modelo ha sumed de manera muy naïve que todas
37260	44280	las salinaciones que yo puedo tener son equiprobables, o sea, ha sumed que yo voy a tener un
44280	48640	conjunto de lineaciones posibles y todas van a tener la vina de probabilidad. Bien, entonces,
48640	54600	la probabilidad de elegir una lineación en particular, si yo tengo un montón de lineaciones,
54600	59640	digamos, la probabilidad de elegir una, una lineación en particular, va a ser uno sobre
59640	63480	la cantidad de lineaciones que tengo, porque en realidad todas van a ser equiprobables.
63480	69280	Bien, entonces, cuántas lineaciones puedo tener entre dos oraciones, una oración en inglés
69280	73160	que tiene largo y una oración española que tiene largo jota, como puedo calcular cuántas
73160	79160	a lineaciones existen.
79160	90400	Más o menos, casi de la jota. Recuerden que el lado de inglés, yo podía, yo tenía ciertas
90400	99200	palabras en inglés tenía la palabra, en inglés era ahí, la palabra 1, 2 hasta,
99200	108000	sui y en español tenía las palabras f1, f2 hasta, f subjota. Entonces, yo podía
108000	113600	atrazar líneas para alinear, pero además en inglés, yo siempre considerado que tenía un
113600	119480	token null. Entonces, todas las palabras que no estaban alineadas del lado del español y van
119480	123000	a parar ahí. Así que en inglés en realidad no tengo
123040	127560	y posibilidades, tengo una más, tengo y más uno. Entonces, cuántas formas tengo yo de
127560	133320	mapear estas jota posibilidades en español con las y en inglés.
133320	136720	Es alto, y más una la jota, porque yo tengo y más una opción para la primera y más
136720	142600	una opción para la segunda, etcétera, que yo al final. Así que son y más uno a las jota
142600	152600	alineaciones, posibles. ¿No voy a tener un cliente medio de la red? ¿No voy?
152600	155960	¿No voy a dar esta porillas a las a las a las a las a las de los múltiples en medio
155960	162000	de la ingestación? Ojo, el null es como una pizadita que hago yo para alinear cosas que
162000	165160	no tienen un correspondiente. O sea, yo tenía una palabra en español que...
165160	172440	¿Tar? Varias de las cefes pueden estar alineadas en español, no importa en qué
172440	179680	orden están. Eso. Bien, entonces, eran y más uno a las jota posibles alineaciones,
179680	188920	por lo tanto. La probabilidad de elegir una alineación a data de la
188920	193480	operación en inglés, la probabilidad de elegir una alineación cualquiera, data, la
193480	199400	oración en inglés, va a ser el producto de la probabilidad de haber sortiado un valor
199400	205400	jota primero que era de epsilon por la probabilidad de elegir una alineación cualquiera para
205400	212560	ese jota, que es uno sobre y más uno a la jota. Bien, entonces esto lo resolvimos como
212560	223280	epsilon sobre y más uno a la jota. Epsilon sobre y más uno a la jota es la probabilidad
223280	229500	de data de una oración en inglés, elegir cierta alineación que yo voy a utilizar.
229500	236840	Bien, ese fue el segundo paso. El tercer paso es una vez que se atengo la alineación,
236840	240640	voy mirando cada palabra de la dolin inglés y le voy poniendo una palabra correspondiente
240640	246320	de la de español. Para acá voy a sumir que yo tengo una tabla de traducción, una tabla de
246320	250080	traducción que me dice que tiene de un lado todas las palabras en español y el otro lado
250080	257040	de las palabras en inglés, entonces mi tabla va a tener una forma como, por ejemplo,
257040	264040	hace una tabla así que de un lado decir las palabras en español como banco, perro,
264040	270480	chato y más cosas y del otro lado va a tener las correspondientes en inglés como banco,
270480	278240	bench, cat, tri y más cosas. Y entonces esta tabla va a decir la probabilidad de traducir
278240	280840	una cosa en la botan. Entonces banco probablemente tenga cierta probabilidad para
280840	292000	avanzar y cierta probabilidad para bench, 0.4 y 0.6, 0.6 y para cat no da ninguna probabilidad
292000	297480	para tri tan poco y después perro no va a tener nada esto, pero si después y cat va a ser
297480	302240	este no sé, 0.8 en este caso, etcétera voy a tener una tabla bastante grande que tiene
302240	311480	toda la posibilidad de traducir una palabra como otra. Entonces, si yo tengo esa tabla lo
311480	318720	que puedo decir es que la forma de calcular la probabilidad de esa oración final que
318720	323080	yo traduce va a depender de cuáles son las palabras que yo elija va a depender de cuáles son las
323080	330920	palabras que yo haya puesto dentro de mi, de mi oración para traducir. Entonces esa tabla que
330920	336800	está ahí definida le llamamos acá en la, en la, en la, la, aparece como T de f su x,
336800	344160	su y y dice que la probabilidad de traducir la palabra su y como f su x. Entonces,
344160	354520	acá hay una cosa importante. Si tenemos la oración en inglés, la oración en inglés
354520	361840	recuerdan que tenía las palabras, es su 1, es su 2, hasta de su 9, la oración en español
361840	369080	tenía las palabras, es su 1, f su 2, hasta de f su jota. Y eso tenía en el medio una función
369080	377320	de la lineación que me decía que palabras se correspondía con cual. Entonces, no era su
377320	390800	vene ni f su jota, era su y y f su jota grande. Esto era su y, esto era f su jota grande.
390800	398200	Entonces, si yo tengo una palabra cualquiera dentro de la oración en español, tengo un f su jota
398200	405200	de chica dentro de la oración en español. Esto se va a corresponder con algún f su y chica en la
405200	409760	oración en inglés, digamos. Yo sé que esto se cumble por la función de la lineación
409760	412560	porque agarra y mape a todas las palabras que están en español con algo que estaba
412560	417880	a la dole inglés. Potencialmente con el doque en vacío, no olvides.
417880	422440	Bien, entonces, tengo una palabra de la dole español que es f su jota y una palabra de la dole
422440	427960	inglés que es f su y. ¿Cuál es la relación entre ese jota y ese y? ¿Cómo es la relación
427960	443480	entre sí? Tiamos. Yo puedo decir que el i es igual a algo de jota. La buena manera.
443480	447920	La función de la lineación, ahí está. O sea, el i es igual a la función de la lineación
447920	455080	aplicada jota. Como la i, el índice de este acá es igual a la función de la lineación
455080	463320	aplicada jota. Entonces, yo puedo decir que la palabra su i es igual a la palabra su
463320	468440	a su jota. Así que puedo decir que en realidad los que están alineados son la palabra
468440	475000	f su jota está alineada con la palabra y su a su jota. Y ahí me sacqué el i de encima,
475000	481200	digamos, simplemente y te eros sobre las palabras y te erando sobre la jota puedo establecer
481200	490160	la correspondencia entre las dos palabras. Y eso es un poco lo que dice acá para terminar
490160	493360	de armar lo que es el modelo de traducción. Para terminar de armar el modelo de traducción
493360	497240	dicen que en el tercer paso yo voy a elegir cuáles son las palabras. Entonces, lo que
497240	503920	voy a hacer es iterar sobre todas las palabras y haciendo el producto de todas las
504000	509440	las probabilidades. O sea, el producto de dado que yo tenía la palabra f su jota,
509440	514800	pero dado que su tenía la palabra eso va su jota en inglés. Entonces, elegir la palabra f su jota
514800	521120	en español. Eso haga una productoria con todos los valores de las distintas palabras.
523680	531680	Bien, entonces ahí, llegue a el último de los valores que quería calcular, que es la
531680	542720	probabilidad de f dado que conozco. Ahí es igual a la productoria con jota igual uno hasta
542720	550680	jota grande, de el valor de la tabla de traducción, que es de su f su jota, t de f su jota
550680	561640	y su vasu jota. Bueno, ta. Entonces, ahí tengo como en cada paso fui calculando cosas
561840	567600	este se correspondía al paso uno del modelo, paso uno, este se corresponde con el paso del modelo.
567600	571680	En realidad, este ya tiene el paso uno del paso dos juntos porque ella tengo el epsilon acá y este
571680	577800	se corresponde con el paso tres del modelo. El paso tres de la historia de generación.
579800	586160	Mi objetivo con todos estos valores que están acá es calcular pdf de hoy.
586240	595160	¿Qué parametro sin traduje? ¿Qué parametro fueron surgiendo a medida que se iba
595160	598400	y derando sobre estos pasos? Bueno, en primer lugar, el epsilon aquel que estaba
598400	602560	moviendo, este es un valor que yo tendría que estimar a partir de mirar en los corcos,
602560	608200	como son los largos y las oraciones relativos. Y el otro parametro importante es aquella
608200	611920	tabla allá, aquella tabla de traducción es que me dice banco, con que probabilidad lo
611920	615920	puede traducir como banco y como que probabilidad lo puede traducir como véns, etcétera, etcétera.
615920	620680	Esta tabla en realidad es un parametro del modelo, es un parametro el sistema que si yo lo tuviera,
620680	626640	me alcanzaría con eso para poder construirme este modelo y calcular la probabilidad de cualquier
626640	627600	par de operaciones.
632600	638840	Bien, y entonces, antes de continuar, vamos a terminar de armar cuál es la imagen de esto,
639080	646840	que es decir, yo en realidad lo quería calcular era pdf da doe, que eso va a ser mi modelo de traducción
646840	652840	y de hecho va a ser el encargado de medida de ecuación de una frase, pdf da doe lo puedo calcular
652840	657640	con esta descomposición de pasos que dice acá en realidad porque luego de la siguiente manera.
669800	681800	Yo quiero calcular pdf da doe, y entonces voy a mirar lo que dice acá pdf da doe, es igual a la sumatoria
681800	690920	en la pdf da doe, que significa eso que para traducir en la generación en español y una versión
690920	695840	en inglés o más bien para la situación, para traducir en una generación en español,
695840	701760	hay muchas formas de alinear las palabras en el inglés en español y una vez que yo elegí una forma
701760	705520	alinear, hay muchas formas de elegir las palabras que vienen después de vamos a mirar a través de
705520	711800	traducción y capaz que hay varias maneras de elegir distintas palabras. Entonces lo que eso significa es que
711800	716960	no existe una sola manera de traducir una versión en inglés a una versión español. Yo puedo encontrar
716960	721200	varias formas de alinear las palabras si darías formas de elegir las palabras de manera de que muchas
721200	729120	alineaciones son posibles. Entonces para saber cuál es la probabilidad de traducir de traducir F da doe.
730400	735400	Entonces yo voy a tener que sumar sobre todas las alineaciones posibles, sobre todas las formas de alinear las
735400	741280	dos oraciones FI, voy a tener que ir a ir a ir sobre eso y para cada una voy a tener que acular la probabilidad
741280	746600	partial. Entonces, digamos, yo tengo cinco formas alinear las dos oraciones,
747280	751000	cinco es un número un poco raro, pero digamos tengo eneformas de alinear las dos oraciones.
751800	758160	Voy a tener que mirar bueno para la primera alineación cuál es la probabilidad de encontrar la
758160	761560	oración F para la segunda alineación cuál es la probabilidad de encontrar la oración F para la tercera
761560	767920	oración y así hasta llegar al final y agarró y sumo todo eso. Eso lo puedo hacer porque las alineaciones son
767920	771960	una descomposición de la espacio de probabilidad, en realidad yo puedo descomponar el espacio de probabilidad,
771960	777760	en pedacitos disjuntos y cada alineación va a ser uno de ellos. Así que digamos que para
777760	782360	cagular el modelo de traducción, pede F da doe, necesito sumar sobre todas las alineaciones posibles.
783360	787200	Ahora, lo que me falta es saber cómo calculo este valor acá.
788200	794480	Así que lo que estoy diciendo es que la probabilidad de F da doe es la suma sobre las alineaciones
794480	800960	de la probabilidad de F y esa alineación da doe. Eso es simplemente lo que dice ahí en la
800960	805400	la Ley. Lo que me falta calcular entonces es esta parte de acá y esa parte de acá,
805400	811480	la calcula esta manera. Yo digo que la probabilidad de F da doe es igual, ahí está más
811480	819320	o menos al resultado final, pero podemos sacar que es lo que tendría que poner de este lado.
831960	840760	Esta, por definición de probabilidad de condicional es pede F da doe, de verdad lo
840760	848960	alian van a ser lo, pero esto se puede definir cómo pede F a e sobre pede, no, por definición
848960	856000	de probabilidad de condicional. Pero además esto si quiero podría llegar a decir esto es lo mismo
856000	875240	que pede F a e sobre pede, por, voy a que me falta va, no, ahí, por pede a e sobre pede a e
875240	882800	pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e
882800	891280	sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e
891280	897320	sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e
897400	908500	definitiva yo que me queda, es si, asociós los dos, meda que dar pede F da do ahh e y si asociós estos
908500	915340	dos de acá sabrón me va a quedar pede aa dagoes qué lo que tra ya.
915340	922320	La probabilidad pede F, que sea de bueno si te los dos, de f, y ya dago... E eh, es igual a la
922320	926660	la roguelidad de desfeitados ahí por la progulidad de a da doy.
926660	930720	Y estos dos valores que están acá no lo sé el equipo casualidad sino que son los
930720	932740	valores que tenían antes en el modelo.
932740	941240	O sea, yo tenía que el pedea da doy, el igual a épsilón sobre y más uno a la jota.
941240	949500	Y el otro era la productoria de jota igual uno hasta jota grande de las valores de
949500	954660	traducción, el efe subjota y el e suba subjota.
954660	959620	Entonces en definitiva puedo calcular pdf a da doy y además puedo calcular haciendo
959620	966700	una suma sobre todas las alienaciones posibles puedo calcular pdf da doy.
966700	971900	Bien, con eso y con todo ese montón de cocciones, llegamos a construir lo que es un modelo
971900	976740	de traducción o sea solamente teniendo una tabla de traducciones que me diga cuál es la
976740	982620	progulidad de traducir una palabra como otra palabra yo puedo llegar a definirme
982620	988140	cuál es la progulidad de traducir una oración da da otra oración.
988140	992660	Bien, y hay una cosa más, bueno esto ya lo estoy moviendo que aplicamos en cada
992660	1001380	paso, y hay una cosa más que es si yo tuviera las dos oraciones digamos la oración
1001380	1005260	en inglés y la oración en español y además tuviera la tabla de esta con todas las
1005260	1008940	de progulidades yo podría hacer un algoritmo de programación dinámica, un algoritmo
1008940	1013020	estilo biter, y que vaya recorriendo alienaciones y media cuál es la lineación más
1013020	1017500	probable. No vamos a ver los detalles de algoritmo, pero viene a forma de decir bueno,
1017500	1021300	voy recorriendo las dos oraciones y me voy quedando con las sus secciones más
1021300	1025780	probable y al final me termina de volviendo cuál es la lineación más probable edadas
1025780	1031900	esas oraciones. O sea que si yo tuviera ya esa tabla de traducciones, esa tabla de
1031900	1038340	progulidades de traducción podría construirme las a la lineaciónes del corpus.
1038340	1043260	Así que bueno, hasta el momento decíamos bueno, suponemos que tenemos esta tabla de traducción
1043260	1048100	que me dice para bank, si se traduce, para bancos, si se traduce como bank o como
1048100	1053940	bench, etcétera, estaba diciendo que tenía esa tabla, pero en realidad la realidad que no
1053940	1059340	tengo esa tabla y me gustaría poder construirla. Entonces, no gustaría poder estimar esas
1059340	1063420	progulidades para construirme esa tabla. Si yo tuviera un corpus paralelo, simplemente
1063420	1067540	podría ir recorriendo el corpus y contando cuántas veces aparece banco al inado con
1067540	1073260	bench y cuántas veces al inado con bank y ahí sacaría una progulidad, pero no tengo
1073260	1079900	las a la lineaciónes. Y como lo que vimos digamos recién, si yo tuviera la tabla, entonces
1079900	1083140	yo va además poder ir recorriendo el corpus y construirme las a la lineaciónes. Así
1083140	1088060	que si yo tuviera las a la lineaciónes podría contar y sacar la tabla, si yo tuviera la tabla
1088060	1092700	podría pasarle un agorismo y construir las a la lineaciónes. Pero la verdad que no tengo
1092700	1097020	ninguna de las dos cosas, entonces se vuelve un problema de hueve la gallina, o sea, si
1097020	1100460	yo tuviera las a la lineaciónes, construiría el modelo, construiría la tabla de
1100460	1103660	progulidades, si yo tuviera la tabla de progulidades podría construir las a la
1103660	1110620	lineaciónes. Parece tipo de problemas en los cuales yo tengo como dos variables interdependentes
1110620	1114500	y no conozco exactamente el valor de ninguna de las dos, si utiliza lo que se conoce como
1114500	1120620	el algoritmo de expectation maximización o maximización de la esperanza. Y bueno, es un algoritmo
1120620	1125340	que sirve exactamente para este tipo de problemas. En realidad lo que va a hacer es el
1125340	1130660	algoritmo citerar, es un algoritmo iterativo que va tratando de convertir una solución y lo
1130660	1135340	que hace es decir, bueno, yo no tengo ninguno de los dos valores, o sea si yo tuviera
1135340	1142140	mi tabla de probabilidad de traducción, me podría calcular las a la lineaciónes y tuviera
1142140	1146780	mi salinación, me podría calcular la probabilidad de traducción. Entonces lo que hace es decir,
1146780	1151900	bueno, a sumo que mi tabla de traducción va a ser uniformes, digamos, cualquier palabra se
1151900	1155620	puede traducir como cualquier otra palabra con la misma probabilidad. A partir de eso, que
1155620	1159060	alculo de la lineaciónes, y a partir de esas nuevas a la lineaciónes, cálculo otra vez
1159060	1166740	la tabla. Y de vuelta con esa tabla que cálculo vuelva, medir las a la lineaciónes y
1166740	1172260	vuelta con esas nuevas a la lineaciónes, vuelvo a calcular la tabla. Entonces, aunque no me
1172260	1177100	crean, esto después de muchas iteraciones va convergiendo a algo, y parece mágico, ¿no?
1177100	1182460	parece como que tal realidad si yo no tengo ninguno de los dos valores, no debería como
1182460	1190340	dar fruta. Pero voy a tratar de comenzar los que en realidad esto si funciona, con un ejemplo.
1191260	1196940	Bien, tenemos. Entonces, vamos a construir un sistema que es de traducción entre frances
1196940	1201300	y lingles, donde hay un cuerpo muy grande, pero bueno, vamos a concentrar sobre el
1201300	1206100	entre pequeñas oración cita que dicen la mesón se traduce como deja, la mesón blu, se traduce
1206100	1211620	como de lujados y la flea o se traduce como de flower. Entonces, al principio lo que hago es decir,
1211620	1216780	bueno, todas las traducciones en todas las palabras son equiprobables, así que lo que me va
1216780	1221100	a quedar es cuando reparten de las salinaciones, todas van a tener el mismo peso. Entre la
1221100	1225780	y mesón, la probabilidad de que la se traduca como de, o que se traduca como javos, va a ser
1225780	1230700	la misma, en realidad, porque todas las salinaciones son equiprobables. En la mesón blu, también
1230700	1234860	va a ser lo mismo, la probabilidad de traducirla como de como blu o como javos, va a ser la misma
1234860	1244640	y en la flea pasa igual. Entonces, eso es la primera, el primer paso, digamos, en el
1244640	1249600	primer paso, yo voy a tener todas las salinaciones equiprobables y todas las los valores
1249600	1264240	de las palabras iguales.
1264240	1271040	Entonces, en mi algorithmo, yo empecé con una tabla de traducción que era todo uniforme.
1271040	1276560	Como yo tenía la probabilidad de traducir cualquier palabra en cualquier otra era la misma.
1276560	1281080	A partir de eso, yo me construí estas salinaciones, que también parece que son todas equiprobables
1281080	1285040	y parece que no tienen como mucha información. Entonces, lo que voy a hacer ahora, a partir
1285040	1289200	de esto, es tratar de construirme de vuelta, la tabla de traducciones, pero mirando estas
1289200	1294480	nuevas salinaciones que hay. Entonces, lo que voy a construir es una tabla que tiene
1294480	1312640	todas las palabras de las diferencias y en el mesón blu, blu, blu, blu, blu, blu, blu, blu, blu.
1312640	1317320	Y para llenar, esta nueva tabla es lo que tengo que hacer es iterar sobre las salinaciones,
1317320	1320960	mirar cada una de las palabras, cuantas veces está linear con las otras y contar, o sea,
1320960	1327440	y sumar los peso de cabunas de las salinaciones. Entonces, la lineación entre la y de
1327440	1331540	en total, mirando ese ejemplo de corpus, cuanto me daría de agua, cual sería el peso de
1331540	1339180	salinación. Para verlo, en realidad lo que hago es contar, miro cuántas veces la y de están
1339180	1345980	lineados. Entonces, tengo 0.5 de peso en la primera, en la segunda tengo 0.293 y en la última
1345980	1354100	tengo 0.5 de vuelta. Así que en total tengo como 1.33 de peso entre la y de. Después,
1354100	1360940	mira, entre la y j, cuanto peso tengo, cuanta masa de probabilidad tengo. Bueno, tengo 0.5 en la
1360940	1368180	primera relación, 0.103 en la segunda y nada en la tercera. Por lo tanto en total, tengo 0.83
1369100	1375300	de probabilidades entre la y j. Después, mira, entre la y blu, cuanto peso tengo.
1379540	1385220	0.303, solamente 0.33, sólo está en la y entre la y fler, cuanto tengo. No, entre
1385220	1391220	la y flavor, cuanto tengo. 0.5, sólo aparece en la del final. Bien, como lo tengo la siguiente,
1391220	1401820	entre msón y de cuanto tendría. 0.83, está en la primera y la segunda, entre msón y
1401820	1410860	j. En la primera y la segunda, entre msón y j. En la segunda, entre msón y j. Si,
1410860	1415500	se ve usted de trepo que aparece en las dos. Bien, entre msón y blu solamente aparece en
1415500	1420820	la segunda, así que voy a tener 0.33 y entre msón y flavor, no tengo nada. Después, entre
1420820	1428060	blu y de solamente aparece en la segunda, así que voy a tener 0.33, entre blu y j. Creo que
1428060	1433580	de vuelta tengo 0.33 y entre blu y blu también, 0.33 y no aparece junto con flavor.
1433580	1443980	Y para después para flar, tengo 0.5, donde 0.jero con j. 0.5 con flavor. Bien, entonces,
1443980	1448940	y si una pasada por todas las salinaciones y me calculé cuáles son los peso relativos de cada
1448940	1454140	una de estos pares. Lo siguiente que hago, como esto va a ser una probabilidad, es normalizar.
1454140	1458740	Entonces, no voy a construir una tabla, digamos, normalizando por, digamos, voy a sumar en cada
1458740	1463660	fila y voy a adir entre la cantidad que aparece para cada fila, así que, igual también.
1463660	1488100	Entonces, lo que voy a hacer es normalizar, entonces, si yo sumo a estos sacas, creo que me da dos
1488100	1496300	centodal, no, tres centodal, tengo los valores acá, vamos a tener que hacer los cálculos, pero
1496300	1502160	sí, me da tres centodal, entonces lo que pasa cuando yo normalizo es que acá me queda 0.24,
1502160	1510700	acá me queda 0.28, acá me queda 0.12 y acá me queda 0.17, pues el segundo también lo normalizo,
1510700	1521540	es entre 2 y me queda 0.42, 0.42, 0.16, 0, el tercero ya suma 1, así que me queda 0.23, 0.23,
1521540	1535980	0.23 y el último también queda igual, 0.5, 0, 0, 0, 0.25. Bien, entonces, me construí una nueva tabla
1535980	1541940	de probabilidad de traducción dado que ahora la salinación es serianistas, y no te lo que pasó
1541940	1552900	acá, si yo miro la fila correspondiente a la que lo que pasa ahora con esta fila, recuerden que yo
1552900	1557900	empecé de deniendo todas las salinaciones, todas las traducciones de pronto, todas las probabilidades
1557900	1563100	de traducción de equipares de palabras eran equiprobables, si yo ahora miro la fila de la que es lo que pasa,
1565980	1579740	es acto, aparece claramente que la asociación entre la idea es más fuerte, tengo un 0.44 de probabilidad de traducir
1579740	1586220	la como de y tengo bastante menos en los otros, tengo 0.28, 0.27 y yo había empezado diciendo que eran
1586220	1592500	equiprobables, entonces yo probablemente tenía 0.25, 0.25, 0.25, 0.25, 0.25 en cada una, y después de
1592500	1600460	un paso de la iteración, descubrió que la idea tiene más chance de ser una traducción
1600460	1606460	de la otra, en vez de traducirla como jados o la como blú o la como flower, eso pasa en
1606460	1611660	el primer paso, en la primera iteración, el tipo descubre, el algoritmo descubre que la
1611660	1618060	asociación entre la idea es bastante más fuerte, como pasa eso, lo que va a pasar es que cuando
1618060	1623120	yo reparto de vuelta en las alinaciones, estas líneas que se corresponden a la asociación
1623120	1628760	entre la idea van a estar más fuertes, van a tener un poco más de peso, y como esto es una
1628760	1633680	distribución de probabilidad es esa masa que ganó la asociación entre la idea, se va a tener
1633680	1637220	que sacar de otras alinaciones posibles, así la asociación va a con de, entonces no está
1637220	1643620	asociada con las otras que están alrededor, entonces esa masa que se pierde, digamos, o sea
1643620	1651000	que gana en la de, se tiene que repartir en las otras alinaciones posibles, o sea, en las
1651000	1656340	que no son entre la idea, entonces después de una iteración la asociación entre la
1656340	1663420	idea empieza a ser más fuerte, y como pasa eso, en la siguiente iteración va a empezar
1663420	1668060	a descubrir que como la estaba alinado con de, entonces me son tiene que estar alinado con jados,
1668060	1675340	y como me son estaba alinado con jados, digamos esa esa misma masa de probabilidad se va a
1675340	1680740	traducir a transferir a la segunda, y lo mismo, como le ha estado alinado con de, entonces
1680740	1687100	fler tiene que estar alinado con flower, entonces si yo sigo iterando en estos pasos, en cada
1687100	1690980	paso lo que va a pasar es que se va a mover un poco más de probabilidad, hasta que al final
1690980	1695980	va a terminar descubriendo cuál es la alinación real de las palabras, o sea va a descubrir
1695980	1702900	que la va, o sea, con de, me son con jados, luego con blue, luego con flower, como que va descubrir
1702900	1707060	eso, porque en cada paso lo que va pasando es que algunas de las asociaciones, como están,
1707060	1712300	como aparecen co-curren, digamos, en más oraciones, tienen más fuerza que otras, entonces el
1712300	1717060	peso que esas asociaciones ganan lo va sacando otro lado, y eso hace que de otro lado se
1717060	1724340	empieza a generar otras alinaciones diferentes, entonces al final esto termina convergiendo que termina
1724340	1728740	revelando lo que es la, la estructura, su yacente de las palabras, y como se alinian unas
1728740	1734500	con otras, bueno, bien, a ver que yo termine de hacer esto, puedo agarrar y construir me efectivamente
1734500	1740060	la tabla final de traducciones, que es simplemente busco cada una de las posibles traducciones,
1740060	1747420	digamos, de los posibles pares y saco las probabilidades, y qué pasó acá, mientras yo
1747420	1752500	estaba construyendo mi modelo traducción, mientras yo estaba construyendo la tabla de traducciones
1752500	1758340	además de, como efectos secundarios se construyó un corpus alinia, un corpus que está alineado
1758340	1771500	nivel de palabras, así que bueno, el algoritmo de espectrexión maximización, funcionan esa manera,
1771500	1777420	tiene siempre dos pasos, un paso de espectrexión y un paso de maximización, en este caso,
1777420	1784380	el espectrexión era decir el paso de espectrexión, se trataba de agarrar la tabla de
1784380	1789780	propiedad traducción que tengo, y con eso me damos alinianciones, y después el de maximización
1789780	1794260	es al revés, agarrar las alinianciones que acabo de construir y me damos una nueva tabla, y voy
1794260	1801660	alterando todos esos pasos hasta que eventualmente converg, bien, dijimos que eran 5 modelos
1801660	1806620	de IBM, nos vamos a ver muy en detrás y los otros, o sea, solo mencionar que empiezan a
1806620	1812420	agregar complejidad, en este modelo uno habíamos dicho que todas las alinianciones eran equiprobables,
1812420	1816900	en el modelo 2 abandonan esa noción y dicen bueno en vez de alinianciones equiprobables, yo voy a
1816900	1822180	tener un modelo de reordinamiento de las palabras para decir bueno, tengo cierta probabilidad de que
1822180	1826940	las palabras que están si yo tengo y palabras en inglés, jota palabras en español, tengo cierta
1826940	1832740	probabilidad de mover la palabra ahí y la palabra jota, y bueno ya sí siguen subiendo en complejidad
1832740	1838460	hasta llegar al modelo 5, que modelos 5 es el que anda mejor, pero de todas maneras estos
1838460	1845180	son modelos que ya no se usan, digamos esto es del año 93 y en general se han obtenido mejores
1845180	1850140	resultados abandonando estos modelos, entonces que vamos a pasar a ver a continuación, es un modelo
1850140	1855860	bastante más moderno que es lo que sí, si utiliza bien día en traductores como los de Google,
1855860	1873100	sí, es que en realidad lo claro, a ver estos modelos está dícicos no utiliza ningún tipo de
1873100	1878100	analizador un boludo jico, hay otros modelos que sí lo hacen, no vamos a dar ningún
1878100	1882580	no en esta clase pero está, hay otros modelos que sí hacen uso de esa información, igual
1882580	1887340	son como un refinamiento, creo que ninguno lo tiene como en la base del modelo, el uso de
1887340	1893380	partos pitch, pero sí cuando no sabes una palabra de una palabra que se conocida en realidad
1893380	1899500	utilizar información sobre el partos pitch y eso probablemente te ayuda, en esto modelo
1899500	1904100	por lo menos no lo habían tenido en cuenta, bien entonces sí lo que vamos a ver ahora es el modelo
1904100	1909140	de frases que es algo más moderno y o sea el Google Translate o Bing Translate se basan
1909380	1913100	el modelo de este estilo, y bueno antes de ver cómo se modió el frases volvamos un poco
1913100	1917500	de lo que era la alineación entre palabras, yo tenía estas frases clásicas, no María no di una
1917500	1924620	ofretada de la bruja verde, en inglés era Merit is Not Slap Greenwich y una alineación
1924620	1928140	entre esas dos oraciones en realidad se vería como algo así, yo tengo que María se alinea con
1928140	1934700	Merit no se alinea con disnot, se alinea con daba una ofretada de se alinea con ala podría ser
1934700	1942220	solamente con la y el que no esté alineona, grince alinea con verde y bruja con Wedch,
1942220	1946660	qué diferencia tiene esto con la otra alineación que habíamos visto hoy,
1946660	1955220	así se les ocurre algo distinto que tiene esta alineación y la que habíamos visto hoy,
1955220	1959820	era Not con No, sí, y que es lo que cambia acá para que pase eso.
1964700	1972580	Lo que estaba pasando hoy era que yo partida de las palabras en español y a las palabras
1972580	1975540	en inglés y yo tenía una función que me me apé a las palabras en español con las
1975540	1979340	palabras en inglés, entonces yo a cada palabra en español como máximo le podía hacer
1979340	1984180	corresponder una palabra en inglés, entonces me quedaba que yo podía expresar cosas como que
1984180	1989740	daba una ofretada daba esta ofretada a Slap una, esta ofretada, esta ofretada, esta ofretada,
1989740	1994260	esa ofretada, eso le podía expresar, pero no podía expresar algo como esto, que no, esta ofretada
1994260	1998340	es Not porque no sería una función, yo no puedo asociar uno de los valores de la función
1998340	2005420	con dos cosas de la olcodomínio y acá en realidad no puedo hacerlo ni en este sentido ni
2005420	2008620	en el otro sentido, con una función no me sirve porque de vuelta me pasa que Slap está
2008620	2012980	asociado tres cosas, entonces con una función de alineación yo no puedo construir este tipo
2012980	2019420	de expresiones, en realidad necesito algo como un poco más poderoso, esto es lo que decíamos,
2019420	2023980	los modelos dbms siempre usan un mapeo de uno a muchos, usan en una función de alineación,
2023980	2027420	mapeo de uno a muchos, pero en realidad lo que necesito para poder capturar realmente
2027420	2031900	vamos a funcionar en el lenguaje es mapeo de muchos a muchos, yo voy a tener que un conjunto
2031900	2036220	de palabra se va a traducir en otro conjunto de palabras, definitiva lo que pasa es que
2036220	2040460	pequeñas frases se traduce en como otras pequeñas frases, por eso necesito un mapeo de
2040460	2046460	muchos a muchos, entonces bueno hay algoritmos que agarran estos mapeos que como
2046460	2051940	el construimos recién el mapeo de uno a muchos en los dos, en las dos direcciones digamos
2051940	2056660	y a partir de eso construyen este mapeo de muchos a muchos, por ejemplo el algoritmo de
2056660	2060820	la herramienta quizás más, lo que hace decir bueno yo tengo un corpus en inglés en español
2060820	2067900	alineo utilizando los modelos dbms, voy alineo por un lado de inglés en español, por
2067900	2073140	otro lado de español en inglés, y acá me quedan dos mapeos de uno a n y vamos dos mapeos
2073140	2077980	con funciones, y después lo que hago es interceptar esos dos esa dosa de alineación que me
2077980	2086500	caron y unirlas, cuando la intercepto o tengo lo que se conoce como puntos de alta confianza no
2086500	2090540	se llegan a ver bien, los puntos negros son los puntos de alta confianza que son los
2090540	2094780	de la intersección y los puntos grises son lo que están en la unión, o sea los que
2094780	2098380	pertenecían algunos de los modelos, entonces la herramienta lo que hace es decir bueno una
2098380	2103340	vez que yo tengo la intersección y la unión hago crecer los puntos que están en la intersección
2103340	2107380	coeleonizando otros puntos que están en la unión, hasta que al final terminó completando
2107380	2111780	digamos todo el imagen, este punto que quedó solito ahí no sería parte de la alineación
2111780	2120740	al final, solo los que puede llegar moviendo de otra vez de puntos ya conocidos, entonces bueno,
2120740	2127380	eso es una forma que utiliza, se llama el algoritmo de ojinei, que partiendo de alineaciones
2127380	2131420	uniraccionales y vamos me permite construir una alineación completa, muchos a muchos entre
2131420	2136980	las palabras, bien, eso le quería mencionar acerca de las alineaciones de palabras y ahora
2136980	2141940	sí vamos a ver cómo funciona un modelo basado en frases, un modelo basado en frases tiene
2141940	2147460	cierto semejanza con el modelo anterior que hay hemos visto, pero es un poco más expresivo
2147460	2151300	en realidad yo parte de una oración, por ejemplo en Aleman que decía Morgan Flick y que
2151300	2156260	las canas de sus conference, lo primero que hace el modelo cuando quiere traducir, digamos
2156260	2161780	en este caso es decir bueno, yo voy a segmentar esa oración de origen en cierta cantidad
2161780	2166820	de frases, después voy a traducir cada una de esas frases usando una tabla de traducción
2166820	2169820	y esta vez no es una tabla de traducción de palabras sino que es una tabla de traducción
2169820	2175060	de frases que me dice para cada frase con que otra frase corresponde, y una vez que
2175060	2179620	es otra duje cada una de esas frases la voy a ordenar de alguna manera buscando que suena
2179620	2185100	el humanatural posible, buscando aumentar la fluidez de esa oración, entonces como que la
2185100	2188020	historia de generación es un poco más simple que la otra, no tenía que ir sorteando
2188020	2195300	cosas, simplemente digo separo mi oración en segmentos que le voy a llamar frases,
2195300	2201140	los traducos y los reordenos, esa segmentación en frases no tiene por que tener una
2201140	2205420	un significado lingüístico, yo no voy a separarla en grupo nominal, grupo global, grupo
2205420	2209140	profesional, etcétera, no tengo por qué, o sea, capas que los segmentos de la frases
2209140	2214260	y justo me queda un grupo preposicional capaz que no, lo único que tiene que pasar es que
2214340	2218460	estos segmentos que yo construyo tienen que estar en mitad de traducción de frases, alcanza
2218460	2221820	con eso como para que yo puedo utilizar los en mi traducción, pero no tienen por qué
2221820	2228900	tener una motivación lingüística, bueno, entonces un modelo basado en frases tiene
2228900	2233660	estos componentes, es parecido al anterior porque de vuelta, yo lo que quiero hacer es encontrar
2233660	2239340	la probabilidad de ese dado de ambos sigo teniendo la misma ecuación fundamental de la traducción
2239340	2245660	automática estadística, la quiero resolver, necesito pdfd y pd, solo que ahora el pdfd lo voy
2245660	2249580	a calcular una manera extinta, voy a decir que para calcular esto tengo un modelo de traducción
2249580	2254260	de frases y un modelo de ordenamiento, un modelo de una gran tabla de frases que me dice
2254260	2258980	cada frase con qué probabilidad la traducción no otra, y después una forma de decir cómo
2258980	2264420	reordenos a frases para tener mejores oraciones, y bueno, como siempre voy a tener otro componente
2264420	2272260	que es el que mide la fluidez que es el modelo de lenguaje, porque los modelos de frases
2272260	2276580	funcionan mejor que los modelos basados en palabras, porque las frases ya tienen cierto
2276580	2281540	contexto, las frases en realidad son como pequeños grupos de palabras que yo puedo traducir
2281540	2289860	uno en el otro, entonces cosas como dar la mano, dar una ofetada, tomar el pelo, etc.
2289940	2293900	esas cosas como expresiones son mucho más fácil de traducir si en realidad eso es así que
2293900	2297300	esta expresión que son tres cuatro palabras, le puedo traducir en esta otra expresión que son tres
2297300	2301980	cuatro palabras, y como más expresivo entonces pueda aprender más cosas, y bueno obviamente
2301980	2306200	cuanto más tenga, cuanto más largo sea el corpo, que yo tengo yo puedo aprender
2306200	2312860	frases más largas, mejores probabilidades, y mejores frases. Bueno, hay un ejemplo de como
2312860	2316580	sería una tabla de traducción de frases, o sea, es parecido la tabla de traducción de
2316580	2320980	palabras, o lo que acá tengo de enforçla, o sea, si yo busco la fila, asociada en
2320980	2324820	forçla, o sea, encontraría todas estas traducciones de proposa, el concediendo de
2324820	2329060	oposición de broalidad, posesivo proposa, el con 10 por ciento, a proposa, el con
2329060	2335180	3 por ciento, etc. O sea, como ven se traducen frases, en frases. Bueno, y como hago
2335180	2342180	para aprender una tabla de traducción de frases, yo parte de esta alineación de
2342180	2345420	palabras, digamos esta alineación completa, que ya no es una función, sino que es
2345420	2351500	digamos una alineación de muchos a muchos, y voy a tratar de encontrar todos los todas las
2351500	2355860	frases, todos los pares de frases que son consistentes con la alineación, a qué me refiero
2355860	2364020	con que son consistentes, a que hay ejemplos, yo quiero decir que mariano y mariano
2364020	2370460	no son un par de frases que son consistentes con esta alineación, en cambio, mariano y mariano
2370460	2375100	no son, como es que miro esto, lo que pasa es que cuando yo tengo mariano y mariano, la
2375100	2381060	palabra no esta alinea con 10 knot y el 10 knot, digamos, el knot no pertenece hasta alineación
2381060	2385540	que yo estoy dando decir, entonces digo que es no consistente, lo mismo pasa con si
2385540	2392020	yo dado alinear, mariano daba y mariano y mariano, lo que pasa es que daba no está, digamos,
2392020	2395020	los puntos de alineación de daba, no están dentro de este cuadrante que estoy dando
2395020	2399700	a buscar, entonces en definitiva digo que no es consistente, las alineaciones consistentes
2399700	2404180	correctas son las que consideran todos los puntos dentro de ese cuadrante, entonces mariano
2404180	2410180	está asociado con mariano de knot y esas y es consistente, así que como aprendo, frases
2410180	2417380	consistentes, en piezo por las alineaciones, digamos, el piezo con la alineación es una palabra,
2417380	2422420	después busco de una palabra y digo bueno, me quedo con todas esas traduciones de palabras
2422420	2426820	y las pongamitables de frases y después voy tomando de 2 y me quedo con todas esas otras
2426820	2431580	frases y la voy agregando, me quedo de frases, después me puedo avanzar en 1, tomar de
2431580	2437740	3, tomar de 4 y llegar a tomar incluso toda la oración como frases, entonces a partir
2437740	2443100	de estas oraciones que tenían, no sé, un 2, 3, 4, 5, 6, 7, 8, no hay palabras, yo termino
2443100	2450300	aprendiendo como 17 frases, digamos, cada vez más grandes y bueno, hoy voy sacando esto
2450300	2456020	de todo el corpus y calculando mitable de probabilidades, de qué manera, calcula esas
2456020	2460380	probabilidades, yo lo que puedo hacer es como siempre ver cuántas veces aparecen el corpus
2460380	2466420	y contar, o si no, si yo tenía construido el modelo anterior, el modelo de la tabla de
2466420	2470580	traduciones de palabra palabra, en realidad lo que puedo hacer es aprovechar ese modelo
2470580	2475540	traducción de palabra palabra y decir bueno, me arma una traducción entre un par de frases
2475540	2479860	basándome en las traduciones palabra palabras, son como formas distintas de construirlo y
2479860	2488500	a veces hasta complementarias, bien eso fue el modelo de frases, los modelos de frases son
2488500	2493180	los más usados hoy en día en realidad en lo que es la traducción automática, son los
2493180	2499060	candados mejor de resultados y bueno, no faltaba una cosa para terminar el toda la imagen
2499060	2506220	de lo que es la traducción automática estadística que es la decodificación, entonces
2506220	2513060	veamos un resumen de lo que teníamos hasta ahora, hasta ahora yo partí de yo quería resolver
2513060	2518460	la cocción fundamental de la traducción automática estadística y yo tenía un corpus paralelo
2518460	2522620	que tenía texto en el idioma origen y el idioma de estino y a partir de siendo analisis
2522620	2528580	estadístico yo me construí un modelo traducción que lo que vimos en esta clase, además yo
2528580	2533340	tenía cierta cantidad de texto del idioma de estino y a partir de cierto analisis estadístico
2533340	2538220	me construí un modelo de lenguaje que me dice que tan fluido es una operación en el lenguaje
2538220	2543700	estino, entonces ahora lo que me falta, recuerden que yo lo que tenía que hacer era
2543700	2547540	y te era sobre todas las oraciones del lenguaje estino y pasar las a través del modelo
2547540	2552260	traducción y del modelo de lenguaje para que me de la probabilidad de esa oración, bueno
2552260	2556980	lo que me falta es el agorismo de codificación que en vez de probar con todas las oraciones
2556980	2561740	de lenguaje estinos me va a decir unas cuantas oraciones para probar, porque me dice 150
2561740	2566700	oraciones para probar sobre las cuales utiliza el modelo traducción en modelo de lenguaje,
2566700	2572860	entonces esto es como un diagrama de modulos en los cuales el agorismo de codificación utiliza
2572860	2580780	los dos modulos, tanto es la traducción como el lenguaje, bueno, como funciona el agorismo
2580780	2588460	de codificación, que vamos a ver es un agorismo de codificación de tipo bean search y bueno
2588460	2592900	la función de acinde manera, yo tengo la oración María no dio una ofetada a la bruja verde
2592900	2598820	y la quiero traducir al inglés y tengo una tabla de traducción de frases
2598820	2604620	entonces mi oración María no dio una ofetada a la bruja verde, yo busco en la tabla de frases
2604620	2610060	¿Cuáles de esas digamos? ¿Cuáles segmento? ¿Cuáles subsegmento de esa oración?
2610060	2613660	yo puedo encontrar en la tabla de traducción de frases, todo lo que me encanta por ejemplo que
2613660	2618500	María lo pota o sí como Mary, no lo busco en la tabla y lo pota o sí como not como
2618500	2625060	not o como no, dio lo pota o sí como guir, pero además no dio esa frase entera, yo le busco
2625060	2630220	en la tabla y me aparece que la pota o sí como not guir, dio una ofetada a toda esa frase
2630220	2637900	lo pota o sí como slape, una ofetada lo pota o sí como aslape y bueno de otras cosas
2637900	2641060	bruja lo pota o sí como witch, verde como green pero además en algún lado de la tabla
2641060	2647700	tengo que brujar verde lo puedo traducir como green witch y así, yo puedo encontrar diferentes
2647700	2652220	maneras de segmentar la oración y además para cada uno de esos segmentos puedo encontrar distintas
2652220	2659620	formas de traducirlo en el lenguaje destino con mitable de frases, entonces el algoritmo de
2659620	2664060	codificación funciona de la siguiente manera, empezamos teniendo en cada paso el algoritmo
2664060	2668820	vamos a tener un conjunto de hipótesis de traducción, se llega a ver ahí lo que dice a
2668820	2683940	ojos, más o menos, bien, acá que eran malos, correctes, bueno, en cada uno de los pasos
2683940	2690700	yo voy a tener un conjunto de hipótesis de traducción, al principio el algoritmo voy a empezar
2690700	2696020	con una hipótesis vacía, como se le este hipótesis dice que lo importante de leer es la parte
2696020	2699220	de la defe que tiene un montón de guiones, significa que no hay ninguna palabra del español
2699220	2704580	cubierta, esas son todas las 9, 9 palabras en español, ninguna esta cubierta y esta hipótesis
2704580	2710860	tiene probabilidad 1, entonces en cada paso el algoritmo lo que voy a hacer es elegir un par de
2710860	2715580	frases, tal que una traducción de la otra y voy a crear un hipótesis nueva a partir de una
2715580	2721020	que ya tengo, entonces en este paso lo que dice fue decir el hijo, el par de frases María
2721020	2727820	Mary y ahí me creo, una nueva hipótesis que cubre la primera palabra, por eso parece una
2727820	2731820	cerita en este caso, el hijo, la frase en inglés Mary y ahora tiene una probabilidad
2731820	2737180	de 0.584, ese número de esa probabilidad va a servir para guiar un poco en el algoritmo
2737180	2740420	pero vamos a ver después como es que se calcula, porabra que él se solamente con el
2740420	2745860	número, bien, pero entonces yo tenía otra opción, en realidad yo podía haber elegido
2745860	2750140	empezar en vez de traducir María por Mary, podía haber elegido empezar por traducir
2750140	2757860	bruja por witch y ahí me crearía otra hipótesis de traducción donde cubro la penúltima
2757860	2764340	de las palabras en español agarró la palabra witch, de el hijo de la palabra witch y tiene
2764340	2770780	una probabilidad de 0.882. Entonces, en cada paso el algoritmo lo que hace es elegir una
2770780	2775540	el hipótesis que tiene elegir un par de frases y expandir, así que lo siguiente que
2775540	2780260	puedo hacer es elegir la frase, dir not, expandirla a partir de la hipótesis que tenía con
2780260	2786620	Mary y bueno eso me cubre ahora dos palabras en español y me tiene medio otra probabilidad
2786620	2792460	y después, si gobanzando y si gobanzando, hasta que llegó a cubrir en algún momento, si
2792460	2796540	yo sigo avanzando y sigo arregando hipótesis, en algún momento voy a llegar a cubrir todas
2796540	2802460	las palabras del idioma español, todas las palabras de elaboración en el idioma español.
2802460	2807340	Entonces ahí una vez que yo cubrito a las palabras digo bueno, esto es una hipótesis completa
2807340	2814180	y esto lo devuelvo como un potencial candidata, digamos, una abracción candidata a traducción.
2814180	2818180	Pero claro, media que yo fie avanzando una cosa que paso es que fui dejando hipótesis
2818180	2823540	colgadas y esas hipótesis podrían tener otras traducciones posible, yo acá lo que devolí era
2823540	2827300	una hipótesis de traducción, pero a medida que yo tenía las otras hipótesis, si yo hubiera
2827300	2833020	seguido por las otras hipótesis hubiera podido devoler otras cosas. Entonces, yo necesito
2833020	2838100	hacer un backtracking para poder devoler todas las posibilidades, poder volver a ver las hipótesis
2838100	2843300	a revisitar las hipótesis y que había dejado cogeadas y volver a explorar los otros caminos.
2843300	2849620	Entonces, necesitarías en un backtracking para recorrer las todas. Y si hago un backtracking,
2849620	2856620	lo que va a pasar es que voy a va a ocurrir una explosión de exponencial de la espacidad
2856620	2861460	de búsqueda, porque en realidad todas las posibilidades que se abren son exponenciales
2861460	2867740	y ahí esto como que se vuelve bastante lento. Entonces, yo quería un decodificador para
2867740	2872060	volver este problema un problema tratable. En vez de agarrar las infinitas oraciones del idioma,
2872060	2877060	me quedo con algunas que sea más probable. Con esta acorrimo de codificación, logré reducir
2877060	2883660	de infinito a algo finito, pero aún así es demasiado lento, porque hay una explosión combinación
2883660	2889980	combinatoria de asipotesis y me quedo una cantidad exponencial de hipótesis. Entonces,
2889980	2894580	como es tan grande este problema, digamos como la cantidad hipótesis de exponencial y este
2894580	2900860	es un problema en EP completo, entonces se utilizan técnicas para reducir el espacio de búsqueda.
2900860	2905340	Y hay como dos tipos de técnicas, algunas son con riesgo y otras son sin riesgo. Las técnicas
2905340	2910000	sin riesgo, lo que quiere decir es que si yo aplico una técnica de reducción de hipótesis,
2910000	2916020	sin riesgo, la solución ideal que yo tenía, dentro de mi búsqueda, no le voy a perder utilizando
2916020	2920140	una técnica sin riesgo. En cambio en la con riesgo, si yo podría llegar a perder la solución
2920140	2926100	óptima. Bien, entonces, la técnica sin riesgo que conocemos es la de recombinación de hipótesis,
2926100	2930300	que dice que si yo tengo dos hipótesis, voy avanzando por dos caminos, dentro del algoritmo
2930300	2935060	y llevo a dos hipótesis iguales, por lo menos dos hipótesis que cubren las mismas palabras,
2935060	2940140	entonces me puedo quedar con la que tiene mayor probabilidad de las dos y descartar la otra.
2940140	2943040	Porque, porque a medida que yo voy a seguir avanzando en el algoritmo, lo que va a pasar
2943040	2946920	es que van a bajar las probabilidades, digamos, elegiendo más palabras y elegiendo más
2946920	2952620	frases, me va a bajar la probabilidad y nunca me va a pasar que una de las hipótesis que
2952620	2956780	tenía menos probabilidad vaya a subir en realidad, siempre va a tener menos. Entonces,
2956780	2961600	en definitiva, yo puedo conseguir de descartar la que tiene menos probabilidad. Bueno,
2961600	2967240	esa es recomendación de hipótesis, pero ni siquiera con eso, alcanza, digamos, para
2967240	2971720	reducir el espacio de búsqueda, lo suficiente, aún queda muchísimas hipótesis. Entonces,
2971720	2976360	sólo utilizar técnicas de podado con riesgo, la técnica de listo grama, la técnica de
2976360	2980360	lumbral, el listo grama significa que, a cada paso, digamos, en cada paso el algoritmo,
2980360	2984920	yo me quedo con los N, las N hipótesis de traducción más probable y descartó las
2984920	2990400	otras. Y la técnica de lumbral dice que, a cada paso el algoritmo, me quedo con la hipótesis
2990400	2995200	de mayor probabilidad y las que estén a una distancia alfa máxima de esa.
2995200	3002040	¿Cuál es el riesgo de las técnicas de podado? Que si la mejor traducción y la traducción
3002040	3006200	óptima tenía algunas frases muy poco probable, es al principio, entonces probablemente yo
3006200	3011720	descarte esa solución en los primeros pasos y no lleguen a contar la solución óptima.
3011720	3018760	La pérdida, por eso yo haber podado. Sin embargo, bueno, tiene como, como ventaja que en realidad
3018760	3026040	reducen muchísimo el espacio de búsqueda y vuelve este problema, un problema tratable.
3026040	3029560	Bueno, y ahora sí, qué significaba esa probabilidad que estaba viendo en cada una de
3029560	3035040	asipótesis. O sea, el podado necesita tener las mejores asipótesis y bueno, para la
3035040	3039360	recomendación también exitos a ver la probabilidad de asipótesis. Bueno, la forma de calcular
3039360	3043320	la probabilidad de asipótesis se divide en dos, digamos, tengo lo que, en contraste al
3043320	3047080	momento, el asipótesis se va a cuidar a cierta cantidad de palabras. Entonces, para
3047080	3051160	esa cantidad para la verdad, que se llevó cubiertas, utilizo los 3 modelos en modelos de
3051160	3055760	traducción, el modelo de rodeonamiento y el modelo de lenguaje, utilizo los 3 modelos para
3055760	3061360	calcular la probabilidad de las frases hasta el momento, pero para lo que me falta traducir,
3061360	3065440	yo no puedo utilizar todo porque no tengo toda la información de traducción, entonces lo
3065440	3069440	que hago es utilizar solamente el modelo de traducción y el modelo de lenguaje. Descarto
3069440	3074080	el modelo de rodeonamiento y bueno, entonces algo, calcula una probabilidad que es una parte
3074080	3079680	de con todos los 3 modelos y otra parte sin el modelo de rodeonamiento. Bien, este algoritmo
3079680	3084680	que acabamos de describir que hace esta búsqueda basándose en hipótesis que utiliza
3084680	3090520	recomendación y podado hipótesis y bueno, calcula las probabilidades de esta manera,
3090520	3095680	se conoce como algoritmo búsqueda asterico, es un algoritmo de vincers que se usa muchísimo
3095680	3101600	en lo que es traducción automática estadística. Por ejemplo, el sistema Moses, acá tenemos
3101600	3108000	este ejemplos de herramientas o pensores o gratuitas que siguen para construcción de traducción
3108000	3113720	automáticos. Es el sistema Moses, es un sistema o pensó para desarrollar este tipo de traducción
3113720	3120800	automáticos estadísticos y hay implementa este algoritmo de codificación de búsqueda asterico.
3120800	3125480	Y bueno, lo que tiene el sistema Moses de Buenio es que en realidad lo que hace además
3125480	3130680	de implementar el de codificadores utiliza a los otros sistemas y los integrar alguna manera.
3130680	3135800	Entonces, integra este otro sistema al ERCTLM que es una herramienta para crear modelos
3135800	3140240	del lenguaje basados en el gramas y el otro sistema es el quiso más más que lo veo, mencionado
3140240	3147320	hoy que es el sistema que me permite alinear corpus de operaciones en los distintos
3147320	3152680	sitiomas llegando a los modelos del 1 ad 5 de traducción de BMS. Bueno, entonces, esta
3152680	3156760	tres herramientas, si uno quiere construir un tradutor automático estadístico, entre cualquier
3156760	3162920	par de idiomas, puede utilizar estas tres herramientas y tenían un corpus paralelo y un corpus
3162920	3168120	monolingue puede construir un tradutor. Pero, bueno, además, otra cosa que me enseñamos en la
3168120	3173160	clase basada, pero eran los sistemas basados en reglas, los sistemas basados en reglas han caído
3173160	3178680	un poco, y a monotiene tanta popularidad como antes. Sin embargo, algunos se siguen usando,
3178680	3182680	y el sistema aperty un sistema o pensor para construir sistema de traducción basados
3182680	3188520	en reglas, que tienen un montón de pares de lenguajes. Y, bueno, ya anda relativamente bien,
3188520	3193520	digamos, entonces, se sigue desarrollando esta hoy, entonces, es una alternativa o pensor que
3193520	3197880	está basada en reglas en vez de estar basado en estas idicas.
3197880	3204400	Y, bueno, esta es un resumen de lo que vimos, así que dejamos por acá.
