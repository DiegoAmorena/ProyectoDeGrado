Bueno, ¿cómo andan? Bienvenidos a la versión 2.16 del curso de Procesamiento del Linguaje
Natural, Introducción al Procesamiento del Linguaje Natural. Somos unos cuantos. Este
es un curso que hace 10 años, si no me equivoco estamos dando, que se ha venido afianzando
y que lo que intenta presentar son los fundamentos de lo que se llama Procesamiento del Linguaje
Natural, que es esencialmente cómo procesar con computadoras de una forma más o menos
eficiente el Linguaje 1. En la clase de hoy la idea es presentar un poquito en qué consiste
el curso y dar una introducción general. Y bueno, en la sucesiva clase lo que vamos
a tratar de recorrer es todos los temas o los grandes temas del Procesamiento del Linguaje
Natural. El Procesamiento del Linguaje Natural van a ver que es una cosa que tiene muchas
etapas y en cada una de ellas se puede profundizar mucho. La idea de este curso como su nombre
lo indica es un curso de introducción, es un curso, a mi me gusta decir que es un curso
más largo que profundo, digamos. O sea, tratamos de cubrir los temas principales y los algoritmos
y métodos principales, pero teniendo claro y presente que en cada u prácticamente todos
los temas que nosotros tocamos se puede profundizar muchísimo. La idea del curso es generar, proveerlos
a ustedes de un set de, primero de los conocimientos básicos que se necesitan para, del dominio,
es decir, la semana que viene con Luis van a tener un par de clases, la que llamamos
volver al liceo, de idioma español, digamos, para entender el dominio del que estamos hablando.
Y luego un conjunto de herramientas y métodos tradicionales tratando de tener una visión
transversal de métodos porque hay métodos que van a ver que se repiten. Un poco ese
es el objetivo. ¿Está? Barraquemos con un poco de los datos del curso. Los docentes somos Luis Chiruso,
que está por acá adelante, y Yacín Cevallo, que se suma este año y que va a encargarse el laboratorio
y que les habla. El horario es los martes en el Salón 306, donde estábamos viendo a ver si entrábamos,
y los jueves en este Salón 301, de 20 a 22 horas. Si les parece que el horario no es
el mejor, piensen que la primera edición del curso era este horario, pero en el Salón
segundo subsuelo ha sido un frío horrible, o sea que siéntanse afortunados de estar acá.
Este es el correo para los docentes del grupo. Esta es la página web, pero en realidad la
página web lo que tiene son los datos fijos y lo que de ahí nos vamos es a Eva inmediatamente,
que es donde vamos a gestionar toda la información del curso. Vamos a tratar de que todo el vínculo
del curso se haga a través de Eva, publicación, entrega, mensaje, bla, bla. ¿De acuerdo?
Si usted si hay alguna duda me interrumpe porque yo no suelo parar. Bueno, la modalidad del curso es,
tratamos de que sea un curso participativo, eso van a ver que tiene que ver también con
cómo evaluamos, pero un poco, el objetivo del curso es ese que yo les decía de presentarle
el procesamiento del guaje natural, pero uno de los objetivos adicionales, lo que para nosotros es
muy importante es tratar de que este curso sea una especie de introducción a la investigación.
También se está dado como posgrado, donde esencialmente la diferencia con los cursos
tradicionales de grado, se podría decir que acá vale que haya preguntas sin respuestas,
o preguntas que queden planteadas, o cosas que podemos decir no sabemos cómo se hace,
que no hay una receta. ¿De acuerdo? Entonces, un poco tratar de transmitir eso que es lo que pasa siempre
cuando uno investiga. Si, se encuentra con cosas que no sabe cómo hacerlas, que no tiene un docente
que sabe todo, que le va a decir, entonces de esta forma. Un poco introducirlos en eso.
Las clases van a ser teóricos prácticas, vamos a hacer presentaciones y vamos a mostrar los
principales algoritmos, y vamos a poner algunos prácticos que nunca nadie los hace, pero bueno,
cada cual. Los que lo hacen, los además. Y además vamos a estar en Open Film a partir de este año,
quiero agradecerme esencialmente a los amigos de Open Film, lo van a poder ver desde su casa. Yo
igual les recomiendo que vengan, venir en realidad no les va a cambiar nada desde el punto de vista
de los temas del curso, porque vamos a publicar todo y las clases van a estar filmadas, pero yo
creo que está bueno venir porque hay cosas que se transmiten mano a mano que sirven, pero es una
decisión de ustedes. Trataremos de hacerlo tan entretenido como se pueda, como para que no
quedáramos solos acá. Y la evaluación del curso va a ser, por un lado un proyecto que va a ser el
35% de los puntos, es un trabajo que presentando más o menos fin de septiembre o una cosa así.
Algunos años hemos hecho dos en tres, una todavía no sabemos bien cómo va a ser, pero es un trabajo
que implica una tarea progresiva, que trata de aproximar algo que no se ve en el teórico, que
es herramientas, es decir, ¿qué pasa en el mundo real, cómo lo aplicamos?, bibliotecas, cosas de esas
y obtener resultados. Una tarea cómo se resuelve. El año pasado, el año pasado les dimos un corpus,
un conjunto de críticas de películas y tratábamos de hacer un clasificador que predijera a partir
del texto si la crítica era positiva o negativa. Supongo que es el tipo de tarea que se hace,
eso se llama sentiment análisis y está bastante de modo. Después también, y para nosotros tiene
que ver con eso que le decíamos del objetivo de la investigación, la idea es que ustedes en grupo,
van a tener que ir pensando en un grupo de tres, en grupo presenten un artículo científico,
vamos a poner una lista de artículos, ¿de acuerdo?, y ustedes van a tener que leerlo,
tratar de entenderlo y exponerlo a los demás. Eso es lo que uno hace esencialmente en una
conferencia, lo metes con artículos propios. Y otro grupo, la idea es que critique ese artículo,
no con el ánimo de hacer que los estudiantes que lo presentaron pierdan, sino con el ánimo de obtener
más conocimiento, que es lo que sucede en las conferencias. Entonces viene por ahí. Tanto el
que presenta como el que lo critica, tratan de entender y de generar conocimiento para todos
los demás. Y ahí como yo les decía, ¿vale decir esto no lo entendí? No vale leerlo una vez si no
lo entendí, hay que buscar referencia en otros lados, porque ahí habla de cosas que uno necesita
para informarse, armar ese paquetito y hacer una presentación. Eso se lleva al 25% de los puntos,
y luego hay al final una prueba individual escrita, que es el 40%, pero que tiene un mínimo del 60%.
Nada en este curso está pensado para que sea horrible y difícil, porque al ser un curso que
tiene muchos temas, pedirles, como les decía, profundizar en un tema sería muy amplio. Serían
demasiados temas, como para yo pedirles que sepan un montón de cada uno. La prueba generalmente lo
que evalúa es si ustedes conocen los principales algoritmos que se presentan en el curso, algunas
preguntas teóricas, pero no está pensada para que sea muy difícil, porque hay otras instancias de
evaluación, no está todo concentrado ahí. Pero sí es parte de la aprobación integral del curso,
que quiere decir que si alguien no aprueba esta prueba, no aprueba todo el curso. Generalmente no,
ponemos dos instancias, una va a ser a fin de año y otra en febrero, donde cada uno puede presentarse
a cualquiera de las dos. Se presenta a la primera y si la pierde se presenta a la segunda o se presenta
derecho a la segunda, etcétera. Si se presenta a la primera y salva no puede presentarse a las
segundas. Bueno, gente que puede querer subir las notas, no se puede, ponen cosas raras. ¿Alguna
duda del funcionamiento? ¿No? ¿Alguna duda? Bueno, el curso tiene esas áreas que dice ahí,
vamos a ver un poquito en detalle hoy en general, vamos a repasarlo. Es un curso que,
aunque el procesamiento del lenguaje natural, como seguramente le hicieron en cualquier diario
que se precie de moderno, ha tenido un desarrollo enorme el último año, no sé qué, este temario
no ha cambiado tanto porque yo lo que quería mostrar es que los fundamentos vienen como muchas
cosas de hace mucho tiempo atrás y lo que han modificado son los métodos y eso vamos a hablar
en el curso. Como ven son unos cuantos temas. El libro del curso es este, Speech and Language
Processing and Introduction to Natural Language Processing, Computation and Linguistics and
Speed Recognition, de Martín Ixurafqui. Este libro está por salir en su tercera edición.
El libro cubre una cantidad de cosas, de temas más que en este curso no se ven y es un libro
muy recomendable y además, idealmente ustedes deberían leer los capítulos, después de cada
tema nosotros presentamos los capítulos correspondientes del libro que se pueden,
en lo que está basado en la clase y ustedes deberían leerlo y saberlo,
cosa que nadie hace pero yo tengo la obligación de recomendarlo y me parece que es realmente
bueno para entender bien de qué se trata. Y bueno, esto es el libro de NLTK que es la
biblioteca que vamos a usar para una de las bibliotecas que vamos a usar para la parte
práctica, para la parte laboratorio y esto está más orientado a la implementación. NLTK es una
biblioteca en Python pensada principalmente para enseñar pero que muchas veces se usa en
producción también, ha evolucionado mucho. La plataforma de software que vamos a usar es
Python y NLTK, no sé cuánto, no les voy a preguntar porque ya se van a tener a leer,
no sé cuánto saben Python pero los que no saben van a tener que aprender y van a estar fascinados
y NLTK es la biblioteca. Freeling es una herramienta especialmente para el procesamiento del idioma
español, en estas cosas hay muchas herramientas que dependen un poco del idioma, si bien eso
vamos a ver que ha cambiado en los últimos años porque los métodos estadísticos son menos idioma
dependientes pero de todos modos no es lo mismo parciar para español que para el inglés y menos
para el chino. Freeling es una herramienta pensada para el español. Los laboratorios se van a
entregar en como notebook de IPython. ¿Conoce IPython? ¿Nadie conoce? Es una forma de distribuir,
ya lo van a ver, mezclar código y documentos. Es como un documento que tiene un motor de
programación atrás, entonces uno puede mostrar el código y explicarlo arriba en el mismo documento.
Y Scikit Learn que es una herramienta de aprendizaje automático genérica. Esto es lo que se pide saber,
formalismo de estado finito, teoría del lenguaje, gramática formales, teoría del lenguaje,
lógica de predicado como 14 materias pero por lo menos lógica, un poco de probabilidad estadística si bien
vamos a revisar porque generalmente uno se olvida de probabilidad estadística en la carrera y nosotros
se lo tratamos de recordar y bueno obviamente un poco de programación. ¿Hay alguien que lo esté tomando
claro que no sea de ingeniería? No. ¿De lingüística? Bueno, muy bien. Siempre está bueno tener gente
de lingüística y tengo que cuidarme con las cosas que digo ¿no? Intercambiaremos chistes con la lingüista,
intercambiaremos dije, hay una gran discusión en todo esto del procesamiento del lenguaje natural,
ya vamos a hablar de eso, ahora entra un rato vamos a hablar de eso. Capaz que para el caso tuyo podemos adaptar
un poco la tarea por el tema de la programación y demás, no hay problema, lo hablamos. Ah, eso me faltó decir,
los estudiantes de popogrado además de hacer el laboratorio, además de aprobar el curso quiero decir,
van a tener que hacer una prueba, un trabajo especial para obtener los créditos de popogrado,
ese es un reglamento solo para los estudiantes de popogrado. ¿Alguna duda hasta acá? ¿No? ¿Ninguna duda?
¿Me faltó algo o no? Ese es el curso. Entonces como no hay duda vamos a arrancar con la clase,
esto necesito cerrarlo, voy a perder todo.
Generalmente las clases duran una hora y media más o menos, nunca uso dos horas, pero tampoco doy pausa
una hora y media y a veces se va un poquito más largo pero no mucho más. No mentira, a veces dura como dos horas.
Bueno, arranquemos con una introducción, esta primera clase lo que trata de mostrar es la big picture,
es decir, que es todo lo que hay en el procesamiento de lenguaje natural o todo,
una descripción general de que es y que es lo que hay en el procesamiento de lenguaje natural.
Y bueno, vamos a empezar por responder ¿qué es?
El procesamiento de lenguaje natural es un conjunto de métodos y técnicas eficientes
desde un punto de vista computacional para la comprensión y la generación del lenguaje natural.
Acá tenemos varias cosas interesantes para mencionar, uno es el eficiente desde un punto de vista computacional,
el procesamiento de lenguaje natural está pensado como una rama ingenieril
y eso es lo que lo diferencia de la lingüística computacional.
La lingüística computacional es la que estudia toda la teoría del lenguaje,
de cómo se produce, de cómo se generan, de los esquemas, no sé qué,
pero esto es algo que busca ser eficiente desde el punto de vista computacional,
el objetivo es para que lo aplique una computadora, para que se resuelva de forma eficiente.
Y hay dos grandes vertientes en el proceso de lenguaje natural que son la comprensión y la generación,
o sea, una cosa es tratar de entender cuando alguien me habla y otra cosa es generar.
En general para una, bueno, en general no, depende de la tarea, es, no, depende de la tarea,
es más fácil de la que yo iba a decir, pero yo iba a decir algo que está mal.
Son dos tareas diferentes, una es entender, descifrar la señal, ahora vamos a la etapa,
y entenderla, y otra es generar lenguaje, generar cosas de nuevo apañol.
¿Qué quiere decir comprender el lenguaje? ¿Qué interpretan ustedes como comprender el lenguaje?
¿Qué quiere decir que una computadora comprenda el lenguaje?
Es una pregunta muy difícil y muy debatida, digamos, porque una de las teorías es poder responder preguntas
Una de las teorías es poder responder preguntas sobre eso, es una forma de ver si se comprende,
pero algo que memorizara mucho texto y que respondiera, no sabemos si está comprendiendo,
pero capaz que responde bien, hay un, en 1950, 50, bueno, por ahí, debería saberlo esto,
Alan Turing escribió un paper que creaba la inteligencia artificial de alguna forma,
que se preguntaba si las máquinas pueden pensar y proponía lo que se llama el test de Turing,
que es, si yo no veo con quién estoy hablando y una computadora logra hacer que yo, ser humano,
crea que estoy hablando con un humano, tendremos en ese, habrá pasado el test de Turing y podríamos estar hablando de inteligencia artificial.
¿De acuerdo? Es como la definición que proponía Turing de inteligencia artificial,
o de qué quiere decir que una máquina piense. Por lo tanto, muchas veces se dice que el lenguaje natural,
que resolver el problema del lenguaje natural no es ni más ni menos que resolver el problema de la inteligencia artificial,
o que es inteligencia artificial completa, ¿verdad? Eso, sobre eso hoy en día hay algunas variantes,
siempre ha habido algunas variantes, porque con la gran capacidad de cómputo que tenemos y la gran cantidad de datos,
podríamos llegar a hacer algo parecido, si bien hay un concurso que se hace todos los años que trata de jugar al test de Turing,
y estamos lejos, lejos, lejos, pero además, pero lo que ha sucedido en los últimos años,
que hay una cantidad de tareas específicas que se ha disparado el funcionamiento,
por ejemplo, la traducción automática, ahora vamos a hablar un poquito, se ha disparado,
pero hay gente que dice, bueno, sí, todo bien, pero esto se ha hecho en base a number crunching,
a mucho procesamiento y a mucho dato, pero en realidad no entendemos.
Si hay un conjunto de preguntas, ahora se me escapa el nombre, pero que son un conjunto de preguntas que para un ser humano son muy sencillas,
ahora vamos a ver algún caso, y las computadoras siguen siendo muy difíciles de resolver, ahora después vamos a volver sobre este tipo de preguntas,
había un alfajor sobre el lavarropa, vino Luis y se lo comió, ¿se comió el lavarropa o el alfajor?
Esa es una pregunta fácil para un ser humano y para un computador es bastante complicada, todavía está más lejos que eso, ¿de acuerdo?
Es decir, preguntas que para nosotros son muy sencillas, pero que exigen algún tipo de comprensión que las computadoras memorizando no alcanzan.
Sin embargo, por otro lado, están los que dicen, sí, pero en cualquier tarea que vos me pongas, la mejora en los últimos 20 años ha sido impresionante.
Ahí es donde discutimos, por ejemplo, con los límites.
El PLN no es igual a la lingüística computacional porque la lingüística computacional es una cosa mucho más rica,
es como la combinación entre la lingüística y la computación involucra el estudio científico del lenguaje en general,
lingüistas, informáticos, lógicos, psicólogos cognitivos, es como la gran tarea.
El PLN puede verse como la rama ingenieril de la lingüística computacional.
Si yo me llego a caer de esta tarima, editan, ahí con él.
Bueno, vamos a ver si podemos, si nos anda el audio, nos anda muy fuerte.
Sí.
Está más que se escucha horrible debajo, ¿no?
Abre las puertas de la pared, por favor, HAL.
Hola, HAL, ¿me escuchas?
Hola, HAL, ¿me escuchas?
¿Me escuchas, HAL?
¿Me escuchas, HAL?
Hola, HAL, ¿me escuchas?
HAL, ¿me escuchas?
¿Me escuchas, HAL?
Afirmativo, Dave.
Te escucho.
Abre las puertas de la pared, HAL.
Lo siento, Dave.
Me temo que no puedo hacerlo.
¿Qué es el problema?
Creo que sabes lo que es el problema, solo como yo.
¿Qué estás hablando, HAL?
Esta misión es muy importante para mí, para que te permitan que te jepardice.
No sé lo que estás hablando, HAL.
Sé que tú y Frank estábamos planeando desconectarme.
Y temo que eso es algo que no puedo permitir que suceda.
Cada vez son menos.
Esto es...
Oficial del espacio.
Y esencialmente, para los que no escucharon bien...
Hay una computadora, que es la que maneja la nave, esa MAHAL, al 9000.
Y acá, por cuestiones de la trama, digamos, le está pidiendo al astronauta que le abra la puerta y amablemente la computadora le dice que no lo va a abrir porque él la quiere desconectar.
HAL 9000 viene a ser como...
HAL 9000 viene a ser como el...
El ejemplo de una computadora que es capaz de procesar completamente el lenguaje natural.
Y si ponemos un poco de atención, ¿qué son las cosas que HAL puede hacer?
Para empezar, es capaz de...
Comprender a los humanos.
¿De acuerdo?
Es capaz de reconocer el habla.
Porque el muchacho le habla y la computadora, por supuesto, lo escucha.
Pero no solo escucha que está hablando, sino que transforma esa señal en algo y comprende lo que le dicen.
Es decir, lo decodifica de forma de poder entender lo que le dicen.
Pero además, es capaz de generar lenguaje, es capaz de hablar, o de generar un modelo, a partir de su modelo, generar algo que quiere decir y emitirlo.
Hoy en día, las computadoras más o menos hacen eso.
Hace 10 años, cuando empezamos este curso, las computadoras más o menos no hacían eso.
Hacían un poco de esto.
En el caso...
Más de eso no podíamos opinar.
Hoy tenemos a Siri y todo eso, que se supone que entienden.
Todavía no entienden mucho.
Todavía sigue siendo bastante divertido hablar con ella, ¿no?
Pero este...
Pero la reman, la reman, digamos, ¿no?
En esa época ni la remaban.
Tenías que hablarle justo en el lenguaje.
Hablarle con tu voz y mea.
Hal no tiene problema porque él le habla normal y Hal le contesta normal.
Normal, digamos, igual que nosotros.
Y si se ponen a pensar, Hal es indistinto, es indistinto, es indistinto.
Se contesta normal.
Normal, digamos, igual que nosotros.
Y si se ponen a pensar, Hal es indistinguible de un ser humano, digamos,
salvo por la voz que es un poquito metálica para ser de un ser humano,
después...
Este...
Pasa, ¿no?
Y que yo creo que le ponen el metálico para dejarlo un poco computadora, digamos,
para no pasarse para el otro lado.
Entonces, puede reconocer y generar.
Y generar una señal sonora o una secuencia de palabras.
¿Sí?
Entonces, ¿qué tiene que saber Hal?
Tiene que saber de fonética.
O sea, de la naturaleza física de los sonidos.
De cómo se...
El sonido y las ondas y cómo se decodifican, etcétera, ¿no?
Todas esas cosas que yo no sé.
Pero también de fonología.
Es decir, cómo los sonidos funcionan en una lengua, en particular en el inglés.
Esa serie de ruidos, cómo se transforman, qué fonemas simbolizan, etcétera, ¿no?
Y con qué letras yo puedo representar esos fonemas.
Y a qué palabras se mapean, bla, bla.
Se llama fonología.
Pero además, Hal tiene que saber que los sustantivos tienen género y número.
Y que casa no es el femenino de caso.
¿Sí?
Si bien perra es el femenino de perro y perros y perras, ¿no?
Y que además no se dice luz.
Y tampoco se dice luces con zetas, sino que se dice luces con c.
Si mal no recuerdo.
Pero también tiene que saber Hal que uno agregándole prefijos y sufijos a las palabras.
Todo esto si Hal habla en español.
Puede formar palabras nuevas y que de creíble puede sacar increíble.
También tiene que saber que de perro no puede sacar imperro.
Y que mente transforma un adjetivo en un adverbio, porque calma se transforma en calmada mente con alguna que otra modificación.
¿Sí?
Pero que tampoco podemos ponérselo a cualquier cosa porque no podemos decir azulmente.
Bueno, sí, sí somos poetas, pero es como otra rama.
O sea que tiene que saber de morfología.
A ver el estudio de la estructura interna de las palabras, cómo las palabras se arman adentro.
También tiene que saber que las palabras uno las tiene que emitir en sentido correcto porque no es lo mismo decir,
Dave, lo siento que no puedo hacerlo, me temo.
Que no puedo hacerlo, me temo.
Que lo puedo, Dave, siento que no me temo.
No funcionan, son las mismas palabras, pero si las desordeno, no.
O sea que tienen que saber de sintaxis, del estudio de la estructuración de las palabras en unidades mayores.
Vamos a hablar de esto, claro que vamos a hablar.
Pero ustedes fíjense que si yo digo abre las compuertas HAL, es lo mismo desde el punto de vista de la estructura de la oración,
o sea un verbo conjugado más un artículo, un sustantivo, un signo puntuación y otro sustantivo que si yo dijera,
saca los dados HAL, o baja las persianas HAL, lo cual no tiene sentido, porque nosotros tenemos que entender el significado de las cosas.
Yo puedo armar oraciones perfectamente sintactivamente válidas, pero que digan fruta, digamos.
Eso es el significado de cada palabra y de que a cada dado no aplica mucho a una situación en la cual uno está en el espacio y tratando de que una máquina le abra una puerta.
Eso se llama semántica léxica, que quiere decir cada palabra, y hay una cantidad de problemas ahí también, bueno en todos hay problemas.
Pero también cómo combinamos las palabras para obtener significados mayores que eso es semántica composicional.
Todas esas cosas que Luis sabe y yo no.
Yo me quedo por acá más o menos, no me conocí, no es mentira.
Pero no solo eso, sino que, y esto me encanta, cuando él le dice, la frase más famosa es I'm sorry Dave, I'm afraid I cannot do that.
Es, lo siento Dave, me temo que no puedo hacerlo.
Está siendo educada, pero además está siendo ligeramente irónica.
En realidad no lo siente y sí puede abrir la puerta, literalmente puede abrir la puerta.
Eso se llama pragmática, es decir el modo en el que el contexto influye en la interpretación de lo que estamos diciendo.
El ejemplo más claro de necesidad de la pragmática y difícil de resolver es la ironía o el sarcasmo.
Seate temprano, estamos en hora.
Y el discurso, el estudio de las unidades mayores a las oraciones y cómo pegamos una oración con la otra.
Cuando digo, había un alfajor arriba al lado de la ropa, vino Luis y lo comió, él lo refiere al alfajor.
Eso se llama anáfora, resolvé anáforas.
Se tiene que ver con el discurso.
Bien, esas son un poco las cosas que tiene que saber Hal.
Y eso es un poco lo que define las cosas que tiene que estudiar el procesamiento de lenguaje natural.
Ahí están todas resumidas.
Acá hay miles de variantes.
Recuerden lo que les digo de este curso, es un curso que trata de cubrir las generalidades porque lo que sucede después es que se mezclan.
Hay un artículo que le gusta mucho a Luis que dice, yo puedo hacer todo esto con un solo modelo.
Pero sí es importante para entender qué implica verlas y ese es el info que tenemos acá.
Bueno, un poquito de historia del procesamiento de lenguaje natural.
El procedimiento de lenguaje natural arranca a fines de los años 40 y los años 50 y en particular del ruso al inglés.
Por razones que son bastante obvias de la guerra fría.
La guerra ha sido un gran promotor o catalizador de la ciencia mal que nos pese.
Y en el caso del procedimiento de lenguaje natural, no hay excepción ni no por nada.
Era la DARPA la que movía estas cosas y ponía funding para estas cosas.
En particular del ruso al inglés, la tarea más vieja del procedimiento de lenguaje natural es la traducción automática.
Y es una de las que en peor nos ha ido.
Si bien en los últimos años hemos mejorado muchísimo.
No anduvo esto, no anduvo.
El original, este es un chiste.
Dicen que, the spirit is willing but the flesh is weak.
Lo traducían al ruso, lo volvían a traducir para atrás y les daba el vodka is strong but the meat is rotten.
Yo el otro día lo probé con, no lo puse, no.
Lo probé y les invito a que lo hagan, andan mucho mejor que él.
Y eso hizo que la traducción automática y el procesamiento de lenguaje natural cayera en uno de sus primeros inviernos.
La inteligencia artificial ha tenido a lo largo de la historia varios inviernos, digamos.
Si ustedes leen sobre los inviernos de inteligencia artificial en Wikipedia están muy interesantes.
Se genera una expectativa muy grande con algo que amenaza con resolver todos los problemas del lenguaje.
El problema del lenguaje no anda y se termina el funding y nadie más investiga, etc.
Esto pasó muchas veces y hay quienes tienen miedo de que esté pasando ahora, que vaya a pasar ahora.
Que estamos poniendo tanta expectativa en la inteligencia artificial y que el campeón mundial de Go y que toque el otro,
que vuelva, con la decepción vuelva a caer.
Yo creo que no, pero es una opinión.
Acá lo que pasó fue eso, que se cayó porque arrancar una tarea demasiado difícil y no funcionó.
Algunos nombres para recordar, a mi me parece que podríamos resumir muy groseramente
y con el conocimiento que tengo yo, digamos, porque hay muchísimas ramas en el procesamiento de lenguaje natural
a lo largo de la historia y su mezcla con la inteligencia artificial y el aprendizaje automático y tal.
Pero nombres grandes, grandes y cómo la historia lo afectó.
Yo les invito a que lean las biografías de estas personas que están acá, porque son muy interesantes.
Es de particular, por supuesto, la de Alan Turing, que supongo que ustedes saben quién fue.
Fue el que, no sé si decir, inventó, descubrió o modeló la computación como hoy la conocemos,
no fue el único, por supuesto, pero fue quien inventó la máquina de Turing, caramba.
Entonces, que es nuestro modelo teórico de computadora más popular.
Pero además Alan Turing, en ese artículo que yo le decía, sentó las bases de la inteligencia artificial en ese sentido.
Entonces, por eso, y porque vale mucho la pena leer la biografía de Alan Turing, es que lo puse acá.
Lo mataron.
Noam Chomsky es un lingüista muy, muy importante, además de ser muy, muy polémico, como ustedes sabrán.
El gran aporte de Chomsky desde el punto de vista del procesamiento del lenguaje natural,
en medio de otro montón de cosas que aportó, fue el de, como si recuerdan, la jerarquía de los lenguajes,
de cómo los lenguajes formales se agrupaban.
Y además de una cantidad de influencias positivas en los vínculos entre la lingüística y la computación.
También es el responsable, hay que decirlo, de alguna serie de malentendidos, no, de afirmaciones,
que hicieron que se frenara la investigación en el procesamiento del lenguaje natural,
dada la enorme influencia que tenía Chomsky en su momento.
Ahora vamos a mencionar alguno, o lo podemos mencionar ahora.
El Chomsky decía cosas como el procesamiento estadístico, no se me ocurre ninguna aplicación útil,
no se me ocurre ninguna noción útil de probabilidad de una oración.
Y con eso frenó por 30 años el estudio de los métodos estadísticos del procesamiento del lenguaje natural.
Es el problema que tienen las personas muy influyentes, por eso yo siempre recomiendo que crean a la gente muy influyente, pero no tanto.
Por ejemplo, a Tim Berners-Lee.
Por allá por los 90 apareció Frédéric Chelinek, que fue el que dijo, yo quiero reactivar todo esto, está claro que son hitos,
la ciencia no avanza así por grandes inventores, no quiero que queden con esa idea, son como símbolos digamos.
Chelinek en IBM en los años 90 retomó el estudio de los métodos estadísticos.
Hay una frase muy, una frase infame atribuida a Chelinek que dice cada vez que he hecho un lingüista mi performance mejora.
Porque decían yo, todos eran estereotipos, yo puedo con métodos numéricos o haciendo cuentas o contando sustituir la tarea de los lingüistas en reconocer y todas esas cosas.
Porque la mayoría de los métodos del lenguaje hasta ese momento eran orientados a reglas, es decir, venía un lingüista trataba de ver cuáles eran las reglas del lenguaje,
cuáles eran las reglas para formar oraciones, cuáles eran las reglas para agrupar palabras y un programador programaba esas reglas, dicho esto muy groseramente.
Chelinek y los métodos estadísticos lo que dicen es, yo puedo aprender de corpus, vamos a ver esto en el curso obviamente,
yo puedo aprender de corpus, inferir esas reglas automáticamente y no necesito a los lingüistas.
Esa relación amor y odio entre los lingüistas y los de la ciencia de la computación, los computer scientists, ha tenido muchas ida y venida a lo largo del tiempo y siempre está esa ida y vuelta.
Yo creo que se está convergiendo a decir, bueno, en realidad, y hay algunos artículos muy recientes, pero yo creo que se está convergiendo a decir, bueno, en realidad,
y hay algunos artículos muy recientes de Manning, por ejemplo, que habla del tema, hay uno que se llamaba Bring the Linguists Back,
porque lo que pasa con los métodos estadísticos es que llegan hasta un cierto punto y hay ahí una parte que no la pueden inferir de los datos.
Todo eso está muy en discusión y siempre está en discusión, pero por lo menos hasta ahora el trabajo interdisciplinario sigue siendo lo mejor que tenemos y los métodos híbridos también.
Por allá por los, esto pasó en los 90, por allá por la década del 2000, la década del 90 son la de los métodos creativos y Vladimir Manning es un señor que modeló o diseñó,
no sé qué palabras se usa ahí, modeló o propuso primero el modelo de la Support Vector Machines, que es un método discriminativo, después vamos a ver, de clasificación, a separar cosas,
esencialmente uno le da puntos y lo separa, vamos a hablar de eso.
Vladimir lo inventó hace un montón de años a eso, pero este tipo de métodos se pusieron de moda en el procesamiento del lenguaje natural y se empezaron a resolver todos los problemas con Support Vector Machines.
Y últimamente resurgen, digo resurgen porque esta gente venía desde los años 80 estudiando el tema de las redes neuronales, Hinton, Jeff Hinton, Joshua Benio y Jean LeCun,
le dicen la Canadia mafia porque estaban todos en Canadá ubicados, alguno era, LeCun creo que es alumno de Benio y Hinton está por ahí cerca, en otra universidad estaba,
reactivaron el tema de las redes neuronales y todo lo que hoy conocemos como Deep Learning, que es la nueva ola del procesamiento del lenguaje natural,
y ya hay gente diciendo, bueno, basta, el paper que dice, vieron esta tarea, bueno ahora la hago con redes neuronales y anda mejor, ya está, ya tuvimos suficiente, volvamos a discutir sobre la teoría.
Estamos en ese nivel porque todos, pero la realidad es que en todas las tareas principales las redes neuronales han mejorado la performance en algunos casos muy significativamente.
¿Por qué este tipo de métodos resurgen? Porque tenemos una, muchas mayores capacidades computacionales y muchas mayores cantidades de datos, grandes, grandes.
Esa es un poco la breve historia del procesamiento del lenguaje natural, yo les recomiendo que vayan a Wikipedia y ahí se pongan a leer si les interesa, para entender un poco qué cosas han pasado.
Bueno, algunas tareas del procesamiento del lenguaje natural, fíjense, no se ve mucho, la traducción automática, yo voy guardando el histórico de esta traducción,
no, no se ve, perdónenme, es la traducción que dice, el campeonato italiano aún no ha comenzado, pero Inter de Milan y Juventus, todos los clubes más poderosos del canal ya están jugando un duelo para quedarse con Forlán, bla, bla.
Y acá es lo que va pasando a lo largo de cada vez que doy el curso, lo traduzco con el Translate, y no ha mejorado mucho, no son, me hubiera gustado tener en el 2010 una traducción un poco peor, digamos, para poder comparar si ha mejorado algo, porque esto va y viene, comete algún error.
Fíjense que acá traduce, was chosen as the best player in the world of South Africa, acá se equivoca, en the world of South Africa no, was voted, acá usa voted en vez de chosen,
y antes decía, in the world of South Africa, pero había una vez que le embocaba, no hay muchas variantes, pero el hecho es que ha ido mejorando en general en los últimos años, no está bueno el ejemplo, no puedo retroceder el tiempo para cambiarlo,
y ahora, esto es interesante, este título lo dejé por razones históricas, porque estaba en la primera PPT que hicimos, decía, vale la pena, porque era tan malo que uno decía, quiero traducir si es una cosa tan espantosa, hoy en día la pregunta ha cambiado un poco, porque traduce bastante bien,
y podríamos llegar a decir, esto lo traduje yo, un pasado, así como sale, en otra época eso era impensable, porque en cualquier zapallo se daban cuenta que eso no era una traducción, pero de todos modos, por eso digo que lo dejé, es porque me sirven,
bueno, si yo les doy este texto en chino mandarín, tener esto ayuda un poco, o sea que la traducción automática no será perfecta, pero es mucho mejor que nada, digo para ustedes que no saben chino mandarín,
hay otras tareas como el resumen automático, también es una tarea muy vieja, los primeros trabajos son de LUM en los años fines de los 50, que la idea central es tratar de condensar el contenido de la información de un documento para el beneficio de un lector,
si me preguntan para mí, esta es la tarea de las que yo conozco, la que estamos más lejos, en la que he visto menos progreso, porque es una tarea muy difícil, porque primero que nada es subjetiva, qué quiere decir resumen, qué tanto lo resumo, en qué sentido lo resumo,
es muy difícil de modelar yo creo la tarea y por eso es en esto que para mi gusto hay poco para hacer, si uno prueba Word y eso tienen resumidores automáticos que son como espantosos,
la extracción de información es, me das un texto y trato de llenar una base de datos, es decir, donde tengo campos a completar, estaría bastante fácil, bastante fácil para llegar al 90%, de ahí para arriba está complicado,
interfaces a base de datos, todo tuvo de moda en una época, últimamente no he escuchado mucho, que es intentar para un dominio acotado hacer que el sistema responda, bueno no, sí que existe, ahora que pienso, Siri es esa tipo de cosas,
¿dónde está el restaurante? ¿Hay algún restaurante cerca? Y la idea es que eso se traduce internamente a algún tipo de consulta, da una base de datos, y por supuesto se ejecuta,
el enfoque funciona bastante bien con lexico y sintaseo retringido, yo creo que hoy en día podemos decir más que esto, más recuperación de información, recuperación de información es Google, da un término que me traiga lo relevante,
y lo más relevante primero, verificadores de gramática y estilo, es otra cosa que a nivel comercial, categorizar documentos, que es, que me den un documento y me digan esto habla de tal cosa, todo habla de fútbol,
responde preguntas, no sé si recuerdan que hace poco, hace poco, no sé si tampoco, la máquina esta de M. Watson era que se llamaba, le ganó al shopper, el campeón del mundo, ahí es más discutible, otra vez no, ahí lo que estaba pasando principalmente era que tenía grandes bases de datos,
si se quiere era una tarea medio restringida por más que parezca maravilloso, y que va que lo sea, pero era bastante restringida la cosa de buscar facts y armarlo, no es tan fácil como yo lo digo, pero era a lo bruto que funcionaba principalmente Watson,
por eso cuando ponen ejemplo de eso que llaman inteligencia cognitiva o una cosa así, yo no la llevo mucho, más viene de number crunch, y en el grupo de procesamiento del lenguaje natural por ejemplo, este es el grupo nuestro,
este curso lo da el grupo de procesamiento del lenguaje natural que somos un conjunto de entusiastas, investigadores y estudiantes relacionados,
y para que te hagas una idea del tipo de cosas que se ven, por ejemplo nosotros tenemos este año un proyecto que es extracción de eventos en la ciudad a partir de medios escritos y redes sociales, descubrí que cosas pasaron,
búsqueda de temas musicales similares utilizando aprendizaje profundo, esto de buscar canciones parecidas a esta, estudio de menciones a personalidades públicas en tweets,
jugador de espectro, no esto es más bien machine learning, representación de palabras en espacio de vectores, esto se defendía hace poco, determinación de la orientación semántica en las opiniones transmitidas en eventos de prensa,
esto quiere decir si una opinión fue positiva o negativa, dictación por tema de texto de prensa, cosas así, de todo un poco,
una de las cosas interesantes es que en esta área todo lo que se hace prácticamente es open source, o sea que todas las herramientas están disponibles rápidamente,
y el conocimiento general también, una de las cosas que para mí es muy importante y que para nosotros es bastante normal pero que en otras áreas no existe,
es que la Association for Computational Linguistics, que es la principal asociación de todo esto, decidió hace unos años atrás que todos sus contenidos estaban libremente disponibles,
entonces nunca hay una barrera para leer un artículo de computational linguistics porque esté en un journal pago o cosas así,
como mucho timbo no usamos digamos porque no lo necesitamos, eso creo que es muy positivo,
¿Por qué estoy acá?
¿Cómo estamos ahora yo? No sé ni donde tengo el celular, ¿Qué tiene el lenguaje natural que no tiene los lenguajes formales?
Ustedes hasta ahora han estudiado muchos lenguajes formales, de hecho todos los lenguajes que han estudiado son formales, ¿Cuál es la gran diferencia del lenguaje natural con los lenguajes formales?
¿Alguna pregunta hasta acá? No, si tienen dudas puedo resolverlo si están aburridos o no, ¿Qué tiene el lenguaje natural que no tiene los lenguajes formales?
¿Cuál es el problema o los problemas que enfrentamos para hacer esto? ¿Por qué hay un área especial dedicada al lenguaje natural y no podemos usar los métodos de parsing y análisis que hay con los lenguajes formales?
Padre, he mentido, te escucho hijo, dije que tenía 33 palenvíos y tenía 24. ¿Dónde está la gracia del chiste? Lo digo en serio, no lo digo así de que se ríe. ¿Dónde está la gracia?
¿Por qué es un chiste? ¿Cuál es el chiste?
No estamos hablando de la misma mentira y también sabemos que cuando uno se confiesa, no confiesa ese tipo de mentira porque es una mentira en el marco de un juego.
Hay que entender que esa mentira no es la misma de la arriba y la de abajo, el mismo tipo de mentira y ese contraste es lo que nos da.
Le recomiendo un proyecto bravo que tuvimos hace poco que se llamaba reconocimiento de humor en tweet que habla bastante de estas cosas.
Vuitres de la Concagua, ¿cuándo los volveré a ver? Rapaces bravos y audaces con sus granidos voraces me enseñaron a querer.
¿Le gustó Mendieta en un ataque de inspiración? Le compuse a Pacos Quín. Don Inodoro, la próxima vez que lo ataque en la inspiración no podría defenderse mejor.
¿Dónde está el chiste?
Que la inspiración no lo atacó. De hecho sí. No es el mismo ataque. Que no es el mismo ataque. Que estoy queriendo decir dos cosas diferentes con ataque.
Es más divertido escucharlos que interpretarlos. Un borracho dijo si ayer fuese mañana, hoy sería viernes. ¿En qué día de la semana el borracho dijo esto?
Seguramente lo recibieron por whatsapp hace unas semanas atrás. Bueno yo sí. ¿Qué día es hoy? No me voy a poner a discutirlo acá con ustedes.
Se lo voy a dejar de beber pero puede ser domingo o miércoles. Y es muy interesante este problema.
Porque lo que sucede es que cuando yo digo hay como dos mundos a la vez. Porque acá hay un fuese. Si ayer fuese es un mundo hipotético.
Si ayer fuese mañana hoy sería viernes. Entonces ahí la ambigüedad aparece porque el ayer y el hoy no sabemos cuál de los dos mundos es.
Si en el hipotético o en el mío. Y no pueden ser en los dos en el mismo. Porque si no, no habría ambigüedad. No habría duda.
Sería inconsistente. Acá lo que sucede es que hay dos mundos introducidos por este sí. Y como el ayer y el hoy puede estar cruzados. Según como yo lo interprete.
Voy a responder o domingo o miércoles. Creo que era domingo o miércoles. Háganlo. Las computadoras no están ni por asomo cerca de entender esto. Pero ni de lejos.
Tengo muchos amigos que todavía me discuten. Imagínense las computadoras. Bueno, tengo amigos que hay computadoras que le comp... bueno.
Y ahora, por supuesto, esto van a tener que aguzar el oído. Si yo me acerco con el micrófono se escucha acá.
No tiene volumen. No, no tiene más volumen. De hecho no tiene. No, no tiene más.
Shakespeare da para todo. Le digo porque yo conozco todo Shakespeare en inglés. No me digas. No lo he leído porque no sé inglés, pero lo conozco.
Y bueno, el hotel es una tragedia terrible. Ah, entonces no lo había visto. No, no, pero es una gran obra. Ah, ¿sí? Sí.
Hotel, el Moro de Venecia. ¿Qué? ¿Era negro? Sí. Ah, ¿sí? Bueno, no todos los moros son negros, pero este era negro.
Hay distintos tipos de moros. Hay los moros del interior y los moros en la costa. Hay moros, claro.
También están los moros de los lugares importantes, los que son de Morondanga. De Morongo. Claro.
Claro, pero este era negro. Ah, sí, sí. Moro chazo.
Es curioso porque el nombre Othello en realidad es un nombre de origen irlandés. No me digas. Claro, es O-thello.
Ah, tiene el... Sí, tiene el apócrifo que le ponen.
Más, mire, si será irlandés, porque en irlandés antiguo, Thello quiere decir alojamiento.
Ah, sí. Y bueno, y Othello, una historia terrible que ya empieza medio mal porque Othello estaba casado con 10 démonas,
que era una hermosa mujer, pero provenían de familias enemigas, los Capuletos y los Montescos.
Y esto de alguna manera lo vincula con las teorías de Darwin.
Claro, porque... Y desdémona se llama una mujer porque el hombre desciende del mono y la mujer desdemona.
Y entonces...
No, algún famoso que entró, seguramente.
Bueno, el asunto es que le estaba contando. Resulta que se habían casado, igual, con la oposición de la familia,
hasta que un día Othello va caminando ahí por las murallas del castillo y se le aparece el fantasma del padre.
Según el actor, algunos actores...
Bueno, el asunto es que se le aparece el fantasma del padre y le dice a Othello...
Pero a Othello lo ve y dice, papá, ¿qué quiere abrazar? No puede porque es un ser etéreo.
Es un espectro, un ser etéreo. Hay fantasmas mono y fantasmas etéreo.
Y este es etéreo.
Claro.
Y este es etéreo.
Etéreo, sí.
Y también lo vincula con las teorías de Darwin, los fantasmas mono.
Son, claro, bueno, teorías discutidas esas. No todos estamos de acuerdo.
Bueno, entonces resulta que lo quiere abrazar, no puede, y el padre le dice,
Othello, vengo a decirte que tu mujer te es infiel.
¡Uh!
Es fantasma ese.
Sí.
Y entonces dice, no, pero ¿por qué me decís eso?
Y desaparece el espectro.
Claro, se quedó sin señal, digamos, Othello.
Y se queda pensando y dice, ya me dijo mi psicóloga que eras un padre ausente.
Claro.
Tipo, se fumó, pero le dejó trabajando la cabeza al pobre Othello.
Volvió al palacio y andaba por los salones dudando, decía, ¿ser o no ser?
Yo no saber.
Y después cruzaba por ahí por la noche en un cementerio y encuentra la famosa calavera.
Era la calavera de un bufón de la corte.
Y agarra la calavera, Othello la mira así, le dice.
Y sí, ¿cómo la va a mirar? Así, sí, siempre se mira así.
Sí, salvo que sea visco.
Claro. Ah, también es cierto. Eso también depende del actor, ¿no?
Claro.
Bueno, la mira sí dice, te noto desmejorado.
Era la calavera de... y la calavera no le contesta nada.
Claro, calavera no chilla.
Entonces, y se queda dudando, Othello dice, ¿qué hago?
Y dice, ¿desdémona será realmente culpable o a lo mejor es inocente?
¿Qué hago? Dice, bueno, yo por las dudas la mato.
Y pobrecita desdémona ajena, todo estaba ahí en su dormitorio mirando televisión.
¿Cómo mirando?
Sí, en Venecia había televisión.
¿No ve que es la ciudad de los canales?
¿Quién es?
Bueno, entonces entra Othello al dormitorio sigilosamente, ella no lo ve.
Entre que está oscuro y él es negro.
Y va, y nomás va ahí la estrangula ahí sobre el tálamo nuptial.
¿En un árbol la estrangula?
¿En un tálamo? ¿Arriba del tálamo o a la sombra del tálamo?
No, no.
¿Se suben al tálamo?
Espere, espere un poco.
Y si ella es mona, puede subir porque...
No, no me entendió.
Son altos algunos tálamos.
Como 30 metros tienen.
Los que tengo en casa son...
hoja caduca, pierde la hoja en invierno.
Se queda sin hoja, pobrecito el tálamo.
No, pero a ver cómo se lo explico.
No, no, desdémona está acostada sobre el lecho nuptial.
Ah, pusieron helechos.
Por si se cae del tálamo están los helechos, está bien.
Está bien.
Es una medida de seguridad porque el lecho siempre es verde, está muelle.
Y si se cae del tálamo se puede matar.
Si la mata no la puede estrangular.
Si no, poder puede.
Pero una vez que está muerta medio...
redundante, ¿no?
Sí, una redundancia.
No, a ver.
¿Cómo que no?
No, estoy tratando de hacerme entender.
No es fácil, le diré.
Desdémona está acostada sobre...
¿la palabra cama la conoces?
Sí.
Sobre la cama.
Y ahí va a hotel y la estrangula.
¿Quitaron los árboles?
Sí, quitaron los árboles.
¿Y los helechos?
Uy, qué noche larga va a ser.
Los helechos lo dejaron.
Ah, ¿lo dejaron?
Sí.
Hubo discusiones, debate público.
La gente opinaba a favor, en contra.
Hasta que el director dejó de hablar ese lecho se quedan.
¿Por qué?
Porque lo importante es el hecho.
Bueno.
Además de reírnos un poco y de acordarnos del mundo...
Perdón, de Rabinuch, que se murió.
¿Por qué?
¿Por qué pasamos esto?
¿Es fácil para una computadora comprender esto?
No.
¿Y con qué juega todo el tiempo Le Luthier?
Y dicen que si esto no existiera sería muy difícil que hubiera humor.
Hacer una cantidad de cosas que tienen que ver con palabras que son sinónimos.
O homonimia, que suenan parecidas.
Como álamo y tálamo, lo importante es el hecho.
Pero también con palabras que quieren decir algo literalmente,
pero que uno sabe que está refiriendo algo porque conocemos el mundo.
No voy a dar ejemplo porque le quita la gracia, digamos.
Pero uno lo ve todo el tiempo.
Mírenlo de vuelta o mírenlo y piénsenlo desde esta óptica.
Después olvídense, porque si no uno no se divierte.
Todo el tiempo usan ese tipo de métodos.
Y Fontana Rosa también.
Otro ejemplo muy claro de eso es Fontana Rosa.
De Inogoro Pereira en particular.
Por eso puse un par de ejemplos.
Todo el tiempo son juegos de palabras.
El cuarteto de Nos también.
No voy a poner ningún ejemplo del cuarteto de Nos.
Acá la mayoría son irreproducibles.
Incluso llegaron a tener un ciclo que se llamaba este lo hablando hablando.
Que se basaba todo en lo que se llaman calembures.
Que son juegos de palabras.
Los calembures son bastante viejos.
Esto es la ambigüedad.
El gran problema de procesamiento de las cosas naturales,
eso responde a esta pregunta, es la ambigüedad.
La ambigüedad de diferentes niveles que son un poco lo que hemos visto en los chistes.
Ambigüo quiere decir que admite distintas interpretaciones.
Empezamos, como decía ya, con la homonimia.
Dos palabras con la misma forma que tienen distinto significado.
Y ahí podemos distinguir la homografía.
O sea que se escriben igual.
Capital, la capital de un país versus el capital que tengo.
O banco.
Pero también pueden ser homófonas.
Para sufrimiento de los escolares y de algunos adultos.
Hola y hola.
Un estudiante de ingeniería también.
Haz, haz, coser y coser.
Osea.
Hay gente que en vez de osea pone osea con tilde.
Hay gente todo el tiempo.
Eso se llama homofonía.
El estudiante juega mucho con la homofonía.
Que bella plebella, ¿no?
Polisemia es cuando hablamos de una palabra que tiene muchos significados.
Y ahí lo dicen, ¿no?
Bueno, desde el mono es parecido.
Pero el hombre desciende del mono y el mono desciende del árbol.
Está claro que estamos hablando del mismo verbo que se conjuga igual.
Que en todo aspecto es igual pero que quiere decir diferentes cosas.
¿Cómo nos podemos dar cuenta de que?
¿De cuál de las dos es?
¿Cómo podemos desambiguar?
Por el contexto.
El contexto es una de nuestras grandes claves para desambiguar en general.
Que bella plebella, dice Lelutien, otro bastante conocido.
Pero esto es muy viejo, ¿no?
Garcilazo de la Vega decía.
El dulce lamentar de los pastores.
Que puede ser ver como el dulce lamentar de los pastores.
Y esta de Quevedo dice la leyenda que apostó que era capaz de decirle a una reina.
Que no me acuerdo cual era.
Que era reina.
Entonces yo venimos a decirle a la reina que era reina.
Entonces baile dice, entre el clavel y la rosa su majestad escoja.
Bueno Shakespeare en el primer verso de Ricardo III dice.
Acá hay todo un juego de palabras porque el símbolo del rey que era Eduardo.
Eduardo no me acuerdo cual.
Era un sol y san y son juega.
Suenen igual obviamente en inglés.
Y eso permite armar todo un juego de palabras que se extiende además.
Son calembures.
Pero también tenemos ambigüedad a nivel morfológico.
Nosotros plantamos papas.
Ustedes que opinan, el oro plantar está conjugado en pasado o en presente?
Puede ser cualquiera de las dos, no podemos saberlo.
Si no tenemos contexto.
Pero también, este es el gran problema del parsing.
Se llama pipi attachment.
Y es, Pedro vio a Juan con el telescopio.
Se puede interpretar perfectamente como,
Pedro que vio con el telescopio a Juan.
Usando el telescopio vio a Juan.
O que vio a Juan con el telescopio.
Que el telescopio es Juan.
Esto es una frase preposicional con el telescopio.
Y saber si con el telescopio va con Juan o con Pedro es como el gran problema del parsing.
Cuando hacemos parsing, cuando tratamos de armar el árbol de parsing, lo vamos a ver.
Esto es en lo que fallan siempre los parsers.
Y este es otro ejemplo de manual que falla el parsing.
Los hombres y las mujeres que hayan cumplido 60 años pueden solicitar una pensión.
Los hombres y las mujeres que hayan cumplido 60 años son los que pueden solicitar.
O los hombres y además las mujeres que hayan cumplido 60 años.
¿Cuál es el adobe para ustedes?
¿Cuál es el adobe para ustedes?
¿Cuál es el adobe para ustedes?
¿Y cuál apostaría en ustedes que es?
¿Por qué?
No tengo contexto yo acá.
¿Por qué sabemos que es la primera?
Conocimiento del mundo, que es como sentido común.
Conocemos el mundo, es decir, conocemos la realidad.
Ese es el gran problema que tiene la computadora y nosotros conocemos el mundo.
Es como Luis comiéndose la ropa.
La perra de mi vecina me la adoró.
Tenemos dos posibilidades.
Mi vecina realmente tiene una perra o yo no tengo un buen trato con mi vecina.
Esa ambigüedad es a nivel semántica.
También tenemos ambigüedad a nivel pragmático.
Si yo digo, bueno, ¿a qué hora llegarás?
O la frase, llego a las ocho, esperame.
Vamos a hablar en uruguay.
¿A qué hora llegarás?
Llevo a las ocho, esperame. Eso es previsión.
Nunca llegarás en hora.
Llevo a las ocho, esperame.
Eso me lo vas a tener que decir cara a cara.
Llevo a las ocho, esperame.
Dependiendo de la situación uno le interpreta...
Y yo exagero con el sonido, pero podría perfectamente no hacerlo
y que solo el contexto de la situación me diera la respuesta.
Muchas veces el juego de la ironía también se basa en eso,
en decirlo sin ninguna expresión y que el otro interprete
que no se dé cuenta si hay ironía o no.
Tenemos el caso de Luis.
Tomé el fajor del escritorio y lo comí.
Tomé el fajor que estaba en el escritorio y me comí el fajor
o tomé el fajor que estaba en el escritorio y me comí el escritorio.
Nuevamente...
Ahora, y acá hay otra cosa.
Nuevamente...
Una de las formas de darle significado a estas cosas
y entender el mundo, asociar los objetos del mundo
y saber que los escritorios no suelen comerse,
eso es una aproximación, identificar las cosas una por una
y decir aquellas que se pueden comer por seres humanos,
porque si estamos hablando de una termita...
Pero otra forma es ver cuántas veces...
¿Por qué los datos cambiaron todo esto?
Porque si yo tengo muchos, muchos datos,
puedo saber que en mis datos, en mis documentos anteriores
que ya tengo analizados, tengo que tener datos
y además alguien lo haya analizado,
no se dio muchas veces que alguien se comiera un escritorio.
A mí apareció varias veces que alguien se comió en el fajor.
Entonces a mí...
contando puedo arrimarme.
Es en eso que se basan los métodos basados en conteo,
métodos estadísticos, digamos.
Y acá ustedes pueden ver aquello que yo decía de los lingüistas o no.
Es decir, si yo logro entender el significado de esto
y asociarlo y mapearlo,
puedo interpretar perfectamente esto.
En este tipo hay mucha casuística diferente.
Contando, yo me puedo arrimar bastante a esto,
pero seguramente no voy a ser tan preciso.
Y además alguien me tiene que interpretar muchas oraciones.
Esa es como la gran dualidad de las reglas
versus el aprendizaje estadístico, digamos,
o el aprendizaje automático.
Vamos a hablar mucho de eso en el curso.
Juan mató al carpincho con la escopeta.
Obviamente no puede ser el carpincho quien lleva la escopeta.
¿Por qué? Por conocimiento del mundo, ¿no?
Puse la camisa en la lavadora y la lavé.
Claro, por supuesto no me puse...
Yo podría llegar a interpretar que me puse a lavar la lavadora, pero...
No es que sea tan raro lavar la lavadora,
pero poner la camisa y después ponerse a lavar la lavadora es como...
Yo tengo que saber que las lavadoras lavan
y que la ropa se lava y hacer esa asociación.
O sea, requerimos conocimiento del mundo.
Entonces, resolver la ambigüedad es como la gran tarea.
¿Y qué métodos se utilizan?
Hay muchos modelos basados en máquinas de estado finito.
Automatas finitos, transductorias, automatas con peso.
O sea, automatas.
Hay muchos sistemas de reglas.
Se usa lógica, sobre todo en la parte de semántica,
para asociar significado.
Se trata de llevar a un modelo de predicado.
Sí, un modelo de predicado.
Modelos probabilísticas, sobre todo aquellos que hacen conteo.
Modelos basados en redes neuronales.
Y esto lo agregué este año, que es Representation Learning,
que es a partir de muchos atributos...
Es muy difícil de explicar si no explico algunos contextos
de aprendizaje automático, que lo vamos a ver.
Pero la idea es que las features se generan solas.
Ya lo veremos.
Hay una batería de métodos, pero curiosamente muchos se repiten.
Y los algoritmos en general son, o muchos son,
o busquedas en espacios de estados.
Es decir, tengo una cantidad de opciones y tengo que elegirlas mejor.
Por ejemplo, tengo una serie de árboles de análisis sintáctico.
¿Cuál es un árbol de análisis sintáctico?
El árbol que modela una oración.
Como los árboles de parsing, pero para oración.
Buscar cuál es el más adecuado para una entrada.
O hacer programación dinámica.
Vamos a ver varios ejemplos de programación dinámica en el curso.
O aprendizaje automático.
Es decir, a partir de un compost yo infiero conceptos
y luego los aplico.
Y eso cuando son, ¿qué hora?
Y media. No estuvo bastante bien, no fue tan larga.
Es todo por hoy. ¿La semana que viene?
¿Alguna duda?
Representation learning es como feature extraction.
No, representation learning es...
En lugar de yo definir cuáles son las features,
en vez de hacer feature extraction,
la genera solo el algoritmo.
El ejemplo de manual es, si yo tengo una imagen,
le mando de entrada todos los pixels,
y la red neuronal, solita,
identifica agrupaciones o patterns
en la figura que le dice, bueno, acá hay una persona.
O acá hay una nariz, digamos.
Pero no es explícito.
La alternativa de eso es yo de alguna forma darle las features.
Es decir, acá hay una curva C y otra curva Z
y eso es representación learning.
¿Alguna duda?
No, hoy son todos conceptos muy generales.
No sé si les queda mucha duda.
La clase que viene,
Luis hace, volvemos al liceo
con idioma español.
Es muy importante este par de clases,
porque son nuestros elementos de dominio.
Cuando hablemos de un parto espicho,
cuando hablemos de un grupo nominal,
son los elementos con los que vamos a hablar.
Nos vemos la semana que viene.
