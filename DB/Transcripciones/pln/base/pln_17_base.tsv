start	end	text
0	6280	En la clase pasada entonces lo que estuvimos viendo es fundamentalmente lo que es
6280	11280	recuperación de información como una aplicación en donde tendemos a utilizar
11280	19080	procesamiento del lenguaje natural en alguna de las tareas que se hacen
19080	24680	sobre todo antes o durante el proceso de recuperación, los algoritmos que
24680	29960	implementan el proceso de de recuperación y después comentamos también lo
29960	34400	que es la extracción de información como otra disciplina diferente a la
34400	38920	recuperación que a veces se entre mezclan o se confunden y que se
38920	42320	está hablando de lo mismo y en realidad son como complementarias, si yo
42320	45120	tengo un proceso de recuperación de información que me recupera
45120	49680	documentos donde se supone que está la información que yo estoy
49680	54560	buscando usuario y el proceso de extracción de información lo que hace es
54560	58440	a partir de un conjunto de documentos que se supone que son de interés
58440	64520	extrae a aquellas partes que efectivamente hablan de lo que yo estoy queriendo
64520	70080	que incluso algunos comentaban que hoy si pensamos en Google que solamente
70080	74840	le ponés las palabras y ya te trae la porción de texto donde están las
74840	78200	palabras que vos estuviste buscando
78720	85560	decíamos la extracción de información es una una disciplina que
85560	91640	típicamente lo que hace es extraer atributos, relaciones, perdonentidades,
91640	100760	relaciones y eventos y comúnmente lo que se hace es se trata de generar una
100760	105440	suerte de plantilla con pares atributo valor donde ahí se cargan los
105440	111120	valores de los valores de los atributos, los nombres de los atributos y el valor que
111120	118360	tienen en función de lo que yo quiero extraer, eso genera una estructura que
118360	125920	es después mucho más manipulable por un usuario experto digamos o algún
125920	132240	sistema que después permita generar hacer otras cosas. Dentro de las tareas
132240	137400	de extracción información y quedamos más o menos en eso, este tenemos, el
137400	141200	reconocimiento de entidades con nombres, la resolución de con referencias,
141200	145040	extracción de relaciones semánticas, entre entidades, resolución y
145040	148120	reconocimiento de expresiones temporales, asignación de relaciones semánticos,
148120	155560	entre otras tareas, lo que queríamos, hoy es ver, algún ejemplo de qué
155560	160880	consiste por ejemplo en la extracción de reconocimiento de entidades con nombres.
162240	170760	Nosotros esencialmente en entidades con nombres lo que tenemos que pensar es que
170760	177120	típicamente lo que uno quiere extraer son tres grandes conjuntos, organizaciones,
177120	184760	personas y lugares. Después uno puede seguir queriendo poniéndole nombres
184760	189400	de otra cosa, pero esencialmente las entidades que tienen nombres son algún tipo de
189400	195800	organización, algún tipo de lugar o algún nombre de persona. Entonces un poco
195800	201560	acabemos el ejemplo y vemos el ejemplo y vemos que un poco lo que es lo que se
201560	208360	pretende mostrar es que ellas dificultades que se pueden presentar. Barcelona
208360	213760	autorizó noticias vieja, autorizó a Luis Suárez a viajar el lunes a Montevideo
213760	219320	para estar a la orden de la selección para los partidos discriminatorios.
221820	226920	¿Qué entidades con nombres ustedes reconocen ahí o que el sistema debería detectar?
226920	236920	Pensamos de vuelta, ¿no? Organizaciones, lugares, personas, empiezan. Luis Suárez,
237480	247600	Barcelona, Argentina, Paraguay, Montevideo, Liga española, bien, lo vemos como una
247600	258080	organización. Eliminatorias, ¿qué sería eliminatorias?
258080	266020	Eliminatorias como el partido de los partidos de la organización, ¿qué es el saberlo?
266020	273120	Sí, bien, me interesa saberlo, pero es una organización, es una persona, es un lugar.
273120	280000	Capaz que me interesa después hacer cosas, un poco justamente, el chiste es
280000	284680	borre, reconocer sentidades con nombres y después lo que vas a querer reconocer son
284680	290400	relaciones entre esas sentidades o cosas por el estilo. Pero es un paso que viene después,
290400	294560	después de que yo detecto las sentidades, empiezo a jugar, empiezo, bueno, ¿qué otra cosa
294560	299960	quiero hacer con esas sentidades? Es como un primer paso, correcto. Capaz que eliminatorias
299960	305040	me puede servir, porque quiero saber, ¿para qué, por ejemplo, para preguntar, ¿para qué
305040	310560	es que este Luis Suárez quería venir a Montevideo? Porque quería venir a jugar las eliminatorias,
311560	315440	pero eso ya entra en la siguiente etapa que sería la detección de las relaciones.
325200	332240	La segunda guerra mundial, ¿cómo lo vas a encasillar? Podría ser un evento, después vamos a hablar
332240	339560	de eventos y de las dificultades de eventos. Pero bueno, es algo que, comúnmente,
339560	344760	uno lo que tiene, o por lo menos para arrancar, o podrías llegar a tener son listas de palabras
344760	350040	que tienen todas las organizaciones, todos los nombres o que se yo. Ahí yo podría usar esas
350040	355560	listas eventualmente para desambiguar y segunda guerra mundial, ahí yo lo tomo, todo como una
355560	362840	sola entidad y es, pero que es una persona, es una organización, es un nombre. No, entonces,
362840	370120	ver cómo lo categorizas. Eso es algo que me va a interesar tenerlo determinado, pero no,
370120	375440	en principio, no es una entidad con nombre. Si viene por el lado de lo que decía el compañero
375440	385760	después de eliminatoria o las relaciones o los eventos. Exacto. Bueno, ahí están, ¿no?
385760	393200	Este Barcelona, Suárez, está, se nota, están más como ahora, vamos a saber. Ahí, en Negritas,
393200	400560	Barcelona, Montevideo, Argentina, Paraguay. Bien, en Negritas están un poquito las entidades que
400560	408480	se encontraron. Después está, encontrar las entidades, tratamos de acá, ponerle el discuito color,
408480	415720	el cuáles son nombres, cuáles son lugares y cuáles son organizaciones. En rojo organizaciones,
415720	423600	en verde, lugares y en azul nombres. Pero Barcelona es este club. Perfecto. Eso quería llegar.
423600	430440	¿Qué Barcelona? Barcelona es un lugar, es una ciudad preciosa que queda allá en el noreste de paña.
430440	438200	Pero no es un club. De hecho, acá está haciendo referencia a un club. Con a la vez pasa lo mismo.
438200	442560	Bueno, en España pasa mucho porque, bueno, porque hay las ciudades, los equipos de fútbol tienen
442560	450040	nombres de ciudades, muchos de ellos. Entonces, acá ya tenemos un problema. ¿Cómo vas a, digamos,
450040	456840	potencialmente tenemos un problema? Es decir, ¿cómo vas a tratar esa entidad como un nombre de
456840	468360	una persona o como un nombre de un lugar? ¿Van? Entonces, si lo podemos acá, como en realidad,
468360	473640	nosotros sabemos que es un club o que acá en el texto está haciendo referencia a club, lo
473640	481240	ponemos en rojo. Pero es algo que yo lo hago o lo debería hacer a posterior y de una
481240	491320	primera reconocimiento. ¿Ok? Y después está lo que interesa de bien. Yo tengo de las
491320	499840	sentidades con nombres y me puedo querer, me pueden querer encontrar relaciones en tres
499840	505960	ascentidades. ¿Cómo se combinan esas sentidades? Y entonces aparece ahí con un color medio rosadito,
505960	513360	este autorizar, ¿no? La organización, Barcelona, autorizó a luizar a viajar. Entonces, ahí tenemos,
513360	519000	más, tenemos. Lo autorizó a viajar, tenemos dos relaciones. O autorizar a viajar podría ser
519000	525440	tratada como uno, todo depende como uno lo, interpreto o lo que quiere hacer. Y ahí aparece, no se nota
525440	530080	mucho, porque hablamos de las tareas de extracción de información y hablamos de las sentidades con
530080	535720	nombres. También dijimos el tema de las correspondencias. Fíjense acá, no sé si se nota que está
535720	545200	con otro colorcito. Pese a que el jugador no fue incluido, ¿quién es el jugador? El Luis Juárez.
545200	553680	O sea, tengo que de alguna forma también determinar que ese término hace referencia en este caso
553680	561200	del Luis Juárez. Lo mismo acá, lo de Club Catalan hace referencia a Barcelona. Ok, estas son
561200	568040	todas cosas o tareas que uno hace en ese proceso de extracción de entidad con nombres. Estraer nombres,
568040	578000	estraer relaciones. Bueno, lo que está diciendo. La mayor parte de los trabajos es traer relaciones,
578000	586240	entre entidades mencionadas en la misma oración. Siempre se trata uno, ya después cuando analiza con
586240	593220	referencia, el texto analizar es un poco más, o puede decir, ser un poco más largo. Las
593220	598520	correspondencias pueden ser en esa misma oración, pero más complicado es cuando la correspondencia está
598520	611360	en otra oración después, ¿verdad? Bueno, esto es una desafío. La mayor parte decía relaciones
611360	618200	predeterminadas, dirección de la empresa, club de jugador, etcétera. Por relaciones de más de
618200	625520	dos argumentos donde muchas veces se habrá de extracción de eventos. Ahora vamos a hablar un
625520	631880	poquito de eventos. Entonces, en relación, lo que decíamos, la relación autorizar que requiere
631880	639600	dos argumentos, A, autoriza, A, B. Y pues, bueno, podemos agregar cuando, ¿a qué, para qué lo
639600	646480	autorizó, etcétera? Entonces, ahí aparece otro concepto que quizás no está puesto acá,
646480	655840	acá el evento podría ser viajar que lo autorizó a viajar. No sé si dice cuando, dice para
655840	663680	qué, para estar a la orden. En fin, hay una serie de textos ahí que uno podría, o de expresiones
663800	671400	que uno podría quedar llegar a determinar. Se tiene, entonces, la idea, bien, viajar, estar a la
671400	678760	orden incluido, como decíamos recién en general se procede por etapas, primero en las entidades y luego
678760	688160	después que tengo las entidades, cuáles son las relaciones. Entonces, otra cosa y otro desafío
688200	694600	importante es lo que podríamos decir la extracción de eventos. Un evento es una actividad en el mundo
694600	702680	real que ocurre durante cierto periodo de tiempo en un cierto espacio geográfico, una definición. Y para
702680	709880	eso yo lo que tengo, o muchas veces tengo, alguna vez se lo puedo reconocer por sacar por lo, por lo que
709880	713840	decíamos recién. Por ejemplo, el evento de las eliminatorias podríamos determinar que es un
713840	723280	evento, que a lo que hablábamos hoy. Pero a veces es una tarea en sí misma la detección de
723280	729960	eventos donde yo tengo un conjunto de también, de términos o de palabras disparadoras del evento y por
729960	738200	ahí me puede llegar a querer interesar encontrar. Fíjense la primera, el primer de los ejemplos,
738200	745800	una tormenta de arriba, perdón, acá. Una tormenta de arriba, centenares de árboles en
745800	751760	Montevideo. ¿Cómo yo puedo detectar? Bueno acá, Montevideo, sería un metíaco nombre,
753040	762600	pero tengo algo que me indica que se dio un evento, ¿qué es? Tormenta. No? Tormenta me da
762600	771600	la idea de que hubo algo, pasó algo. Un motociclista de 38 años falleció en un accidente de
771600	780840	tránsito, tal vez la palabra accidente sea el evento. También bueno, que falleció, pero accidente
780840	786960	es una palabra disparadora que me dice, bueno acá hay un evento y está ya es un desafío más
786960	795520	grande que se está. Colóñe y requenas una mugre, a priori por qué va a ser un evento,
795520	803560	pero en realidad sí me está marcando un evento de que hay un problema de limpieza en Colóñe y
803560	812640	requena. Entonces a veces yo tengo palabras disparadoras que me ayúen a detectar eventos y a veces
813360	825000	tengo que encontrar alguna otra técnica para detectar esos eventos. ¿De acuerdo? Bien, arquitectura
825000	832880	genérica, esta es una propuesta que hizo Hobbes en la década en los 80, si más no recuerdo,
832880	837440	que plantea cuál es una arquitectura en general de un sistema de extracción de información.
838440	846800	Como ven aparecen un montón de cosas y determinos que estuvimos haciendo, análisis lexico
846800	852040	gráfico, nos basamos en diccionarios, análisis sintáctico, reconocimiento de entidades,
852040	856400	reconocimiento de patrones, siempre acá en realidad todos estos reconocimientos de patrones
856400	862760	de alguna manera, análisis sintáctico, con referencias y acá abajo, lo que decíamos generación
862760	871640	de plantillas, donde se van a cargar esos datos. Y lo se enfoque para la construcción de un sistema
871640	878560	de extracción de información, tengo por un lado reglas o por otro lado los sistemas mediante
878560	887240	aprendizaje automático. No voy a entrar a ser juicio de valor, yo creo que los dos son
887240	897360	válidos, el término de generar reglas, requiere un conocimiento lingüístico, sin duda,
897360	902880	técnicas de reconocimiento de patrones, voy a tener que generar esas listas que me permitan a mí,
902880	907040	porque yo lo puedo hacer todas estas cosas que estuvimos viendo, lo puedo hacer con grandes listas,
907040	915680	y no necesito entrenar nada, pero tengo que tener claro este tipo de cosas, ¿no?
917680	923680	como Barcelona o Uruguay, ¿qué es? ¿A qué estoy haciendo referencia? Es un lugar,
923680	932000	es el Rio Uruguay, es el país Uruguay, es la selección Uruguaya, ¿se entiende? Entonces tengo
932000	941560	dificultades que por ahí las tengo que resolver más adelante, ¿no? Con sistemas, bueno, la contra
941560	947160	que puede llegar a tener los sistemas de reglas es en algún caso que no tengo las capacidades ni
947160	953240	los recursos como para poder hacer todo eso. Además, si yo le quiero incorporar después nuevos
953240	959600	documentos por ahí, tengo que entrar a redefinir reglas y esas reglas nuevas que agrego, capaz
959600	966440	que me repercuten en las que ya tenía, en fin, es un proceso que es muy bueno, que funciona,
966440	971240	pero tiene algunas limitaciones por el lado de los recursos y por el lado de las escrituras de las
971240	984360	reglas. Para esto la clave es que lo que yo necesito que es, para estos.
989600	997160	Claro, datos, corpus. Necesito corpus en los sistemas de Machine Learning, de aprendizaje
997160	1003440	automático, si no tengo datos prácticamente seguramente tenga problemas a la hora de resolver
1003440	1010800	un desafío. La clave está en la cantidad de datos que yo tengo para entrenar mi modelo.
1010800	1021040	Bueno, lo estamos diciendo, los criterios para decidir un enfoque de punidad de recursos,
1021040	1026160	por la posibilidad de escritura de reglas, los datos de entrenamiento, cambios posibles en
1026160	1031160	la especificación y la performance. El capaz que algún algoritmo puede ser un poco más
1031160	1040720	eficiente que otro. Bien, la idea es ahora hablar de un par de temitas más en donde también
1041440	1052800	el procedimiento del lenguaje natural tiene una participación, porque cuando estamos manejando
1052800	1059120	texto, estas técnicas que estamos hablando se aplican a muchas otras, a muchos otros temas,
1059120	1061920	a los que nosotros nos interesa a esa procesamiento de texto.
1061920	1071000	Uno es clasterin y el otro es la detección del vuelvo del lado de tópicos. Entonces, lo
1071000	1081080	primero que lo gustaría ser una cierta precisión es porque nosotros hasta ahora vimos, creo que
1081080	1087160	no sé si lo vieron con la idea, la creo que con Luis, el tema de clasificación. Entonces,
1087720	1095440	muchas veces hacer clasterin implica que yo en definitiva estoy haciendo clasificación,
1095440	1101800	lo que yo estoy haciendo es o qué significa clasterin es agrupar, dar un conjunto de datos,
1101800	1108480	ir agrupándose en datos que tengan un comportamiento similar o sean similares en algún sentido.
1109440	1118440	Cuando yo hago clasificación es un método en donde yo ya sé que es lo que yo pretendo
1118440	1125640	clasificar, que se yo autos de determinado tipo o determinada marca, entonces los tengo
1125640	1132360	donde autos y los clasificos, por lo si es algo, mientras que es y además está asociado a
1132360	1138280	técnicas de aprendizaje supervisado, yo tengo un conjunto de datos en donde yo ya sé y cuando
1138280	1145840	cae un nuevo dato sea donde lo mando, o debería saber, ya está pre establecido cuáles son los
1145840	1154040	términos de clasificación. En clasterin está más asociado a lo que serían técnicas de
1154040	1160000	aprendizaje no supervisado, donde en general no necesariamente, dependiendo del algoritmo que
1160000	1169760	yo utilice, sé la cantidad de conjuntos o clasters que yo voy a determinar. La estrategia es después
1169760	1177080	ver en base a qué es que yo genero esos clasters, esos agrupamientos, qué es lo que hace de que
1178080	1189040	dos datos o dos textos sean similares y ese justamente es el desafío, entonces simplemente
1189040	1195520	presentar el tema, presentar dos modelos un poquito distintos o dos enfocues de algoritmos de
1195520	1207960	clasterización y en una donde yo, a priori digo bueno quiero que tenga x que cae clasidad de
1207960	1215440	clasters, entonces en función de eso no sé cuáles son pero lo que hace el algoritmo es tratar de
1215440	1224800	encontrarlos esos agrupamientos, tienen sus prosios contra. Entonces el clasterin es como
1224800	1228840	decíamos, si en una tarea que tiene como finalidad lograr agrupamiento de conjuntos de objetos
1228840	1239000	que están no etiquetados y esos agrupamientos reciben el nombre de clasters. Los elementos de
1239000	1245240	cada uno de esos conjuntos poseen algunas características que los distinguen de otro, esto es importante porque
1245240	1250840	la idea es que cada uno de los elementos pertenezca a uno y solo uno de los conjuntos determinados.
1259240	1266240	Esa última oración acá queda nuestro criterio darles una interpretación semántica, por ahí yo
1266240	1272280	no sé por qué los estoy agrupando de esa manera y muchas veces sucede, después de que los agrupe
1272280	1278040	yo trato de ver y de ponerle un nombre a cada uno de esos conjuntos, se entiende, a priori no
1278040	1287160	necesariamente tengo por qué conocer de qué trata cada uno de esos clasters, simplemente los agrupo
1287160	1295760	y después le pongo un nombre. Algunos usos de técnicas de clasterin, algunos son más conocidos,
1296760	1306200	seguramente o enseguida le suene, la biología en el estudio de las células, en medio ambiente,
1306200	1316720	en marketing, en marketing, segmentación de mercado, muchas veces se habla de hacer clasterin en marketing,
1316720	1321880	lo que estamos haciendo es segmentar, tratar de hacer agrupaciones de clientes, con determinado
1321880	1327160	perfil, determinado comportamiento, y eso es justamente un determinado claster a donde yo le
1327160	1334280	voy a mandar o mi empresa le va a mandar esa tal o buena información. En sociología, bueno, en
1334280	1343440	análisis de redes sociales, eso se hace mucho cuando se estudian los perfiles de lo que actúan
1343440	1350600	en redes sociales, y bueno, en función de eso, te tiran en Twitter, por ejemplo, y te tiran
1350600	1356640	qué, cómo es, qué tweet, promocionado, determinado producto, te puedes llegar a interesar, eso
1356640	1363560	está relacionado en las dos, es algo de segmentación de mercado, pero también implica análisis de redes sociales.
1363560	1369480	Por mismo, lo que veis, tipo, es una gran social, no va a sacar todos los tweets, por ejemplo,
1370000	1375860	los tweets primero y ahí también en segmentación en clasters, ¿qué haces? ¿Puedes
1375860	1379920	abrubar a su usuario con intereses, no? Pero eso es lo que haría vos después,
1379920	1385480	está, eso lo haces, lo haces vos después cuando tenés los tweets, yo me sé que cuando
1385480	1389600	empezaste, ahora pensé que hablabas de cuando vos entras a Twitter y ves lo que le aparece, yo me
1389600	1394880	refería a que vos entras a Twitter y de repente te parece algo, un tweet que no sabe es por qué
1394880	1400420	te lo ponen, y eso es porque alguien sabe, a este le gusta el futo, entonces seguramente le
1400420	1406040	va a hacer un tweet de la final de la Copa, esta que está haciendo ahora, ¿enderte? Porque
1406040	1411660	detectan que hay un interés en vos, entonces ese tipo de cosas agrupan, claro, el tweet no te
1411660	1418340	lo mandan a vos, te lo mandan a todos aquellas personas que tienen un perfil similar, entonces es un
1418340	1435680	poco en ese sentido. Bien, hay como dos clases de algoritmos principales, por decirlo alguna
1435680	1445580	forma, manera, uno es el que se denomina camins, que es el que en el que yo sé a priori, como
1445580	1454100	decía, quiero conseguir cada cláster distintos, ese algoritmo de camins en donde yo prefijo un
1454100	1462580	K es, trato de terminar en un, este es un dibujito para que se entienda más fácil en dos dimensiones,
1464300	1472740	ahí hay un montón de, piensen que pueden ser documentos, pueden ser, no importa qué, demasiado,
1472740	1478460	representados por mundos, entonces el algoritmo de camins, lo que dice es bueno, ¿cuánto
1478460	1488180	vale cada tres? Entonces trata de determinar tres puntos que son, van a ser los centróides de esos
1488180	1501180	cláster, de esos conjuntos. Cada cláster se representa mediante un punto en el espacio, tengo cada
1501180	1508220	de esos puntos, los puntos que queden más cerca del centroide, se subí, que de cualquier otro
1508220	1516300	centroide corresponden al cláster, se subí. Y eso es un proceso iterativo, es decir, yo agaro y
1516300	1527060	pongo, ahí elijo, tres puntos, a priori cualquiera, y empiezo a calcular las distancias, y ahí está
1527060	1533620	la clave, que es lo que utilizo para que fórmula es la que utilizo para calcular la distancia de
1533620	1539660	cada uno de los puntos a esos que constituirían mis centróides. Esos centróides en definitiva,
1539660	1547820	por eso que dice que es un proceso iterativo, yo voy a cambiarlo, es decir, yo tiro una vez y
1547820	1554540	empiezo a grupar, y después eventualmente en función de lo que me da, puedo determinar nuevos
1554540	1560780	centróides, porque algunos me quedaron medios lejos o lo que sea, digo, capaz que hay otra
1560780	1567940	agrupación, que es un poco mejor, acá es como en el ejemplito, este es como bastante obvio,
1567940	1575300	que en definitiva, si yo eligiera, un puntito acá, un puntito acá y un puntito por acá,
1575300	1581500	en seguida esos grupos, pareciera que están cerca de esos puntos, pero si yo hubiera puesto
1582460	1590540	una de las x por acá arriba, o por acá, bueno esto, el puntito, capaz que los agrupamientos
1590540	1595780	hubieran sido otros, y entonces necesito más de una iteración para armarlos los conjuntos que
1595780	1610420	aparecen ahí, ¿ok? Entonces, como decía recién, acá todo depende de cuántos conjuntos o cuánto
1610420	1615820	vale acá, acá yo podría decir, bueno, yo tengo todos estos puntos y quiero hacer dos
1615820	1624700	clasters, entonces parece intuitivo que están agrupados de esa manera, y así podría elegir
1624700	1630460	6 clasters, entonces en definitiva los puntitos que están más cerca, o sea no está marcado
1630460	1639980	acá cuál es el centro oído, pero un poco podemos intubir en función de los colores, ¿ok?
1639980	1648340	Bueno, 2 clasters, 6 clasters, 4 clasters, lo que fuera, para el cálculo de la distancia entre
1648340	1654820	los puntos, lo que se utiliza es la distancia euclidia, también se podría utilizar el
1654820	1661820	coceno del ángulo, entre eso que se forma entre los 2 puntos, en general es un algoritmo muy rápido
1662700	1670580	que convergen pocas iteraciones y esto es una cosa importante, en los clasters no hay solapamiento
1670580	1679180	de objetos, es decir, cada uno de los elementos va a partencer a un conjunto sol, el desafío obviamente
1679180	1691060	va a hacer elegir los mejores casas centroides, acá hay un ejemplo justamente que iteran más de un
1691060	1696500	caso que muestra lo que lo que decíamos hace un ratito, yo tengo un conjunto de puntos,
1697700	1707860	ahí los verdes y elijo estos dos, como centroides, estas dos x en azul y en rojo, entonces en una
1707860	1715820	primera pasada del algoritmo lo que me dice es divido así y así, ese agrupamiento, algunos son
1715820	1726460	azul y otros, pero será la mejor iteración, vuelvo a iterar, elijo, cálculo de estos puntos
1726460	1731820	que yo harás tan todos azul, a ver si no hay algún otro x, no sé si se ve ahí, acá hay otro,
1733340	1741980	acá está la x y acá hay está la x en rojo, entonces si yo defino esos otros centroides,
1741980	1754660	el agrupamiento es distinto, itero de vuelta, centro de acá, centro de acá y el agrupamiento
1754660	1763060	algunos cambian, pero después de, acá muestra que después de un par de iteraciones ya no cambia más,
1763060	1769940	entonces la partición final sería este, o sea tiende a converger después de un cierto número de
1769940	1775500	pasos, ¿cómo pasa la información de esas alas dimensiones para ayudar a hacer el ejemplo?
1775500	1781380	No, pero esto es como, esto es un ejemplo, no más, de visualización, acá lo están mostrando en
1781380	1786020	dos dimensiones, vuelvo a que podés tener si pueden ser en dimensiones de ellos, que es ellos, el espacio
1786020	1795700	en edimensional, en principio, esto va a mostrar más que nada el ejemplo, entonces un modelo de clástering
1795700	1805180	es el camins, y otro modelo, otro, otro esquema es el modelo gerárquico, en donde al revés del camins,
1805180	1810860	donde yo conocía a los K, sabía que yo quería hacer K-conjuntos, en el gerárquico yo no tengo
1810860	1815460	predeterminido a priori, ¿cuáles son esos K-conjuntos que yo quiero determinar?
1816460	1827220	Entonces, yo se plantea como que los datos o las observaciones o los textos, si fueran textos,
1827220	1833660	serían las hojas, y en principio trato de ver alguna forma en que estén correlacionadas
1833660	1840300	cierta similitud, y ahí tendremos que ver cuál es pueden ser las distancias de similitud entre
1840300	1847420	si son documentos, o si son este, que si yo cualquier otro caso, esto, a ver, como decíamos
1847420	1852300	hoy, esto se aplica a lo que sea, a nosotros nos interesa ver cómo estas cosas las aplicamos a
1852300	1862220	los documentos, a los textos, pero en principio son algoritmos de clástering genericos, cada hoja
1862220	1869300	representa un elemento de observación, repito para nuestro caso serían documentos, y a medida
1869300	1874660	de que se sube, alguna de esas hojas se van funcionando en función de cierto grado de
1874660	1885100	similitud, algunas características comunes, y la idea en este ejemplo que está puesto acá es que
1885100	1894140	a nivel horizontal yo voy marcando, oí, yo voy marcando, acá sería en la de apositiva
1894140	1901580	de la izquierda sería un solo cláster, son todos iguales, pero los cortes estos horizontales acá
1901580	1910180	en las ramas es como que si yo digo, bueno acá marco estos tengo dos clásters, tengo dos
1910180	1914260	conjuntos de elemento que se parecen, y este de la izquierda tengo tres, dependiendo aquí
1914260	1922940	altura corto es donde yo a grupo conjuntos de elementos que se consideran parecidos, que tengan
1922940	1937740	algún verado de similitud, hay otro, algunos otros, perdón, hay otro modelo, también que
1937740	1946540	se llama Debescan, que también se utiliza, se utiliza en clástering de textos, en donde es
1946540	1950860	un algoritmo que también se basa en la densidad de puntos, en la representación como veíamos
1950900	1958780	hoy en el Camins, pero también es un modelo que no conoce de prioridad lo ca, sino que yo
1958780	1966940	voy tratando de agrupar conjuntos que tengan alguna similitud, el problema que puedes llegar a tener
1966940	1972380	es que yo lo que hago es para cada uno de los puntitos de mis observaciones o mis textos,
1972380	1982580	trato de generar un cierto círculo, digamos un cierto epsilom de cercanía, de correlación,
1982580	1988340	y en función de eso voy agrupando aquellos que se queden cerca, esta es el concepto de lo que
1988340	1995220	están adentro, lo que están en la frontera o lo que están quedan muy lejos, y en función de eso
1995220	2004300	yo voy viendo cuáles son los que puedo ir agrupando de alguna manera, lo que pasa ahí es que como
2004300	2008500	en cualquiera de estos otros casos yo puedo tener documentos que no se parezcan a nada y que me
2008500	2015700	quedan muy aslados, y entonces también en cualquiera estos algoritmos, eso puede generarme,
2017860	2024060	puede generarme, si son muy dispersos, los documentos muy distintos, documentos, digo documentos
2024060	2032980	o elementos, puede generarme algunos elementos que no estén relacionados con ninguno de los clasca,
2032980	2044300	entonces bueno hay que ver qué tratamiento se hace con eso, preguntas, seguimos, bien,
2044300	2054260	y el otro tema es que queríamos comentar, bueno es el modelado de tópicos,
2058780	2066220	que es un tópico, que es un tópico, también lo que es un tópico, más allá del que está acá,
2066220	2072620	vamos a hacer así, si no le hicieron rápido, qué es un tópico, qué le se llama un tópico,
2072620	2081060	escucharnos en el tema modelado de tópicos, tópico modeling, no le suena, bien,
2082580	2091620	qué es un tópico, un tema, ¿por qué usamos la palabra tópico?
2092620	2100980	hay ninguna circunstancia, ¿eh? para vos era la tema, bien, claro, se utiliza algunos,
2100980	2108180	se habló de determinado tópico, y eso es, se habló de determinado tema, correcto,
2108180	2115220	es que es un poco esa idea, lo que pasa es que no necesariamente, y esa es un poco,
2115220	2120900	vamos a, primero vamos a ver un par de definiciones de la RAI, fíjense en la cincada,
2120900	2128180	es lo que vos decís, tema, ¿no? elemento de una enunciado, fíjense acá, esta buena también,
2128180	2133100	elemento de una enunciado normalmente es helado entre pausas que introduce alguno de los elementos
2133100	2137700	de la radiación, o bien aporta el marco del punto vista pertinente para la enunciación,
2137700	2151540	en definitiva la pregunta o la dos es tópico, es igual la tema, si como yo determino o como
2151540	2159620	debería yo tener las formas de identificar los tópicos o los temas,
2159620	2178100	es decir, cuando yo hago model, modelado de tópicos, lo que trato hacer y ahora nos vamos
2178100	2185780	a concentrar directamente en textos, pensemos en textos en palabras, yo trato de ver o de agrupar,
2185780	2197100	tratar de detectar de qué tópico habla, tal o cual documento en función de las palabras
2197100	2202940	que tienen ese documento, pensemos en un texto que no sabemos nada y que hemos de decir determinar
2202940	2208020	de qué tópico habla, para eso lo que hago es analizó las palabras que contiene,
2208020	2220900	analizó las palabras que contiene, y después ya hay algunas discusiones, ¿no?, porque bueno,
2220900	2229340	claro, las palabras que contenga si son palabras que hablan, están siempre aparecen medio relacionadas
2229340	2235060	en todos los tópicos en perdón, en todos los documentos, capaz que están hablando de lo mismo,
2235060	2245740	universidad, estudiante, clase, materia, profesor, capaz que todo eso está relacionado a algo
2245740	2256020	que podríamos decir tópico, educación, se entiende? y le estamos dando, le estamos dando como un
2256580	2265580	justamente un tema semántico, pero sin retrocedemos un casillero y lo pensamos como conjunto
2265580	2271700	de palabras, hay un ejemplo que está muy lindo que yo digo, bueno, en primer lugar, en segundo
2271700	2277660	lugar, en tercer lugar, finalmente, son ciertos marcadores o palabras que también suelen
2277660	2282420	aparecer juntas en un montón de documentos, pero en realidad de qué están hablando, ¿cuál
2282420	2287940	es el tópico? ¿qué que están hablando? son palabras que sí están relacionadas en algún
2287940	2291540	sentido, porque aparecen siempre juntas, lo que decía Marciano, aparecen siempre juntas,
2291540	2297180	pero en realidad no tienen un tema semántico, entonces hay que saber discriminar ese tipo de cosas,
2299860	2305780	se ve la dificultad o se ve el tema, ¿sí?
2305780	2315620	El origen de todo esto es lo que se conoce con el nombre de las colocaciones, o podríamos decir que
2315620	2322220	uno de los orígenes, que es una combinación, que son las colocaciones, es una combinación de
2322220	2339860	palabras, cerrar una ventana, cometer un error, que tienden a aparecer juntas, mientras estas
2339860	2348940	otras términos que aparecen acá, meter la pata, tomar el pelo, cortar por los anos, son palabras
2348940	2356860	que aparecen juntas, pero que en realidad tienen significado en sí mismo, o sea todas juntas
2356860	2373260	constituyen un solo elemento o un término, meter la pata que es, cuando decís meter la pata y si
2373260	2379940	te agomada el cometís un error, entonces yo tendría que mi algoritmo, tendría que determinar que
2379940	2386300	meterla, si aparece, meter la pata, o cometer un error deberían de estar juntas, por decir algo,
2388700	2395180	entonces esos son el tipo de cosas o los desafíos que uno puede llegar a encontrar cuando estás
2395180	2412420	haciendo estas cosas, bien, tópicos, es definitiva, o debería ser el asunto principal del que
2412420	2419660	se habla, del que se predica, o del que se comunica alguna cuestión, y el tema es que dado un
2419660	2427580	documento no necesariamente fácil determinar el tópico, y ese es justamente el desafío que se
2427580	2438300	que convoca cuando uno hace modelado de tópicos o tópicos de diga, tratar de encontrar o determinar
2438420	2448860	el tema o un determinado tema del que hable un documento. Fíjense este ejemplo muy lindo,
2448860	2458780	leamos arriba, a partir de este martes cada club solo podrá sumar nueve puntos, unidades que
2458780	2464900	solo definirán el último módulo del Campeonato Uruguayo, sino que también decidirán quiénes se
2464900	2476780	mantienen en primera, ¿de qué hablar eso? Ahora, tiene un montón de palabras,
2479280	2487460	enseguida y te cuenta que hablaba de fútbol. Cambía club por estudiante Campeonato Uruguayo por curso y
2487460	2494760	primera por carrera y le damos la segunda abrasión. A partir de este martes cada estudiante solo podrá
2494760	2500520	sumar nueve puntos, unidades que solo definirán el último módulo del curso actual, sino que también
2500520	2508440	decidirán quiénes se mantienen en carrera. ¿Y qué estamos hablando acá? A puntar, estudios,
2508440	2521280	educación, entonces la clave está en ver cuáles son las palabras que en definitiva son las
2521280	2530760	que me marcan el tópico y hay un montón de palabras que pueden aparecer en varios textos y
2530760	2539960	en varios tópicos, porque la palabra martes aparece en tanto en los tópicos de carrera como en el
2539960	2549800	tópico de fútbol. ¿Se entiende? Entonces, ¿pero qué pasa? En alguna va a aparecer o más frequentemente,
2549800	2561720	o menos frecuentemente, y ahí la estrategia o el modelado que más se adecúa a este tema es
2561720	2565720	trabajar con provenidades y hacer distribución de probabilidades.
2570640	2576960	Entonces, y ya vamos a eso. El modelado tópico nos permite organizar, entender y resumir grandes
2576960	2584520	colectiones de documentos, intentar detectar patrones de ocurrencia de las palabras, agrupándolas
2584520	2589120	en base a distribuciones de esas palabras en un conjunto de documentos, un poco lo que estábamos
2589120	2597600	comentando con ese ejemplo. Es útil identificar los temas para poder agrupados, eso está claro.
2598480	2604600	Entonces, ¿en qué consiste el modelo de tópicos? En construir un modelo justamente que busque y
2604600	2612760	encuentre las palabras que están relacionadas de alguna manera. Esas agrupaciones de palabras lo
2612760	2621280	que van a conformar, justamente son clasters. Y esa, o sea, que lo que estuvimos viendo antes está
2621280	2630480	implicitamente relacionado con esto que está moviendo ahora. Y la estrategia claramente es que
2631480	2638800	mis tópicos, los distintos clasters que yo vas a juntar sean los más distintos que pueda, entre
2638800	2645720	sí. Pero eso no necesariamente lo puedo, porque lo que nos va a estar pasando es que
2645720	2654720	palabras, muchas palabras pueden aparecer en muchos tópicos, lo que va a tener, lo que van a tener,
2654720	2661000	o lo que deberían detener son distintas frecuencias de aparición, o distintas probabilidades que
2661000	2664280	ocurran en tal o cual palabra, en tal o cual tópico.
2668320	2671880	¿Pero puede tener un documento que habla de los tópicos?
2671880	2675800	¿Dió? Porque en el clasterín, un clasterín, un claster.
2675800	2682960	Sí, exacto. Y ese es todo un desafío. Porque justamente lo que va a estar a tener no solamente
2683960	2691600	un documento va a pertenecer, ahora lo vamos a ver, el acorismo tradicional de esto,
2691600	2698640	es el LDA, que lo que hace es justamente una distribución de donde este documento puede quedar
2698640	2707120	en este tópico, en este tópico o en este tópico. Entonces, pero con distintas probabilidades y ese
2707120	2713560	es justamente el desafío. No solamente tengo palabras que pueden pertenecer a más de
2713560	2719520	un documento y a más de un tópico, sino documentos que pueden pertenecer a más de un tópico.
2719520	2725680	Y ese es todo un problema, sí duda. Lo que pasa aquí es lo que yo trato de hacer es generar un
2725680	2736160	modelo en base a distribuciones de probabilidades. En el modelado de tópicas, yo tengo que cada tópico
2736160	2744280	es una bolsa de palabras y que cada documento es una mezcla de tópicos, que era un poco la pregunta que
2744280	2752920	vos hacía. Cada documento puede tener cierto porcentaje de palabras que con mayor o menor
2752920	2759560	frecuencia aparecen en más de un tópico. Y eso es justamente la estrategia que hacen los
2759560	2775560	algoritmos de tópico de língua. Tengo un conjunto de documentos y lo que trato de hacer es
2775560	2784160	agruparlos bajo un determinado tópico. Claro, uno me dirán, pero pensemos y pensamos noticias de
2784160	2795360	prensa. Por lo general, tengo ya metadatos, que me dice de hecho pasa, esto pertenece a economía o
2795360	2801240	esta es una noticia de fútbol o esta es una noticia de, ahí pueden haber tópicos que están
2801240	2808760	fregamente determinados, pero no necesariamente tengo esos metadatos en donde yo me pueda basar para
2809280	2820840	aplicar mi tópico de línguamos, mi modelado. Y no necesariamente, o sea, acá yo le estoy diciendo esto.
2824880	2833600	T1, T2 y T3, yo después a este T1, T2 y T3, le voy a poner una etiqueta. Y el desafío va a ser
2833600	2838720	después, bueno, y cuando yo le incorporo un nuevo texto a ver si encajan a algunos de esos tres
2838720	2845280	que definía ahí, o tengo que hacer un nuevo, una nueva pasada para determinar capas otra cosa.
2848280	2853640	Tampoco es una cuestión de que yo diga, bueno hago un modelado tópico, voy a seleccionar en
2853640	2862240	10 tópicos, porque 10, capas que son 5, capas que son 20, capas que son 50, capas que son 2,
2862240	2867960	o sea, tampoco necesariamente se conocen a priori, cuáles son los tópicos o la cantidad de tópicos
2867960	2883120	que existen en un corpus. Y hay 12 foques, ¿no? Por un lado, me vuelta, la lista de palabras y por
2883120	2891160	otro lado es tratar de detectar patrones de aquellas ocurrencias de palabras que se agrupen en
2891160	2897840	base a ciertas distribuciones dentro del conjunto de documentos. Ahora son 12 foques distintos.
2899680	2906320	Y uno podía hacer este, hace un tiempo habíamos hecho un trabajo con la gente de cisces
2906320	2912000	económicas, entonces justamente trataban para otra cosa, el estudio de un indicador y que se
2912000	2920400	basaba en cosa de este estilo, trataba de ver cuáles son aquellas palabras que hablan de
2920400	2928080	determinado tópico o determinado tema, ¿no? Ahí dice economía, económica, económica, economista,
2928080	2934800	comercio, inflación, entonces el tópico es economía, insertidumbre, inserto, inserta, riesgo,
2934800	2943800	país, insertidumbre. Fíjense que riesgo país lo toman como un token, o sea no estamos necesariamente
2943800	2950920	hablando de palabras, sino que estamos hablando de tokens. Esto también le da la pauta, hoy no lo
2950920	2962360	vimos en el ejemplo, que entonces estas cosas, yo cada vez que vaya a aplicar, y ahí ya metemos
2962360	2968800	un peléne, antes de aplicar estas cosas, que lo que tengo que hacer con los textos, que yo les
2968800	2978640	dije que está minimizada esa tarea cuando hacemos peléne. Depurar, preprocesar, limpiar el texto,
2978640	2984600	sacar un URL, ver que hacer con las fechas, normalizar, ver que hacer con los puntos,
2985600	2990220	he decidido esa tarea de preprocesamiento, la tengo que hacer antes, qué hago con las palabras?
2993820	3000320	Las limpios, las consideros no las considero, se entiende, esas palabras, estas palabras,
3000320	3008000	estos temas no, algunos algoritmos las dejan adentro, pero claro esas me van a aparecer en
3008000	3012320	todos los tópicos, se aparecen en casi todos los documentos, conjunciones, artículos,
3013840	3019520	estas van a aparecer en todos los documentos, esas no son palabras que me identifiquen un tema. De hecho,
3019520	3024940	algunas veces uno lo que hace, algunos algoritmos dicen bueno, genero todo un tópico con las
3024940	3029180	etropores, y algunas palabras que no creen contenido, y te hacen un tópico con eso.
3029180	3042460	Para este tipo de cosas, cuando uno trabaja con listas de palabras, ahí lo que se requiere es el
3042460	3047580	conocimiento de un juicio experto, también de que diga bueno, cuáles son las palabras asociadas
3047580	3054780	a tal tópico. O sea que hay un trabajo no solamente de algoritmos que tratan de identificar,
3054780	3059540	sino un trabajo de arranque que me identifique, cuáles son aquellos, aquellas palabras asociadas
3059540	3072140	a tal tópico. Bueno, por otro lado tenemos algoritmos un enfoque basado en distribución de las
3072140	3083380	palabras. El EDA es un algoritmo bastante, es el de los más utilizados, el EDA y algunas
3083380	3090820	variantes, en esto de modelado estópicos, sobre todo en este último tiempo. Pero fíjense
3090820	3097300	que aparecen, son trabajos que aparecen ya en la década del 2000, ¿no? Y leí es uno de los que
3097300	3106500	es el que propone el algoritmo del EDA. El EDA genera tópicos proponiendo una distribución de
3106500	3112380	todas las palabras del corpus y calcula una distribución de estos tópicos en cada documento.
3113380	3122300	Entonces, cada documento en ese corpus es atribuible con una cierta probabilidad a alguno de los
3122300	3130060	tópicos. O sea, un poco de la pregunta que vas a hacidas, un documento puede pertenecer,
3130060	3137260	ser del tópico T1 con un 95% de probabilidad, pero tiene un 5% de probabilidad de que ese tópico
3137260	3142380	también pertenece, ese documento también pertenece al tópico T2. Y es un poco lo que hace
3142380	3149700	el EDA. ¿Cuea con eso? Pero un documento puede tener más de todo. Exacto.
3149700	3156420	Pero no tengo probabilidad, sino que hablo de buscoso. Bueno, ese es otro tema, pero vos
3156420	3165100	y vos querés encasiñarlo en uno de los tópicos, es decir, este habla de 95 por 50% de economía
3165100	3175140	y 50% de política, política. Exacto. Y te lo deja así. Después vos después tendrás que ver
3175140	3184100	qué lo que haces con eso. Pero sí, exacto, puede pasar. Bueno, un poco lo que decíamos recién.
3184100	3188980	Cada tópico es una distribución probabilística de palabras, entonces tengo el tópico turismo,
3188980	3199540	como educación, economía. Y entonces, como ven hay palabras que aparecen, estos son números
3199540	3207900	truchos, ¿no? Pero palabras que aparecen o que pueden aparecer en más duto pico. Turismo,
3207900	3221980	argentinos, bilateral, blu, educación. Bueno, ven acá en economía también. Aparecen
3221980	3231260	blu, pesos, dólar. Entonces, hay palabras que capaz que blu, cuando tengas que procesar
3231260	3235540	un documento, bueno, adónde lo pongo y tiene la palabra blu muchas veces y bueno,
3235540	3240820	capaz que lo pongo en el tópico turismo, pues es más probable que el tópico economía.
3240820	3249460	Pero bueno, es parte de las cosas que yo tengo que decidir cuando aplico este tipo de
3249460	3256660	voces. Entonces, decíamos, cada tópico es una distribución probabilística de palabras.
3256660	3263860	Y cada documento es una distribución probabilística de tópicos, de vuelta lo que decíamos es
3263860	3268700	un rato. Entonces, si yo tengo este texto que está acá, un poco en base a lo que preguntaba
3268700	3275780	a vos, y bueno, en función de lo que aparece ahí, va así para el Ministerio de Turismo,
3275780	3279740	el Observatorio de Nado por el Economista, Javier Adedea, señaló que el primer trimestre
3279740	3286420	este año el gasto de gruvaya alcanzó, no sé cuánto, tanto de los uruguayos, millones.
3286420	3291900	Bueno, parece acá el tema, no parece la palabra dólar, aparece el signo, un poco lo que
3291900	3297920	decíamos hoy de el prepresentamiento. En fin, aparece acá sí, la aparece la palabra
3297920	3305460	dólar, aparece el blue, aparece pesos. En fin, el proceso me podría decir que este
3305460	3312260	documento tiene un 25% de que sea de turismo, un 7% de educación, porque capaz que tiene
3312260	3318700	algunas palabras del tópico educación y un 19% de economía, por decir algo. Y otras
3318700	3329260	que por ahí no aparecen ahí, ¿ok? Bien, se asinen inicialmente una probabilidad y lo que
3329260	3336460	la D es de Dirichlet, porque lo que utiliza es la distribución de Dirichlet, una distribución
3336460	3344940	de Dirichlet. Permite que un documento sea parte de varios tópicos, cada uno con un peso
3344940	3353260	diferente, y lo interesante es esto, que son las métricas, ¿cómo yo mido? Si mi algoritmo
3353260	3361140	es bueno o malo, se comporta bien, se comporta mal. Lo puedo medir con cuerencia y perplejidad,
3361140	3371980	perplejidades, ¿cómo se comporta cuando yo le agrego un documento? Sabes donde ir,
3371980	3377820	encajan en uno de los tópicos que ya definimos, o no, entonces una medida de perplejidad me dice a
3377820	3382900	mi cuál efectivo es el algoritmo que yo acabo de aplicar. Y cuerencia es bueno, que haya una
3382900	3392980	cuerencia, sea completo entre en su globalidad, que sea cuarente lo que acabo de mi distribución
3392980	3399620	de documentos, a lo largo de todo el corpus, de que todos estén dentro de algunos de los tópicos
3399620	3410860	que he estado trabajando. Hay algunas variantes de la idea STM, BTM, la STM es una variante que
3410860	3416620	lo que hace es cambiar la distribución de probabilidad por una normal logística. BTM está bueno, es
3416620	3422620	una variante, ¿por qué que pasa? El idea, estamos acostumbrados a trabajar con textos largos,
3422620	3427140	donde tienen una gran cantidad de palabras, entonces bueno, eso juego con la frecuencia de las
3427140	3438820	palabras de STM y BTM lo que hace es incluir el concepto de BTM y es de ver si utiliza es como una
3438820	3445220	versión aplicada a textos cortos, como podrían ser textos de Twitter o cosas por el estilo,
3445220	3452900	en donde yo puedo tratar de encontrar pequeñas palabras que ocurren en un texto, es la misma idea,
3452900	3465460	pero para textos mucho más cortitos. Es interesante que si yo son ejemplos, después hay literatura
3465460	3476340	que hable de estos acolípticos. Quería llegar a este. Esta es una eleda extendida con
3476340	3485100	embeddings, es una propuesta bastante reciente en donde yo hago una representación de mi
3485100	3498900	conjunto de documentos vectorial, entonces un vector de dimensiones de las palabras de un
3498900	3506300	vocabulario de conjunto de todas las palabras del vocabulario. Y lo interesante es que utiliza
3506300	3512740	aventores para determinar, o sea, para representar a los documentos y para representar a los tópicos,
3512740	3518180	los documentos están representados por palabras y los tópicos están representados por palabras.
3518180	3527500	Entonces para saber, cuando un nuevo documento entra en tal o cual tópico calcula la distancia
3527500	3533780	euclidia o la distancia cosena entre los vectores del tópico y el documento que estoy agregando,
3533780	3546380	o sea, lo que le agrega este, este m, es al LEDA vectores, embeddings.
3546380	3557820	Entonces, yo tengo ahí ciertos hiperparámetros, ¿cuál es el número de tópicos que yo quiero
3557820	3563940	inferir, cuál es el espacio, la dimensión de los vectores, tal y la cantidad de vocabularios.
3563940	3569380	Entonces, tengo una matriz, bueno, embeddings con dimensión de por B, una matriz de tópicos,
3569380	3578980	una red neuronal, con entrada de tamaño B y salida de tamaño B. Entonces, un esquema
3578980	3587940	simplemente de lo que como haría para un nuevo documento entra la red y metida. ¿Cuáles
3587940	3596900	son los tópicos inferidos por la red con su porcentaje de probabilidad y cuál va a ser la distribución
3596900	3604500	de las palabras de ese texto en esos tópicos, o sea, las dos cosas. Es más probable que
3604500	3611740	tenga sea de economía o de política, tal probabilidad y bueno, y el porcentaje de estas palabras
3611740	3620340	y yo después de Pueblo, si lo ve pa' adelante, si sí o si no, ya queda en función del usuario.
3621060	3630100	Esto es simplemente un ejemplo para bajar a tierra estos conceptos, ¿no? Yo tengo estas palabras,
3630100	3637140	¿no? Club, campeonato, primera, tantos medios por acá, este cláster de palabras,
3637140	3643820	están medio juntos, por acá tengo estudiante, carrera, curso, creo que son los mismos
3643820	3647420	ejemplos que estaban en el anterior, ¿no? Y tengo esta noticia,
3652020	3654380	¿qué quiero ver a dónde va?
3658020	3666220	Tengo el tópico 1, ¿oops, que está acá? Tópico 1, fíjense, lo del centro y el que decíamos
3666940	3677940	hoy, tengo el tópico 2, yo lo que tengo que ver es calcular la distancia del vector de esta noticia
3677940	3685180	con respecto a cada uno de los tópicos, de los factores de los tópicos, y bueno, esto
3685180	3691500	es simplemente a modo de ejemplo, me dio que esta noticia, fíjense, hablamos de texto,
3691500	3696980	hablamos de multimedia, ¿no? Acá está propósito para mostrarles de que aparece una fotito
3696980	3703340	que probablemente sea de deporte de esa noticia, pero bueno, en función de las palabras que
3703340	3712020	tiene el texto, esto dice que pertenece al tópico T1, 90 y al tópico T2, 10, con esa probabilidad,
3712020	3718260	y esta es la distribución de probabilidad de las palabras de la noticia que aparece ahí,
3719260	3727260	esto es simplemente a modo de ejemplo, ¿qué está la probabilidad de las palabras del tópico?
3733260	3737460	Bien, ¿se entendió? ¿Alguna pregunta?
3738460	3747860	Obviamente, devuelta, ¿dónde engancha BLN acá, prácticamente en todas las etapas?
3747860	3753900	Rickamente en todas las etapas estoy aplicando técnicas de procedimiento de lenguaje natural,
3753900	3758380	porque trabajo con las palabras, trabajo con documentos, en cualquiera de estas dos casos,
3758380	3764140	más ya que clástarlo mismo con algunos ejemplitos medios aislados, el mismo, acá aparece el mismo
3764660	3771380	concepto de agrupamiento, de agrupamiento de palabras, de agrupamiento de documentos, y bueno,
3771380	3778340	después está la manera de cómo yo represento esos documentos para luego procesados.
3781140	3782980	Bien, ¿no hay preguntas?
3785140	3792660	Estos sabrimos, son unos prohibizados, no le decís el tópico en el maíz, exacto, exacto,
3794140	3800700	es más, hoy lo, en este ejemplito, ¿no?
3803700	3811460	O sea, los tópicos son t1, t2, t3, después yo, humano, bueno, mira, al t1 me fijó en las
3811460	3818380	palabras y digo economía, al t2 le pongo deportes. Si pensamos en noticias, ¿no?
3818900	3827020	Pensamos en noticias de un diario, no necesariamente un diario que lo coloque en el tópico política,
3827020	3837420	capaz que en realidad para mí es el tópico economía. O sea, me puede servir tener esos metadatos,
3837420	3842460	si fueran, si estuvieran analizando texto o emprensa y tengo los metadatos, me puede servir como
3842460	3852980	para validar o no validar. Pero a priori, el tipo te tira, t1, t2, t3, t4, t5, los que vos quieras,
3852980	3859460	o digamos, de vuelta, esto se va refinando, llega un punto donde vos desis, ¿no?
3859460	3866540	Llego hasta diez tópicos, o llegó hasta cuatro tópicos, o llegó hasta 20 tópicos, porque después
3866540	3873260	ya la distribución es la misma, no cambia, por más que a grande el número de tópicos esto no
3873260	3883420	cambia, o sea, no va a borear la economía. Pero bueno, después se requiere de un juicio experto
3883420	3889820	que te diga, bueno, t1 es tal, t2 es tal, y cuando venga un nuevo documento entre hace el
3889820	3896460	algoritmo, y ves, si enganchó en el t1, que era la economía, y ahí como que validas si estaba bien
3896460	3915380	o está mal. No preguntas? Bien, bueno, entonces dejamos por acá, fin del curso, y seguimos ahora
3915380	3922540	la semana que viene libre, y luego empezamos con las presentaciones. En el foro tienen para
3922540	3931980	preguntar por la tarea laboratorio, vamos a tratar de estar atentos a las preguntas. Y
3931980	3939020	tal, y después ya les digo, hoy publicamos en un rato publicamos la nómina de artículos
3939020	3940020	de cada uno de los grupos.
