start	end	text
0	26220	En la clase de hoy vamos a ver un tema nuevo que es el de los modelos del
26220	35860	lenguaje. Si ya fueran en la clase pasada, vimos que era bastante diferente, el de los
35860	44860	transductores para resolver el tema de la morfología de Taufinito, unos artefactos de
44860	55460	Taufinito que permiten resolver temas a través de un método de reglas. Yo defino reglas de
55460	64340	como se conforman las palabras, las combino de cierta forma y de esa forma resuelvo el
64340	73220	tema de convertir de la palabra a su análisis y viceversa. Y después vimos la segunda parte
73220	78420	de un método que era bastante diferente, su concepción, que es un método estadístico,
78420	86260	que lo que hacía era aplicando el modelo del canal ruidoso, aproximarse al problema de
86260	91540	corregir el rojo de ortográfico. Cuando yo hablo un modelo probabilista, lo que estoy diciendo
91540	101100	es que además de, por ejemplo, clasificar o sugerir una solución, lo que haces es asignarle
101100	108180	probabilidades a las posibles respuestas. Un método probabilista, típicamente no da una
108180	113380	respuesta, sino que devuelve una distribución de probabilidad. Si yo tengo varios eventos
113380	124180	posibles, una distribución de probabilidad es un número, entre 0 y 1, que yo asigno a cada
124180	129820	evento posible, de forma que la suma de todos los eventos de en 1, eso es lo que llamamos
129820	133900	una distribución de probabilidad. Entre 0 y 1 son todos, son todos mayores o iguales
133900	138060	que 0, menores iguales que 1 y además su suma da 1, eso es una distribución de probabilidad.
138060	142780	0, 5, 0, 25, 0, 25 es una distribución de probabilidad. Si el evento 1 tiene probabilidad
142780	147740	0, 5, el otro es 0, 25 y el otro es 0, 25, eso es una distribución de probabilidad. Si no
147740	155340	suma 1, no son una distribución de probabilidad. Y si yo, por ejemplo, tengo un evento que
155340	159340	ocurre 10 veces, si por ejemplo hago conteo de frecuencia, por ejemplo no digo hay un evento
159340	167340	1, que ocurre 10 veces, hay un evento 2, que ocurre 5 y hay un evento 3, que ocurre
167340	177660	5, eso no es una distribución de probabilidad, porque esto no está entre 0 y 1, porque no
177660	186980	suman 1. ¿Cómo hago yo para convertir esto en una distribución de probabilidad? Lo que
186980	199580	hago es dividir por el total de ocurrencia, ¿verdad? Que en este caso es 20 y eso me da la
199580	205380	proporción respecto a 1 y eso es siempre una distribución de probabilidad. Entonces,
205380	212460	se llama normalizar para obtener una probabilidad. Y ustedes lo van a ver que lo vamos a ver
212460	220020	en varias veces. El método de este de corrección utilizaba fuertemente la regla de valles
220020	228980	para modelar la situación. Hasta ahora hemos hablado en todas las cosas que hemos tratado
228980	233540	de palabras aisladas, ¿no? La morfología estudia, en primero hablamos de cómo separar
233540	237860	las palabras y después vimos cómo analizaba la intamimente, pero siempre hablábamos de palabras
237860	244380	aisladas. Acá lo que vamos a empezar a mirar es ¿qué pasa cuando las palabras aparecen
244380	259860	juntas? Es decir, nosotros lo que vamos a hablar es de la
259860	278180	probabilidad de una secuencia de palabras. ¿Por qué esto importa? Porque como ustedes bien
278180	283080	sabrán, las palabras en el idioma pañón nos aparecen solas y no cualquier palabra
283080	290080	así o otra palabra. Nosotros tenemos una cantidad de reglas para expresar en el idioma
290080	302200	que hace que el orden importe. Y de lo que se trata es ver cómo se orden, cómo tener
302200	305600	en cuenta se orden, no puede ayudar a otra estaria. Creo que con algún ejemplo lo vamos
305600	313080	a ver más claro. Primero que nada vamos a recordar a Chonky, que esto yo lo comentaba
313080	320400	en la primera clase, aquello de que Chonky dijo la noción de probabilidad de una oración
320400	328180	es completamente inútil bajo cualquier interpretación de este término y trancó por 20 años la
328180	334300	investigación hasta que apareció, Shellinet que volvió a revivir el tema de los métodos
334300	341140	probabilistas o basados en conteos para aproximárselo el problema de procedimiento en el
341140	347020	lenguaje natural. Chonky lo que decía esencialmente es cuando nosotros hacemos conteos y sacamos
347020	350940	conclusiones en base a cuenta, en base a número, en base a experiencia, que es típicamente
350940	356620	lo que vamos a ver en este caso de los enigramos. Estamos obteniendo soluciones a problemas,
356620	359860	no estamos entendiendo qué es lo que está pasando. Y eso es una discusión catalía de
359860	367940	hoy sigue, es decir, hay una famosa discusión por ahí en internet entre Chonky, esto te
367940	377340	hablando hace dos o tres años, o cinco años, entre Chonky y Peter Norby, que discute un
377340	382580	poco esto, es decir, si esto que estamos haciendo ahora y que ha tenido tan buenos resultados
382580	387100	del punto de vista de reconocimiento de labla y el procedimiento de los enigramos natural
387100	391340	es en realidad inteligencia artificial o de solamente en number crunching que no nos aporta
391340	396940	mucho. Norby en lo que le dice, bueno, de hecho, la ciencia siempre en modo menos funcionó
396940	403540	así. Bueno, entonces ¿cuál es el objetivo de lo que vamos a ver acá son de modelos
403540	407700	del lenguaje? El objetivo del modelo del lenguaje es calcular la probabilidad de una
407700	415260	secuencia palabra, es decir, ¿qué tan probable es en mi lenguaje que una secuencia se
415260	425660	es? ¿De acuerdo? ¿Para qué no puede servir eso? Bueno, imagínense que ustedes, y acabamos
425660	432260	a recordarlo otra vez el modelo del canal ruidozo, del otra vez, imagínense que tengo este
432260	442140	texto escrito, ¿sí? Y por medio de un método que no sé cuál es, tengo dos oraciones
442140	453820	candidatas, bueno, dos textos candidatos, uno que es preneva para el curso de PLN y prueba
453820	462740	para el curso de PLN. ¿De acuerdo? Y además supongamos que el método que utilicé para
462740	470940	reconocer la escritura me dice que este es más probable que este. Nosotros ¿qué vamos
470940	481860	a elegir? Vamos a elegirle abajo. ¿Por qué? Porque esto no es una palabra válida, pero
481860	490220	aun siendo una palabra válida, o aun suponiendo que fuera una palabra válida, podría darse
490220	494620	un caso donde yo identifico una palabra válida, se ponen los correcciones, aún así yo
494620	506020	podía decir bueno, pero en este lugar, en este lugar, esa palabra no calza, digamos, ¿sí
506020	511340	alguna forma yo sé? Es decir, si yo logro detectar que esta oración es más probable que
511340	517940	esta de alguna forma, eso me va a ayudar en la tarea de reconocimiento. Lo mismo pasa con el
517940	521980	reconocimiento de la habla de lo que hablamos y lo otro día con el espíritu de reconocimiento
521980	527140	y cuando yo hablo y digo una palabra, ustedes me escuchan. Entonces, los modelos de
527140	532020	nevoje sirven para ayudar en este tipo de tarea, típicamente los modelos de nevoje
532020	541940	ayudan y no tratarían. Nos abregan mucha información. Entonces, cuando nosotros hacemos
541940	550940	reconocimiento de escritura, luego lo que decimos es, ¿cuál es la probabilidad de la oración
550940	557420	origen, dada la observación que tengo? Yo tengo una observación, ¿sí? ¿Cuál es la
557420	563300	probabilidad de una oración origen? Es proporcionar a la probabilidad de la observación,
563300	572420	dada la oración por la probabilidad de la oración. ¿Y esto qué es? Eso es valles, en la rir
572420	579500	de valles. Entonces, nosotros por valles sabemos eso. Y como ven, acá aparece la noción
579500	583740	de probabilidad de la oración. Por eso es que nos interesa conocer la probabilidad de
583740	593060	las variaciones. Ahora, ¿cómo calculamos la probabilidad de la oración? Bueno, hay un ejemplo
593060	604940	más, ¿no? Por ejemplo, en la traducion automática, si tenemos estas tres candidatos, nuevamente
604940	610300	a mí me va a ayudar con conocer el orden o saber cuál es la más probable en mi linguaje.
613740	622700	En las corrección de errores, como vimos la vez pasada, hordas de botero es una secuencia
622700	641980	muy de poca probabilidad. Y pensemos un poquito. ¿Preguntemos, no? ¿Por qué? Esta
641980	648620	oración no les parece que sea muy probable. ¿Qué nos podría determinar que esta oración
648620	666460	no es muy probable? O esta, implementación a la educación ley. ¿Por qué podemos suponer
666460	678260	que esa no es probable? Bueno, a mí me ocurre en dos razones, principales o dos, pero
678260	685940	sí mansiones. ¿Una es por las sintaxis, ¿no? La sintaxis del día de mapeñón no es así. No
685940	693580	decimos educación ley, educación... ¿Por qué no? ¿Por qué no? ¿Por qué no?
693580	703460	La sección es su y de botero, como publican la verdad. Ah, bueno, ¿Precio pudiera ser un
703460	708420	suh de un tercero, ¿no? Acá seguramente lo que hay es lo que hay es un error autográfico
708420	714140	de sus gordas de botero. O sea, acá, acá tenemos un tema de sintaxis, acá no tenemos un tema
714140	721420	de sintaxis. Deberíamos conocer un poco de semántica para asociar botero que pintaba
721420	727220	mujeres gordas. Entonces, una aproximación un poco más humilde, es la segunda, es la
727220	732900	alguna aproximación más étadística, porque si nosotros, y que juega con el hecho de que
732900	736900	tenemos grandes volúmenes de texto y ahí el cambio de los modelos probabilísticos, es
736900	745500	que sus gordas de botero seguramente apareció antes en mis cuerpos de texto y hordas de botero,
745500	750740	eso es una aproximación mucho más étadística, eso es lo que vamos a hacer en los modelos
750740	755860	de negra más justamente. A partir de grande volúmenes de texto, detectar, calcular la
755860	762340	probabilidad. Es una aproximación puramente étadística, es bien salvoaje, yo no sé qué
762340	766660	estructura tiene esto, pero sé que esto no se dio nunca y que gordas de botero sí, muchas
766660	779620	veces. Entonces, les más probaré que más equivocado. A ver, relacionado con esto, ahora
779620	784060	vamos a ver por qué está relacionado, está el tema de la predicción de la siguiente
784060	795340	palabra. ¿Cuáles se imaginan que es la siguiente palabra a la primera relación? ¿Cuál
795340	803580	puede ser la siguiente palabra? ¿Quién? Para y no meten mi tío pronóstico para, qué
803580	814340	otra cosa puede ser? Para es una preposición ¿no? ¿Qué más? ¿Qué otra cosa puede
814340	826020	ser ahí? ¿Cuál por ejemplo? ¿Un pronóstico alentador? O puede decir un pronóstico terrible
826020	832780	o un pronóstico... ¿Qué otra cosa más? Hay un más común para mí. El mitío pronóstico
832780	842780	con meteorológico ¿no? A raíz de este fenómeno se sucederán tormentas, fuertes, importantes,
842780	850260	muy, no creo que ahí diga tormentas gatito ¿no? gatito no es muy probable que sea la palabra
850260	855380	siguiente. Nuevamente, ¿por qué sabemos esto? Y porque es muy raro que hay en diga tormentas
855380	865900	gatitos ¿no? Entonces, esto que tenemos acá es la posibilidad de que hay de siguiente
865900	871580	palabra. ¿Dada todas las anteriores? Si yo tengo todo el contexto lo que se llama
871580	879460	contexto, dado el contexto de la palabra que sigue acá. ¿Sí? Una de las, lo que nosotros
879460	883220	vamos a querer hacer en un modelo de lenguaje como camino para calcular la protección
883220	891220	de honoración es dado el contexto calcular la palabra. Siguiente. ¿Sí?
893220	903020	¿Rachas de viento fuerte de componente? Veremos que. Bueno, no resulta hacer que de
903020	907580	los ejemplos que yo tomé a Buenos Aires, puse viento fuerte de componente, perdón. El
907580	911660	lino me demitió pronóstico especial, o sea que le ramos, se sucederán tormentas fuertes,
911660	915420	viento fuerte, componente subo este. Por ejemplo, perdición.
919500	926020	Vamos a poner un poquito de notación antes de seguir, porque vamos a ver cómo enfrentamos
926020	930820	este problema, es decir, cómo calculamos esa protección. Un poco de notación para seguir
930820	940900	eso. Yo lo que estoy diciendo es la probabilidad de que una variable aleatoria ahí valga,
940900	945820	tome el valor con ocimiento, en este caso tendría una variable aleatoria por cada posición
945820	950860	del texto, ¿verdad? Tengo una X1 que la primera palabra ha equidó que es la segunda
950860	955860	X3, son variables aleatoria que lo variable aleatoria esencialmente un mapeo, es una
955860	966740	función que me apega, de un evento un número entre cero y un. La probabilidad, perdón.
966740	973180	Perdón, perdón. Bueno, no, mientras definí, me apega con un real y la probabilidad me
973180	978420	devuelve un número entre cero y un. Es decir, yo defino la probabilidad de una variable aleatoria
978420	985300	como la distribución de probabilidad de una variable aleatoria es la dado de los diferentes
985300	992620	valores que puede tomar, cuál es el valor de cada uno de ellos, ¿sí? Y esto cuál es
992620	999420	el rango, ¿qué valores probable tiene cada una variable aleatoria que refira palabras?
999420	1008540	El todo el vocabulario, todas las palabras diferentes que yo puedo tener. Entonces nosotros
1008540	1014100	vamos a poner estos notaciones probabilidades con ocimiento, de que la palabra sea
1014100	1027780	conocimiento. Vamos a denotar W1 a la N1N a la secuencia de palabras W1, W2, WN, por ejemplo
1027780	1034860	en una nación y vamos a decir que la vamos a hablar de la probabilidad de la secuencia
1034860	1039340	de palabras queriendo decir, bueno, la probabilidad de la que la primera sea W1, que la segunda
1039340	1047540	sea W2, etcétera. ¿De acuerdo? O sea que esta distribución de probabilidad tiene como
1047540	1057660	rango todas las secuencias posibles de palabras. O sea que si mi vocabulario es V, tengo N
1057660	1071620	y a la V, V a la N, V a la N. O sea que es enorme, especialmente, si todas las posibles
1071620	1082900	secuencias y vamos a recordar la chain rule o la regla de multiplicación de las probabilidades
1082900	1092060	que es, si yo tengo la probabilidad de una secuencia de palabras W1, WN, esto es la probabilidad
1092060	1098020	de la primera palabra, que de alguna forma la calculo, por la probabilidad de la segunda
1098020	1106120	da la primera, da que la primera, da que la primera fue W1, observen acá que no son
1106120	1111960	independientes, es decir, la palabra por definición acá, no son eventos independientes,
1111960	1117880	es decir, tengo una cierta probabilidad de que empiece con W1, la multiplicación por
1117880	1122800	la probabilidad de que la segunda sea W2, da que la primera fue W1, por la probabilidad
1122800	1130440	que la tercera sea W3, da que las dos primeras fueron uno de ahí así. ¿De acuerdo?
1130440	1138160	de esa forma con esta regla yo y al final WN la última da toda la santería, esto se
1138160	1146840	llama regla de la cadena, yo con la regla de la cadena puedo calcular la probabilidad
1146840	1154600	de una secuencia o de una oración, da la secuencia, si logro calcular estas probabilidades, o sea
1154600	1164040	si logro calcular predecir las palabras correctamente, voy a poder predecir la secuencia,
1164040	1171040	esa forma paso de la predicción al cálculo de toda la probabilidad de la oración. ¿Entienden?
1171040	1182040	Bien, entonces vamos a quedarnos con esa notación, entonces yo digo bueno, un ejemplo
1182040	1186720	¿no? Si yo quiero saber la probabilidad de viento fuerte, de componente sudeste como
1186720	1192480	el que está soplando, no sé si es componente sudeste, pero fuerte, es la probabilidad de
1192480	1197780	viento por la probabilidad de fuerte, dado viento por la probabilidad de dado viento fuerte
1197780	1211400	etcétera, ¿no? Nada menos que la regla de la cadena. Entonces yo quiero saber la última
1211400	1217040	P de sudeste, dado viento fuerte, de componente y vos con Google por ejemplo digo bueno,
1217040	1224880	fuerte, de componente aparece 9.230 veces, viento fuerte, componente sudeste aparece
1224880	1234040	347 veces, y yo entonces voy a estimar la probabilidad de esa por medio de conteos, entonces
1234040	1237840	la cantidad de veces que ha aparecido viento fuerte, componente sudeste, dividido la cantidad
1237840	1246560	de veces que aparece fuerte, componente, 347 veces dividido, no, 9.230. Aguardo, y esta
1246560	1251960	es la probabilidad de que la siguiente palabra sea sudeste, en mi estimación. Si ustedes
1251960	1260360	desfijan, esto es una probabilidad porque contando todas las palabras posibles que pueden
1260360	1267720	seguir acá, si yo logro determinar cuáles son, yo sé que van a ver 9.230, van a sumar
1267720	1274160	9.230, ¿no? En todo lo caso posible, mira todos los casos, junto a lo que son
1274160	1280680	la siguiente palabra, eso hace que como esto me va a dar 9.230, la suma de todas las
1280680	1286360	cuantidades, esto va a dar uno, entonces esto sí es una distribución de probabilidad,
1286360	1291480	entonces que estamos bien, efectivamente que yo des una probabilidad. Aguardo, esto es
1291480	1308280	lo que me dices, bueno, el 3,76% de las veces es sudeste, la siguiente palabra. Eso
1308280	1315840	que acabamos de hacer es estimar la probabilidad a partir de la frecuencia de ocurrencia en un
1315840	1322720	corpo grande, eso Google es un corpo grande, muy grande. Y eso se llama principio máximo,
1322720	1328360	pero similitud que lo vimos la de pasada, es, trato de hacer, calcular la probabilidad
1328360	1336120	en base a lo mejor posible a los datos que tengo, es decir, considero, yo estoy considerando
1336120	1341160	que los datos que tengo, es decir, el corpo de Google es una buena aproximación del mundo
1341160	1347640	real, del lenguaje en realidad, yo no sé si en realidad efectivamente cuando los seres
1347640	1357560	humanos hablamos, hay un 3,76% de probabilidad de que, después decir bien tofuerte componente,
1357560	1362440	viene sudó este, pero el corpo de Google es que es lo mejor que tengo como aproximación,
1362440	1367560	me dices eso, y eso es lo que yo utilizo, como un estimador de máxima de la similitud,
1367560	1372120	lo mejor que puedo acercarme con el corpo que tengo, eso es lo que vamos a hacer todo el
1372120	1381160	tiempo acá, calcular componentes de máxima de la similitud. Pero tenemos algún problema,
1381160	1387880	¿no? Y es, en el otro caso, dice, a raíz estos fenómenos se producirán tormentas fuertes,
1387880	1395800	la próegue fuertes, y a raíz estos fenómenos se producirán tormentas, tiene un problema,
1395800	1401880	ahí es que, nunca apareció en mi corpus, a raíz estos fenómenos se producirán tormentas,
1401880	1406520	y nunca apareció en mi corpus, a raíz estos fenómenos se producirán tormentas fuertes,
1406520	1415720	¿sí? Y eso nos da una horrible edición por cero, que queremos evitar, o sea que no
1415720	1424800	está probabilidad, ah, infinito, no sé, no está definida, esto, una pregunta, esto les parece
1424800	1431560	que es un fenómeno común o no, que nos puede pasar cuando estemos estimando, todo el tiempo,
1431560	1440520	porque por más grande que sea el corpus, el lenguaje es muy creativo, entonces tenemos que buscar
1440520	1446040	forma y además, porque estamos haciendo un conteo de palabras, de relación muy largas,
1446040	1452440	o sea que la rila de la cadena no resuelve en mi problema, porque yo, una aproximación bien
1452440	1457680	naïf para que el culo de la probabilidad de calcular toda la secuencia posible, ¿cuánta
1457680	1461320	vez se aparece la secuencia que quiero calcular en la elaboración del total de raciones,
1461320	1464920	lo cual es un disparate, pues no tengo corpus, evidentemente grande, pero esta aproximación
1464920	1471480	tampoco nos ayuda mucho, porque sigo teniendo contexto muy largo, porque si ustedes se fijan,
1471480	1477200	en la rila de la cadena, bueno, en lo que acabamos de hacer, la última probabilidad es casi
1477200	1491040	la misma que la primera, con menos una palabra, tengo que con una forma a chicar eso. Entonces,
1491040	1501560	una de las ideas fuerza para computar esta probabilidad es el lugar de tomar todas las palabras,
1501560	1512080	tomar sobre las últimas, es decir, yo me quedo con las últimas N menos un palabras, N menos
1512080	1521840	N, bueno, ¿sí? N, N, esto es en gran, ¿no? Y las otras no las considero, digo bueno,
1521840	1529320	la con, mi, mi, mi, mi humilde aproximación para que esto se pueda volver manejable, es decir,
1529320	1533080	bueno, yo en realidad solamente me importan las, solo las últimas palabras afectan en la que
1533080	1542720	voy a predecir, solo la última idea. Y de eso se tratan los modelos en grama, que utilizan
1542720	1546480	lo que se llama, eso que acabo de decir, yo llamo hipote sigue marco, hipote sigue marcoviana,
1546480	1558480	solamente las últimas palabras afectan la siguiente, hay un límite, ¿tá? Y fíjense que en la hipote
1558480	1565160	se divide grama, yo digo, cada palabra la aproximo por la anterior, simplemente, es decir, estoy
1565160	1571120	diciendo una cosa tan sencilla como la última palabra es la única, cada palabra condición
1571120	1576720	en la siguiente, pero en la anterior, ¿no? Es muy fuerte, ¿no? Y de trigramas son dos y con
1576720	1585000	N en grama son N, ¿no? Sí, con la hipote sigue divide grama, mi proviezo mucho más
1585000	1591720	sencilla que antes, porque es como, cada palabra, solo depende, vamos a mire, uno bueno, uno
1591720	1601040	no está más, pero cada palabra depende del anterior, simplemente me queda que la probabilidad
1601040	1612960	de una secuencia, es la probabilidad de la primera, por la probabilidad de la segunda
1612960	1634280	de la primera, por la probabilidad de la tercera de la segunda, etcétera, y aguardó, acá
1634280	1640520	nos falta este PW1 en esa fórmula, pero no nos preocupa demasiado porque eso lo resolvemos
1640520	1645040	poniendo una marca al comienzo de la secuencia que siempre vale uno su probabilidad, es decir
1645040	1651600	que todas las variaciones empiezan con una marca, y si no, multiplico acá, ¿no? Si no, si
1651600	1656960	lo quiere hacer de otra forma, agrega un PW, es su cero, acá y lo mismo, pero esencialmente
1656960	1661240	lo importante acá es que esto se transforma en una simple multiplicación de probabilidades
1661240	1675160	de una palabra a la anterior, y cómo hago para calcular esto, cómo puedo calcular esto acá,
1675160	1680920	cómo calcular la probabilidad de una palabra, da en anterior, contando, pero solamente
1680920	1688320	den cuenta a dos, lo cual lo vuelvo poniendo mucho más manejable, y eso es justo lo que vamos
1688320	1694280	a hacer, un modelo de lenguaje intenta predecir la próxima palabra de una oración a partir
1694280	1700920	de las n menos una anterior, y por supuesto que importa el orden en ese cálculo, ¿no?
1706520	1712600	También tenemos que plantearnos cuando hagamos los enegramos, cuando calculemos la probabilidad
1712600	1718160	en general, bueno, cosas que ya hemos conversado, ¿qué elemento vamos a contar? Sí, por
1718160	1725800	ejemplo, tengo un tema de tokenización, esta coma, la tengo que considerar un diagrama
1725800	1731080	o no la tengo que considerar un diagrama, ¿sí? La tengo que considerar un token o no la tengo
1731080	1736000	considerar un token, me interesa, bueno eso seguramente va a depender un poco de la aplicación
1736000	1742560	en la que les aplican a los que les utilizan, o tengo un cuerpo oral donde tengo
1742560	1748080	de fluencia, de fluencia, creo que ya me ha estado. ¿Qué tengo que hacer con las
1748080	1753240	mayúsculas? ¿Qué hago con la forma flexionada? Todo lo problema de la tokenización me
1753240	1758760	parece en el diagrama, es decir, esto son cascadas y amo, ¿no? Yo acabé a tener la tokenización
1758760	1762240	realizada, lo que ya no hay respuesto universal depende de la tarea que estamos haciendo, por
1762240	1770560	ejemplo, típicamente los cuerpos orales están todos pasados a mayúsculas, como son
1770560	1779920	más continuos, no hay la identificación de raciones, no es tan importante. Si yo voy
1779920	1785480	a hacer análisis, si estoy haciendo un análisis de cómo se usan los signos de puntuación
1785480	1791480	en mi lenguaje, obviamente la coma la tengo que identificar, sino que para que no me interese,
1791480	1798480	o me puede interesar, todo esto es mapearlos a una cosa sola que se llama signos de puntuación
1798480	1805440	y juntar los puntos con las coma. Bueno, tiene que hacer eso en el laboratorio, ya se van a
1805440	1809800	escolar. Bueno, nada, se necesita un pretetamiento, disponible al menos palabras, yo ni
1809800	1818200	el modelo, no hay modelos generales. También va a depender un poco, nuestros números van
1818200	1826160	a depender de la cantidad de palabras. El diccionario, el Oxford English Dictionary tiene
1826160	1832960	290.000 entradas, el trezor de la sangre francés tiene 54.000 y el diccionario de la radio
1832960	1844680	es 88.000. ¿Por qué les parece que tienen tantas más acacagadas? Porque el diccionario
1844680	1849480	no parece en la forma flexionada y el español está mucho más flexionado que el número.
1850480	1857360	O sea, el inmune se va a tener que arreglar más solito. Bueno, y después tenemos corpos,
1857360	1863320	esto ya hablamos un poco, y aquellos distinguyen entre el número de toques en que son la cantidad
1863320	1867600	de ocurrencias que hay en el texto y el número de palabras distintas, el vocabular.
1874080	1877760	Acá está la respuesta a la pregunta de que hacíamos antes, ¿cómo estimamos lo vigilan más?
1877760	1882160	Utilizando otra vez lo que se llama un estimador de máximo a ver el similitud, lo que se llama
1882160	1887600	métodos de frecuencias relativas, que es cuento, la cantidad de veces que apareció una
1887600	1901720	palabra con, por ejemplo, la probabilidad de fuerte, dado viento, se aproxima como la cantidad
1901720	1917280	de veces que aparece bien tofuerte, por la dividida de la cantidad de veces que apareció,
1917280	1934840	dividido todas las posibles continuaciones, ¿de acuerdo? Viento fuerte, viento calmo, viento,
1934840	1944120	viento diles, viento, no sé, lo que quieras. Y sumo todas las posibles, estoy haciendo normalizando
1944120	1949360	como hablamos al principio de como hablamos acá, estoy normalizando. Ahora, esto aquí es equivalente,
1950520	1952120	¿cómo puedo simplificar esto?
1958480	1968000	Si yo tengo todas las disica, parece viento fuerte, viento calmo, no sé, ¿qué es la suma de todo eso?
1974120	1979960	Y la cantidad es de la peseamiento, estoy igual a la cantidad de veces que aparece bien tof, en el corp.
1980720	1985360	¿Cómo guardó? ¿Cómo son todas las posibles ocurrencias?
1991040	1996080	Ahí tenemos la simplificación y además para tener en cuenta la primera y última palabra
1996080	2000760	de honoración, le vamos a agregar siempre los símbolos de comienzo y de fin, eso para asegurarnos
2001760	2008080	de que para no tener que calcularse parada la probabilidad de la primera palabra. Yo sé que la primera
2008080	2016360	palabra siempre es ese y calculo la probabilidad de la primera en el texto, digamos, ponerle él
2016360	2025800	dado que la anterior era ese, ¿de acuerdo? Y así lo dejo en una sola forma. Por ejemplo,
2025800	2034440	si supongamos que yo tengo ese corp, ¿no? Oan, abrió la puerta, el viento abrió la puerta,
2034440	2041200	el negro abrió limones en tus mejillas nuevas, Juan recoge limones. Y quiero saber la probabilidad
2041200	2047400	de estas oraciones. Evidentemente, no las tengo en el corp, ya que no es poco tan directamente,
2049800	2053480	pero quiero utilizar un modelo de diagramas para calcular.
2055800	2064840	Y con lo que sabemos es bastante sencillo. Primero que nada, decimos bueno, la probabilidad de
2064840	2076440	Juan abrió limones es probabilidad de Juan dado el comienzo, probabilidad de abrió dado Juan,
2076440	2085440	probabilidad de limones de abrió, etcétera, ¿no? Fíjense que la probabilidad Juan dado el comienzo
2085440	2089800	de la cantidad de veces que apareció Juan en la marca del comienzo, dividido en la cantidad de
2089800	2092840	marca del comienzo que es uno. Entonces, he tomado...
2097240	2100240	2 de 4. Ah, ¿por qué hay cuatro oraciones?
2100240	2104960	Claro, claro, porque yo estoy haciendo contegos directamente, no estoy haciendo probabilidad.
2104960	2115400	2 de 4 veces arrancó con Juan, ¿sí? Juan abrió es una de 2, ya había parecido
2115400	2123560	Juan abrió en el corpus y Juan apareció 2 veces. O sea, de 2 veces la pareció Juan en la siguiente
2123560	2129400	apareció una vez abrió. Y así sigo multiplicando y como ve, multiplica la fracción y me da, bueno,
2130400	2133400	0,042, esa es la probabilidad de Juan abrió limones.
2137400	2144040	Enero abrió la puerta, 0,17, también tiene mucho sentido, ¿no? A ver,
2144040	2148640	justamente el hecho de que sigo un ejemplo de jubete le hace perder la gracia todo esto,
2148640	2157440	porque esto funciona porque tengo grandes volúmenes, sino no es una paba. Y acá que nos pasó,
2157440	2159840	¿qué puede haber pasado acá?
2175040	2190280	La palabra come nunca está. Y en la puerta, en la puerta está. La primera se explica
2190280	2215240	porque come nunca está. Creo que está así, perdón, la si, la puerta, ¿por qué es
2215240	2224600	la 0? Porque lo que no está es en la, en la, no aparece nunca, si ustedes miren acá la probabilidad
2224600	2229840	de, perdón, la cantidad de, la probabilidad de esto es la probabilidad de que empiece con él,
2229840	2232880	ya tenemos un problema con el comienzo con él, porque creo que no hay ninguna.
2238080	2244120	Ningún empieza con él, y tú ya tienes un problema y además en la tampoco está, o sea que el
2244120	2253240	conteo me da 0, si el vigrama no aparece en el cuerpo de entrenamiento, siempre mi problema
2253240	2261080	me da 0, y más interesante aún, si cualquier vigrama de todos los que aparecen en la oración,
2261080	2271680	da 0, la probabilidad de la oración es 0, eso es un gran problema. Resolver el problema de eso
2271680	2275720	y lo que se llama el suavizado de negra más que vamos a ver cómo, tenemos que ir una forma de
2275720	2281560	resolver eso que nos va a pasar siempre, es decir, como nuestro cuerpo, nunca puede ser tan,
2281560	2286440	aunque solo sean dos palabras, igual puede aparecer mi pareja de palabras que no aparecieron y yo
2286440	2289200	no me puedo transcar con eso, ¿de acuerdo?
2289200	2301320	Bueno, nos queda ese pendiente del cielo que lo vamos a ver después porque ya te quiero comentar
2301320	2304360	y con una cosa, pero vamos a acordarnos de eso, y tú y tenéis un buen problema pendiente.
2308840	2315440	Bien, en general ustedes eran, bueno, pero ¿cuál es el mejor ene? ¿No? ¿Por qué? ¿Cuál es el tema? Es
2316240	2328320	cuánto, cuánto, más largo sea el tirama que yo utilizo, más información tengo de contexto,
2328320	2333360	es decir, intuitivamente mejor estimar con 5 palabras que con una.
2337360	2340520	Vamos a guardar con eso. ¿Cuál es el problema de los 3 más largos?
2340520	2346720	¿Por qué no puedo usar el 15?
2352080	2357000	Porque tenemos mi problema, porque llegamos acá, que con 15 no tengo corpos fíjendemente grande
2357000	2366040	como para que aparecan esa ocurrencia. Entonces, ese balance entre cantidad de ocurrencia,
2366040	2369720	porque yo no tengo una buena estimación de la cantidad de ocurrencia, no voy a poder estimar
2369720	2373560	bien la probabilidad. Con eso bien que yo estoy atimada la probabilidad de partir en contegos,
2373560	2379960	si yo tengo una, dos, tres ocurrencias seguramente esa probabilidad artificial, pues si hubo
2379960	2384040	una ocurrencia en un corpo de miles de millones de palabras, no me está diciendo mucho.
2387040	2396000	Realmente en igual 3 se obtienen buenos resultados, por lo menos para aproximarse de
2396000	2403480	la cantidad de cada muy bien, Google hace unos años atrás sacó un corpo de negra, un sí,
2403480	2409480	una lista de negra más de hasta 5, no recordo que no está ahí porque venían en 7.
2413080	2416360	O sea que determinaré, ¿ne va a depender un poco la tarea y ese se me dio a ojos?
2416360	2420560	Digamos, pues yo me estaría un poco bóblica. Ahora vamos a ver un poco de evaluación,
2421560	2426080	y tal y lo que decíamos, ¿no se agregan? Cuando son 3g más tengo que agregar 2 símbolos,
2426080	2438200	el comienzo de la oración. Te voy a poner enero abrió, porque yo necesito 2 de contexto para
2438200	2446320	calcular el triunfo en detalle. ¿Cuándo? Ahí no te caas, así que no.
2451360	2453880	Y bueno, y la pregunta es ¿cómo calculamos?
2457040	2460960	De ver punto de vista metodológico, ¿cómo hacemos para calcular buenas probabilidades?
2460960	2466840	Ya vimos cómo se hace el conteo. Ahora quiero ver cómo organizo el corpo, y me parece
2466840	2471240	que es interesante ver esto porque nos va a pasar en muchas cosas, en este tema,
2471240	2476880	el procedimiento de lengua natural, y que muchas veces induce el mal uso metodológico de estas
2476880	2488640	cosas, lleva error. Entonces me parece que va de la pena comentarlo esto. Yo dije que
2488640	2493440	iba a ser conteo para calcular las probabilidades, ¿no? Entonces yo por acá tengo un corpus,
2493440	2506080	un corpus de texto, ¿si? Entonces, sencillamente lo que tengo son muchos textos, ¿no?
2506080	2510920	Obviamente, sencillamente no, tengo muchos textos, esa es la definición de corno.
2510920	2526280	Y yo voy a crear un modelo de un modelo de un lenguaje, es decir, yo lo que quiero
2526280	2532840	construir con esto de las probabilidades de las eleaciones es un modelo del idioma pañol.
2532840	2536000	Yo tengo un corpus de texto en español, y quiero hacer un modelo del idioma pañol.
2536000	2542200	Supongo que yo entreno un modelo, entrenar el modelo en este caso que es decir calcular
2542200	2551000	todas esas probabilidades. ¿Cómo hago para saber qué tan bueno es? ¿Sí? ¿Cómo lo
2551000	2558160	evaluó? Supongo que yo ahora voy a modular el cual es la medida, pero supongo que yo tengo
2558160	2566280	una medida de performance que me dice bueno, aplicale tu modelo a este texto, sí, supongamos
2566280	2569440	que la medida es el que le asigne, ahora vamos a ver por qué, pero el que le asigne
2569440	2577560	mayor probabilidad a todo el texto a las oraciones del texto es el mejor, el mejor modelo
2577560	2586960	es que la asigna probabilidad mayor a la oración en que tengo el texto. Si yo aplico mi
2586960	2592600	método, mi modelo, o sea, el lugo, mi modelo, sobre este mismo corpus, ¿qué problema tengo?
2594360	2600560	Que me va a dar barro, porque los calculé ahí, es decir, yo nunca puedo, nunca, pero nunca
2600560	2606360	nunca, he valuado un modelo en el mismo corpus en el que entrenes. Esto aplica siempre, cada vez
2606360	2610960	que es un difícil métodotadístico, pensado automático, lo más importante es saber en la
2610960	2617440	pensado automático, nunca, el lugo es tu modelo en un corpus, en el mismo corpus que entrenaste,
2617440	2623600	porque por definición estás haciendo trampa, eso lo llama sobre ajustes, sobre ajustas
2623600	2631840	a tu corpus de entrenamiento. Entonces yo lo que voy a hacer es dividir mi corpus en
2631840	2641960	dos, y voy a decir, este es el corpus de entrenamiento, voy a poner en inglés y el corpus
2641960	2658320	de evaluación. Entonces lo que yo voy a hacer es entrenar y cuánto se paró acá. Bueno,
2661920	2664640	la regla más o menos es 80 20.
2672040	2676800	Pregunto, ¿por qué me interesaría que esto fuera lo más grande posible?
2683600	2690840	Para que tener más información, ¿y por qué no uso 90 10 o 95 o 97 3?
2692840	2694840	¿Cómo?
2697840	2702520	Tengo que solucionar ese balance, no entretener una cantidad razonable de datos, porque si yo le
2702520	2709880	valú sobre una oración, la variance es muy grande, es decir, la posibilidad de equivocarme
2709880	2716800	es muy grande. Entonces, una regla es más o menos 80 20, ¿sí?
2722840	2729840	Y bueno, ahí habla de 90 10, yo tengo la regla de 80 20.
2732840	2741560	Va a solucir un problema adicional acá y es que ahora lo voy a ver es, por ejemplo,
2741560	2755280	si yo quiero saber cuántos elegir el N, ¿no? Yo quiero elegir el N, yo necesito lo que
2755280	2768360	va a hacer es prevo con un N acá, modelo 1, en igual 2 y aún modelo 2, en igual 3.
2772560	2780720	Y esto es un poco más útil, y lo valúa acá y digo M1 y M2, y me quedó con el
2780720	2787120	que me da mejor. Y esos métodologicamente no están bien, ¿por qué?
2790760	2796760	Y esto es una de las cosas que es más difícil entender a veces, es, si yo prevo los dos
2796760	2800920	modelos acá, de alguna forma también estoy haciendo trampa, porque supongan que yo tengo
2800920	2806840	no dos parámetros, porque acá tengo o un parámetro que tiene dos valores. Supongamos
2806840	2812680	que yo quiero ajustar otro parámetro de mi método, que puede tomar 500 valores posible.
2812680	2823760	Si yo hago 500 en realidad, y 500 pruebas, sí, muy probablemente también estoy ajustando
2823760	2828600	acá, estoy ajustando acá, porque estoy elegiendo de los 500 y a veces puede ser miles o
2828600	2833680	300 de miles, el que mejora anda en este corpo de evaluación, o sea que estoy
2833680	2839240	sobre ajustando el corpo de evaluación. Entonces, para la ajuste de parámetro yo
2839240	2849800	usualmente lo que tengo que hacer es definir dividir este corpus, sacar un pedacito
2849800	2862320	del corpo en trainamiento, que lo llamo corpus, gel dauto, corpus de desarrollo, y lo que
2862320	2870040	hago es entrenos sobre esta parte y evaluos sobre el gel dauto, y me reservo este de evaluación,
2870040	2874720	solamente para cuando tengo mi modelo definitivo, y quiero saber su performance, con su
2874720	2884960	medio de evaluación. ¿Aguardo? Esto lo van a tener que presentar en el laboratorio,
2884960	2892480	es decir, cómo evaluarían el método, un método. Hay otras posibilidades que no implican
2892480	2898720	un cuerpo gel dauto, por ejemplo, hacer lo que se llama coros validation, que es separo
2898720	2907280	este pedacito, entrenos sobre esto y evaluos sobre este, después separo otra franjita y entrenos
2907280	2914120	sobre el resto y evaluos sobre la franjita, y así con cas franjas y saco el promedio. Eso
2914120	2918320	me sirve para no desperdiciar, digamos, esta parte del corpus, para poder utilizar todo
2918320	2929720	el corpus entrenadito. Esa más, cros validation. Vamos a volver a hablar un poquito
2929720	2934200	cros validation cuando le hemos clasificación, pero lo que me interesa es que le quede claro
2934200	2943880	la diferencia entre estos corpus, y cuando tengo el modelo final, uso esto solamente para
2943880	2950440	evaluar la performance, es una medida que determinaré según mi tarea. ¿Cómo evaluamos
2950440	2955760	un modelo bueno? La manera correcta de evaluar un modelo debería sería empíricamente,
2955760	2960200	es decir, yo quiero evaluar un modelo del lenguaje y lo estoy usando para el reconocimiento
2960200	2966240	de la habla, debería ser una evaluación de que también reconozco el habla, o que también
2966240	2970520	reconozco la escritura, pero eso puede ser muy costoso a veces. Yo puedo estar haciendo un
2970520	2974800	modelo lenguaje, no sé para qué se va a usar. Entonces, me interesa mucho, me puede
2974800	2986560	interesar tener una medida intrínseca de la performance de mi modelo. Entonces, vamos
2986560	2993040	a ver una forma de evaluar. A mi esta parte, de esta parte, en el libro está apuesta como
2993040	3004280	un tema avanzado, pero a mí me parece interesante mostrarlo, porque la entropía es un concepto
3004280	3007360	que aparece muchas veces en el procedimiento de lenguaje natural de otras cosas, y me
3007360	3012960	pese que le va a ir la pena por lo menos aproximarse. Supongo que yo tengo una variabilidad
3012960	3018320	aleatoria y todo esto voy a llegar a una forma de evaluar un modelo, no hay que empezar
3018320	3025400	a hablar de todo esto. Supongo que sí que yo tengo una variabilidad aleatoria que tiene
3025400	3034080	varios eventos posibles, en otro caso dijimos que eran las palabras posibles. La entropía,
3034080	3039640	la entropía es una variabilidad aleatoria que es un concepto que viene de la teoría
3039640	3054600	de información, de CloudXanon, la teleinformación lo que hablaba era, bueno, algunos capacicieron,
3054600	3060040	lo vieron a algún curso, pero la teleinformación lo que trataba era de medir cuánto me cuesta
3060040	3063400	a mí transmitir un mensaje. ¿Cómo puedo transmitir un mensaje de forma óptima? Digamos
3063400	3072520	un poco la idea, o que hay atrás de una comunicación. La noción de entropía, estas
3072520	3078560	funciones, tengo el evento que quiero hacer, la probabilidad del evento, por el hogarismo
3078560	3084400	de esa probabilidad. La entropía tiene como característica fundamental que es una medida
3084400	3092400	que si hay un evento que tiene toda la masa de probabilidad, la entropía es mínima, es
3092400	3097920	decir, si yo tengo un dado que está tan carregado y una forma en algo que valentemente
3097920	3103080	se puede decir que la entropía a mí es mirado de incertidumbre sobre un evento. Si yo
3103080	3107320	tengo un dado que está tan carregado, que cabe que lo tiro, sé que siempre vas a salir
3107320	3118480	seis, no tengo incertidumbre, mi entropía es cero. En cambio, si el dado está perfectamente
3118480	3129480	calibrado, equilibrado, mi entropía es máxima. Es decir, ¿cómo está definida la entropía?
3129480	3139800	No puedo tener etropía más alta que cuando los eventos están equipos lo hablan. Entonces
3139800	3144760	justamente la entropía es generalmente lo que uno mide con la entropía de eso, ¿qué
3144760	3150680	están parecidos? Son los resultados que están balanceados, están de alguna forma. Cuanto
3150680	3153840	más incertidumbre tengo, porque están más balanceados. Si yo no tengo ni la menor
3153840	3168360	idea de la palabra que sigue, mi entropía es máxima. Y además tiene otra característica
3168360	3177000	que es que si lo haríamos es en base dos. Este número, la entropía me mide la cantidad
3177000	3188240	de bits que yo necesito mínimos para transmitir los eventos. Esto es lo mejor forma de
3188240	3194280	hacerlo con un ejemplo. Supongamos, y es el ejemplo que aparece en el libro. Supongamos
3194280	3201000	que yo tengo ocho caballos. Tengo ocho caballos que quiero transmitirlas las apuestas
3201000	3205160	que se están haciendo por un cable. Entonces digo, bueno, una forma cantada de transmitir
3205160	3223560	lo directa de transmitir, llamar al primer caballo 0-1, 0-10, 0-11, 101, 110, 111.
3223560	3234320	De acuerdo, acá yo uso ocho bits. Cada vez que se apuesta por el caballo 1, yo poco
3234320	3240800	0-0, 0-1, blabla. Entonces en total yo utilizo tres bits para transmitirlas por un cable,
3240800	3247440	tres bits por cada apuesta, ¿no? Ahora, cuando nosotros vemos las apuestas, descubrimos
3247440	3257920	que la mitad de las veces se apuesta por el caballo 1. Un cuarto del caballo 2, un tercio blabla,
3257920	3263160	un octavo del caballo 3, un disiseo del caballo 4, y todos estos se apuesta mucho menos.
3263160	3268640	Teniendo en cuenta eso, yo lo que trato de hacer ahora es decir, bueno, quiero proponer
3268640	3277480	una codificación mejor que hace que yo, los caballos que se apuesta más, o sea que
3277480	3284880	tengo que transmitir más seguido, los codificos con menos bits. De acuerdo, la mitad
3284880	3290440	de los bits, el primer bit, lo utilizo solo para el caballo 1, es decir, que si es un
3290440	3304240	0 es que transmitir el caballo 1, necesita un solo bit. Si es un 1, si es un 1 y un 0 después
3304240	3311920	es el caballo 2. Si son 2, 1 y un 0 después es el caballo 3. Si son 3, 1 y un 0, fíjense
3311920	3321640	que yo para transmitir esto caballo utilizo 1, 2, 3, 4, 5, 6 bits. Utilizo más bits,
3321640	3329160	pero como son mucho menos probable, mi entropía me da 2 bits, o sea, el promedio de bits
3329160	3338000	que yo utilizo según la distribución es 2 bits, que es más baja que los 3 bits originales.
3338560	3346720	¿Centiende? Incorporando la información de la distribución bajo. Podemos mejorar eso, no
3346720	3350720	podemos mejorar eso. Nunca vamos a hacer el etropía, lo que lo dice es eso, nunca vas a encontrar
3350720	3356320	una, porque justamente la etropía 2, como la etropía 2, la etropía me da una cota inferior
3356320	3365680	sobre cuánto puedo llegar, con menos de 2 bits no puedo. ¿También te acuerdo? Te dice
3365680	3374320	preguntarán para qué sirve esto. De hecho no, la etropía es una cota, lo que decía, una
3374320	3380560	cota mínima para el número de bits necesaria. A partir de la etropía yo puedo calcular la
3380560	3392680	etropía de una secuencia, la etropía de una secuencia es de todas las combinaciones
3392680	3399240	posibles de una secuencia, la probabilidad de esa combinación es lo mismo para aplicar
3399240	3403280	la secuencia, entonces si lo ven es un número muy complicado, porque es la sumatoria de una
3403280	3407760	cantidad impresionante de número, porque son todas las combinaciones posibles de secuencia.
3407760	3418560	Eso es lo que me mide la etropía de la secuencia, ¿qué tanta incertidumbre hay en una secuencia?
3427880	3435240	Y la tasa de etropía sería eso debido a N, es decir el promedio,
3435240	3442840	porque si no la secuencia malarga o no tiene entropía más alto, el promedio por palabra
3442840	3459960	de la etropía. Entonces la etropía de un lenguaje, que sería como la medida de qué tanta
3459960	3471520	incertidumbre hay en un lenguaje, ¿qué tan, digamos, qué tanto pollo llegar a predecir
3471520	3477080	lo que va a seguir diciendo el lenguaje? Esa límite, pero como valoso, no en un contexto
3477080	3482200	general en el lenguaje, es una medida para el lenguaje. Esa límite cuando la secuencia
3482200	3488040	tiene infinito de la tasa de etropía, ¿sí?
3498040	3502440	Y que es que acá es la suma, como decíamos, es la suma de todas las secuencias posibles,
3502440	3507920	o sea que es una cosa imposible calcular, pero hay un teorema que es el de llano,
3507920	3513600	como a mi la embraiman que dice que es el lenguaje, él es estacionario y ergólico. Estacionario
3513600	3519480	y ergólico quiere decir que no importa dónde yo esté parado en una secuencia, todas las
3519480	3526560	posiciones van en las probabilidades o en las mismas de la limidad, lo cual no es así en
3526560	3530400	un lenguaje, porque lo que yo digo ahora y sí dentro de lo que estoy diciendo entre un
3530400	3536680	minuto más, no, no hay aleatorio de lo más, pero suponiendo eso es una simplificación,
3536680	3544280	lo que me permite es simplemente para calcular la entropía, la tasa de entropía, el lenguaje
3544280	3549560	es simplemente unos sobre nes divididos logarimos, fíjense que perdí la probabilidad de cada
3549560	3555440	una de las secuencias, es como que si yo tomo una secuencia suficientemente larga del lenguaje,
3555440	3562360	voy a incluir a todas las secuencias, o sea que si yo una secuencia suficientemente larga
3562360	3567160	puede ser el corpo de evaluación, yo puedo calcular la entropía sobre el corpo de evaluación,
3578120	3584840	y entonces, esto es un número, ahora lo que dije acá es un número, no sabemos por qué tengo
3584840	3591600	esto, ¿no? Pero fíjense que si yo puedo calcular lo que se llama la entropía cruzada,
3591680	3598200	porque yo que tengo, yo tengo un lenguaje que genera las palabras con una cierta distribución
3598200	3604000	de probabilidad, que es lo que queremos averiguar, que es lo que es lo que es lo que es nuestra
3604000	3609280	problema original, es como da las palabras anteriores y se genera la siguiente, eso es algo que
3609280	3614000	he desconocido, no sabemos como es, porque es el del lenguaje español, que yo quiero calcular,
3614000	3621000	pero yo tengo un modelo M, que es el modelo de negramas, está, la entropía cruzada, lo que
3621000	3631560	dice, bueno, calculamos esta hache utilizando la probabilidad original por el lovarismo
3631560	3638360	del, de la probabilidad sin nada por el modelo, la probabilidad de la secuencia es la que tenía
3638360	3644160	los movilidades, no la conozco, y la probabilidad, y en lovarimos sí, o sea, esa distancia
3644160	3651720	es a largo embítesis del modelo, seguramente tenemos otra vez, ya lo manmelan, yo puedo sacar esta
3651720	3656560	probabilidad simplificando la suponiendo que es el gode y que lo la, y digo bueno, la entropía cruzada
3658800	3668360	es, depende sólo lovarismo de, de la probabilidad sin nada por lenguaje, por el modelo, y esto es
3668360	3677680	interesante, cualquier, cualquier entropía cruzada que yo tenga, que yo calcule con un modelo,
3677680	3687960	va a ser mayor necesariamente que la entropía es del lenguaje, cualquier modelo va a
3687960	3693200	ensinarme una entropía mayor a la del lenguaje, entonces la, la, la, la, la cota inferior,
3709200	3712280	entonces fíjense que como son todas mayores,
3712280	3719720	cuanto más parecido sea mi modelo, al modelo, al modelo de lenguaje, al, al, al, cuanto más
3719720	3724240	modelo, más parecido, así que mi probabilidad es más parecida de las de acá, por cómo está definido,
3725360	3734440	va a ser mejor, de acuerdo, entonces, cuanto menor sea la entropía cruzada de mi modelo,
3734440	3739040	evaluado sobre una secuencia suficientemente larga, decir sobre el corpo de evaluación,
3740000	3746800	mejor va a ser mi aproximación, y justamente la medida de esa intrínsega que está buscando era
3751640	3762280	es esto, que es dos, porque dos no lo sé, porque lo mismo, es dos, es para sacarlo lovarimos nada más,
3762840	3772520	es dos a la entropía cruzada a este valor, y esto se llama perplejida, la perplejida es lo que
3772520	3786840	mide el, el, lo que mide que tan bueno es interisidamente mi modelo sobre, sobre mi cuerpo de
3786840	3792040	entrenamiento, sobre mi cuerpo de evaluación, es decir, si yo tengo dos modelos, el que así me
3792040	3799440	mayor probabilidad, menor propiedad, mayor probabilidad, al corpón de evaluación es mejor desde
3799440	3803960	ese punto de vista, lo consideramos mejor, porque porque tiene menos dudas de cómo se comporta,
3803960	3813400	porque la perplejida es, es como la incertidumbre que yo tengo ante, dada una palabra, cuando
3813560	3819160	sume para una palabra, cuál es mi incertidumbre, mi branching factor, en cuántas se puede abrir la
3819160	3824920	siguiente palabra en promedio, un poco eso es lo que captura la perplejida, mi lenguaje va a tener un
3824920	3830480	branching factor, es decir, no es que es cero, pero mi modelo siempre va a calcular algo mayor
3830480	3836360	igual a ese branching factor, cuanto más bajo, es que si yo me estoy acercando mal a la perplejida
3836360	3842280	posta, por eso la perplejida es la medida de que también hace la cosa, acuerdo,
3848120	3858520	bueno, no, eso es su cuenta, por ejemplo, si nosotros entrenamos un ígrama, más ígrama,
3858520	3862760	más ígrama, en un cuerpo de artículo de Wall Street Journal, de 38 millones de palabras,
3862760	3870920	probaron el cuerpo sobre un modelo, ni un cuerpo de prueba de 1,5 millones de palabras,
3870920	3878360	y calcularon la perplejida, y fíjense que la perplejida con los unigramos desde 962,
3880520	3885840	no sabemos cuál es el mínimo esto, no sabemos cuánto puede bajar, pero sabemos que con
3885840	3891520	vígrama llegó a 170 y contrígrama a 109, es decir, si yo tengo dos palabras antes, puedo
3891520	3897280	predecir con mejor, porque acá es con un ígrama, es la probabilidad de la palabra, no dice mucho,
3897280	3903760	si yo tengo el anterior, rápidamente baja, y si se fija cuando abre un tercero baja, pero no tanto,
3903760	3914560	ni de cerca tanto, no, bueno, lo último que nos queda hablar,
3916800	3924400	no dice, no pasó con las probabilidades nudes, se acuerdan que nos quedaban las probabilidades nudes
3924400	3929360	cuando no había contigo, bueno, uno de los problemas es las palabras que no existen,
3930880	3934960	las palabras que no existen, lo único que podemos hacer, o lo que típicamente se hace es
3936160	3941840	crear un vocabulario fijo y sustituyo las palabras desconocidas por un especial,
3941840	3946640	esto es típicamente lo que se hace, es decir, todas las palabras desconocidas las considero una
3946640	3953520	sola palabra que no se equivale, y cuando aparecen enigradas más que no ocurren,
3953520	3959440	tiene el caso de comer, que no aparecidas, pero puede ser que la enigrama no ocurra lo que
3959440	3973760	voy a hacer, son técnicas de suavizado, yo tengo, se acuerdan, tengo el contador de,
3976320	3982160	por ejemplo, acá es un migra a mano, contador de la palabra, de cantidad de veces la palabra
3982160	3993280	dividido el total de token que hay, y así calculo las probabilidades, la técnica de la plaza,
3995040	3999280	lo que dice es bueno, le agrego uno a cada contador, o sea que nunca me va a dar cero,
3999280	4004640	lo hago a los bestia, digamos, no, para que no me decero le sumo uno, y le sumo ve y se acuerdan
4004640	4008120	el nuevo poquito de una clase pasada, le sumo ve para que esto me siga dando una distribución de
4008120	4024520	probabilidad, esto es simplemente lo que hace es calcular un contador ajustado,
4026280	4034200	me explica por té y divide por temas, si me explica por el juvenil y divide por esto, por el PWI,
4038600	4048040	por ejemplo, si yo digo, si este es mi corpo entrenamiento, esta es la historia de un hombre y la
4048040	4061880	ciudad que creó, fíjense que mi conteo da uno, la habla y quiso me da cero, perdón, este es el
4061880	4068240	conteo, ahí va, conteo de este es uno, de la es dos y de quiso es cero, la probabilidad de este es
4068240	4076640	uno y divide 13, total de palabras, una es esta y es 0 0 8, la es 2 divide 13 y quiso me da cero en la
4076640	4082960	probabilidad que nos queremos que nos da cero, si nosotros aplicamos la plaza, lo que me da es
4082960	4090160	sumo 25, son 12 palabras en el vocabulario, porque la unidad está repetida es la
4091960	4102880	sí, o sea que tengo 12 en el vocabulario no 13, 13 es T y 12 es B, entonces ya hago 2 divide 25 y así me da
4102880	4110240	las nuevas probabilidades y acá quiso dejar de ser cero, el contador ajustado de lo que nos permite
4110240	4115760	es comparar lo que teníamos antes con lo que teníamos ahora, por ejemplo, esta valía 1 y
4115760	4135320	baja a 0 96, perdón, la valía 2 y baja a 1 44 y quiso va a de 0 a 0 48, si se fijan acá el
4135320	4141400	descuento, lo que se llama descuento que es la división entre los dos valores, lo permite ver
4141400	4152960	que le estoy sacando más masa de probabilidad a la que hay que quedar casi igual, es decir, la
4152960	4157880	meta le la tiene a la plaza el problema, por qué es lo que está pasando acá, esto es lo que me
4157880	4165960	muestra es que yo le tengo que sacar masa de probabilidad a los que aparecen, porque todo me
4165960	4170600	tiene que sumar 1, toda la probabilidad me tiene que sumar 1, si yo ya agregar 5 gramas que antes
4170600	4177120	estaban en cero, tengo que sacarle probabilidad a los que está, pues no me es un mamá que 1, entonces
4177120	4184600	esto es lo que tiene que castiga mucho a los más frecuentes, le sacan mucho probabilidad a los
4184600	4190880	más frecuentes y como que premia demasiado a los que no aparecen, hay otras técnicas no, no,
4190880	4197320	vamos a entrar en eso, que tratan de ajustarlo un poco mejor, pero ahora vamos a mover alguna
4201960	4213080	muy demasiada probabilidad, otra posibilidad es usar un delta en lugar de 1 y ese delta
4213080	4218040	te va a calcularlo, se acuerdan lo que hablamos del cuerpo, siempre que yo tengo esos parámetros
4218040	4230800	para calcular los calculos sobre el cuerpo de desarrollo, finalmente hay otro, esa es una
4230800	4236560	aproximación, es decir, con técnicas sobre el contencio, hay otra posibilidad que son un poco
4236560	4245200	más evolucios avanzadas, digamos que es, cuando yo quiero estimar, por ejemplo en técnicas de
4245200	4255440	trigrama, una palabra, a partir de las dos anteriores y no existen casos de las dos anteriores
4255440	4267960	en el texto, de las dos anteriores seguida doble, ¿no? Acá es doble, perdón, lo que hago es hacer lo
4267960	4273560	que se llama BACOF, hacia calcularlo a través de la probabilidad de la anterior, si no tengo la
4273560	4284080	anterior prueba con la anterior, eso llamas BACOF, el BACOF, tenés que resolver también que ahora
4284080	4289400	otra vez está introduciendo en nuevas, luego caso que no tenías antes, estas probabilidades
4289400	4297920	que calcularle y darle masa de probabilidad, otra vez tengo que mover probabilidad, cuando los
4297920	4306440	corpos son muy muy grandes, una forma alternativa y es un método muy nuevo, se llama Stupid BACOF,
4306440	4313560	que es como mi corpos muy grande, típicamente el corpos de Google, es no normalizo nada de las
4313560	4319280	probabilidades, este conteo, no más como me fue, si una no me da prueba con la anterior, si igual
4319280	4330120	tengo un montón de edad, o también se puede hacer interpolación, es decir, la probabilidad
4330120	4340400	de una palabra daba las dos anteriores, es la probabilidad de la palabra, la probabilidad
4340400	4345240	nueva, es la probabilidad original de la palabra daba las dos anteriores por un cierto
4345280	4351280	lambda, un cierto lambda 2 por la probabilidad de la palabra daba el sol en el vigrama, más la
4351280	4357640	probabilidad de un vigrama, y convino las tres a la vez, es como convino las tres tínias a la
4357640	4365720	vez, es decir, le doy un cierto peso a las probabilidades que yo quiero, de esta forma,
4365720	4370560	porque acá podría ser que existiera el vigrama anterior, pero existiera una vez sola, entonces
4370560	4376040	yo no le tengo mucha confianza a esa, puede sucederme y no le tengo mucha confianza, entonces
4376040	4379800	le doy un cierto peso a este también, y capa que le doy un peso un poquito más alto a este,
4379800	4384600	o sea, si este existe, está todo bien, pero este es siempre una ayuda, y de esa forma
4384600	4394080	balanceo, como calculo esto es lambda y con el corpos de valo, tengo que, de alguna forma
4394080	4403760	calcularlo sobre el cuerpo de desarrollo, o el cuerpo gelado, también hay interpolación
4405760	4411360	condicionada por el contexto, o sea, hay un lambda, acá ya lo que pasa es un poco más raro,
4411360	4416640	y un poco más moderno, digamos que es que más de estas épocas, digamos, donde a mí ya no me
4416640	4421520	preocupa tanto tener muchos parámetros, acá estoy definiendo un parámetro para cada combinación de
4421520	4437320	palabras, y hasta aquí llegamos hoy, esto es este capítulo que tengo acá, capítulo 4 del libro
4437320	4446000	Yurazki, tiene algunas cositas más, presencialmente es eso, y es lo que vamos a hablar de en este curso
4446000	4450760	de Nigrama, la clase que viene, presentamos la baratocha.
