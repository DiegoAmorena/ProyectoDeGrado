{"text": " Una vez que eleg\u00ed en mi, con el paso 1, eleg\u00ed cu\u00e1ntas palabras en espa\u00f1ol y bolsar en el paso 2, es lo que voy a elegir es una lineaci\u00f3n, una funci\u00f3n de lineaci\u00f3n que me dice cada palabra, con cual se va a corresponder, cada palabra, el lado de espa\u00f1ol, con que palabra en ingl\u00e9s se va a corresponder. Este modelo ha sumed de manera muy na\u00efve que todas las salinaciones que yo puedo tener son equiprobables, o sea, ha sumed que yo voy a tener un conjunto de lineaciones posibles y todas van a tener la vina de probabilidad. Bien, entonces, la probabilidad de elegir una lineaci\u00f3n en particular, si yo tengo un mont\u00f3n de lineaciones, digamos, la probabilidad de elegir una, una lineaci\u00f3n en particular, va a ser uno sobre la cantidad de lineaciones que tengo, porque en realidad todas van a ser equiprobables. Bien, entonces, cu\u00e1ntas lineaciones puedo tener entre dos oraciones, una oraci\u00f3n en ingl\u00e9s que tiene largo y una oraci\u00f3n espa\u00f1ola que tiene largo jota, como puedo calcular cu\u00e1ntas a lineaciones existen. M\u00e1s o menos, casi de la jota. Recuerden que el lado de ingl\u00e9s, yo pod\u00eda, yo ten\u00eda ciertas palabras en ingl\u00e9s ten\u00eda la palabra, en ingl\u00e9s era ah\u00ed, la palabra 1, 2 hasta, sui y en espa\u00f1ol ten\u00eda las palabras f1, f2 hasta, f subjota. Entonces, yo pod\u00eda atrazar l\u00edneas para alinear, pero adem\u00e1s en ingl\u00e9s, yo siempre considerado que ten\u00eda un token null. Entonces, todas las palabras que no estaban alineadas del lado del espa\u00f1ol y van a parar ah\u00ed. As\u00ed que en ingl\u00e9s en realidad no tengo y posibilidades, tengo una m\u00e1s, tengo y m\u00e1s uno. Entonces, cu\u00e1ntas formas tengo yo de mapear estas jota posibilidades en espa\u00f1ol con las y en ingl\u00e9s. Es alto, y m\u00e1s una la jota, porque yo tengo y m\u00e1s una opci\u00f3n para la primera y m\u00e1s una opci\u00f3n para la segunda, etc\u00e9tera, que yo al final. As\u00ed que son y m\u00e1s uno a las jota alineaciones, posibles. \u00bfNo voy a tener un cliente medio de la red? \u00bfNo voy? \u00bfNo voy a dar esta porillas a las a las a las a las a las de los m\u00faltiples en medio de la ingestaci\u00f3n? Ojo, el null es como una pizadita que hago yo para alinear cosas que no tienen un correspondiente. O sea, yo ten\u00eda una palabra en espa\u00f1ol que... \u00bfTar? Varias de las cefes pueden estar alineadas en espa\u00f1ol, no importa en qu\u00e9 orden est\u00e1n. Eso. Bien, entonces, eran y m\u00e1s uno a las jota posibles alineaciones, por lo tanto. La probabilidad de elegir una alineaci\u00f3n a data de la operaci\u00f3n en ingl\u00e9s, la probabilidad de elegir una alineaci\u00f3n cualquiera, data, la oraci\u00f3n en ingl\u00e9s, va a ser el producto de la probabilidad de haber sortiado un valor jota primero que era de epsilon por la probabilidad de elegir una alineaci\u00f3n cualquiera para ese jota, que es uno sobre y m\u00e1s uno a la jota. Bien, entonces esto lo resolvimos como epsilon sobre y m\u00e1s uno a la jota. Epsilon sobre y m\u00e1s uno a la jota es la probabilidad de data de una oraci\u00f3n en ingl\u00e9s, elegir cierta alineaci\u00f3n que yo voy a utilizar. Bien, ese fue el segundo paso. El tercer paso es una vez que se atengo la alineaci\u00f3n, voy mirando cada palabra de la dolin ingl\u00e9s y le voy poniendo una palabra correspondiente de la de espa\u00f1ol. Para ac\u00e1 voy a sumir que yo tengo una tabla de traducci\u00f3n, una tabla de traducci\u00f3n que me dice que tiene de un lado todas las palabras en espa\u00f1ol y el otro lado de las palabras en ingl\u00e9s, entonces mi tabla va a tener una forma como, por ejemplo, hace una tabla as\u00ed que de un lado decir las palabras en espa\u00f1ol como banco, perro, chato y m\u00e1s cosas y del otro lado va a tener las correspondientes en ingl\u00e9s como banco, bench, cat, tri y m\u00e1s cosas. Y entonces esta tabla va a decir la probabilidad de traducir una cosa en la botan. Entonces banco probablemente tenga cierta probabilidad para avanzar y cierta probabilidad para bench, 0.4 y 0.6, 0.6 y para cat no da ninguna probabilidad para tri tan poco y despu\u00e9s perro no va a tener nada esto, pero si despu\u00e9s y cat va a ser este no s\u00e9, 0.8 en este caso, etc\u00e9tera voy a tener una tabla bastante grande que tiene toda la posibilidad de traducir una palabra como otra. Entonces, si yo tengo esa tabla lo que puedo decir es que la forma de calcular la probabilidad de esa oraci\u00f3n final que yo traduce va a depender de cu\u00e1les son las palabras que yo elija va a depender de cu\u00e1les son las palabras que yo haya puesto dentro de mi, de mi oraci\u00f3n para traducir. Entonces esa tabla que est\u00e1 ah\u00ed definida le llamamos ac\u00e1 en la, en la, en la, la, aparece como T de f su x, su y y dice que la probabilidad de traducir la palabra su y como f su x. Entonces, ac\u00e1 hay una cosa importante. Si tenemos la oraci\u00f3n en ingl\u00e9s, la oraci\u00f3n en ingl\u00e9s recuerdan que ten\u00eda las palabras, es su 1, es su 2, hasta de su 9, la oraci\u00f3n en espa\u00f1ol ten\u00eda las palabras, es su 1, f su 2, hasta de f su jota. Y eso ten\u00eda en el medio una funci\u00f3n de la lineaci\u00f3n que me dec\u00eda que palabras se correspond\u00eda con cual. Entonces, no era su vene ni f su jota, era su y y f su jota grande. Esto era su y, esto era f su jota grande. Entonces, si yo tengo una palabra cualquiera dentro de la oraci\u00f3n en espa\u00f1ol, tengo un f su jota de chica dentro de la oraci\u00f3n en espa\u00f1ol. Esto se va a corresponder con alg\u00fan f su y chica en la oraci\u00f3n en ingl\u00e9s, digamos. Yo s\u00e9 que esto se cumble por la funci\u00f3n de la lineaci\u00f3n porque agarra y mape a todas las palabras que est\u00e1n en espa\u00f1ol con algo que estaba a la dole ingl\u00e9s. Potencialmente con el doque en vac\u00edo, no olvides. Bien, entonces, tengo una palabra de la dole espa\u00f1ol que es f su jota y una palabra de la dole ingl\u00e9s que es f su y. \u00bfCu\u00e1l es la relaci\u00f3n entre ese jota y ese y? \u00bfC\u00f3mo es la relaci\u00f3n entre s\u00ed? Tiamos. Yo puedo decir que el i es igual a algo de jota. La buena manera. La funci\u00f3n de la lineaci\u00f3n, ah\u00ed est\u00e1. O sea, el i es igual a la funci\u00f3n de la lineaci\u00f3n aplicada jota. Como la i, el \u00edndice de este ac\u00e1 es igual a la funci\u00f3n de la lineaci\u00f3n aplicada jota. Entonces, yo puedo decir que la palabra su i es igual a la palabra su a su jota. As\u00ed que puedo decir que en realidad los que est\u00e1n alineados son la palabra f su jota est\u00e1 alineada con la palabra y su a su jota. Y ah\u00ed me sacqu\u00e9 el i de encima, digamos, simplemente y te eros sobre las palabras y te erando sobre la jota puedo establecer la correspondencia entre las dos palabras. Y eso es un poco lo que dice ac\u00e1 para terminar de armar lo que es el modelo de traducci\u00f3n. Para terminar de armar el modelo de traducci\u00f3n dicen que en el tercer paso yo voy a elegir cu\u00e1les son las palabras. Entonces, lo que voy a hacer es iterar sobre todas las palabras y haciendo el producto de todas las las probabilidades. O sea, el producto de dado que yo ten\u00eda la palabra f su jota, pero dado que su ten\u00eda la palabra eso va su jota en ingl\u00e9s. Entonces, elegir la palabra f su jota en espa\u00f1ol. Eso haga una productoria con todos los valores de las distintas palabras. Bien, entonces ah\u00ed, llegue a el \u00faltimo de los valores que quer\u00eda calcular, que es la probabilidad de f dado que conozco. Ah\u00ed es igual a la productoria con jota igual uno hasta jota grande, de el valor de la tabla de traducci\u00f3n, que es de su f su jota, t de f su jota y su vasu jota. Bueno, ta. Entonces, ah\u00ed tengo como en cada paso fui calculando cosas este se correspond\u00eda al paso uno del modelo, paso uno, este se corresponde con el paso del modelo. En realidad, este ya tiene el paso uno del paso dos juntos porque ella tengo el epsilon ac\u00e1 y este se corresponde con el paso tres del modelo. El paso tres de la historia de generaci\u00f3n. Mi objetivo con todos estos valores que est\u00e1n ac\u00e1 es calcular pdf de hoy. \u00bfQu\u00e9 parametro sin traduje? \u00bfQu\u00e9 parametro fueron surgiendo a medida que se iba y derando sobre estos pasos? Bueno, en primer lugar, el epsilon aquel que estaba moviendo, este es un valor que yo tendr\u00eda que estimar a partir de mirar en los corcos, como son los largos y las oraciones relativos. Y el otro parametro importante es aquella tabla all\u00e1, aquella tabla de traducci\u00f3n es que me dice banco, con que probabilidad lo puede traducir como banco y como que probabilidad lo puede traducir como v\u00e9ns, etc\u00e9tera, etc\u00e9tera. Esta tabla en realidad es un parametro del modelo, es un parametro el sistema que si yo lo tuviera, me alcanzar\u00eda con eso para poder construirme este modelo y calcular la probabilidad de cualquier par de operaciones. Bien, y entonces, antes de continuar, vamos a terminar de armar cu\u00e1l es la imagen de esto, que es decir, yo en realidad lo quer\u00eda calcular era pdf da doe, que eso va a ser mi modelo de traducci\u00f3n y de hecho va a ser el encargado de medida de ecuaci\u00f3n de una frase, pdf da doe lo puedo calcular con esta descomposici\u00f3n de pasos que dice ac\u00e1 en realidad porque luego de la siguiente manera. Yo quiero calcular pdf da doe, y entonces voy a mirar lo que dice ac\u00e1 pdf da doe, es igual a la sumatoria en la pdf da doe, que significa eso que para traducir en la generaci\u00f3n en espa\u00f1ol y una versi\u00f3n en ingl\u00e9s o m\u00e1s bien para la situaci\u00f3n, para traducir en una generaci\u00f3n en espa\u00f1ol, hay muchas formas de alinear las palabras en el ingl\u00e9s en espa\u00f1ol y una vez que yo eleg\u00ed una forma alinear, hay muchas formas de elegir las palabras que vienen despu\u00e9s de vamos a mirar a trav\u00e9s de traducci\u00f3n y capaz que hay varias maneras de elegir distintas palabras. Entonces lo que eso significa es que no existe una sola manera de traducir una versi\u00f3n en ingl\u00e9s a una versi\u00f3n espa\u00f1ol. Yo puedo encontrar varias formas de alinear las palabras si dar\u00edas formas de elegir las palabras de manera de que muchas alineaciones son posibles. Entonces para saber cu\u00e1l es la probabilidad de traducir de traducir F da doe. Entonces yo voy a tener que sumar sobre todas las alineaciones posibles, sobre todas las formas de alinear las dos oraciones FI, voy a tener que ir a ir a ir sobre eso y para cada una voy a tener que acular la probabilidad partial. Entonces, digamos, yo tengo cinco formas alinear las dos oraciones, cinco es un n\u00famero un poco raro, pero digamos tengo eneformas de alinear las dos oraciones. Voy a tener que mirar bueno para la primera alineaci\u00f3n cu\u00e1l es la probabilidad de encontrar la oraci\u00f3n F para la segunda alineaci\u00f3n cu\u00e1l es la probabilidad de encontrar la oraci\u00f3n F para la tercera oraci\u00f3n y as\u00ed hasta llegar al final y agarr\u00f3 y sumo todo eso. Eso lo puedo hacer porque las alineaciones son una descomposici\u00f3n de la espacio de probabilidad, en realidad yo puedo descomponar el espacio de probabilidad, en pedacitos disjuntos y cada alineaci\u00f3n va a ser uno de ellos. As\u00ed que digamos que para cagular el modelo de traducci\u00f3n, pede F da doe, necesito sumar sobre todas las alineaciones posibles. Ahora, lo que me falta es saber c\u00f3mo calculo este valor ac\u00e1. As\u00ed que lo que estoy diciendo es que la probabilidad de F da doe es la suma sobre las alineaciones de la probabilidad de F y esa alineaci\u00f3n da doe. Eso es simplemente lo que dice ah\u00ed en la la Ley. Lo que me falta calcular entonces es esta parte de ac\u00e1 y esa parte de ac\u00e1, la calcula esta manera. Yo digo que la probabilidad de F da doe es igual, ah\u00ed est\u00e1 m\u00e1s o menos al resultado final, pero podemos sacar que es lo que tendr\u00eda que poner de este lado. Esta, por definici\u00f3n de probabilidad de condicional es pede F da doe, de verdad lo alian van a ser lo, pero esto se puede definir c\u00f3mo pede F a e sobre pede, no, por definici\u00f3n de probabilidad de condicional. Pero adem\u00e1s esto si quiero podr\u00eda llegar a decir esto es lo mismo que pede F a e sobre pede, por, voy a que me falta va, no, ah\u00ed, por pede a e sobre pede a e pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e definitiva yo que me queda, es si, asoci\u00f3s los dos, meda que dar pede F da do ahh e y si asoci\u00f3s estos dos de ac\u00e1 sabr\u00f3n me va a quedar pede aa dagoes qu\u00e9 lo que tra ya. La probabilidad pede F, que sea de bueno si te los dos, de f, y ya dago... E eh, es igual a la la roguelidad de desfeitados ah\u00ed por la progulidad de a da doy. Y estos dos valores que est\u00e1n ac\u00e1 no lo s\u00e9 el equipo casualidad sino que son los valores que ten\u00edan antes en el modelo. O sea, yo ten\u00eda que el pedea da doy, el igual a \u00e9psil\u00f3n sobre y m\u00e1s uno a la jota. Y el otro era la productoria de jota igual uno hasta jota grande de las valores de traducci\u00f3n, el efe subjota y el e suba subjota. Entonces en definitiva puedo calcular pdf a da doy y adem\u00e1s puedo calcular haciendo una suma sobre todas las alienaciones posibles puedo calcular pdf da doy. Bien, con eso y con todo ese mont\u00f3n de cocciones, llegamos a construir lo que es un modelo de traducci\u00f3n o sea solamente teniendo una tabla de traducciones que me diga cu\u00e1l es la progulidad de traducir una palabra como otra palabra yo puedo llegar a definirme cu\u00e1l es la progulidad de traducir una oraci\u00f3n da da otra oraci\u00f3n. Bien, y hay una cosa m\u00e1s, bueno esto ya lo estoy moviendo que aplicamos en cada paso, y hay una cosa m\u00e1s que es si yo tuviera las dos oraciones digamos la oraci\u00f3n en ingl\u00e9s y la oraci\u00f3n en espa\u00f1ol y adem\u00e1s tuviera la tabla de esta con todas las de progulidades yo podr\u00eda hacer un algoritmo de programaci\u00f3n din\u00e1mica, un algoritmo estilo biter, y que vaya recorriendo alienaciones y media cu\u00e1l es la lineaci\u00f3n m\u00e1s probable. No vamos a ver los detalles de algoritmo, pero viene a forma de decir bueno, voy recorriendo las dos oraciones y me voy quedando con las sus secciones m\u00e1s probable y al final me termina de volviendo cu\u00e1l es la lineaci\u00f3n m\u00e1s probable edadas esas oraciones. O sea que si yo tuviera ya esa tabla de traducciones, esa tabla de progulidades de traducci\u00f3n podr\u00eda construirme las a la lineaci\u00f3nes del corpus. As\u00ed que bueno, hasta el momento dec\u00edamos bueno, suponemos que tenemos esta tabla de traducci\u00f3n que me dice para bank, si se traduce, para bancos, si se traduce como bank o como bench, etc\u00e9tera, estaba diciendo que ten\u00eda esa tabla, pero en realidad la realidad que no tengo esa tabla y me gustar\u00eda poder construirla. Entonces, no gustar\u00eda poder estimar esas progulidades para construirme esa tabla. Si yo tuviera un corpus paralelo, simplemente podr\u00eda ir recorriendo el corpus y contando cu\u00e1ntas veces aparece banco al inado con bench y cu\u00e1ntas veces al inado con bank y ah\u00ed sacar\u00eda una progulidad, pero no tengo las a la lineaci\u00f3nes. Y como lo que vimos digamos reci\u00e9n, si yo tuviera la tabla, entonces yo va adem\u00e1s poder ir recorriendo el corpus y construirme las a la lineaci\u00f3nes. As\u00ed que si yo tuviera las a la lineaci\u00f3nes podr\u00eda contar y sacar la tabla, si yo tuviera la tabla podr\u00eda pasarle un agorismo y construir las a la lineaci\u00f3nes. Pero la verdad que no tengo ninguna de las dos cosas, entonces se vuelve un problema de hueve la gallina, o sea, si yo tuviera las a la lineaci\u00f3nes, construir\u00eda el modelo, construir\u00eda la tabla de progulidades, si yo tuviera la tabla de progulidades podr\u00eda construir las a la lineaci\u00f3nes. Parece tipo de problemas en los cuales yo tengo como dos variables interdependentes y no conozco exactamente el valor de ninguna de las dos, si utiliza lo que se conoce como el algoritmo de expectation maximizaci\u00f3n o maximizaci\u00f3n de la esperanza. Y bueno, es un algoritmo que sirve exactamente para este tipo de problemas. En realidad lo que va a hacer es el algoritmo citerar, es un algoritmo iterativo que va tratando de convertir una soluci\u00f3n y lo que hace es decir, bueno, yo no tengo ninguno de los dos valores, o sea si yo tuviera mi tabla de probabilidad de traducci\u00f3n, me podr\u00eda calcular las a la lineaci\u00f3nes y tuviera mi salinaci\u00f3n, me podr\u00eda calcular la probabilidad de traducci\u00f3n. Entonces lo que hace es decir, bueno, a sumo que mi tabla de traducci\u00f3n va a ser uniformes, digamos, cualquier palabra se puede traducir como cualquier otra palabra con la misma probabilidad. A partir de eso, que alculo de la lineaci\u00f3nes, y a partir de esas nuevas a la lineaci\u00f3nes, c\u00e1lculo otra vez la tabla. Y de vuelta con esa tabla que c\u00e1lculo vuelva, medir las a la lineaci\u00f3nes y vuelta con esas nuevas a la lineaci\u00f3nes, vuelvo a calcular la tabla. Entonces, aunque no me crean, esto despu\u00e9s de muchas iteraciones va convergiendo a algo, y parece m\u00e1gico, \u00bfno? parece como que tal realidad si yo no tengo ninguno de los dos valores, no deber\u00eda como dar fruta. Pero voy a tratar de comenzar los que en realidad esto si funciona, con un ejemplo. Bien, tenemos. Entonces, vamos a construir un sistema que es de traducci\u00f3n entre frances y lingles, donde hay un cuerpo muy grande, pero bueno, vamos a concentrar sobre el entre peque\u00f1as oraci\u00f3n cita que dicen la mes\u00f3n se traduce como deja, la mes\u00f3n blu, se traduce como de lujados y la flea o se traduce como de flower. Entonces, al principio lo que hago es decir, bueno, todas las traducciones en todas las palabras son equiprobables, as\u00ed que lo que me va a quedar es cuando reparten de las salinaciones, todas van a tener el mismo peso. Entre la y mes\u00f3n, la probabilidad de que la se traduca como de, o que se traduca como javos, va a ser la misma, en realidad, porque todas las salinaciones son equiprobables. En la mes\u00f3n blu, tambi\u00e9n va a ser lo mismo, la probabilidad de traducirla como de como blu o como javos, va a ser la misma y en la flea pasa igual. Entonces, eso es la primera, el primer paso, digamos, en el primer paso, yo voy a tener todas las salinaciones equiprobables y todas las los valores de las palabras iguales. Entonces, en mi algorithmo, yo empec\u00e9 con una tabla de traducci\u00f3n que era todo uniforme. Como yo ten\u00eda la probabilidad de traducir cualquier palabra en cualquier otra era la misma. A partir de eso, yo me constru\u00ed estas salinaciones, que tambi\u00e9n parece que son todas equiprobables y parece que no tienen como mucha informaci\u00f3n. Entonces, lo que voy a hacer ahora, a partir de esto, es tratar de construirme de vuelta, la tabla de traducciones, pero mirando estas nuevas salinaciones que hay. Entonces, lo que voy a construir es una tabla que tiene todas las palabras de las diferencias y en el mes\u00f3n blu, blu, blu, blu, blu, blu, blu, blu, blu. Y para llenar, esta nueva tabla es lo que tengo que hacer es iterar sobre las salinaciones, mirar cada una de las palabras, cuantas veces est\u00e1 linear con las otras y contar, o sea, y sumar los peso de cabunas de las salinaciones. Entonces, la lineaci\u00f3n entre la y de en total, mirando ese ejemplo de corpus, cuanto me dar\u00eda de agua, cual ser\u00eda el peso de salinaci\u00f3n. Para verlo, en realidad lo que hago es contar, miro cu\u00e1ntas veces la y de est\u00e1n lineados. Entonces, tengo 0.5 de peso en la primera, en la segunda tengo 0.293 y en la \u00faltima tengo 0.5 de vuelta. As\u00ed que en total tengo como 1.33 de peso entre la y de. Despu\u00e9s, mira, entre la y j, cuanto peso tengo, cuanta masa de probabilidad tengo. Bueno, tengo 0.5 en la primera relaci\u00f3n, 0.103 en la segunda y nada en la tercera. Por lo tanto en total, tengo 0.83 de probabilidades entre la y j. Despu\u00e9s, mira, entre la y blu, cuanto peso tengo. 0.303, solamente 0.33, s\u00f3lo est\u00e1 en la y entre la y fler, cuanto tengo. No, entre la y flavor, cuanto tengo. 0.5, s\u00f3lo aparece en la del final. Bien, como lo tengo la siguiente, entre ms\u00f3n y de cuanto tendr\u00eda. 0.83, est\u00e1 en la primera y la segunda, entre ms\u00f3n y j. En la primera y la segunda, entre ms\u00f3n y j. En la segunda, entre ms\u00f3n y j. Si, se ve usted de trepo que aparece en las dos. Bien, entre ms\u00f3n y blu solamente aparece en la segunda, as\u00ed que voy a tener 0.33 y entre ms\u00f3n y flavor, no tengo nada. Despu\u00e9s, entre blu y de solamente aparece en la segunda, as\u00ed que voy a tener 0.33, entre blu y j. Creo que de vuelta tengo 0.33 y entre blu y blu tambi\u00e9n, 0.33 y no aparece junto con flavor. Y para despu\u00e9s para flar, tengo 0.5, donde 0.jero con j. 0.5 con flavor. Bien, entonces, y si una pasada por todas las salinaciones y me calcul\u00e9 cu\u00e1les son los peso relativos de cada una de estos pares. Lo siguiente que hago, como esto va a ser una probabilidad, es normalizar. Entonces, no voy a construir una tabla, digamos, normalizando por, digamos, voy a sumar en cada fila y voy a adir entre la cantidad que aparece para cada fila, as\u00ed que, igual tambi\u00e9n. Entonces, lo que voy a hacer es normalizar, entonces, si yo sumo a estos sacas, creo que me da dos centodal, no, tres centodal, tengo los valores ac\u00e1, vamos a tener que hacer los c\u00e1lculos, pero s\u00ed, me da tres centodal, entonces lo que pasa cuando yo normalizo es que ac\u00e1 me queda 0.24, ac\u00e1 me queda 0.28, ac\u00e1 me queda 0.12 y ac\u00e1 me queda 0.17, pues el segundo tambi\u00e9n lo normalizo, es entre 2 y me queda 0.42, 0.42, 0.16, 0, el tercero ya suma 1, as\u00ed que me queda 0.23, 0.23, 0.23 y el \u00faltimo tambi\u00e9n queda igual, 0.5, 0, 0, 0, 0.25. Bien, entonces, me constru\u00ed una nueva tabla de probabilidad de traducci\u00f3n dado que ahora la salinaci\u00f3n es serianistas, y no te lo que pas\u00f3 ac\u00e1, si yo miro la fila correspondiente a la que lo que pasa ahora con esta fila, recuerden que yo empec\u00e9 de deniendo todas las salinaciones, todas las traducciones de pronto, todas las probabilidades de traducci\u00f3n de equipares de palabras eran equiprobables, si yo ahora miro la fila de la que es lo que pasa, es acto, aparece claramente que la asociaci\u00f3n entre la idea es m\u00e1s fuerte, tengo un 0.44 de probabilidad de traducir la como de y tengo bastante menos en los otros, tengo 0.28, 0.27 y yo hab\u00eda empezado diciendo que eran equiprobables, entonces yo probablemente ten\u00eda 0.25, 0.25, 0.25, 0.25, 0.25 en cada una, y despu\u00e9s de un paso de la iteraci\u00f3n, descubri\u00f3 que la idea tiene m\u00e1s chance de ser una traducci\u00f3n de la otra, en vez de traducirla como jados o la como bl\u00fa o la como flower, eso pasa en el primer paso, en la primera iteraci\u00f3n, el tipo descubre, el algoritmo descubre que la asociaci\u00f3n entre la idea es bastante m\u00e1s fuerte, como pasa eso, lo que va a pasar es que cuando yo reparto de vuelta en las alinaciones, estas l\u00edneas que se corresponden a la asociaci\u00f3n entre la idea van a estar m\u00e1s fuertes, van a tener un poco m\u00e1s de peso, y como esto es una distribuci\u00f3n de probabilidad es esa masa que gan\u00f3 la asociaci\u00f3n entre la idea, se va a tener que sacar de otras alinaciones posibles, as\u00ed la asociaci\u00f3n va a con de, entonces no est\u00e1 asociada con las otras que est\u00e1n alrededor, entonces esa masa que se pierde, digamos, o sea que gana en la de, se tiene que repartir en las otras alinaciones posibles, o sea, en las que no son entre la idea, entonces despu\u00e9s de una iteraci\u00f3n la asociaci\u00f3n entre la idea empieza a ser m\u00e1s fuerte, y como pasa eso, en la siguiente iteraci\u00f3n va a empezar a descubrir que como la estaba alinado con de, entonces me son tiene que estar alinado con jados, y como me son estaba alinado con jados, digamos esa esa misma masa de probabilidad se va a traducir a transferir a la segunda, y lo mismo, como le ha estado alinado con de, entonces fler tiene que estar alinado con flower, entonces si yo sigo iterando en estos pasos, en cada paso lo que va a pasar es que se va a mover un poco m\u00e1s de probabilidad, hasta que al final va a terminar descubriendo cu\u00e1l es la alinaci\u00f3n real de las palabras, o sea va a descubrir que la va, o sea, con de, me son con jados, luego con blue, luego con flower, como que va descubrir eso, porque en cada paso lo que va pasando es que algunas de las asociaciones, como est\u00e1n, como aparecen co-curren, digamos, en m\u00e1s oraciones, tienen m\u00e1s fuerza que otras, entonces el peso que esas asociaciones ganan lo va sacando otro lado, y eso hace que de otro lado se empieza a generar otras alinaciones diferentes, entonces al final esto termina convergiendo que termina revelando lo que es la, la estructura, su yacente de las palabras, y como se alinian unas con otras, bueno, bien, a ver que yo termine de hacer esto, puedo agarrar y construir me efectivamente la tabla final de traducciones, que es simplemente busco cada una de las posibles traducciones, digamos, de los posibles pares y saco las probabilidades, y qu\u00e9 pas\u00f3 ac\u00e1, mientras yo estaba construyendo mi modelo traducci\u00f3n, mientras yo estaba construyendo la tabla de traducciones adem\u00e1s de, como efectos secundarios se construy\u00f3 un corpus alinia, un corpus que est\u00e1 alineado nivel de palabras, as\u00ed que bueno, el algoritmo de espectrexi\u00f3n maximizaci\u00f3n, funcionan esa manera, tiene siempre dos pasos, un paso de espectrexi\u00f3n y un paso de maximizaci\u00f3n, en este caso, el espectrexi\u00f3n era decir el paso de espectrexi\u00f3n, se trataba de agarrar la tabla de propiedad traducci\u00f3n que tengo, y con eso me damos alinianciones, y despu\u00e9s el de maximizaci\u00f3n es al rev\u00e9s, agarrar las alinianciones que acabo de construir y me damos una nueva tabla, y voy alterando todos esos pasos hasta que eventualmente converg, bien, dijimos que eran 5 modelos de IBM, nos vamos a ver muy en detr\u00e1s y los otros, o sea, solo mencionar que empiezan a agregar complejidad, en este modelo uno hab\u00edamos dicho que todas las alinianciones eran equiprobables, en el modelo 2 abandonan esa noci\u00f3n y dicen bueno en vez de alinianciones equiprobables, yo voy a tener un modelo de reordinamiento de las palabras para decir bueno, tengo cierta probabilidad de que las palabras que est\u00e1n si yo tengo y palabras en ingl\u00e9s, jota palabras en espa\u00f1ol, tengo cierta probabilidad de mover la palabra ah\u00ed y la palabra jota, y bueno ya s\u00ed siguen subiendo en complejidad hasta llegar al modelo 5, que modelos 5 es el que anda mejor, pero de todas maneras estos son modelos que ya no se usan, digamos esto es del a\u00f1o 93 y en general se han obtenido mejores resultados abandonando estos modelos, entonces que vamos a pasar a ver a continuaci\u00f3n, es un modelo bastante m\u00e1s moderno que es lo que s\u00ed, si utiliza bien d\u00eda en traductores como los de Google, s\u00ed, es que en realidad lo claro, a ver estos modelos est\u00e1 d\u00edcicos no utiliza ning\u00fan tipo de analizador un boludo jico, hay otros modelos que s\u00ed lo hacen, no vamos a dar ning\u00fan no en esta clase pero est\u00e1, hay otros modelos que s\u00ed hacen uso de esa informaci\u00f3n, igual son como un refinamiento, creo que ninguno lo tiene como en la base del modelo, el uso de partos pitch, pero s\u00ed cuando no sabes una palabra de una palabra que se conocida en realidad utilizar informaci\u00f3n sobre el partos pitch y eso probablemente te ayuda, en esto modelo por lo menos no lo hab\u00edan tenido en cuenta, bien entonces s\u00ed lo que vamos a ver ahora es el modelo de frases que es algo m\u00e1s moderno y o sea el Google Translate o Bing Translate se basan el modelo de este estilo, y bueno antes de ver c\u00f3mo se modi\u00f3 el frases volvamos un poco de lo que era la alineaci\u00f3n entre palabras, yo ten\u00eda estas frases cl\u00e1sicas, no Mar\u00eda no di una ofretada de la bruja verde, en ingl\u00e9s era Merit is Not Slap Greenwich y una alineaci\u00f3n entre esas dos oraciones en realidad se ver\u00eda como algo as\u00ed, yo tengo que Mar\u00eda se alinea con Merit no se alinea con disnot, se alinea con daba una ofretada de se alinea con ala podr\u00eda ser solamente con la y el que no est\u00e9 alineona, grince alinea con verde y bruja con Wedch, qu\u00e9 diferencia tiene esto con la otra alineaci\u00f3n que hab\u00edamos visto hoy, as\u00ed se les ocurre algo distinto que tiene esta alineaci\u00f3n y la que hab\u00edamos visto hoy, era Not con No, s\u00ed, y que es lo que cambia ac\u00e1 para que pase eso. Lo que estaba pasando hoy era que yo partida de las palabras en espa\u00f1ol y a las palabras en ingl\u00e9s y yo ten\u00eda una funci\u00f3n que me me ap\u00e9 a las palabras en espa\u00f1ol con las palabras en ingl\u00e9s, entonces yo a cada palabra en espa\u00f1ol como m\u00e1ximo le pod\u00eda hacer corresponder una palabra en ingl\u00e9s, entonces me quedaba que yo pod\u00eda expresar cosas como que daba una ofretada daba esta ofretada a Slap una, esta ofretada, esta ofretada, esta ofretada, esa ofretada, eso le pod\u00eda expresar, pero no pod\u00eda expresar algo como esto, que no, esta ofretada es Not porque no ser\u00eda una funci\u00f3n, yo no puedo asociar uno de los valores de la funci\u00f3n con dos cosas de la olcodom\u00ednio y ac\u00e1 en realidad no puedo hacerlo ni en este sentido ni en el otro sentido, con una funci\u00f3n no me sirve porque de vuelta me pasa que Slap est\u00e1 asociado tres cosas, entonces con una funci\u00f3n de alineaci\u00f3n yo no puedo construir este tipo de expresiones, en realidad necesito algo como un poco m\u00e1s poderoso, esto es lo que dec\u00edamos, los modelos dbms siempre usan un mapeo de uno a muchos, usan en una funci\u00f3n de alineaci\u00f3n, mapeo de uno a muchos, pero en realidad lo que necesito para poder capturar realmente vamos a funcionar en el lenguaje es mapeo de muchos a muchos, yo voy a tener que un conjunto de palabra se va a traducir en otro conjunto de palabras, definitiva lo que pasa es que peque\u00f1as frases se traduce en como otras peque\u00f1as frases, por eso necesito un mapeo de muchos a muchos, entonces bueno hay algoritmos que agarran estos mapeos que como el construimos reci\u00e9n el mapeo de uno a muchos en los dos, en las dos direcciones digamos y a partir de eso construyen este mapeo de muchos a muchos, por ejemplo el algoritmo de la herramienta quiz\u00e1s m\u00e1s, lo que hace decir bueno yo tengo un corpus en ingl\u00e9s en espa\u00f1ol alineo utilizando los modelos dbms, voy alineo por un lado de ingl\u00e9s en espa\u00f1ol, por otro lado de espa\u00f1ol en ingl\u00e9s, y ac\u00e1 me quedan dos mapeos de uno a n y vamos dos mapeos con funciones, y despu\u00e9s lo que hago es interceptar esos dos esa dosa de alineaci\u00f3n que me caron y unirlas, cuando la intercepto o tengo lo que se conoce como puntos de alta confianza no se llegan a ver bien, los puntos negros son los puntos de alta confianza que son los de la intersecci\u00f3n y los puntos grises son lo que est\u00e1n en la uni\u00f3n, o sea los que pertenec\u00edan algunos de los modelos, entonces la herramienta lo que hace es decir bueno una vez que yo tengo la intersecci\u00f3n y la uni\u00f3n hago crecer los puntos que est\u00e1n en la intersecci\u00f3n coeleonizando otros puntos que est\u00e1n en la uni\u00f3n, hasta que al final termin\u00f3 completando digamos todo el imagen, este punto que qued\u00f3 solito ah\u00ed no ser\u00eda parte de la alineaci\u00f3n al final, solo los que puede llegar moviendo de otra vez de puntos ya conocidos, entonces bueno, eso es una forma que utiliza, se llama el algoritmo de ojinei, que partiendo de alineaciones uniraccionales y vamos me permite construir una alineaci\u00f3n completa, muchos a muchos entre las palabras, bien, eso le quer\u00eda mencionar acerca de las alineaciones de palabras y ahora s\u00ed vamos a ver c\u00f3mo funciona un modelo basado en frases, un modelo basado en frases tiene cierto semejanza con el modelo anterior que hay hemos visto, pero es un poco m\u00e1s expresivo en realidad yo parte de una oraci\u00f3n, por ejemplo en Aleman que dec\u00eda Morgan Flick y que las canas de sus conference, lo primero que hace el modelo cuando quiere traducir, digamos en este caso es decir bueno, yo voy a segmentar esa oraci\u00f3n de origen en cierta cantidad de frases, despu\u00e9s voy a traducir cada una de esas frases usando una tabla de traducci\u00f3n y esta vez no es una tabla de traducci\u00f3n de palabras sino que es una tabla de traducci\u00f3n de frases que me dice para cada frase con que otra frase corresponde, y una vez que es otra duje cada una de esas frases la voy a ordenar de alguna manera buscando que suena el humanatural posible, buscando aumentar la fluidez de esa oraci\u00f3n, entonces como que la historia de generaci\u00f3n es un poco m\u00e1s simple que la otra, no ten\u00eda que ir sorteando cosas, simplemente digo separo mi oraci\u00f3n en segmentos que le voy a llamar frases, los traducos y los reordenos, esa segmentaci\u00f3n en frases no tiene por que tener una un significado ling\u00fc\u00edstico, yo no voy a separarla en grupo nominal, grupo global, grupo profesional, etc\u00e9tera, no tengo por qu\u00e9, o sea, capas que los segmentos de la frases y justo me queda un grupo preposicional capaz que no, lo \u00fanico que tiene que pasar es que estos segmentos que yo construyo tienen que estar en mitad de traducci\u00f3n de frases, alcanza con eso como para que yo puedo utilizar los en mi traducci\u00f3n, pero no tienen por qu\u00e9 tener una motivaci\u00f3n ling\u00fc\u00edstica, bueno, entonces un modelo basado en frases tiene estos componentes, es parecido al anterior porque de vuelta, yo lo que quiero hacer es encontrar la probabilidad de ese dado de ambos sigo teniendo la misma ecuaci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica estad\u00edstica, la quiero resolver, necesito pdfd y pd, solo que ahora el pdfd lo voy a calcular una manera extinta, voy a decir que para calcular esto tengo un modelo de traducci\u00f3n de frases y un modelo de ordenamiento, un modelo de una gran tabla de frases que me dice cada frase con qu\u00e9 probabilidad la traducci\u00f3n no otra, y despu\u00e9s una forma de decir c\u00f3mo reordenos a frases para tener mejores oraciones, y bueno, como siempre voy a tener otro componente que es el que mide la fluidez que es el modelo de lenguaje, porque los modelos de frases funcionan mejor que los modelos basados en palabras, porque las frases ya tienen cierto contexto, las frases en realidad son como peque\u00f1os grupos de palabras que yo puedo traducir uno en el otro, entonces cosas como dar la mano, dar una ofetada, tomar el pelo, etc. esas cosas como expresiones son mucho m\u00e1s f\u00e1cil de traducir si en realidad eso es as\u00ed que esta expresi\u00f3n que son tres cuatro palabras, le puedo traducir en esta otra expresi\u00f3n que son tres cuatro palabras, y como m\u00e1s expresivo entonces pueda aprender m\u00e1s cosas, y bueno obviamente cuanto m\u00e1s tenga, cuanto m\u00e1s largo sea el corpo, que yo tengo yo puedo aprender frases m\u00e1s largas, mejores probabilidades, y mejores frases. Bueno, hay un ejemplo de como ser\u00eda una tabla de traducci\u00f3n de frases, o sea, es parecido la tabla de traducci\u00f3n de palabras, o lo que ac\u00e1 tengo de enfor\u00e7la, o sea, si yo busco la fila, asociada en for\u00e7la, o sea, encontrar\u00eda todas estas traducciones de proposa, el concediendo de oposici\u00f3n de broalidad, posesivo proposa, el con 10 por ciento, a proposa, el con 3 por ciento, etc. O sea, como ven se traducen frases, en frases. Bueno, y como hago para aprender una tabla de traducci\u00f3n de frases, yo parte de esta alineaci\u00f3n de palabras, digamos esta alineaci\u00f3n completa, que ya no es una funci\u00f3n, sino que es digamos una alineaci\u00f3n de muchos a muchos, y voy a tratar de encontrar todos los todas las frases, todos los pares de frases que son consistentes con la alineaci\u00f3n, a qu\u00e9 me refiero con que son consistentes, a que hay ejemplos, yo quiero decir que mariano y mariano no son un par de frases que son consistentes con esta alineaci\u00f3n, en cambio, mariano y mariano no son, como es que miro esto, lo que pasa es que cuando yo tengo mariano y mariano, la palabra no esta alinea con 10 knot y el 10 knot, digamos, el knot no pertenece hasta alineaci\u00f3n que yo estoy dando decir, entonces digo que es no consistente, lo mismo pasa con si yo dado alinear, mariano daba y mariano y mariano, lo que pasa es que daba no est\u00e1, digamos, los puntos de alineaci\u00f3n de daba, no est\u00e1n dentro de este cuadrante que estoy dando a buscar, entonces en definitiva digo que no es consistente, las alineaciones consistentes correctas son las que consideran todos los puntos dentro de ese cuadrante, entonces mariano est\u00e1 asociado con mariano de knot y esas y es consistente, as\u00ed que como aprendo, frases consistentes, en piezo por las alineaciones, digamos, el piezo con la alineaci\u00f3n es una palabra, despu\u00e9s busco de una palabra y digo bueno, me quedo con todas esas traduciones de palabras y las pongamitables de frases y despu\u00e9s voy tomando de 2 y me quedo con todas esas otras frases y la voy agregando, me quedo de frases, despu\u00e9s me puedo avanzar en 1, tomar de 3, tomar de 4 y llegar a tomar incluso toda la oraci\u00f3n como frases, entonces a partir de estas oraciones que ten\u00edan, no s\u00e9, un 2, 3, 4, 5, 6, 7, 8, no hay palabras, yo termino aprendiendo como 17 frases, digamos, cada vez m\u00e1s grandes y bueno, hoy voy sacando esto de todo el corpus y calculando mitable de probabilidades, de qu\u00e9 manera, calcula esas probabilidades, yo lo que puedo hacer es como siempre ver cu\u00e1ntas veces aparecen el corpus y contar, o si no, si yo ten\u00eda construido el modelo anterior, el modelo de la tabla de traduciones de palabra palabra, en realidad lo que puedo hacer es aprovechar ese modelo traducci\u00f3n de palabra palabra y decir bueno, me arma una traducci\u00f3n entre un par de frases bas\u00e1ndome en las traduciones palabra palabras, son como formas distintas de construirlo y a veces hasta complementarias, bien eso fue el modelo de frases, los modelos de frases son los m\u00e1s usados hoy en d\u00eda en realidad en lo que es la traducci\u00f3n autom\u00e1tica, son los candados mejor de resultados y bueno, no faltaba una cosa para terminar el toda la imagen de lo que es la traducci\u00f3n autom\u00e1tica estad\u00edstica que es la decodificaci\u00f3n, entonces veamos un resumen de lo que ten\u00edamos hasta ahora, hasta ahora yo part\u00ed de yo quer\u00eda resolver la cocci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica estad\u00edstica y yo ten\u00eda un corpus paralelo que ten\u00eda texto en el idioma origen y el idioma de estino y a partir de siendo analisis estad\u00edstico yo me constru\u00ed un modelo traducci\u00f3n que lo que vimos en esta clase, adem\u00e1s yo ten\u00eda cierta cantidad de texto del idioma de estino y a partir de cierto analisis estad\u00edstico me constru\u00ed un modelo de lenguaje que me dice que tan fluido es una operaci\u00f3n en el lenguaje estino, entonces ahora lo que me falta, recuerden que yo lo que ten\u00eda que hacer era y te era sobre todas las oraciones del lenguaje estino y pasar las a trav\u00e9s del modelo traducci\u00f3n y del modelo de lenguaje para que me de la probabilidad de esa oraci\u00f3n, bueno lo que me falta es el agorismo de codificaci\u00f3n que en vez de probar con todas las oraciones de lenguaje estinos me va a decir unas cuantas oraciones para probar, porque me dice 150 oraciones para probar sobre las cuales utiliza el modelo traducci\u00f3n en modelo de lenguaje, entonces esto es como un diagrama de modulos en los cuales el agorismo de codificaci\u00f3n utiliza los dos modulos, tanto es la traducci\u00f3n como el lenguaje, bueno, como funciona el agorismo de codificaci\u00f3n, que vamos a ver es un agorismo de codificaci\u00f3n de tipo bean search y bueno la funci\u00f3n de acinde manera, yo tengo la oraci\u00f3n Mar\u00eda no dio una ofetada a la bruja verde y la quiero traducir al ingl\u00e9s y tengo una tabla de traducci\u00f3n de frases entonces mi oraci\u00f3n Mar\u00eda no dio una ofetada a la bruja verde, yo busco en la tabla de frases \u00bfCu\u00e1les de esas digamos? \u00bfCu\u00e1les segmento? \u00bfCu\u00e1les subsegmento de esa oraci\u00f3n? yo puedo encontrar en la tabla de traducci\u00f3n de frases, todo lo que me encanta por ejemplo que Mar\u00eda lo pota o s\u00ed como Mary, no lo busco en la tabla y lo pota o s\u00ed como not como not o como no, dio lo pota o s\u00ed como guir, pero adem\u00e1s no dio esa frase entera, yo le busco en la tabla y me aparece que la pota o s\u00ed como not guir, dio una ofetada a toda esa frase lo pota o s\u00ed como slape, una ofetada lo pota o s\u00ed como aslape y bueno de otras cosas bruja lo pota o s\u00ed como witch, verde como green pero adem\u00e1s en alg\u00fan lado de la tabla tengo que brujar verde lo puedo traducir como green witch y as\u00ed, yo puedo encontrar diferentes maneras de segmentar la oraci\u00f3n y adem\u00e1s para cada uno de esos segmentos puedo encontrar distintas formas de traducirlo en el lenguaje destino con mitable de frases, entonces el algoritmo de codificaci\u00f3n funciona de la siguiente manera, empezamos teniendo en cada paso el algoritmo vamos a tener un conjunto de hip\u00f3tesis de traducci\u00f3n, se llega a ver ah\u00ed lo que dice a ojos, m\u00e1s o menos, bien, ac\u00e1 que eran malos, correctes, bueno, en cada uno de los pasos yo voy a tener un conjunto de hip\u00f3tesis de traducci\u00f3n, al principio el algoritmo voy a empezar con una hip\u00f3tesis vac\u00eda, como se le este hip\u00f3tesis dice que lo importante de leer es la parte de la defe que tiene un mont\u00f3n de guiones, significa que no hay ninguna palabra del espa\u00f1ol cubierta, esas son todas las 9, 9 palabras en espa\u00f1ol, ninguna esta cubierta y esta hip\u00f3tesis tiene probabilidad 1, entonces en cada paso el algoritmo lo que voy a hacer es elegir un par de frases, tal que una traducci\u00f3n de la otra y voy a crear un hip\u00f3tesis nueva a partir de una que ya tengo, entonces en este paso lo que dice fue decir el hijo, el par de frases Mar\u00eda Mary y ah\u00ed me creo, una nueva hip\u00f3tesis que cubre la primera palabra, por eso parece una cerita en este caso, el hijo, la frase en ingl\u00e9s Mary y ahora tiene una probabilidad de 0.584, ese n\u00famero de esa probabilidad va a servir para guiar un poco en el algoritmo pero vamos a ver despu\u00e9s como es que se calcula, porabra que \u00e9l se solamente con el n\u00famero, bien, pero entonces yo ten\u00eda otra opci\u00f3n, en realidad yo pod\u00eda haber elegido empezar en vez de traducir Mar\u00eda por Mary, pod\u00eda haber elegido empezar por traducir bruja por witch y ah\u00ed me crear\u00eda otra hip\u00f3tesis de traducci\u00f3n donde cubro la pen\u00faltima de las palabras en espa\u00f1ol agarr\u00f3 la palabra witch, de el hijo de la palabra witch y tiene una probabilidad de 0.882. Entonces, en cada paso el algoritmo lo que hace es elegir una el hip\u00f3tesis que tiene elegir un par de frases y expandir, as\u00ed que lo siguiente que puedo hacer es elegir la frase, dir not, expandirla a partir de la hip\u00f3tesis que ten\u00eda con Mary y bueno eso me cubre ahora dos palabras en espa\u00f1ol y me tiene medio otra probabilidad y despu\u00e9s, si gobanzando y si gobanzando, hasta que lleg\u00f3 a cubrir en alg\u00fan momento, si yo sigo avanzando y sigo arregando hip\u00f3tesis, en alg\u00fan momento voy a llegar a cubrir todas las palabras del idioma espa\u00f1ol, todas las palabras de elaboraci\u00f3n en el idioma espa\u00f1ol. Entonces ah\u00ed una vez que yo cubrito a las palabras digo bueno, esto es una hip\u00f3tesis completa y esto lo devuelvo como un potencial candidata, digamos, una abracci\u00f3n candidata a traducci\u00f3n. Pero claro, media que yo fie avanzando una cosa que paso es que fui dejando hip\u00f3tesis colgadas y esas hip\u00f3tesis podr\u00edan tener otras traducciones posible, yo ac\u00e1 lo que devol\u00ed era una hip\u00f3tesis de traducci\u00f3n, pero a medida que yo ten\u00eda las otras hip\u00f3tesis, si yo hubiera seguido por las otras hip\u00f3tesis hubiera podido devoler otras cosas. Entonces, yo necesito hacer un backtracking para poder devoler todas las posibilidades, poder volver a ver las hip\u00f3tesis a revisitar las hip\u00f3tesis y que hab\u00eda dejado cogeadas y volver a explorar los otros caminos. Entonces, necesitar\u00edas en un backtracking para recorrer las todas. Y si hago un backtracking, lo que va a pasar es que voy a va a ocurrir una explosi\u00f3n de exponencial de la espacidad de b\u00fasqueda, porque en realidad todas las posibilidades que se abren son exponenciales y ah\u00ed esto como que se vuelve bastante lento. Entonces, yo quer\u00eda un decodificador para volver este problema un problema tratable. En vez de agarrar las infinitas oraciones del idioma, me quedo con algunas que sea m\u00e1s probable. Con esta acorrimo de codificaci\u00f3n, logr\u00e9 reducir de infinito a algo finito, pero a\u00fan as\u00ed es demasiado lento, porque hay una explosi\u00f3n combinaci\u00f3n combinatoria de asipotesis y me quedo una cantidad exponencial de hip\u00f3tesis. Entonces, como es tan grande este problema, digamos como la cantidad hip\u00f3tesis de exponencial y este es un problema en EP completo, entonces se utilizan t\u00e9cnicas para reducir el espacio de b\u00fasqueda. Y hay como dos tipos de t\u00e9cnicas, algunas son con riesgo y otras son sin riesgo. Las t\u00e9cnicas sin riesgo, lo que quiere decir es que si yo aplico una t\u00e9cnica de reducci\u00f3n de hip\u00f3tesis, sin riesgo, la soluci\u00f3n ideal que yo ten\u00eda, dentro de mi b\u00fasqueda, no le voy a perder utilizando una t\u00e9cnica sin riesgo. En cambio en la con riesgo, si yo podr\u00eda llegar a perder la soluci\u00f3n \u00f3ptima. Bien, entonces, la t\u00e9cnica sin riesgo que conocemos es la de recombinaci\u00f3n de hip\u00f3tesis, que dice que si yo tengo dos hip\u00f3tesis, voy avanzando por dos caminos, dentro del algoritmo y llevo a dos hip\u00f3tesis iguales, por lo menos dos hip\u00f3tesis que cubren las mismas palabras, entonces me puedo quedar con la que tiene mayor probabilidad de las dos y descartar la otra. Porque, porque a medida que yo voy a seguir avanzando en el algoritmo, lo que va a pasar es que van a bajar las probabilidades, digamos, elegiendo m\u00e1s palabras y elegiendo m\u00e1s frases, me va a bajar la probabilidad y nunca me va a pasar que una de las hip\u00f3tesis que ten\u00eda menos probabilidad vaya a subir en realidad, siempre va a tener menos. Entonces, en definitiva, yo puedo conseguir de descartar la que tiene menos probabilidad. Bueno, esa es recomendaci\u00f3n de hip\u00f3tesis, pero ni siquiera con eso, alcanza, digamos, para reducir el espacio de b\u00fasqueda, lo suficiente, a\u00fan queda much\u00edsimas hip\u00f3tesis. Entonces, s\u00f3lo utilizar t\u00e9cnicas de podado con riesgo, la t\u00e9cnica de listo grama, la t\u00e9cnica de lumbral, el listo grama significa que, a cada paso, digamos, en cada paso el algoritmo, yo me quedo con los N, las N hip\u00f3tesis de traducci\u00f3n m\u00e1s probable y descart\u00f3 las otras. Y la t\u00e9cnica de lumbral dice que, a cada paso el algoritmo, me quedo con la hip\u00f3tesis de mayor probabilidad y las que est\u00e9n a una distancia alfa m\u00e1xima de esa. \u00bfCu\u00e1l es el riesgo de las t\u00e9cnicas de podado? Que si la mejor traducci\u00f3n y la traducci\u00f3n \u00f3ptima ten\u00eda algunas frases muy poco probable, es al principio, entonces probablemente yo descarte esa soluci\u00f3n en los primeros pasos y no lleguen a contar la soluci\u00f3n \u00f3ptima. La p\u00e9rdida, por eso yo haber podado. Sin embargo, bueno, tiene como, como ventaja que en realidad reducen much\u00edsimo el espacio de b\u00fasqueda y vuelve este problema, un problema tratable. Bueno, y ahora s\u00ed, qu\u00e9 significaba esa probabilidad que estaba viendo en cada una de asip\u00f3tesis. O sea, el podado necesita tener las mejores asip\u00f3tesis y bueno, para la recomendaci\u00f3n tambi\u00e9n exitos a ver la probabilidad de asip\u00f3tesis. Bueno, la forma de calcular la probabilidad de asip\u00f3tesis se divide en dos, digamos, tengo lo que, en contraste al momento, el asip\u00f3tesis se va a cuidar a cierta cantidad de palabras. Entonces, para esa cantidad para la verdad, que se llev\u00f3 cubiertas, utilizo los 3 modelos en modelos de traducci\u00f3n, el modelo de rodeonamiento y el modelo de lenguaje, utilizo los 3 modelos para calcular la probabilidad de las frases hasta el momento, pero para lo que me falta traducir, yo no puedo utilizar todo porque no tengo toda la informaci\u00f3n de traducci\u00f3n, entonces lo que hago es utilizar solamente el modelo de traducci\u00f3n y el modelo de lenguaje. Descarto el modelo de rodeonamiento y bueno, entonces algo, calcula una probabilidad que es una parte de con todos los 3 modelos y otra parte sin el modelo de rodeonamiento. Bien, este algoritmo que acabamos de describir que hace esta b\u00fasqueda bas\u00e1ndose en hip\u00f3tesis que utiliza recomendaci\u00f3n y podado hip\u00f3tesis y bueno, calcula las probabilidades de esta manera, se conoce como algoritmo b\u00fasqueda asterico, es un algoritmo de vincers que se usa much\u00edsimo en lo que es traducci\u00f3n autom\u00e1tica estad\u00edstica. Por ejemplo, el sistema Moses, ac\u00e1 tenemos este ejemplos de herramientas o pensores o gratuitas que siguen para construcci\u00f3n de traducci\u00f3n autom\u00e1ticos. Es el sistema Moses, es un sistema o pens\u00f3 para desarrollar este tipo de traducci\u00f3n autom\u00e1ticos estad\u00edsticos y hay implementa este algoritmo de codificaci\u00f3n de b\u00fasqueda asterico. Y bueno, lo que tiene el sistema Moses de Buenio es que en realidad lo que hace adem\u00e1s de implementar el de codificadores utiliza a los otros sistemas y los integrar alguna manera. Entonces, integra este otro sistema al ERCTLM que es una herramienta para crear modelos del lenguaje basados en el gramas y el otro sistema es el quiso m\u00e1s m\u00e1s que lo veo, mencionado hoy que es el sistema que me permite alinear corpus de operaciones en los distintos sitiomas llegando a los modelos del 1 ad 5 de traducci\u00f3n de BMS. Bueno, entonces, esta tres herramientas, si uno quiere construir un tradutor autom\u00e1tico estad\u00edstico, entre cualquier par de idiomas, puede utilizar estas tres herramientas y ten\u00edan un corpus paralelo y un corpus monolingue puede construir un tradutor. Pero, bueno, adem\u00e1s, otra cosa que me ense\u00f1amos en la clase basada, pero eran los sistemas basados en reglas, los sistemas basados en reglas han ca\u00eddo un poco, y a monotiene tanta popularidad como antes. Sin embargo, algunos se siguen usando, y el sistema aperty un sistema o pensor para construir sistema de traducci\u00f3n basados en reglas, que tienen un mont\u00f3n de pares de lenguajes. Y, bueno, ya anda relativamente bien, digamos, entonces, se sigue desarrollando esta hoy, entonces, es una alternativa o pensor que est\u00e1 basada en reglas en vez de estar basado en estas idicas. Y, bueno, esta es un resumen de lo que vimos, as\u00ed que dejamos por ac\u00e1.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 23.68, "text": " Una vez que eleg\u00ed en mi, con el paso 1, eleg\u00ed cu\u00e1ntas palabras en espa\u00f1ol y bolsar en el", "tokens": [50364, 15491, 5715, 631, 14459, 870, 465, 2752, 11, 416, 806, 29212, 502, 11, 14459, 870, 44256, 296, 35240, 465, 31177, 288, 8986, 82, 289, 465, 806, 51548], "temperature": 0.0, "avg_logprob": -0.49242068204012784, "compression_ratio": 1.381679389312977, "no_speech_prob": 0.09566617757081985}, {"id": 1, "seek": 0, "start": 23.68, "end": 27.8, "text": " paso 2, es lo que voy a elegir es una lineaci\u00f3n, una funci\u00f3n de lineaci\u00f3n que me dice", "tokens": [51548, 29212, 568, 11, 785, 450, 631, 7552, 257, 14459, 347, 785, 2002, 1622, 3482, 11, 2002, 43735, 368, 1622, 3482, 631, 385, 10313, 51754], "temperature": 0.0, "avg_logprob": -0.49242068204012784, "compression_ratio": 1.381679389312977, "no_speech_prob": 0.09566617757081985}, {"id": 2, "seek": 2780, "start": 27.8, "end": 31.0, "text": " cada palabra, con cual se va a corresponder, cada palabra, el lado de espa\u00f1ol, con que", "tokens": [50364, 8411, 31702, 11, 416, 10911, 369, 2773, 257, 6805, 260, 11, 8411, 31702, 11, 806, 11631, 368, 31177, 11, 416, 631, 50524], "temperature": 0.0, "avg_logprob": -0.3837153737137957, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.2299921065568924}, {"id": 3, "seek": 2780, "start": 31.0, "end": 37.260000000000005, "text": " palabra en ingl\u00e9s se va a corresponder. Este modelo ha sumed de manera muy na\u00efve que todas", "tokens": [50524, 31702, 465, 49766, 369, 2773, 257, 6805, 260, 13, 16105, 27825, 324, 2408, 292, 368, 13913, 5323, 1667, 15487, 303, 631, 10906, 50837], "temperature": 0.0, "avg_logprob": -0.3837153737137957, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.2299921065568924}, {"id": 4, "seek": 2780, "start": 37.260000000000005, "end": 44.28, "text": " las salinaciones que yo puedo tener son equiprobables, o sea, ha sumed que yo voy a tener un", "tokens": [50837, 2439, 1845, 259, 9188, 631, 5290, 21612, 11640, 1872, 5037, 16614, 2965, 11, 277, 4158, 11, 324, 2408, 292, 631, 5290, 7552, 257, 11640, 517, 51188], "temperature": 0.0, "avg_logprob": -0.3837153737137957, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.2299921065568924}, {"id": 5, "seek": 2780, "start": 44.28, "end": 48.64, "text": " conjunto de lineaciones posibles y todas van a tener la vina de probabilidad. Bien, entonces,", "tokens": [51188, 37776, 368, 1622, 9188, 1366, 14428, 288, 10906, 3161, 257, 11640, 635, 371, 1426, 368, 31959, 4580, 13, 16956, 11, 13003, 11, 51406], "temperature": 0.0, "avg_logprob": -0.3837153737137957, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.2299921065568924}, {"id": 6, "seek": 2780, "start": 48.64, "end": 54.6, "text": " la probabilidad de elegir una lineaci\u00f3n en particular, si yo tengo un mont\u00f3n de lineaciones,", "tokens": [51406, 635, 31959, 4580, 368, 14459, 347, 2002, 1622, 3482, 465, 1729, 11, 1511, 5290, 13989, 517, 45259, 368, 1622, 9188, 11, 51704], "temperature": 0.0, "avg_logprob": -0.3837153737137957, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.2299921065568924}, {"id": 7, "seek": 5460, "start": 54.6, "end": 59.64, "text": " digamos, la probabilidad de elegir una, una lineaci\u00f3n en particular, va a ser uno sobre", "tokens": [50364, 36430, 11, 635, 31959, 4580, 368, 14459, 347, 2002, 11, 2002, 1622, 3482, 465, 1729, 11, 2773, 257, 816, 8526, 5473, 50616], "temperature": 0.0, "avg_logprob": -0.33352700956575165, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.0270269475877285}, {"id": 8, "seek": 5460, "start": 59.64, "end": 63.480000000000004, "text": " la cantidad de lineaciones que tengo, porque en realidad todas van a ser equiprobables.", "tokens": [50616, 635, 33757, 368, 1622, 9188, 631, 13989, 11, 4021, 465, 25635, 10906, 3161, 257, 816, 5037, 16614, 2965, 13, 50808], "temperature": 0.0, "avg_logprob": -0.33352700956575165, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.0270269475877285}, {"id": 9, "seek": 5460, "start": 63.480000000000004, "end": 69.28, "text": " Bien, entonces, cu\u00e1ntas lineaciones puedo tener entre dos oraciones, una oraci\u00f3n en ingl\u00e9s", "tokens": [50808, 16956, 11, 13003, 11, 44256, 296, 1622, 9188, 21612, 11640, 3962, 4491, 420, 9188, 11, 2002, 420, 3482, 465, 49766, 51098], "temperature": 0.0, "avg_logprob": -0.33352700956575165, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.0270269475877285}, {"id": 10, "seek": 5460, "start": 69.28, "end": 73.16, "text": " que tiene largo y una oraci\u00f3n espa\u00f1ola que tiene largo jota, como puedo calcular cu\u00e1ntas", "tokens": [51098, 631, 7066, 31245, 288, 2002, 420, 3482, 25726, 4711, 631, 7066, 31245, 361, 5377, 11, 2617, 21612, 2104, 17792, 44256, 296, 51292], "temperature": 0.0, "avg_logprob": -0.33352700956575165, "compression_ratio": 1.7572815533980584, "no_speech_prob": 0.0270269475877285}, {"id": 11, "seek": 7316, "start": 73.16, "end": 79.16, "text": " a lineaciones existen.", "tokens": [50364, 257, 1622, 9188, 2514, 268, 13, 50664], "temperature": 0.0, "avg_logprob": -0.5020217895507812, "compression_ratio": 1.4214285714285715, "no_speech_prob": 0.08737444877624512}, {"id": 12, "seek": 7316, "start": 79.16, "end": 90.4, "text": " M\u00e1s o menos, casi de la jota. Recuerden que el lado de ingl\u00e9s, yo pod\u00eda, yo ten\u00eda ciertas", "tokens": [50664, 376, 2490, 277, 8902, 11, 22567, 368, 635, 361, 5377, 13, 9647, 5486, 1556, 631, 806, 11631, 368, 49766, 11, 5290, 45588, 11, 5290, 23718, 49252, 296, 51226], "temperature": 0.0, "avg_logprob": -0.5020217895507812, "compression_ratio": 1.4214285714285715, "no_speech_prob": 0.08737444877624512}, {"id": 13, "seek": 7316, "start": 90.4, "end": 99.19999999999999, "text": " palabras en ingl\u00e9s ten\u00eda la palabra, en ingl\u00e9s era ah\u00ed, la palabra 1, 2 hasta,", "tokens": [51226, 35240, 465, 49766, 23718, 635, 31702, 11, 465, 49766, 4249, 12571, 11, 635, 31702, 502, 11, 568, 10764, 11, 51666], "temperature": 0.0, "avg_logprob": -0.5020217895507812, "compression_ratio": 1.4214285714285715, "no_speech_prob": 0.08737444877624512}, {"id": 14, "seek": 9920, "start": 99.2, "end": 108.0, "text": " sui y en espa\u00f1ol ten\u00eda las palabras f1, f2 hasta, f subjota. Entonces, yo pod\u00eda", "tokens": [50364, 459, 72, 288, 465, 31177, 23718, 2439, 35240, 283, 16, 11, 283, 17, 10764, 11, 283, 1422, 73, 5377, 13, 15097, 11, 5290, 45588, 50804], "temperature": 0.0, "avg_logprob": -0.3552411036057906, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.03622209653258324}, {"id": 15, "seek": 9920, "start": 108.0, "end": 113.60000000000001, "text": " atrazar l\u00edneas para alinear, pero adem\u00e1s en ingl\u00e9s, yo siempre considerado que ten\u00eda un", "tokens": [50804, 44192, 26236, 16118, 716, 296, 1690, 419, 533, 289, 11, 4768, 21251, 465, 49766, 11, 5290, 12758, 1949, 1573, 631, 23718, 517, 51084], "temperature": 0.0, "avg_logprob": -0.3552411036057906, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.03622209653258324}, {"id": 16, "seek": 9920, "start": 113.60000000000001, "end": 119.48, "text": " token null. Entonces, todas las palabras que no estaban alineadas del lado del espa\u00f1ol y van", "tokens": [51084, 14862, 18184, 13, 15097, 11, 10906, 2439, 35240, 631, 572, 36713, 419, 533, 6872, 1103, 11631, 1103, 31177, 288, 3161, 51378], "temperature": 0.0, "avg_logprob": -0.3552411036057906, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.03622209653258324}, {"id": 17, "seek": 9920, "start": 119.48, "end": 123.0, "text": " a parar ah\u00ed. As\u00ed que en ingl\u00e9s en realidad no tengo", "tokens": [51378, 257, 37193, 12571, 13, 17419, 631, 465, 49766, 465, 25635, 572, 13989, 51554], "temperature": 0.0, "avg_logprob": -0.3552411036057906, "compression_ratio": 1.5679611650485437, "no_speech_prob": 0.03622209653258324}, {"id": 18, "seek": 12300, "start": 123.04, "end": 127.56, "text": " y posibilidades, tengo una m\u00e1s, tengo y m\u00e1s uno. Entonces, cu\u00e1ntas formas tengo yo de", "tokens": [50366, 288, 1366, 11607, 10284, 11, 13989, 2002, 3573, 11, 13989, 288, 3573, 8526, 13, 15097, 11, 44256, 296, 33463, 13989, 5290, 368, 50592], "temperature": 0.0, "avg_logprob": -0.34548381883270884, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.21972204744815826}, {"id": 19, "seek": 12300, "start": 127.56, "end": 133.32, "text": " mapear estas jota posibilidades en espa\u00f1ol con las y en ingl\u00e9s.", "tokens": [50592, 463, 494, 289, 13897, 361, 5377, 1366, 11607, 10284, 465, 31177, 416, 2439, 288, 465, 49766, 13, 50880], "temperature": 0.0, "avg_logprob": -0.34548381883270884, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.21972204744815826}, {"id": 20, "seek": 12300, "start": 133.32, "end": 136.72, "text": " Es alto, y m\u00e1s una la jota, porque yo tengo y m\u00e1s una opci\u00f3n para la primera y m\u00e1s", "tokens": [50880, 2313, 21275, 11, 288, 3573, 2002, 635, 361, 5377, 11, 4021, 5290, 13989, 288, 3573, 2002, 999, 5687, 1690, 635, 17382, 288, 3573, 51050], "temperature": 0.0, "avg_logprob": -0.34548381883270884, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.21972204744815826}, {"id": 21, "seek": 12300, "start": 136.72, "end": 142.6, "text": " una opci\u00f3n para la segunda, etc\u00e9tera, que yo al final. As\u00ed que son y m\u00e1s uno a las jota", "tokens": [51050, 2002, 999, 5687, 1690, 635, 21978, 11, 5183, 526, 23833, 11, 631, 5290, 419, 2572, 13, 17419, 631, 1872, 288, 3573, 8526, 257, 2439, 361, 5377, 51344], "temperature": 0.0, "avg_logprob": -0.34548381883270884, "compression_ratio": 1.7164948453608246, "no_speech_prob": 0.21972204744815826}, {"id": 22, "seek": 14260, "start": 142.6, "end": 152.6, "text": " alineaciones, posibles. \u00bfNo voy a tener un cliente medio de la red? \u00bfNo voy?", "tokens": [50364, 419, 533, 9188, 11, 1366, 14428, 13, 3841, 4540, 7552, 257, 11640, 517, 6423, 68, 22123, 368, 635, 2182, 30, 3841, 4540, 7552, 30, 50864], "temperature": 0.0, "avg_logprob": -0.6647467476981027, "compression_ratio": 1.5990338164251208, "no_speech_prob": 0.11996428668498993}, {"id": 23, "seek": 14260, "start": 152.6, "end": 155.95999999999998, "text": " \u00bfNo voy a dar esta porillas a las a las a las a las a las de los m\u00faltiples en medio", "tokens": [50864, 3841, 4540, 7552, 257, 4072, 5283, 1515, 373, 296, 257, 2439, 257, 2439, 257, 2439, 257, 2439, 257, 2439, 368, 1750, 275, 43447, 72, 2622, 465, 22123, 51032], "temperature": 0.0, "avg_logprob": -0.6647467476981027, "compression_ratio": 1.5990338164251208, "no_speech_prob": 0.11996428668498993}, {"id": 24, "seek": 14260, "start": 155.95999999999998, "end": 162.0, "text": " de la ingestaci\u00f3n? Ojo, el null es como una pizadita que hago yo para alinear cosas que", "tokens": [51032, 368, 635, 3957, 377, 3482, 30, 422, 5134, 11, 806, 18184, 785, 2617, 2002, 280, 590, 345, 2786, 631, 38721, 5290, 1690, 419, 533, 289, 12218, 631, 51334], "temperature": 0.0, "avg_logprob": -0.6647467476981027, "compression_ratio": 1.5990338164251208, "no_speech_prob": 0.11996428668498993}, {"id": 25, "seek": 14260, "start": 162.0, "end": 165.16, "text": " no tienen un correspondiente. O sea, yo ten\u00eda una palabra en espa\u00f1ol que...", "tokens": [51334, 572, 12536, 517, 6805, 8413, 13, 422, 4158, 11, 5290, 23718, 2002, 31702, 465, 31177, 631, 485, 51492], "temperature": 0.0, "avg_logprob": -0.6647467476981027, "compression_ratio": 1.5990338164251208, "no_speech_prob": 0.11996428668498993}, {"id": 26, "seek": 16516, "start": 165.16, "end": 172.44, "text": " \u00bfTar? Varias de las cefes pueden estar alineadas en espa\u00f1ol, no importa en qu\u00e9", "tokens": [50364, 3841, 51, 289, 30, 32511, 296, 368, 2439, 269, 5666, 279, 14714, 8755, 419, 533, 6872, 465, 31177, 11, 572, 33218, 465, 8057, 50728], "temperature": 0.0, "avg_logprob": -0.5627773072984483, "compression_ratio": 1.3505747126436782, "no_speech_prob": 0.058206524699926376}, {"id": 27, "seek": 16516, "start": 172.44, "end": 179.68, "text": " orden est\u00e1n. Eso. Bien, entonces, eran y m\u00e1s uno a las jota posibles alineaciones,", "tokens": [50728, 28615, 10368, 13, 27795, 13, 16956, 11, 13003, 11, 32762, 288, 3573, 8526, 257, 2439, 361, 5377, 1366, 14428, 419, 533, 9188, 11, 51090], "temperature": 0.0, "avg_logprob": -0.5627773072984483, "compression_ratio": 1.3505747126436782, "no_speech_prob": 0.058206524699926376}, {"id": 28, "seek": 16516, "start": 179.68, "end": 188.92, "text": " por lo tanto. La probabilidad de elegir una alineaci\u00f3n a data de la", "tokens": [51090, 1515, 450, 10331, 13, 2369, 31959, 4580, 368, 14459, 347, 2002, 419, 533, 3482, 257, 1412, 368, 635, 51552], "temperature": 0.0, "avg_logprob": -0.5627773072984483, "compression_ratio": 1.3505747126436782, "no_speech_prob": 0.058206524699926376}, {"id": 29, "seek": 18892, "start": 188.92, "end": 193.48, "text": " operaci\u00f3n en ingl\u00e9s, la probabilidad de elegir una alineaci\u00f3n cualquiera, data, la", "tokens": [50364, 2208, 3482, 465, 49766, 11, 635, 31959, 4580, 368, 14459, 347, 2002, 419, 533, 3482, 10911, 35134, 11, 1412, 11, 635, 50592], "temperature": 0.0, "avg_logprob": -0.31407988801294445, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.07979270815849304}, {"id": 30, "seek": 18892, "start": 193.48, "end": 199.39999999999998, "text": " oraci\u00f3n en ingl\u00e9s, va a ser el producto de la probabilidad de haber sortiado un valor", "tokens": [50592, 420, 3482, 465, 49766, 11, 2773, 257, 816, 806, 47583, 368, 635, 31959, 4580, 368, 15811, 1333, 72, 1573, 517, 15367, 50888], "temperature": 0.0, "avg_logprob": -0.31407988801294445, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.07979270815849304}, {"id": 31, "seek": 18892, "start": 199.39999999999998, "end": 205.39999999999998, "text": " jota primero que era de epsilon por la probabilidad de elegir una alineaci\u00f3n cualquiera para", "tokens": [50888, 361, 5377, 21289, 631, 4249, 368, 17889, 1515, 635, 31959, 4580, 368, 14459, 347, 2002, 419, 533, 3482, 10911, 35134, 1690, 51188], "temperature": 0.0, "avg_logprob": -0.31407988801294445, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.07979270815849304}, {"id": 32, "seek": 18892, "start": 205.39999999999998, "end": 212.56, "text": " ese jota, que es uno sobre y m\u00e1s uno a la jota. Bien, entonces esto lo resolvimos como", "tokens": [51188, 10167, 361, 5377, 11, 631, 785, 8526, 5473, 288, 3573, 8526, 257, 635, 361, 5377, 13, 16956, 11, 13003, 7433, 450, 7923, 85, 8372, 2617, 51546], "temperature": 0.0, "avg_logprob": -0.31407988801294445, "compression_ratio": 1.8586387434554974, "no_speech_prob": 0.07979270815849304}, {"id": 33, "seek": 21256, "start": 212.56, "end": 223.28, "text": " epsilon sobre y m\u00e1s uno a la jota. Epsilon sobre y m\u00e1s uno a la jota es la probabilidad", "tokens": [50364, 17889, 5473, 288, 3573, 8526, 257, 635, 361, 5377, 13, 462, 1878, 388, 266, 5473, 288, 3573, 8526, 257, 635, 361, 5377, 785, 635, 31959, 4580, 50900], "temperature": 0.0, "avg_logprob": -0.3061751937866211, "compression_ratio": 1.6761904761904762, "no_speech_prob": 0.035952068865299225}, {"id": 34, "seek": 21256, "start": 223.28, "end": 229.5, "text": " de data de una oraci\u00f3n en ingl\u00e9s, elegir cierta alineaci\u00f3n que yo voy a utilizar.", "tokens": [50900, 368, 1412, 368, 2002, 420, 3482, 465, 49766, 11, 14459, 347, 39769, 1328, 419, 533, 3482, 631, 5290, 7552, 257, 24060, 13, 51211], "temperature": 0.0, "avg_logprob": -0.3061751937866211, "compression_ratio": 1.6761904761904762, "no_speech_prob": 0.035952068865299225}, {"id": 35, "seek": 21256, "start": 229.5, "end": 236.84, "text": " Bien, ese fue el segundo paso. El tercer paso es una vez que se atengo la alineaci\u00f3n,", "tokens": [51211, 16956, 11, 10167, 9248, 806, 17954, 29212, 13, 2699, 38103, 29212, 785, 2002, 5715, 631, 369, 412, 30362, 635, 419, 533, 3482, 11, 51578], "temperature": 0.0, "avg_logprob": -0.3061751937866211, "compression_ratio": 1.6761904761904762, "no_speech_prob": 0.035952068865299225}, {"id": 36, "seek": 21256, "start": 236.84, "end": 240.64000000000001, "text": " voy mirando cada palabra de la dolin ingl\u00e9s y le voy poniendo una palabra correspondiente", "tokens": [51578, 7552, 3149, 1806, 8411, 31702, 368, 635, 360, 5045, 49766, 288, 476, 7552, 9224, 7304, 2002, 31702, 6805, 8413, 51768], "temperature": 0.0, "avg_logprob": -0.3061751937866211, "compression_ratio": 1.6761904761904762, "no_speech_prob": 0.035952068865299225}, {"id": 37, "seek": 24064, "start": 240.64, "end": 246.32, "text": " de la de espa\u00f1ol. Para ac\u00e1 voy a sumir que yo tengo una tabla de traducci\u00f3n, una tabla de", "tokens": [50364, 368, 635, 368, 31177, 13, 11107, 23496, 7552, 257, 2408, 347, 631, 5290, 13989, 2002, 4421, 875, 368, 2479, 1311, 5687, 11, 2002, 4421, 875, 368, 50648], "temperature": 0.0, "avg_logprob": -0.3088662028312683, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.1559859663248062}, {"id": 38, "seek": 24064, "start": 246.32, "end": 250.07999999999998, "text": " traducci\u00f3n que me dice que tiene de un lado todas las palabras en espa\u00f1ol y el otro lado", "tokens": [50648, 2479, 1311, 5687, 631, 385, 10313, 631, 7066, 368, 517, 11631, 10906, 2439, 35240, 465, 31177, 288, 806, 11921, 11631, 50836], "temperature": 0.0, "avg_logprob": -0.3088662028312683, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.1559859663248062}, {"id": 39, "seek": 24064, "start": 250.07999999999998, "end": 257.03999999999996, "text": " de las palabras en ingl\u00e9s, entonces mi tabla va a tener una forma como, por ejemplo,", "tokens": [50836, 368, 2439, 35240, 465, 49766, 11, 13003, 2752, 4421, 875, 2773, 257, 11640, 2002, 8366, 2617, 11, 1515, 13358, 11, 51184], "temperature": 0.0, "avg_logprob": -0.3088662028312683, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.1559859663248062}, {"id": 40, "seek": 24064, "start": 257.03999999999996, "end": 264.03999999999996, "text": " hace una tabla as\u00ed que de un lado decir las palabras en espa\u00f1ol como banco, perro,", "tokens": [51184, 10032, 2002, 4421, 875, 8582, 631, 368, 517, 11631, 10235, 2439, 35240, 465, 31177, 2617, 45498, 11, 680, 340, 11, 51534], "temperature": 0.0, "avg_logprob": -0.3088662028312683, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.1559859663248062}, {"id": 41, "seek": 26404, "start": 264.04, "end": 270.48, "text": " chato y m\u00e1s cosas y del otro lado va a tener las correspondientes en ingl\u00e9s como banco,", "tokens": [50364, 417, 2513, 288, 3573, 12218, 288, 1103, 11921, 11631, 2773, 257, 11640, 2439, 6805, 20135, 465, 49766, 2617, 45498, 11, 50686], "temperature": 0.0, "avg_logprob": -0.4293040084838867, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.2596082389354706}, {"id": 42, "seek": 26404, "start": 270.48, "end": 278.24, "text": " bench, cat, tri y m\u00e1s cosas. Y entonces esta tabla va a decir la probabilidad de traducir", "tokens": [50686, 10638, 11, 3857, 11, 1376, 288, 3573, 12218, 13, 398, 13003, 5283, 4421, 875, 2773, 257, 10235, 635, 31959, 4580, 368, 2479, 1311, 347, 51074], "temperature": 0.0, "avg_logprob": -0.4293040084838867, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.2596082389354706}, {"id": 43, "seek": 26404, "start": 278.24, "end": 280.84000000000003, "text": " una cosa en la botan. Entonces banco probablemente tenga cierta probabilidad para", "tokens": [51074, 2002, 10163, 465, 635, 10592, 282, 13, 15097, 45498, 21759, 4082, 36031, 39769, 1328, 31959, 4580, 1690, 51204], "temperature": 0.0, "avg_logprob": -0.4293040084838867, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.2596082389354706}, {"id": 44, "seek": 26404, "start": 280.84000000000003, "end": 292.0, "text": " avanzar y cierta probabilidad para bench, 0.4 y 0.6, 0.6 y para cat no da ninguna probabilidad", "tokens": [51204, 42444, 289, 288, 39769, 1328, 31959, 4580, 1690, 10638, 11, 1958, 13, 19, 288, 1958, 13, 21, 11, 1958, 13, 21, 288, 1690, 3857, 572, 1120, 36073, 31959, 4580, 51762], "temperature": 0.0, "avg_logprob": -0.4293040084838867, "compression_ratio": 1.733009708737864, "no_speech_prob": 0.2596082389354706}, {"id": 45, "seek": 29200, "start": 292.0, "end": 297.48, "text": " para tri tan poco y despu\u00e9s perro no va a tener nada esto, pero si despu\u00e9s y cat va a ser", "tokens": [50364, 1690, 1376, 7603, 10639, 288, 15283, 680, 340, 572, 2773, 257, 11640, 8096, 7433, 11, 4768, 1511, 15283, 288, 3857, 2773, 257, 816, 50638], "temperature": 0.0, "avg_logprob": -0.28769238141118264, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.06336429715156555}, {"id": 46, "seek": 29200, "start": 297.48, "end": 302.24, "text": " este no s\u00e9, 0.8 en este caso, etc\u00e9tera voy a tener una tabla bastante grande que tiene", "tokens": [50638, 4065, 572, 7910, 11, 1958, 13, 23, 465, 4065, 9666, 11, 5183, 526, 23833, 7552, 257, 11640, 2002, 4421, 875, 14651, 8883, 631, 7066, 50876], "temperature": 0.0, "avg_logprob": -0.28769238141118264, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.06336429715156555}, {"id": 47, "seek": 29200, "start": 302.24, "end": 311.48, "text": " toda la posibilidad de traducir una palabra como otra. Entonces, si yo tengo esa tabla lo", "tokens": [50876, 11687, 635, 1366, 33989, 368, 2479, 1311, 347, 2002, 31702, 2617, 13623, 13, 15097, 11, 1511, 5290, 13989, 11342, 4421, 875, 450, 51338], "temperature": 0.0, "avg_logprob": -0.28769238141118264, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.06336429715156555}, {"id": 48, "seek": 29200, "start": 311.48, "end": 318.72, "text": " que puedo decir es que la forma de calcular la probabilidad de esa oraci\u00f3n final que", "tokens": [51338, 631, 21612, 10235, 785, 631, 635, 8366, 368, 2104, 17792, 635, 31959, 4580, 368, 11342, 420, 3482, 2572, 631, 51700], "temperature": 0.0, "avg_logprob": -0.28769238141118264, "compression_ratio": 1.6181818181818182, "no_speech_prob": 0.06336429715156555}, {"id": 49, "seek": 31872, "start": 318.72, "end": 323.08000000000004, "text": " yo traduce va a depender de cu\u00e1les son las palabras que yo elija va a depender de cu\u00e1les son las", "tokens": [50364, 5290, 2479, 4176, 2773, 257, 1367, 3216, 368, 2702, 842, 904, 1872, 2439, 35240, 631, 5290, 806, 20642, 2773, 257, 1367, 3216, 368, 2702, 842, 904, 1872, 2439, 50582], "temperature": 0.0, "avg_logprob": -0.3610565941613, "compression_ratio": 1.8109452736318408, "no_speech_prob": 0.07594224810600281}, {"id": 50, "seek": 31872, "start": 323.08000000000004, "end": 330.92, "text": " palabras que yo haya puesto dentro de mi, de mi oraci\u00f3n para traducir. Entonces esa tabla que", "tokens": [50582, 35240, 631, 5290, 24693, 35136, 10856, 368, 2752, 11, 368, 2752, 420, 3482, 1690, 2479, 1311, 347, 13, 15097, 11342, 4421, 875, 631, 50974], "temperature": 0.0, "avg_logprob": -0.3610565941613, "compression_ratio": 1.8109452736318408, "no_speech_prob": 0.07594224810600281}, {"id": 51, "seek": 31872, "start": 330.92, "end": 336.8, "text": " est\u00e1 ah\u00ed definida le llamamos ac\u00e1 en la, en la, en la, la, aparece como T de f su x,", "tokens": [50974, 3192, 12571, 1561, 2887, 476, 16848, 2151, 23496, 465, 635, 11, 465, 635, 11, 465, 635, 11, 635, 11, 37863, 2617, 314, 368, 283, 459, 2031, 11, 51268], "temperature": 0.0, "avg_logprob": -0.3610565941613, "compression_ratio": 1.8109452736318408, "no_speech_prob": 0.07594224810600281}, {"id": 52, "seek": 31872, "start": 336.8, "end": 344.16, "text": " su y y dice que la probabilidad de traducir la palabra su y como f su x. Entonces,", "tokens": [51268, 459, 288, 288, 10313, 631, 635, 31959, 4580, 368, 2479, 1311, 347, 635, 31702, 459, 288, 2617, 283, 459, 2031, 13, 15097, 11, 51636], "temperature": 0.0, "avg_logprob": -0.3610565941613, "compression_ratio": 1.8109452736318408, "no_speech_prob": 0.07594224810600281}, {"id": 53, "seek": 34416, "start": 344.16, "end": 354.52000000000004, "text": " ac\u00e1 hay una cosa importante. Si tenemos la oraci\u00f3n en ingl\u00e9s, la oraci\u00f3n en ingl\u00e9s", "tokens": [50364, 23496, 4842, 2002, 10163, 9416, 13, 4909, 9914, 635, 420, 3482, 465, 49766, 11, 635, 420, 3482, 465, 49766, 50882], "temperature": 0.0, "avg_logprob": -0.3751164206975623, "compression_ratio": 1.740506329113924, "no_speech_prob": 0.044198717921972275}, {"id": 54, "seek": 34416, "start": 354.52000000000004, "end": 361.84000000000003, "text": " recuerdan que ten\u00eda las palabras, es su 1, es su 2, hasta de su 9, la oraci\u00f3n en espa\u00f1ol", "tokens": [50882, 39092, 10312, 631, 23718, 2439, 35240, 11, 785, 459, 502, 11, 785, 459, 568, 11, 10764, 368, 459, 1722, 11, 635, 420, 3482, 465, 31177, 51248], "temperature": 0.0, "avg_logprob": -0.3751164206975623, "compression_ratio": 1.740506329113924, "no_speech_prob": 0.044198717921972275}, {"id": 55, "seek": 34416, "start": 361.84000000000003, "end": 369.08000000000004, "text": " ten\u00eda las palabras, es su 1, f su 2, hasta de f su jota. Y eso ten\u00eda en el medio una funci\u00f3n", "tokens": [51248, 23718, 2439, 35240, 11, 785, 459, 502, 11, 283, 459, 568, 11, 10764, 368, 283, 459, 361, 5377, 13, 398, 7287, 23718, 465, 806, 22123, 2002, 43735, 51610], "temperature": 0.0, "avg_logprob": -0.3751164206975623, "compression_ratio": 1.740506329113924, "no_speech_prob": 0.044198717921972275}, {"id": 56, "seek": 36908, "start": 369.08, "end": 377.32, "text": " de la lineaci\u00f3n que me dec\u00eda que palabras se correspond\u00eda con cual. Entonces, no era su", "tokens": [50364, 368, 635, 1622, 3482, 631, 385, 37599, 631, 35240, 369, 6805, 2686, 416, 10911, 13, 15097, 11, 572, 4249, 459, 50776], "temperature": 0.0, "avg_logprob": -0.29403463224085363, "compression_ratio": 1.7329192546583851, "no_speech_prob": 0.058133214712142944}, {"id": 57, "seek": 36908, "start": 377.32, "end": 390.79999999999995, "text": " vene ni f su jota, era su y y f su jota grande. Esto era su y, esto era f su jota grande.", "tokens": [50776, 371, 1450, 3867, 283, 459, 361, 5377, 11, 4249, 459, 288, 288, 283, 459, 361, 5377, 8883, 13, 20880, 4249, 459, 288, 11, 7433, 4249, 283, 459, 361, 5377, 8883, 13, 51450], "temperature": 0.0, "avg_logprob": -0.29403463224085363, "compression_ratio": 1.7329192546583851, "no_speech_prob": 0.058133214712142944}, {"id": 58, "seek": 36908, "start": 390.79999999999995, "end": 398.2, "text": " Entonces, si yo tengo una palabra cualquiera dentro de la oraci\u00f3n en espa\u00f1ol, tengo un f su jota", "tokens": [51450, 15097, 11, 1511, 5290, 13989, 2002, 31702, 10911, 35134, 10856, 368, 635, 420, 3482, 465, 31177, 11, 13989, 517, 283, 459, 361, 5377, 51820], "temperature": 0.0, "avg_logprob": -0.29403463224085363, "compression_ratio": 1.7329192546583851, "no_speech_prob": 0.058133214712142944}, {"id": 59, "seek": 39820, "start": 398.2, "end": 405.2, "text": " de chica dentro de la oraci\u00f3n en espa\u00f1ol. Esto se va a corresponder con alg\u00fan f su y chica en la", "tokens": [50364, 368, 417, 2262, 10856, 368, 635, 420, 3482, 465, 31177, 13, 20880, 369, 2773, 257, 6805, 260, 416, 26300, 283, 459, 288, 417, 2262, 465, 635, 50714], "temperature": 0.0, "avg_logprob": -0.39759315982941656, "compression_ratio": 1.777327935222672, "no_speech_prob": 0.04867613688111305}, {"id": 60, "seek": 39820, "start": 405.2, "end": 409.76, "text": " oraci\u00f3n en ingl\u00e9s, digamos. Yo s\u00e9 que esto se cumble por la funci\u00f3n de la lineaci\u00f3n", "tokens": [50714, 420, 3482, 465, 49766, 11, 36430, 13, 7616, 7910, 631, 7433, 369, 12713, 638, 1515, 635, 43735, 368, 635, 1622, 3482, 50942], "temperature": 0.0, "avg_logprob": -0.39759315982941656, "compression_ratio": 1.777327935222672, "no_speech_prob": 0.04867613688111305}, {"id": 61, "seek": 39820, "start": 409.76, "end": 412.56, "text": " porque agarra y mape a todas las palabras que est\u00e1n en espa\u00f1ol con algo que estaba", "tokens": [50942, 4021, 623, 289, 424, 288, 463, 494, 257, 10906, 2439, 35240, 631, 10368, 465, 31177, 416, 8655, 631, 17544, 51082], "temperature": 0.0, "avg_logprob": -0.39759315982941656, "compression_ratio": 1.777327935222672, "no_speech_prob": 0.04867613688111305}, {"id": 62, "seek": 39820, "start": 412.56, "end": 417.88, "text": " a la dole ingl\u00e9s. Potencialmente con el doque en vac\u00edo, no olvides.", "tokens": [51082, 257, 635, 360, 306, 49766, 13, 9145, 26567, 4082, 416, 806, 360, 1077, 465, 2842, 20492, 11, 572, 43851, 1875, 13, 51348], "temperature": 0.0, "avg_logprob": -0.39759315982941656, "compression_ratio": 1.777327935222672, "no_speech_prob": 0.04867613688111305}, {"id": 63, "seek": 39820, "start": 417.88, "end": 422.44, "text": " Bien, entonces, tengo una palabra de la dole espa\u00f1ol que es f su jota y una palabra de la dole", "tokens": [51348, 16956, 11, 13003, 11, 13989, 2002, 31702, 368, 635, 360, 306, 31177, 631, 785, 283, 459, 361, 5377, 288, 2002, 31702, 368, 635, 360, 306, 51576], "temperature": 0.0, "avg_logprob": -0.39759315982941656, "compression_ratio": 1.777327935222672, "no_speech_prob": 0.04867613688111305}, {"id": 64, "seek": 42244, "start": 422.44, "end": 427.96, "text": " ingl\u00e9s que es f su y. \u00bfCu\u00e1l es la relaci\u00f3n entre ese jota y ese y? \u00bfC\u00f3mo es la relaci\u00f3n", "tokens": [50364, 49766, 631, 785, 283, 459, 288, 13, 3841, 35222, 11447, 785, 635, 37247, 3962, 10167, 361, 5377, 288, 10167, 288, 30, 3841, 28342, 785, 635, 37247, 50640], "temperature": 0.0, "avg_logprob": -0.34627703011754046, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.16391268372535706}, {"id": 65, "seek": 42244, "start": 427.96, "end": 443.48, "text": " entre s\u00ed? Tiamos. Yo puedo decir que el i es igual a algo de jota. La buena manera.", "tokens": [50640, 3962, 8600, 30, 314, 2918, 329, 13, 7616, 21612, 10235, 631, 806, 741, 785, 10953, 257, 8655, 368, 361, 5377, 13, 2369, 25710, 13913, 13, 51416], "temperature": 0.0, "avg_logprob": -0.34627703011754046, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.16391268372535706}, {"id": 66, "seek": 42244, "start": 443.48, "end": 447.92, "text": " La funci\u00f3n de la lineaci\u00f3n, ah\u00ed est\u00e1. O sea, el i es igual a la funci\u00f3n de la lineaci\u00f3n", "tokens": [51416, 2369, 43735, 368, 635, 1622, 3482, 11, 12571, 3192, 13, 422, 4158, 11, 806, 741, 785, 10953, 257, 635, 43735, 368, 635, 1622, 3482, 51638], "temperature": 0.0, "avg_logprob": -0.34627703011754046, "compression_ratio": 1.644578313253012, "no_speech_prob": 0.16391268372535706}, {"id": 67, "seek": 44792, "start": 447.92, "end": 455.08000000000004, "text": " aplicada jota. Como la i, el \u00edndice de este ac\u00e1 es igual a la funci\u00f3n de la lineaci\u00f3n", "tokens": [50364, 18221, 1538, 361, 5377, 13, 11913, 635, 741, 11, 806, 18645, 273, 573, 368, 4065, 23496, 785, 10953, 257, 635, 43735, 368, 635, 1622, 3482, 50722], "temperature": 0.0, "avg_logprob": -0.28955712091355096, "compression_ratio": 1.8526315789473684, "no_speech_prob": 0.14636418223381042}, {"id": 68, "seek": 44792, "start": 455.08000000000004, "end": 463.32, "text": " aplicada jota. Entonces, yo puedo decir que la palabra su i es igual a la palabra su", "tokens": [50722, 18221, 1538, 361, 5377, 13, 15097, 11, 5290, 21612, 10235, 631, 635, 31702, 459, 741, 785, 10953, 257, 635, 31702, 459, 51134], "temperature": 0.0, "avg_logprob": -0.28955712091355096, "compression_ratio": 1.8526315789473684, "no_speech_prob": 0.14636418223381042}, {"id": 69, "seek": 44792, "start": 463.32, "end": 468.44, "text": " a su jota. As\u00ed que puedo decir que en realidad los que est\u00e1n alineados son la palabra", "tokens": [51134, 257, 459, 361, 5377, 13, 17419, 631, 21612, 10235, 631, 465, 25635, 1750, 631, 10368, 419, 533, 4181, 1872, 635, 31702, 51390], "temperature": 0.0, "avg_logprob": -0.28955712091355096, "compression_ratio": 1.8526315789473684, "no_speech_prob": 0.14636418223381042}, {"id": 70, "seek": 44792, "start": 468.44, "end": 475.0, "text": " f su jota est\u00e1 alineada con la palabra y su a su jota. Y ah\u00ed me sacqu\u00e9 el i de encima,", "tokens": [51390, 283, 459, 361, 5377, 3192, 419, 533, 1538, 416, 635, 31702, 288, 459, 257, 459, 361, 5377, 13, 398, 12571, 385, 4899, 16412, 806, 741, 368, 40265, 11, 51718], "temperature": 0.0, "avg_logprob": -0.28955712091355096, "compression_ratio": 1.8526315789473684, "no_speech_prob": 0.14636418223381042}, {"id": 71, "seek": 47500, "start": 475.0, "end": 481.2, "text": " digamos, simplemente y te eros sobre las palabras y te erando sobre la jota puedo establecer", "tokens": [50364, 36430, 11, 33190, 288, 535, 1189, 329, 5473, 2439, 35240, 288, 535, 1189, 1806, 5473, 635, 361, 5377, 21612, 37444, 1776, 50674], "temperature": 0.0, "avg_logprob": -0.2709738358207371, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.0657174214720726}, {"id": 72, "seek": 47500, "start": 481.2, "end": 490.16, "text": " la correspondencia entre las dos palabras. Y eso es un poco lo que dice ac\u00e1 para terminar", "tokens": [50674, 635, 6805, 10974, 3962, 2439, 4491, 35240, 13, 398, 7287, 785, 517, 10639, 450, 631, 10313, 23496, 1690, 36246, 51122], "temperature": 0.0, "avg_logprob": -0.2709738358207371, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.0657174214720726}, {"id": 73, "seek": 47500, "start": 490.16, "end": 493.36, "text": " de armar lo que es el modelo de traducci\u00f3n. Para terminar de armar el modelo de traducci\u00f3n", "tokens": [51122, 368, 3726, 289, 450, 631, 785, 806, 27825, 368, 2479, 1311, 5687, 13, 11107, 36246, 368, 3726, 289, 806, 27825, 368, 2479, 1311, 5687, 51282], "temperature": 0.0, "avg_logprob": -0.2709738358207371, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.0657174214720726}, {"id": 74, "seek": 47500, "start": 493.36, "end": 497.24, "text": " dicen que en el tercer paso yo voy a elegir cu\u00e1les son las palabras. Entonces, lo que", "tokens": [51282, 33816, 631, 465, 806, 38103, 29212, 5290, 7552, 257, 14459, 347, 2702, 842, 904, 1872, 2439, 35240, 13, 15097, 11, 450, 631, 51476], "temperature": 0.0, "avg_logprob": -0.2709738358207371, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.0657174214720726}, {"id": 75, "seek": 47500, "start": 497.24, "end": 503.92, "text": " voy a hacer es iterar sobre todas las palabras y haciendo el producto de todas las", "tokens": [51476, 7552, 257, 6720, 785, 17138, 289, 5473, 10906, 2439, 35240, 288, 20509, 806, 47583, 368, 10906, 2439, 51810], "temperature": 0.0, "avg_logprob": -0.2709738358207371, "compression_ratio": 1.8818565400843883, "no_speech_prob": 0.0657174214720726}, {"id": 76, "seek": 50392, "start": 504.0, "end": 509.44, "text": " las probabilidades. O sea, el producto de dado que yo ten\u00eda la palabra f su jota,", "tokens": [50368, 2439, 31959, 10284, 13, 422, 4158, 11, 806, 47583, 368, 29568, 631, 5290, 23718, 635, 31702, 283, 459, 361, 5377, 11, 50640], "temperature": 0.0, "avg_logprob": -0.36912649054276314, "compression_ratio": 1.7, "no_speech_prob": 0.02806909941136837}, {"id": 77, "seek": 50392, "start": 509.44, "end": 514.8000000000001, "text": " pero dado que su ten\u00eda la palabra eso va su jota en ingl\u00e9s. Entonces, elegir la palabra f su jota", "tokens": [50640, 4768, 29568, 631, 459, 23718, 635, 31702, 7287, 2773, 459, 361, 5377, 465, 49766, 13, 15097, 11, 14459, 347, 635, 31702, 283, 459, 361, 5377, 50908], "temperature": 0.0, "avg_logprob": -0.36912649054276314, "compression_ratio": 1.7, "no_speech_prob": 0.02806909941136837}, {"id": 78, "seek": 50392, "start": 514.8000000000001, "end": 521.12, "text": " en espa\u00f1ol. Eso haga una productoria con todos los valores de las distintas palabras.", "tokens": [50908, 465, 31177, 13, 27795, 46726, 2002, 1674, 8172, 416, 6321, 1750, 38790, 368, 2439, 31489, 296, 35240, 13, 51224], "temperature": 0.0, "avg_logprob": -0.36912649054276314, "compression_ratio": 1.7, "no_speech_prob": 0.02806909941136837}, {"id": 79, "seek": 50392, "start": 523.6800000000001, "end": 531.6800000000001, "text": " Bien, entonces ah\u00ed, llegue a el \u00faltimo de los valores que quer\u00eda calcular, que es la", "tokens": [51352, 16956, 11, 13003, 12571, 11, 11234, 622, 257, 806, 21013, 368, 1750, 38790, 631, 37869, 2104, 17792, 11, 631, 785, 635, 51752], "temperature": 0.0, "avg_logprob": -0.36912649054276314, "compression_ratio": 1.7, "no_speech_prob": 0.02806909941136837}, {"id": 80, "seek": 53168, "start": 531.68, "end": 542.7199999999999, "text": " probabilidad de f dado que conozco. Ah\u00ed es igual a la productoria con jota igual uno hasta", "tokens": [50364, 31959, 4580, 368, 283, 29568, 631, 416, 15151, 1291, 13, 49924, 785, 10953, 257, 635, 1674, 8172, 416, 361, 5377, 10953, 8526, 10764, 50916], "temperature": 0.0, "avg_logprob": -0.3569245282341452, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.04294171556830406}, {"id": 81, "seek": 53168, "start": 542.7199999999999, "end": 550.68, "text": " jota grande, de el valor de la tabla de traducci\u00f3n, que es de su f su jota, t de f su jota", "tokens": [50916, 361, 5377, 8883, 11, 368, 806, 15367, 368, 635, 4421, 875, 368, 2479, 1311, 5687, 11, 631, 785, 368, 459, 283, 459, 361, 5377, 11, 256, 368, 283, 459, 361, 5377, 51314], "temperature": 0.0, "avg_logprob": -0.3569245282341452, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.04294171556830406}, {"id": 82, "seek": 53168, "start": 550.68, "end": 561.64, "text": " y su vasu jota. Bueno, ta. Entonces, ah\u00ed tengo como en cada paso fui calculando cosas", "tokens": [51314, 288, 459, 11481, 84, 361, 5377, 13, 16046, 11, 1846, 13, 15097, 11, 12571, 13989, 2617, 465, 8411, 29212, 27863, 4322, 1806, 12218, 51862], "temperature": 0.0, "avg_logprob": -0.3569245282341452, "compression_ratio": 1.5168539325842696, "no_speech_prob": 0.04294171556830406}, {"id": 83, "seek": 56164, "start": 561.84, "end": 567.6, "text": " este se correspond\u00eda al paso uno del modelo, paso uno, este se corresponde con el paso del modelo.", "tokens": [50374, 4065, 369, 6805, 2686, 419, 29212, 8526, 1103, 27825, 11, 29212, 8526, 11, 4065, 369, 6805, 68, 416, 806, 29212, 1103, 27825, 13, 50662], "temperature": 0.0, "avg_logprob": -0.36069920298817393, "compression_ratio": 1.890625, "no_speech_prob": 0.05183481052517891}, {"id": 84, "seek": 56164, "start": 567.6, "end": 571.68, "text": " En realidad, este ya tiene el paso uno del paso dos juntos porque ella tengo el epsilon ac\u00e1 y este", "tokens": [50662, 2193, 25635, 11, 4065, 2478, 7066, 806, 29212, 8526, 1103, 29212, 4491, 33868, 4021, 18823, 13989, 806, 17889, 23496, 288, 4065, 50866], "temperature": 0.0, "avg_logprob": -0.36069920298817393, "compression_ratio": 1.890625, "no_speech_prob": 0.05183481052517891}, {"id": 85, "seek": 56164, "start": 571.68, "end": 577.8, "text": " se corresponde con el paso tres del modelo. El paso tres de la historia de generaci\u00f3n.", "tokens": [50866, 369, 6805, 68, 416, 806, 29212, 15890, 1103, 27825, 13, 2699, 29212, 15890, 368, 635, 18385, 368, 1337, 3482, 13, 51172], "temperature": 0.0, "avg_logprob": -0.36069920298817393, "compression_ratio": 1.890625, "no_speech_prob": 0.05183481052517891}, {"id": 86, "seek": 56164, "start": 579.8, "end": 586.16, "text": " Mi objetivo con todos estos valores que est\u00e1n ac\u00e1 es calcular pdf de hoy.", "tokens": [51272, 10204, 29809, 416, 6321, 12585, 38790, 631, 10368, 23496, 785, 2104, 17792, 280, 45953, 368, 13775, 13, 51590], "temperature": 0.0, "avg_logprob": -0.36069920298817393, "compression_ratio": 1.890625, "no_speech_prob": 0.05183481052517891}, {"id": 87, "seek": 58616, "start": 586.24, "end": 595.16, "text": " \u00bfQu\u00e9 parametro sin traduje? \u00bfQu\u00e9 parametro fueron surgiendo a medida que se iba", "tokens": [50368, 3841, 15137, 6220, 302, 340, 3343, 2479, 84, 2884, 30, 3841, 15137, 6220, 302, 340, 28739, 19560, 7304, 257, 32984, 631, 369, 33423, 50814], "temperature": 0.0, "avg_logprob": -0.39625613920150266, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.11648421734571457}, {"id": 88, "seek": 58616, "start": 595.16, "end": 598.4, "text": " y derando sobre estos pasos? Bueno, en primer lugar, el epsilon aquel que estaba", "tokens": [50814, 288, 1163, 1806, 5473, 12585, 1736, 329, 30, 16046, 11, 465, 12595, 11467, 11, 806, 17889, 2373, 338, 631, 17544, 50976], "temperature": 0.0, "avg_logprob": -0.39625613920150266, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.11648421734571457}, {"id": 89, "seek": 58616, "start": 598.4, "end": 602.56, "text": " moviendo, este es un valor que yo tendr\u00eda que estimar a partir de mirar en los corcos,", "tokens": [50976, 2402, 7304, 11, 4065, 785, 517, 15367, 631, 5290, 3928, 37183, 631, 8017, 289, 257, 13906, 368, 3149, 289, 465, 1750, 1181, 6877, 11, 51184], "temperature": 0.0, "avg_logprob": -0.39625613920150266, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.11648421734571457}, {"id": 90, "seek": 58616, "start": 602.56, "end": 608.1999999999999, "text": " como son los largos y las oraciones relativos. Y el otro parametro importante es aquella", "tokens": [51184, 2617, 1872, 1750, 11034, 329, 288, 2439, 420, 9188, 21960, 329, 13, 398, 806, 11921, 6220, 302, 340, 9416, 785, 2373, 9885, 51466], "temperature": 0.0, "avg_logprob": -0.39625613920150266, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.11648421734571457}, {"id": 91, "seek": 58616, "start": 608.1999999999999, "end": 611.92, "text": " tabla all\u00e1, aquella tabla de traducci\u00f3n es que me dice banco, con que probabilidad lo", "tokens": [51466, 4421, 875, 30642, 11, 2373, 9885, 4421, 875, 368, 2479, 1311, 5687, 785, 631, 385, 10313, 45498, 11, 416, 631, 31959, 4580, 450, 51652], "temperature": 0.0, "avg_logprob": -0.39625613920150266, "compression_ratio": 1.6374045801526718, "no_speech_prob": 0.11648421734571457}, {"id": 92, "seek": 61192, "start": 611.92, "end": 615.92, "text": " puede traducir como banco y como que probabilidad lo puede traducir como v\u00e9ns, etc\u00e9tera, etc\u00e9tera.", "tokens": [50364, 8919, 2479, 1311, 347, 2617, 45498, 288, 2617, 631, 31959, 4580, 450, 8919, 2479, 1311, 347, 2617, 371, 3516, 82, 11, 5183, 526, 23833, 11, 5183, 526, 23833, 13, 50564], "temperature": 0.0, "avg_logprob": -0.3820650870339912, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.040994904935359955}, {"id": 93, "seek": 61192, "start": 615.92, "end": 620.68, "text": " Esta tabla en realidad es un parametro del modelo, es un parametro el sistema que si yo lo tuviera,", "tokens": [50564, 20547, 4421, 875, 465, 25635, 785, 517, 6220, 302, 340, 1103, 27825, 11, 785, 517, 6220, 302, 340, 806, 13245, 631, 1511, 5290, 450, 38177, 10609, 11, 50802], "temperature": 0.0, "avg_logprob": -0.3820650870339912, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.040994904935359955}, {"id": 94, "seek": 61192, "start": 620.68, "end": 626.64, "text": " me alcanzar\u00eda con eso para poder construirme este modelo y calcular la probabilidad de cualquier", "tokens": [50802, 385, 50200, 21178, 416, 7287, 1690, 8152, 38445, 1398, 4065, 27825, 288, 2104, 17792, 635, 31959, 4580, 368, 21004, 51100], "temperature": 0.0, "avg_logprob": -0.3820650870339912, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.040994904935359955}, {"id": 95, "seek": 61192, "start": 626.64, "end": 627.5999999999999, "text": " par de operaciones.", "tokens": [51100, 971, 368, 2208, 9188, 13, 51148], "temperature": 0.0, "avg_logprob": -0.3820650870339912, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.040994904935359955}, {"id": 96, "seek": 61192, "start": 632.5999999999999, "end": 638.8399999999999, "text": " Bien, y entonces, antes de continuar, vamos a terminar de armar cu\u00e1l es la imagen de esto,", "tokens": [51398, 16956, 11, 288, 13003, 11, 11014, 368, 29980, 11, 5295, 257, 36246, 368, 3726, 289, 44318, 785, 635, 40652, 368, 7433, 11, 51710], "temperature": 0.0, "avg_logprob": -0.3820650870339912, "compression_ratio": 1.6983471074380165, "no_speech_prob": 0.040994904935359955}, {"id": 97, "seek": 63884, "start": 639.08, "end": 646.84, "text": " que es decir, yo en realidad lo quer\u00eda calcular era pdf da doe, que eso va a ser mi modelo de traducci\u00f3n", "tokens": [50376, 631, 785, 10235, 11, 5290, 465, 25635, 450, 37869, 2104, 17792, 4249, 280, 45953, 1120, 360, 68, 11, 631, 7287, 2773, 257, 816, 2752, 27825, 368, 2479, 1311, 5687, 50764], "temperature": 0.0, "avg_logprob": -0.34492645707241326, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.020443342626094818}, {"id": 98, "seek": 63884, "start": 646.84, "end": 652.84, "text": " y de hecho va a ser el encargado de medida de ecuaci\u00f3n de una frase, pdf da doe lo puedo calcular", "tokens": [50764, 288, 368, 13064, 2773, 257, 816, 806, 2058, 289, 30135, 368, 32984, 368, 11437, 84, 3482, 368, 2002, 38406, 11, 280, 45953, 1120, 360, 68, 450, 21612, 2104, 17792, 51064], "temperature": 0.0, "avg_logprob": -0.34492645707241326, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.020443342626094818}, {"id": 99, "seek": 63884, "start": 652.84, "end": 657.64, "text": " con esta descomposici\u00f3n de pasos que dice ac\u00e1 en realidad porque luego de la siguiente manera.", "tokens": [51064, 416, 5283, 730, 21541, 329, 15534, 368, 1736, 329, 631, 10313, 23496, 465, 25635, 4021, 17222, 368, 635, 25666, 13913, 13, 51304], "temperature": 0.0, "avg_logprob": -0.34492645707241326, "compression_ratio": 1.6324324324324324, "no_speech_prob": 0.020443342626094818}, {"id": 100, "seek": 66884, "start": 669.8000000000001, "end": 681.8000000000001, "text": " Yo quiero calcular pdf da doe, y entonces voy a mirar lo que dice ac\u00e1 pdf da doe, es igual a la sumatoria", "tokens": [50412, 7616, 16811, 2104, 17792, 280, 45953, 1120, 360, 68, 11, 288, 13003, 7552, 257, 3149, 289, 450, 631, 10313, 23496, 280, 45953, 1120, 360, 68, 11, 785, 10953, 257, 635, 2408, 1639, 654, 51012], "temperature": 0.0, "avg_logprob": -0.405657080716865, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.06395509093999863}, {"id": 101, "seek": 66884, "start": 681.8000000000001, "end": 690.9200000000001, "text": " en la pdf da doe, que significa eso que para traducir en la generaci\u00f3n en espa\u00f1ol y una versi\u00f3n", "tokens": [51012, 465, 635, 280, 45953, 1120, 360, 68, 11, 631, 19957, 7287, 631, 1690, 2479, 1311, 347, 465, 635, 1337, 3482, 465, 31177, 288, 2002, 47248, 51468], "temperature": 0.0, "avg_logprob": -0.405657080716865, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.06395509093999863}, {"id": 102, "seek": 66884, "start": 690.9200000000001, "end": 695.84, "text": " en ingl\u00e9s o m\u00e1s bien para la situaci\u00f3n, para traducir en una generaci\u00f3n en espa\u00f1ol,", "tokens": [51468, 465, 49766, 277, 3573, 3610, 1690, 635, 29343, 11, 1690, 2479, 1311, 347, 465, 2002, 1337, 3482, 465, 31177, 11, 51714], "temperature": 0.0, "avg_logprob": -0.405657080716865, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.06395509093999863}, {"id": 103, "seek": 69584, "start": 695.84, "end": 701.76, "text": " hay muchas formas de alinear las palabras en el ingl\u00e9s en espa\u00f1ol y una vez que yo eleg\u00ed una forma", "tokens": [50364, 4842, 16072, 33463, 368, 419, 533, 289, 2439, 35240, 465, 806, 49766, 465, 31177, 288, 2002, 5715, 631, 5290, 14459, 870, 2002, 8366, 50660], "temperature": 0.0, "avg_logprob": -0.298948974609375, "compression_ratio": 2.0883534136546187, "no_speech_prob": 0.06028033792972565}, {"id": 104, "seek": 69584, "start": 701.76, "end": 705.52, "text": " alinear, hay muchas formas de elegir las palabras que vienen despu\u00e9s de vamos a mirar a trav\u00e9s de", "tokens": [50660, 419, 533, 289, 11, 4842, 16072, 33463, 368, 14459, 347, 2439, 35240, 631, 49298, 15283, 368, 5295, 257, 3149, 289, 257, 24463, 368, 50848], "temperature": 0.0, "avg_logprob": -0.298948974609375, "compression_ratio": 2.0883534136546187, "no_speech_prob": 0.06028033792972565}, {"id": 105, "seek": 69584, "start": 705.52, "end": 711.8000000000001, "text": " traducci\u00f3n y capaz que hay varias maneras de elegir distintas palabras. Entonces lo que eso significa es que", "tokens": [50848, 2479, 1311, 5687, 288, 35453, 631, 4842, 37496, 587, 6985, 368, 14459, 347, 31489, 296, 35240, 13, 15097, 450, 631, 7287, 19957, 785, 631, 51162], "temperature": 0.0, "avg_logprob": -0.298948974609375, "compression_ratio": 2.0883534136546187, "no_speech_prob": 0.06028033792972565}, {"id": 106, "seek": 69584, "start": 711.8000000000001, "end": 716.96, "text": " no existe una sola manera de traducir una versi\u00f3n en ingl\u00e9s a una versi\u00f3n espa\u00f1ol. Yo puedo encontrar", "tokens": [51162, 572, 16304, 2002, 34162, 13913, 368, 2479, 1311, 347, 2002, 47248, 465, 49766, 257, 2002, 47248, 31177, 13, 7616, 21612, 17525, 51420], "temperature": 0.0, "avg_logprob": -0.298948974609375, "compression_ratio": 2.0883534136546187, "no_speech_prob": 0.06028033792972565}, {"id": 107, "seek": 69584, "start": 716.96, "end": 721.2, "text": " varias formas de alinear las palabras si dar\u00edas formas de elegir las palabras de manera de que muchas", "tokens": [51420, 37496, 33463, 368, 419, 533, 289, 2439, 35240, 1511, 4072, 10025, 33463, 368, 14459, 347, 2439, 35240, 368, 13913, 368, 631, 16072, 51632], "temperature": 0.0, "avg_logprob": -0.298948974609375, "compression_ratio": 2.0883534136546187, "no_speech_prob": 0.06028033792972565}, {"id": 108, "seek": 72120, "start": 721.2, "end": 729.12, "text": " alineaciones son posibles. Entonces para saber cu\u00e1l es la probabilidad de traducir de traducir F da doe.", "tokens": [50364, 419, 533, 9188, 1872, 1366, 14428, 13, 15097, 1690, 12489, 44318, 785, 635, 31959, 4580, 368, 2479, 1311, 347, 368, 2479, 1311, 347, 479, 1120, 360, 68, 13, 50760], "temperature": 0.0, "avg_logprob": -0.3588037150246756, "compression_ratio": 2.0454545454545454, "no_speech_prob": 0.10420433431863785}, {"id": 109, "seek": 72120, "start": 730.4000000000001, "end": 735.4000000000001, "text": " Entonces yo voy a tener que sumar sobre todas las alineaciones posibles, sobre todas las formas de alinear las", "tokens": [50824, 15097, 5290, 7552, 257, 11640, 631, 2408, 289, 5473, 10906, 2439, 419, 533, 9188, 1366, 14428, 11, 5473, 10906, 2439, 33463, 368, 419, 533, 289, 2439, 51074], "temperature": 0.0, "avg_logprob": -0.3588037150246756, "compression_ratio": 2.0454545454545454, "no_speech_prob": 0.10420433431863785}, {"id": 110, "seek": 72120, "start": 735.4000000000001, "end": 741.2800000000001, "text": " dos oraciones FI, voy a tener que ir a ir a ir sobre eso y para cada una voy a tener que acular la probabilidad", "tokens": [51074, 4491, 420, 9188, 479, 40, 11, 7552, 257, 11640, 631, 3418, 257, 3418, 257, 3418, 5473, 7287, 288, 1690, 8411, 2002, 7552, 257, 11640, 631, 696, 1040, 635, 31959, 4580, 51368], "temperature": 0.0, "avg_logprob": -0.3588037150246756, "compression_ratio": 2.0454545454545454, "no_speech_prob": 0.10420433431863785}, {"id": 111, "seek": 72120, "start": 741.2800000000001, "end": 746.6, "text": " partial. Entonces, digamos, yo tengo cinco formas alinear las dos oraciones,", "tokens": [51368, 14641, 13, 15097, 11, 36430, 11, 5290, 13989, 21350, 33463, 419, 533, 289, 2439, 4491, 420, 9188, 11, 51634], "temperature": 0.0, "avg_logprob": -0.3588037150246756, "compression_ratio": 2.0454545454545454, "no_speech_prob": 0.10420433431863785}, {"id": 112, "seek": 74660, "start": 747.28, "end": 751.0, "text": " cinco es un n\u00famero un poco raro, pero digamos tengo eneformas de alinear las dos oraciones.", "tokens": [50398, 21350, 785, 517, 14959, 517, 10639, 367, 9708, 11, 4768, 36430, 13989, 465, 68, 837, 296, 368, 419, 533, 289, 2439, 4491, 420, 9188, 13, 50584], "temperature": 0.0, "avg_logprob": -0.38377681676892267, "compression_ratio": 2.0233463035019454, "no_speech_prob": 0.06626078486442566}, {"id": 113, "seek": 74660, "start": 751.8000000000001, "end": 758.16, "text": " Voy a tener que mirar bueno para la primera alineaci\u00f3n cu\u00e1l es la probabilidad de encontrar la", "tokens": [50624, 25563, 257, 11640, 631, 3149, 289, 11974, 1690, 635, 17382, 419, 533, 3482, 44318, 785, 635, 31959, 4580, 368, 17525, 635, 50942], "temperature": 0.0, "avg_logprob": -0.38377681676892267, "compression_ratio": 2.0233463035019454, "no_speech_prob": 0.06626078486442566}, {"id": 114, "seek": 74660, "start": 758.16, "end": 761.5600000000001, "text": " oraci\u00f3n F para la segunda alineaci\u00f3n cu\u00e1l es la probabilidad de encontrar la oraci\u00f3n F para la tercera", "tokens": [50942, 420, 3482, 479, 1690, 635, 21978, 419, 533, 3482, 44318, 785, 635, 31959, 4580, 368, 17525, 635, 420, 3482, 479, 1690, 635, 1796, 41034, 51112], "temperature": 0.0, "avg_logprob": -0.38377681676892267, "compression_ratio": 2.0233463035019454, "no_speech_prob": 0.06626078486442566}, {"id": 115, "seek": 74660, "start": 761.5600000000001, "end": 767.9200000000001, "text": " oraci\u00f3n y as\u00ed hasta llegar al final y agarr\u00f3 y sumo todo eso. Eso lo puedo hacer porque las alineaciones son", "tokens": [51112, 420, 3482, 288, 8582, 10764, 24892, 419, 2572, 288, 623, 2284, 812, 288, 2408, 78, 5149, 7287, 13, 27795, 450, 21612, 6720, 4021, 2439, 419, 533, 9188, 1872, 51430], "temperature": 0.0, "avg_logprob": -0.38377681676892267, "compression_ratio": 2.0233463035019454, "no_speech_prob": 0.06626078486442566}, {"id": 116, "seek": 74660, "start": 767.9200000000001, "end": 771.96, "text": " una descomposici\u00f3n de la espacio de probabilidad, en realidad yo puedo descomponar el espacio de probabilidad,", "tokens": [51430, 2002, 730, 21541, 329, 15534, 368, 635, 7089, 326, 1004, 368, 31959, 4580, 11, 465, 25635, 5290, 21612, 7471, 8586, 266, 289, 806, 33845, 368, 31959, 4580, 11, 51632], "temperature": 0.0, "avg_logprob": -0.38377681676892267, "compression_ratio": 2.0233463035019454, "no_speech_prob": 0.06626078486442566}, {"id": 117, "seek": 77196, "start": 771.96, "end": 777.76, "text": " en pedacitos disjuntos y cada alineaci\u00f3n va a ser uno de ellos. As\u00ed que digamos que para", "tokens": [50364, 465, 5670, 326, 11343, 717, 73, 2760, 329, 288, 8411, 419, 533, 3482, 2773, 257, 816, 8526, 368, 16353, 13, 17419, 631, 36430, 631, 1690, 50654], "temperature": 0.0, "avg_logprob": -0.28261429298925034, "compression_ratio": 1.792, "no_speech_prob": 0.05178046226501465}, {"id": 118, "seek": 77196, "start": 777.76, "end": 782.36, "text": " cagular el modelo de traducci\u00f3n, pede F da doe, necesito sumar sobre todas las alineaciones posibles.", "tokens": [50654, 269, 559, 1040, 806, 27825, 368, 2479, 1311, 5687, 11, 280, 4858, 479, 1120, 360, 68, 11, 11909, 3528, 2408, 289, 5473, 10906, 2439, 419, 533, 9188, 1366, 14428, 13, 50884], "temperature": 0.0, "avg_logprob": -0.28261429298925034, "compression_ratio": 1.792, "no_speech_prob": 0.05178046226501465}, {"id": 119, "seek": 77196, "start": 783.36, "end": 787.2, "text": " Ahora, lo que me falta es saber c\u00f3mo calculo este valor ac\u00e1.", "tokens": [50934, 18840, 11, 450, 631, 385, 22111, 785, 12489, 12826, 4322, 78, 4065, 15367, 23496, 13, 51126], "temperature": 0.0, "avg_logprob": -0.28261429298925034, "compression_ratio": 1.792, "no_speech_prob": 0.05178046226501465}, {"id": 120, "seek": 77196, "start": 788.2, "end": 794.48, "text": " As\u00ed que lo que estoy diciendo es que la probabilidad de F da doe es la suma sobre las alineaciones", "tokens": [51176, 17419, 631, 450, 631, 15796, 42797, 785, 631, 635, 31959, 4580, 368, 479, 1120, 360, 68, 785, 635, 2408, 64, 5473, 2439, 419, 533, 9188, 51490], "temperature": 0.0, "avg_logprob": -0.28261429298925034, "compression_ratio": 1.792, "no_speech_prob": 0.05178046226501465}, {"id": 121, "seek": 77196, "start": 794.48, "end": 800.96, "text": " de la probabilidad de F y esa alineaci\u00f3n da doe. Eso es simplemente lo que dice ah\u00ed en la", "tokens": [51490, 368, 635, 31959, 4580, 368, 479, 288, 11342, 419, 533, 3482, 1120, 360, 68, 13, 27795, 785, 33190, 450, 631, 10313, 12571, 465, 635, 51814], "temperature": 0.0, "avg_logprob": -0.28261429298925034, "compression_ratio": 1.792, "no_speech_prob": 0.05178046226501465}, {"id": 122, "seek": 80096, "start": 800.96, "end": 805.4000000000001, "text": " la Ley. Lo que me falta calcular entonces es esta parte de ac\u00e1 y esa parte de ac\u00e1,", "tokens": [50364, 635, 36794, 13, 6130, 631, 385, 22111, 4322, 289, 13003, 785, 5283, 6975, 368, 23496, 288, 11342, 6975, 368, 23496, 11, 50586], "temperature": 0.0, "avg_logprob": -0.45731110042995876, "compression_ratio": 1.5227272727272727, "no_speech_prob": 0.02358882874250412}, {"id": 123, "seek": 80096, "start": 805.4000000000001, "end": 811.48, "text": " la calcula esta manera. Yo digo que la probabilidad de F da doe es igual, ah\u00ed est\u00e1 m\u00e1s", "tokens": [50586, 635, 4322, 64, 5283, 13913, 13, 7616, 22990, 631, 635, 31959, 4580, 368, 479, 1120, 360, 68, 785, 10953, 11, 12571, 3192, 3573, 50890], "temperature": 0.0, "avg_logprob": -0.45731110042995876, "compression_ratio": 1.5227272727272727, "no_speech_prob": 0.02358882874250412}, {"id": 124, "seek": 80096, "start": 811.48, "end": 819.32, "text": " o menos al resultado final, pero podemos sacar que es lo que tendr\u00eda que poner de este lado.", "tokens": [50890, 277, 8902, 419, 28047, 2572, 11, 4768, 12234, 43823, 631, 785, 450, 631, 3928, 37183, 631, 19149, 368, 4065, 11631, 13, 51282], "temperature": 0.0, "avg_logprob": -0.45731110042995876, "compression_ratio": 1.5227272727272727, "no_speech_prob": 0.02358882874250412}, {"id": 125, "seek": 83096, "start": 831.96, "end": 840.76, "text": " Esta, por definici\u00f3n de probabilidad de condicional es pede F da doe, de verdad lo", "tokens": [50414, 20547, 11, 1515, 1561, 15534, 368, 31959, 4580, 368, 2224, 33010, 785, 280, 4858, 479, 1120, 360, 68, 11, 368, 13692, 450, 50854], "temperature": 0.0, "avg_logprob": -0.4756669264573317, "compression_ratio": 1.680722891566265, "no_speech_prob": 0.1593359410762787}, {"id": 126, "seek": 83096, "start": 840.76, "end": 848.96, "text": " alian van a ser lo, pero esto se puede definir c\u00f3mo pede F a e sobre pede, no, por definici\u00f3n", "tokens": [50854, 419, 952, 3161, 257, 816, 450, 11, 4768, 7433, 369, 8919, 1561, 347, 12826, 280, 4858, 479, 257, 308, 5473, 280, 4858, 11, 572, 11, 1515, 1561, 15534, 51264], "temperature": 0.0, "avg_logprob": -0.4756669264573317, "compression_ratio": 1.680722891566265, "no_speech_prob": 0.1593359410762787}, {"id": 127, "seek": 83096, "start": 848.96, "end": 856.0, "text": " de probabilidad de condicional. Pero adem\u00e1s esto si quiero podr\u00eda llegar a decir esto es lo mismo", "tokens": [51264, 368, 31959, 4580, 368, 2224, 33010, 13, 9377, 21251, 7433, 1511, 16811, 27246, 24892, 257, 10235, 7433, 785, 450, 12461, 51616], "temperature": 0.0, "avg_logprob": -0.4756669264573317, "compression_ratio": 1.680722891566265, "no_speech_prob": 0.1593359410762787}, {"id": 128, "seek": 85600, "start": 856.0, "end": 875.24, "text": " que pede F a e sobre pede, por, voy a que me falta va, no, ah\u00ed, por pede a e sobre pede a e", "tokens": [50364, 631, 280, 4858, 479, 257, 308, 5473, 280, 4858, 11, 1515, 11, 7552, 257, 631, 385, 22111, 2773, 11, 572, 11, 12571, 11, 1515, 280, 4858, 257, 308, 5473, 280, 4858, 257, 308, 51326], "temperature": 0.0, "avg_logprob": -0.48710467364336996, "compression_ratio": 1.2602739726027397, "no_speech_prob": 0.5169376730918884}, {"id": 129, "seek": 87524, "start": 875.24, "end": 882.8, "text": " pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e", "tokens": [50364, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 50742], "temperature": 0.0, "avg_logprob": -0.38784945944081184, "compression_ratio": 9.966666666666667, "no_speech_prob": 0.6089498996734619}, {"id": 130, "seek": 87524, "start": 882.8, "end": 891.28, "text": " sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e", "tokens": [50742, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 51166], "temperature": 0.0, "avg_logprob": -0.38784945944081184, "compression_ratio": 9.966666666666667, "no_speech_prob": 0.6089498996734619}, {"id": 131, "seek": 87524, "start": 891.28, "end": 897.32, "text": " sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e sobre pede F a e", "tokens": [51166, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 5473, 280, 4858, 479, 257, 308, 51468], "temperature": 0.0, "avg_logprob": -0.38784945944081184, "compression_ratio": 9.966666666666667, "no_speech_prob": 0.6089498996734619}, {"id": 132, "seek": 89732, "start": 897.4000000000001, "end": 908.5, "text": " definitiva yo que me queda, es si, asoci\u00f3s los dos, meda que dar pede F da do ahh e y si asoci\u00f3s estos", "tokens": [50368, 28781, 5931, 5290, 631, 385, 23314, 11, 785, 1511, 11, 382, 78, 537, 12994, 1750, 4491, 11, 1205, 64, 631, 4072, 280, 4858, 479, 1120, 220, 2595, 28612, 308, 288, 1511, 382, 78, 537, 12994, 12585, 50923], "temperature": 1.0, "avg_logprob": -1.7910686374939595, "compression_ratio": 1.5371428571428571, "no_speech_prob": 0.3934841454029083}, {"id": 133, "seek": 89732, "start": 908.5, "end": 915.34, "text": " dos de ac\u00e1 sabr\u00f3n me va a quedar pede aa dagoes qu\u00e9 lo que tra ya.", "tokens": [50923, 4491, 368, 23496, 5560, 81, 1801, 385, 2773, 257, 39244, 280, 4858, 40079, 274, 6442, 279, 8057, 450, 631, 944, 2478, 13, 51265], "temperature": 1.0, "avg_logprob": -1.7910686374939595, "compression_ratio": 1.5371428571428571, "no_speech_prob": 0.3934841454029083}, {"id": 134, "seek": 89732, "start": 915.34, "end": 922.32, "text": " La probabilidad pede F, que sea de bueno si te los dos, de f, y ya dago... E eh, es igual a la", "tokens": [51265, 2369, 31959, 4580, 280, 4858, 479, 11, 631, 4158, 368, 11974, 1511, 535, 1750, 4491, 11, 368, 283, 11, 288, 2478, 274, 6442, 485, 462, 7670, 11, 785, 10953, 257, 635, 51614], "temperature": 1.0, "avg_logprob": -1.7910686374939595, "compression_ratio": 1.5371428571428571, "no_speech_prob": 0.3934841454029083}, {"id": 135, "seek": 92232, "start": 922.32, "end": 926.6600000000001, "text": " la roguelidad de desfeitados ah\u00ed por la progulidad de a da doy.", "tokens": [50364, 635, 744, 2794, 338, 4580, 368, 730, 2106, 270, 4181, 12571, 1515, 635, 447, 70, 425, 4580, 368, 257, 1120, 360, 88, 13, 50581], "temperature": 0.4, "avg_logprob": -0.6680099487304687, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.4300670027732849}, {"id": 136, "seek": 92232, "start": 926.6600000000001, "end": 930.72, "text": " Y estos dos valores que est\u00e1n ac\u00e1 no lo s\u00e9 el equipo casualidad sino que son los", "tokens": [50581, 398, 12585, 4491, 38790, 631, 10368, 23496, 572, 450, 7910, 806, 30048, 13052, 4580, 18108, 631, 1872, 1750, 50784], "temperature": 0.4, "avg_logprob": -0.6680099487304687, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.4300670027732849}, {"id": 137, "seek": 92232, "start": 930.72, "end": 932.74, "text": " valores que ten\u00edan antes en el modelo.", "tokens": [50784, 38790, 631, 47596, 11014, 465, 806, 27825, 13, 50885], "temperature": 0.4, "avg_logprob": -0.6680099487304687, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.4300670027732849}, {"id": 138, "seek": 92232, "start": 932.74, "end": 941.24, "text": " O sea, yo ten\u00eda que el pedea da doy, el igual a \u00e9psil\u00f3n sobre y m\u00e1s uno a la jota.", "tokens": [50885, 422, 4158, 11, 5290, 23718, 631, 806, 5670, 29791, 1120, 360, 88, 11, 806, 10953, 257, 1136, 1878, 388, 1801, 5473, 288, 3573, 8526, 257, 635, 361, 5377, 13, 51310], "temperature": 0.4, "avg_logprob": -0.6680099487304687, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.4300670027732849}, {"id": 139, "seek": 92232, "start": 941.24, "end": 949.5, "text": " Y el otro era la productoria de jota igual uno hasta jota grande de las valores de", "tokens": [51310, 398, 806, 11921, 4249, 635, 1674, 8172, 368, 361, 5377, 10953, 8526, 10764, 361, 5377, 8883, 368, 2439, 38790, 368, 51723], "temperature": 0.4, "avg_logprob": -0.6680099487304687, "compression_ratio": 1.6497695852534562, "no_speech_prob": 0.4300670027732849}, {"id": 140, "seek": 94950, "start": 949.5, "end": 954.66, "text": " traducci\u00f3n, el efe subjota y el e suba subjota.", "tokens": [50364, 2479, 1311, 5687, 11, 806, 308, 2106, 1422, 73, 5377, 288, 806, 308, 1422, 64, 1422, 73, 5377, 13, 50622], "temperature": 0.0, "avg_logprob": -0.34025118643777413, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.05400828272104263}, {"id": 141, "seek": 94950, "start": 954.66, "end": 959.62, "text": " Entonces en definitiva puedo calcular pdf a da doy y adem\u00e1s puedo calcular haciendo", "tokens": [50622, 15097, 465, 28781, 5931, 21612, 2104, 17792, 280, 45953, 257, 1120, 360, 88, 288, 21251, 21612, 2104, 17792, 20509, 50870], "temperature": 0.0, "avg_logprob": -0.34025118643777413, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.05400828272104263}, {"id": 142, "seek": 94950, "start": 959.62, "end": 966.7, "text": " una suma sobre todas las alienaciones posibles puedo calcular pdf da doy.", "tokens": [50870, 2002, 2408, 64, 5473, 10906, 2439, 12319, 9188, 1366, 14428, 21612, 2104, 17792, 280, 45953, 1120, 360, 88, 13, 51224], "temperature": 0.0, "avg_logprob": -0.34025118643777413, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.05400828272104263}, {"id": 143, "seek": 94950, "start": 966.7, "end": 971.9, "text": " Bien, con eso y con todo ese mont\u00f3n de cocciones, llegamos a construir lo que es un modelo", "tokens": [51224, 16956, 11, 416, 7287, 288, 416, 5149, 10167, 45259, 368, 598, 35560, 11, 11234, 2151, 257, 38445, 450, 631, 785, 517, 27825, 51484], "temperature": 0.0, "avg_logprob": -0.34025118643777413, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.05400828272104263}, {"id": 144, "seek": 94950, "start": 971.9, "end": 976.74, "text": " de traducci\u00f3n o sea solamente teniendo una tabla de traducciones que me diga cu\u00e1l es la", "tokens": [51484, 368, 2479, 1311, 5687, 277, 4158, 27814, 2064, 7304, 2002, 4421, 875, 368, 2479, 1311, 23469, 631, 385, 2528, 64, 44318, 785, 635, 51726], "temperature": 0.0, "avg_logprob": -0.34025118643777413, "compression_ratio": 1.7366071428571428, "no_speech_prob": 0.05400828272104263}, {"id": 145, "seek": 97674, "start": 976.74, "end": 982.62, "text": " progulidad de traducir una palabra como otra palabra yo puedo llegar a definirme", "tokens": [50364, 447, 70, 425, 4580, 368, 2479, 1311, 347, 2002, 31702, 2617, 13623, 31702, 5290, 21612, 24892, 257, 1561, 347, 1398, 50658], "temperature": 0.0, "avg_logprob": -0.3194894368669628, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.15705569088459015}, {"id": 146, "seek": 97674, "start": 982.62, "end": 988.14, "text": " cu\u00e1l es la progulidad de traducir una oraci\u00f3n da da otra oraci\u00f3n.", "tokens": [50658, 44318, 785, 635, 447, 70, 425, 4580, 368, 2479, 1311, 347, 2002, 420, 3482, 1120, 1120, 13623, 420, 3482, 13, 50934], "temperature": 0.0, "avg_logprob": -0.3194894368669628, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.15705569088459015}, {"id": 147, "seek": 97674, "start": 988.14, "end": 992.66, "text": " Bien, y hay una cosa m\u00e1s, bueno esto ya lo estoy moviendo que aplicamos en cada", "tokens": [50934, 16956, 11, 288, 4842, 2002, 10163, 3573, 11, 11974, 7433, 2478, 450, 15796, 2402, 7304, 631, 18221, 2151, 465, 8411, 51160], "temperature": 0.0, "avg_logprob": -0.3194894368669628, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.15705569088459015}, {"id": 148, "seek": 97674, "start": 992.66, "end": 1001.38, "text": " paso, y hay una cosa m\u00e1s que es si yo tuviera las dos oraciones digamos la oraci\u00f3n", "tokens": [51160, 29212, 11, 288, 4842, 2002, 10163, 3573, 631, 785, 1511, 5290, 38177, 10609, 2439, 4491, 420, 9188, 36430, 635, 420, 3482, 51596], "temperature": 0.0, "avg_logprob": -0.3194894368669628, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.15705569088459015}, {"id": 149, "seek": 97674, "start": 1001.38, "end": 1005.26, "text": " en ingl\u00e9s y la oraci\u00f3n en espa\u00f1ol y adem\u00e1s tuviera la tabla de esta con todas las", "tokens": [51596, 465, 49766, 288, 635, 420, 3482, 465, 31177, 288, 21251, 38177, 10609, 635, 4421, 875, 368, 5283, 416, 10906, 2439, 51790], "temperature": 0.0, "avg_logprob": -0.3194894368669628, "compression_ratio": 1.8144796380090498, "no_speech_prob": 0.15705569088459015}, {"id": 150, "seek": 100526, "start": 1005.26, "end": 1008.9399999999999, "text": " de progulidades yo podr\u00eda hacer un algoritmo de programaci\u00f3n din\u00e1mica, un algoritmo", "tokens": [50364, 368, 447, 70, 425, 10284, 5290, 27246, 6720, 517, 3501, 50017, 3280, 368, 1461, 3482, 3791, 19524, 2262, 11, 517, 3501, 50017, 3280, 50548], "temperature": 0.0, "avg_logprob": -0.3839543309705011, "compression_ratio": 1.913533834586466, "no_speech_prob": 0.10613833367824554}, {"id": 151, "seek": 100526, "start": 1008.9399999999999, "end": 1013.02, "text": " estilo biter, y que vaya recorriendo alienaciones y media cu\u00e1l es la lineaci\u00f3n m\u00e1s", "tokens": [50548, 37470, 857, 260, 11, 288, 631, 47682, 850, 284, 470, 3999, 12319, 9188, 288, 3021, 44318, 785, 635, 1622, 3482, 3573, 50752], "temperature": 0.0, "avg_logprob": -0.3839543309705011, "compression_ratio": 1.913533834586466, "no_speech_prob": 0.10613833367824554}, {"id": 152, "seek": 100526, "start": 1013.02, "end": 1017.5, "text": " probable. No vamos a ver los detalles de algoritmo, pero viene a forma de decir bueno,", "tokens": [50752, 21759, 13, 883, 5295, 257, 1306, 1750, 1141, 37927, 368, 3501, 50017, 3280, 11, 4768, 19561, 257, 8366, 368, 10235, 11974, 11, 50976], "temperature": 0.0, "avg_logprob": -0.3839543309705011, "compression_ratio": 1.913533834586466, "no_speech_prob": 0.10613833367824554}, {"id": 153, "seek": 100526, "start": 1017.5, "end": 1021.3, "text": " voy recorriendo las dos oraciones y me voy quedando con las sus secciones m\u00e1s", "tokens": [50976, 7552, 850, 284, 470, 3999, 2439, 4491, 420, 9188, 288, 385, 7552, 13617, 1806, 416, 2439, 3291, 907, 23469, 3573, 51166], "temperature": 0.0, "avg_logprob": -0.3839543309705011, "compression_ratio": 1.913533834586466, "no_speech_prob": 0.10613833367824554}, {"id": 154, "seek": 100526, "start": 1021.3, "end": 1025.78, "text": " probable y al final me termina de volviendo cu\u00e1l es la lineaci\u00f3n m\u00e1s probable edadas", "tokens": [51166, 21759, 288, 419, 2572, 385, 1433, 1426, 368, 1996, 85, 7304, 44318, 785, 635, 1622, 3482, 3573, 21759, 1257, 345, 296, 51390], "temperature": 0.0, "avg_logprob": -0.3839543309705011, "compression_ratio": 1.913533834586466, "no_speech_prob": 0.10613833367824554}, {"id": 155, "seek": 100526, "start": 1025.78, "end": 1031.9, "text": " esas oraciones. O sea que si yo tuviera ya esa tabla de traducciones, esa tabla de", "tokens": [51390, 23388, 420, 9188, 13, 422, 4158, 631, 1511, 5290, 38177, 10609, 2478, 11342, 4421, 875, 368, 2479, 1311, 23469, 11, 11342, 4421, 875, 368, 51696], "temperature": 0.0, "avg_logprob": -0.3839543309705011, "compression_ratio": 1.913533834586466, "no_speech_prob": 0.10613833367824554}, {"id": 156, "seek": 103190, "start": 1031.9, "end": 1038.3400000000001, "text": " progulidades de traducci\u00f3n podr\u00eda construirme las a la lineaci\u00f3nes del corpus.", "tokens": [50364, 447, 70, 425, 10284, 368, 2479, 1311, 5687, 27246, 38445, 1398, 2439, 257, 635, 1622, 3482, 279, 1103, 1181, 31624, 13, 50686], "temperature": 0.0, "avg_logprob": -0.33088597530076486, "compression_ratio": 1.8619246861924685, "no_speech_prob": 0.07015759497880936}, {"id": 157, "seek": 103190, "start": 1038.3400000000001, "end": 1043.26, "text": " As\u00ed que bueno, hasta el momento dec\u00edamos bueno, suponemos que tenemos esta tabla de traducci\u00f3n", "tokens": [50686, 17419, 631, 11974, 11, 10764, 806, 9333, 979, 16275, 11974, 11, 9331, 266, 4485, 631, 9914, 5283, 4421, 875, 368, 2479, 1311, 5687, 50932], "temperature": 0.0, "avg_logprob": -0.33088597530076486, "compression_ratio": 1.8619246861924685, "no_speech_prob": 0.07015759497880936}, {"id": 158, "seek": 103190, "start": 1043.26, "end": 1048.1000000000001, "text": " que me dice para bank, si se traduce, para bancos, si se traduce como bank o como", "tokens": [50932, 631, 385, 10313, 1690, 3765, 11, 1511, 369, 2479, 4176, 11, 1690, 39612, 329, 11, 1511, 369, 2479, 4176, 2617, 3765, 277, 2617, 51174], "temperature": 0.0, "avg_logprob": -0.33088597530076486, "compression_ratio": 1.8619246861924685, "no_speech_prob": 0.07015759497880936}, {"id": 159, "seek": 103190, "start": 1048.1000000000001, "end": 1053.94, "text": " bench, etc\u00e9tera, estaba diciendo que ten\u00eda esa tabla, pero en realidad la realidad que no", "tokens": [51174, 10638, 11, 5183, 526, 23833, 11, 17544, 42797, 631, 23718, 11342, 4421, 875, 11, 4768, 465, 25635, 635, 25635, 631, 572, 51466], "temperature": 0.0, "avg_logprob": -0.33088597530076486, "compression_ratio": 1.8619246861924685, "no_speech_prob": 0.07015759497880936}, {"id": 160, "seek": 103190, "start": 1053.94, "end": 1059.3400000000001, "text": " tengo esa tabla y me gustar\u00eda poder construirla. Entonces, no gustar\u00eda poder estimar esas", "tokens": [51466, 13989, 11342, 4421, 875, 288, 385, 45896, 8152, 38445, 875, 13, 15097, 11, 572, 45896, 8152, 8017, 289, 23388, 51736], "temperature": 0.0, "avg_logprob": -0.33088597530076486, "compression_ratio": 1.8619246861924685, "no_speech_prob": 0.07015759497880936}, {"id": 161, "seek": 105934, "start": 1059.34, "end": 1063.4199999999998, "text": " progulidades para construirme esa tabla. Si yo tuviera un corpus paralelo, simplemente", "tokens": [50364, 447, 70, 425, 10284, 1690, 38445, 1398, 11342, 4421, 875, 13, 4909, 5290, 38177, 10609, 517, 1181, 31624, 26009, 10590, 11, 33190, 50568], "temperature": 0.0, "avg_logprob": -0.2962204326282848, "compression_ratio": 2.0265151515151514, "no_speech_prob": 0.23183438181877136}, {"id": 162, "seek": 105934, "start": 1063.4199999999998, "end": 1067.54, "text": " podr\u00eda ir recorriendo el corpus y contando cu\u00e1ntas veces aparece banco al inado con", "tokens": [50568, 27246, 3418, 850, 284, 470, 3999, 806, 1181, 31624, 288, 660, 1806, 44256, 296, 17054, 37863, 45498, 419, 294, 1573, 416, 50774], "temperature": 0.0, "avg_logprob": -0.2962204326282848, "compression_ratio": 2.0265151515151514, "no_speech_prob": 0.23183438181877136}, {"id": 163, "seek": 105934, "start": 1067.54, "end": 1073.26, "text": " bench y cu\u00e1ntas veces al inado con bank y ah\u00ed sacar\u00eda una progulidad, pero no tengo", "tokens": [50774, 10638, 288, 44256, 296, 17054, 419, 294, 1573, 416, 3765, 288, 12571, 43823, 2686, 2002, 447, 70, 425, 4580, 11, 4768, 572, 13989, 51060], "temperature": 0.0, "avg_logprob": -0.2962204326282848, "compression_ratio": 2.0265151515151514, "no_speech_prob": 0.23183438181877136}, {"id": 164, "seek": 105934, "start": 1073.26, "end": 1079.8999999999999, "text": " las a la lineaci\u00f3nes. Y como lo que vimos digamos reci\u00e9n, si yo tuviera la tabla, entonces", "tokens": [51060, 2439, 257, 635, 1622, 3482, 279, 13, 398, 2617, 450, 631, 49266, 36430, 4214, 3516, 11, 1511, 5290, 38177, 10609, 635, 4421, 875, 11, 13003, 51392], "temperature": 0.0, "avg_logprob": -0.2962204326282848, "compression_ratio": 2.0265151515151514, "no_speech_prob": 0.23183438181877136}, {"id": 165, "seek": 105934, "start": 1079.8999999999999, "end": 1083.1399999999999, "text": " yo va adem\u00e1s poder ir recorriendo el corpus y construirme las a la lineaci\u00f3nes. As\u00ed", "tokens": [51392, 5290, 2773, 21251, 8152, 3418, 850, 284, 470, 3999, 806, 1181, 31624, 288, 38445, 1398, 2439, 257, 635, 1622, 3482, 279, 13, 17419, 51554], "temperature": 0.0, "avg_logprob": -0.2962204326282848, "compression_ratio": 2.0265151515151514, "no_speech_prob": 0.23183438181877136}, {"id": 166, "seek": 105934, "start": 1083.1399999999999, "end": 1088.06, "text": " que si yo tuviera las a la lineaci\u00f3nes podr\u00eda contar y sacar la tabla, si yo tuviera la tabla", "tokens": [51554, 631, 1511, 5290, 38177, 10609, 2439, 257, 635, 1622, 3482, 279, 27246, 27045, 288, 43823, 635, 4421, 875, 11, 1511, 5290, 38177, 10609, 635, 4421, 875, 51800], "temperature": 0.0, "avg_logprob": -0.2962204326282848, "compression_ratio": 2.0265151515151514, "no_speech_prob": 0.23183438181877136}, {"id": 167, "seek": 108806, "start": 1088.06, "end": 1092.7, "text": " podr\u00eda pasarle un agorismo y construir las a la lineaci\u00f3nes. Pero la verdad que no tengo", "tokens": [50364, 27246, 25344, 306, 517, 623, 284, 6882, 288, 38445, 2439, 257, 635, 1622, 3482, 279, 13, 9377, 635, 13692, 631, 572, 13989, 50596], "temperature": 0.0, "avg_logprob": -0.28229864438374835, "compression_ratio": 2.003787878787879, "no_speech_prob": 0.11210664361715317}, {"id": 168, "seek": 108806, "start": 1092.7, "end": 1097.02, "text": " ninguna de las dos cosas, entonces se vuelve un problema de hueve la gallina, o sea, si", "tokens": [50596, 36073, 368, 2439, 4491, 12218, 11, 13003, 369, 20126, 303, 517, 12395, 368, 24967, 303, 635, 8527, 1426, 11, 277, 4158, 11, 1511, 50812], "temperature": 0.0, "avg_logprob": -0.28229864438374835, "compression_ratio": 2.003787878787879, "no_speech_prob": 0.11210664361715317}, {"id": 169, "seek": 108806, "start": 1097.02, "end": 1100.46, "text": " yo tuviera las a la lineaci\u00f3nes, construir\u00eda el modelo, construir\u00eda la tabla de", "tokens": [50812, 5290, 38177, 10609, 2439, 257, 635, 1622, 3482, 279, 11, 38445, 2686, 806, 27825, 11, 38445, 2686, 635, 4421, 875, 368, 50984], "temperature": 0.0, "avg_logprob": -0.28229864438374835, "compression_ratio": 2.003787878787879, "no_speech_prob": 0.11210664361715317}, {"id": 170, "seek": 108806, "start": 1100.46, "end": 1103.6599999999999, "text": " progulidades, si yo tuviera la tabla de progulidades podr\u00eda construir las a la", "tokens": [50984, 447, 70, 425, 10284, 11, 1511, 5290, 38177, 10609, 635, 4421, 875, 368, 447, 70, 425, 10284, 27246, 38445, 2439, 257, 635, 51144], "temperature": 0.0, "avg_logprob": -0.28229864438374835, "compression_ratio": 2.003787878787879, "no_speech_prob": 0.11210664361715317}, {"id": 171, "seek": 108806, "start": 1103.6599999999999, "end": 1110.62, "text": " lineaci\u00f3nes. Parece tipo de problemas en los cuales yo tengo como dos variables interdependentes", "tokens": [51144, 1622, 3482, 279, 13, 45419, 9746, 368, 20720, 465, 1750, 46932, 5290, 13989, 2617, 4491, 9102, 728, 36763, 9240, 51492], "temperature": 0.0, "avg_logprob": -0.28229864438374835, "compression_ratio": 2.003787878787879, "no_speech_prob": 0.11210664361715317}, {"id": 172, "seek": 108806, "start": 1110.62, "end": 1114.5, "text": " y no conozco exactamente el valor de ninguna de las dos, si utiliza lo que se conoce como", "tokens": [51492, 288, 572, 416, 15151, 1291, 48686, 806, 15367, 368, 36073, 368, 2439, 4491, 11, 1511, 4976, 13427, 450, 631, 369, 33029, 384, 2617, 51686], "temperature": 0.0, "avg_logprob": -0.28229864438374835, "compression_ratio": 2.003787878787879, "no_speech_prob": 0.11210664361715317}, {"id": 173, "seek": 111450, "start": 1114.5, "end": 1120.62, "text": " el algoritmo de expectation maximizaci\u00f3n o maximizaci\u00f3n de la esperanza. Y bueno, es un algoritmo", "tokens": [50364, 806, 3501, 50017, 3280, 368, 14334, 5138, 27603, 277, 5138, 27603, 368, 635, 10045, 20030, 13, 398, 11974, 11, 785, 517, 3501, 50017, 3280, 50670], "temperature": 0.0, "avg_logprob": -0.2948169708251953, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.1858632117509842}, {"id": 174, "seek": 111450, "start": 1120.62, "end": 1125.34, "text": " que sirve exactamente para este tipo de problemas. En realidad lo que va a hacer es el", "tokens": [50670, 631, 4735, 303, 48686, 1690, 4065, 9746, 368, 20720, 13, 2193, 25635, 450, 631, 2773, 257, 6720, 785, 806, 50906], "temperature": 0.0, "avg_logprob": -0.2948169708251953, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.1858632117509842}, {"id": 175, "seek": 111450, "start": 1125.34, "end": 1130.66, "text": " algoritmo citerar, es un algoritmo iterativo que va tratando de convertir una soluci\u00f3n y lo", "tokens": [50906, 3501, 50017, 3280, 269, 1681, 289, 11, 785, 517, 3501, 50017, 3280, 17138, 18586, 631, 2773, 21507, 1806, 368, 7620, 347, 2002, 24807, 5687, 288, 450, 51172], "temperature": 0.0, "avg_logprob": -0.2948169708251953, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.1858632117509842}, {"id": 176, "seek": 111450, "start": 1130.66, "end": 1135.34, "text": " que hace es decir, bueno, yo no tengo ninguno de los dos valores, o sea si yo tuviera", "tokens": [51172, 631, 10032, 785, 10235, 11, 11974, 11, 5290, 572, 13989, 17210, 12638, 368, 1750, 4491, 38790, 11, 277, 4158, 1511, 5290, 38177, 10609, 51406], "temperature": 0.0, "avg_logprob": -0.2948169708251953, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.1858632117509842}, {"id": 177, "seek": 111450, "start": 1135.34, "end": 1142.14, "text": " mi tabla de probabilidad de traducci\u00f3n, me podr\u00eda calcular las a la lineaci\u00f3nes y tuviera", "tokens": [51406, 2752, 4421, 875, 368, 31959, 4580, 368, 2479, 1311, 5687, 11, 385, 27246, 2104, 17792, 2439, 257, 635, 1622, 3482, 279, 288, 38177, 10609, 51746], "temperature": 0.0, "avg_logprob": -0.2948169708251953, "compression_ratio": 1.7751937984496124, "no_speech_prob": 0.1858632117509842}, {"id": 178, "seek": 114214, "start": 1142.14, "end": 1146.7800000000002, "text": " mi salinaci\u00f3n, me podr\u00eda calcular la probabilidad de traducci\u00f3n. Entonces lo que hace es decir,", "tokens": [50364, 2752, 1845, 259, 3482, 11, 385, 27246, 2104, 17792, 635, 31959, 4580, 368, 2479, 1311, 5687, 13, 15097, 450, 631, 10032, 785, 10235, 11, 50596], "temperature": 0.0, "avg_logprob": -0.3277719671075994, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.16767899692058563}, {"id": 179, "seek": 114214, "start": 1146.7800000000002, "end": 1151.9, "text": " bueno, a sumo que mi tabla de traducci\u00f3n va a ser uniformes, digamos, cualquier palabra se", "tokens": [50596, 11974, 11, 257, 2408, 78, 631, 2752, 4421, 875, 368, 2479, 1311, 5687, 2773, 257, 816, 9452, 279, 11, 36430, 11, 21004, 31702, 369, 50852], "temperature": 0.0, "avg_logprob": -0.3277719671075994, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.16767899692058563}, {"id": 180, "seek": 114214, "start": 1151.9, "end": 1155.6200000000001, "text": " puede traducir como cualquier otra palabra con la misma probabilidad. A partir de eso, que", "tokens": [50852, 8919, 2479, 1311, 347, 2617, 21004, 13623, 31702, 416, 635, 24946, 31959, 4580, 13, 316, 13906, 368, 7287, 11, 631, 51038], "temperature": 0.0, "avg_logprob": -0.3277719671075994, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.16767899692058563}, {"id": 181, "seek": 114214, "start": 1155.6200000000001, "end": 1159.0600000000002, "text": " alculo de la lineaci\u00f3nes, y a partir de esas nuevas a la lineaci\u00f3nes, c\u00e1lculo otra vez", "tokens": [51038, 419, 25436, 368, 635, 1622, 3482, 279, 11, 288, 257, 13906, 368, 23388, 42817, 257, 635, 1622, 3482, 279, 11, 6476, 75, 25436, 13623, 5715, 51210], "temperature": 0.0, "avg_logprob": -0.3277719671075994, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.16767899692058563}, {"id": 182, "seek": 114214, "start": 1159.0600000000002, "end": 1166.74, "text": " la tabla. Y de vuelta con esa tabla que c\u00e1lculo vuelva, medir las a la lineaci\u00f3nes y", "tokens": [51210, 635, 4421, 875, 13, 398, 368, 41542, 416, 11342, 4421, 875, 631, 6476, 75, 25436, 20126, 2757, 11, 1205, 347, 2439, 257, 635, 1622, 3482, 279, 288, 51594], "temperature": 0.0, "avg_logprob": -0.3277719671075994, "compression_ratio": 1.8617886178861789, "no_speech_prob": 0.16767899692058563}, {"id": 183, "seek": 116674, "start": 1166.74, "end": 1172.26, "text": " vuelta con esas nuevas a la lineaci\u00f3nes, vuelvo a calcular la tabla. Entonces, aunque no me", "tokens": [50364, 41542, 416, 23388, 42817, 257, 635, 1622, 3482, 279, 11, 20126, 3080, 257, 2104, 17792, 635, 4421, 875, 13, 15097, 11, 21962, 572, 385, 50640], "temperature": 0.0, "avg_logprob": -0.4325276543112362, "compression_ratio": 1.5102880658436213, "no_speech_prob": 0.03962916508316994}, {"id": 184, "seek": 116674, "start": 1172.26, "end": 1177.1, "text": " crean, esto despu\u00e9s de muchas iteraciones va convergiendo a algo, y parece m\u00e1gico, \u00bfno?", "tokens": [50640, 1197, 282, 11, 7433, 15283, 368, 16072, 17138, 9188, 2773, 9652, 70, 7304, 257, 8655, 11, 288, 14120, 12228, 70, 2789, 11, 3841, 1771, 30, 50882], "temperature": 0.0, "avg_logprob": -0.4325276543112362, "compression_ratio": 1.5102880658436213, "no_speech_prob": 0.03962916508316994}, {"id": 185, "seek": 116674, "start": 1177.1, "end": 1182.46, "text": " parece como que tal realidad si yo no tengo ninguno de los dos valores, no deber\u00eda como", "tokens": [50882, 14120, 2617, 631, 4023, 25635, 1511, 5290, 572, 13989, 17210, 12638, 368, 1750, 4491, 38790, 11, 572, 29671, 2686, 2617, 51150], "temperature": 0.0, "avg_logprob": -0.4325276543112362, "compression_ratio": 1.5102880658436213, "no_speech_prob": 0.03962916508316994}, {"id": 186, "seek": 116674, "start": 1182.46, "end": 1190.34, "text": " dar fruta. Pero voy a tratar de comenzar los que en realidad esto si funciona, con un ejemplo.", "tokens": [51150, 4072, 431, 12093, 13, 9377, 7552, 257, 42549, 368, 29564, 289, 1750, 631, 465, 25635, 7433, 1511, 26210, 11, 416, 517, 13358, 13, 51544], "temperature": 0.0, "avg_logprob": -0.4325276543112362, "compression_ratio": 1.5102880658436213, "no_speech_prob": 0.03962916508316994}, {"id": 187, "seek": 119034, "start": 1191.26, "end": 1196.9399999999998, "text": " Bien, tenemos. Entonces, vamos a construir un sistema que es de traducci\u00f3n entre frances", "tokens": [50410, 16956, 11, 9914, 13, 15097, 11, 5295, 257, 38445, 517, 13245, 631, 785, 368, 2479, 1311, 5687, 3962, 431, 2676, 50694], "temperature": 0.0, "avg_logprob": -0.36427074670791626, "compression_ratio": 1.80859375, "no_speech_prob": 0.1815149486064911}, {"id": 188, "seek": 119034, "start": 1196.9399999999998, "end": 1201.3, "text": " y lingles, donde hay un cuerpo muy grande, pero bueno, vamos a concentrar sobre el", "tokens": [50694, 288, 22949, 904, 11, 10488, 4842, 517, 20264, 5323, 8883, 11, 4768, 11974, 11, 5295, 257, 5512, 5352, 5473, 806, 50912], "temperature": 0.0, "avg_logprob": -0.36427074670791626, "compression_ratio": 1.80859375, "no_speech_prob": 0.1815149486064911}, {"id": 189, "seek": 119034, "start": 1201.3, "end": 1206.1, "text": " entre peque\u00f1as oraci\u00f3n cita que dicen la mes\u00f3n se traduce como deja, la mes\u00f3n blu, se traduce", "tokens": [50912, 3962, 19132, 32448, 420, 3482, 269, 2786, 631, 33816, 635, 3813, 1801, 369, 2479, 4176, 2617, 38260, 11, 635, 3813, 1801, 888, 84, 11, 369, 2479, 4176, 51152], "temperature": 0.0, "avg_logprob": -0.36427074670791626, "compression_ratio": 1.80859375, "no_speech_prob": 0.1815149486064911}, {"id": 190, "seek": 119034, "start": 1206.1, "end": 1211.62, "text": " como de lujados y la flea o se traduce como de flower. Entonces, al principio lo que hago es decir,", "tokens": [51152, 2617, 368, 287, 4579, 4181, 288, 635, 7025, 64, 277, 369, 2479, 4176, 2617, 368, 8617, 13, 15097, 11, 419, 34308, 450, 631, 38721, 785, 10235, 11, 51428], "temperature": 0.0, "avg_logprob": -0.36427074670791626, "compression_ratio": 1.80859375, "no_speech_prob": 0.1815149486064911}, {"id": 191, "seek": 119034, "start": 1211.62, "end": 1216.78, "text": " bueno, todas las traducciones en todas las palabras son equiprobables, as\u00ed que lo que me va", "tokens": [51428, 11974, 11, 10906, 2439, 2479, 1311, 23469, 465, 10906, 2439, 35240, 1872, 5037, 16614, 2965, 11, 8582, 631, 450, 631, 385, 2773, 51686], "temperature": 0.0, "avg_logprob": -0.36427074670791626, "compression_ratio": 1.80859375, "no_speech_prob": 0.1815149486064911}, {"id": 192, "seek": 121678, "start": 1216.78, "end": 1221.1, "text": " a quedar es cuando reparten de las salinaciones, todas van a tener el mismo peso. Entre la", "tokens": [50364, 257, 39244, 785, 7767, 1085, 11719, 368, 2439, 1845, 259, 9188, 11, 10906, 3161, 257, 11640, 806, 12461, 28149, 13, 27979, 635, 50580], "temperature": 0.0, "avg_logprob": -0.30406215641048406, "compression_ratio": 1.9416666666666667, "no_speech_prob": 0.06956826895475388}, {"id": 193, "seek": 121678, "start": 1221.1, "end": 1225.78, "text": " y mes\u00f3n, la probabilidad de que la se traduca como de, o que se traduca como javos, va a ser", "tokens": [50580, 288, 3813, 1801, 11, 635, 31959, 4580, 368, 631, 635, 369, 2479, 1311, 64, 2617, 368, 11, 277, 631, 369, 2479, 1311, 64, 2617, 361, 706, 329, 11, 2773, 257, 816, 50814], "temperature": 0.0, "avg_logprob": -0.30406215641048406, "compression_ratio": 1.9416666666666667, "no_speech_prob": 0.06956826895475388}, {"id": 194, "seek": 121678, "start": 1225.78, "end": 1230.7, "text": " la misma, en realidad, porque todas las salinaciones son equiprobables. En la mes\u00f3n blu, tambi\u00e9n", "tokens": [50814, 635, 24946, 11, 465, 25635, 11, 4021, 10906, 2439, 1845, 259, 9188, 1872, 5037, 16614, 2965, 13, 2193, 635, 3813, 1801, 888, 84, 11, 6407, 51060], "temperature": 0.0, "avg_logprob": -0.30406215641048406, "compression_ratio": 1.9416666666666667, "no_speech_prob": 0.06956826895475388}, {"id": 195, "seek": 121678, "start": 1230.7, "end": 1234.86, "text": " va a ser lo mismo, la probabilidad de traducirla como de como blu o como javos, va a ser la misma", "tokens": [51060, 2773, 257, 816, 450, 12461, 11, 635, 31959, 4580, 368, 2479, 1311, 347, 875, 2617, 368, 2617, 888, 84, 277, 2617, 361, 706, 329, 11, 2773, 257, 816, 635, 24946, 51268], "temperature": 0.0, "avg_logprob": -0.30406215641048406, "compression_ratio": 1.9416666666666667, "no_speech_prob": 0.06956826895475388}, {"id": 196, "seek": 121678, "start": 1234.86, "end": 1244.6399999999999, "text": " y en la flea pasa igual. Entonces, eso es la primera, el primer paso, digamos, en el", "tokens": [51268, 288, 465, 635, 7025, 64, 20260, 10953, 13, 15097, 11, 7287, 785, 635, 17382, 11, 806, 12595, 29212, 11, 36430, 11, 465, 806, 51757], "temperature": 0.0, "avg_logprob": -0.30406215641048406, "compression_ratio": 1.9416666666666667, "no_speech_prob": 0.06956826895475388}, {"id": 197, "seek": 124464, "start": 1244.64, "end": 1249.6000000000001, "text": " primer paso, yo voy a tener todas las salinaciones equiprobables y todas las los valores", "tokens": [50364, 12595, 29212, 11, 5290, 7552, 257, 11640, 10906, 2439, 1845, 259, 9188, 5037, 16614, 2965, 288, 10906, 2439, 1750, 38790, 50612], "temperature": 0.0, "avg_logprob": -0.33296921293614273, "compression_ratio": 1.36, "no_speech_prob": 0.02761460840702057}, {"id": 198, "seek": 124464, "start": 1249.6000000000001, "end": 1264.24, "text": " de las palabras iguales.", "tokens": [50612, 368, 2439, 35240, 10953, 279, 13, 51344], "temperature": 0.0, "avg_logprob": -0.33296921293614273, "compression_ratio": 1.36, "no_speech_prob": 0.02761460840702057}, {"id": 199, "seek": 124464, "start": 1264.24, "end": 1271.0400000000002, "text": " Entonces, en mi algorithmo, yo empec\u00e9 con una tabla de traducci\u00f3n que era todo uniforme.", "tokens": [51344, 15097, 11, 465, 2752, 9284, 78, 11, 5290, 846, 494, 13523, 416, 2002, 4421, 875, 368, 2479, 1311, 5687, 631, 4249, 5149, 9452, 68, 13, 51684], "temperature": 0.0, "avg_logprob": -0.33296921293614273, "compression_ratio": 1.36, "no_speech_prob": 0.02761460840702057}, {"id": 200, "seek": 127104, "start": 1271.04, "end": 1276.56, "text": " Como yo ten\u00eda la probabilidad de traducir cualquier palabra en cualquier otra era la misma.", "tokens": [50364, 11913, 5290, 23718, 635, 31959, 4580, 368, 2479, 1311, 347, 21004, 31702, 465, 21004, 13623, 4249, 635, 24946, 13, 50640], "temperature": 0.0, "avg_logprob": -0.21730651203383747, "compression_ratio": 1.844, "no_speech_prob": 0.1648114025592804}, {"id": 201, "seek": 127104, "start": 1276.56, "end": 1281.08, "text": " A partir de eso, yo me constru\u00ed estas salinaciones, que tambi\u00e9n parece que son todas equiprobables", "tokens": [50640, 316, 13906, 368, 7287, 11, 5290, 385, 12946, 870, 13897, 1845, 259, 9188, 11, 631, 6407, 14120, 631, 1872, 10906, 5037, 16614, 2965, 50866], "temperature": 0.0, "avg_logprob": -0.21730651203383747, "compression_ratio": 1.844, "no_speech_prob": 0.1648114025592804}, {"id": 202, "seek": 127104, "start": 1281.08, "end": 1285.04, "text": " y parece que no tienen como mucha informaci\u00f3n. Entonces, lo que voy a hacer ahora, a partir", "tokens": [50866, 288, 14120, 631, 572, 12536, 2617, 25248, 21660, 13, 15097, 11, 450, 631, 7552, 257, 6720, 9923, 11, 257, 13906, 51064], "temperature": 0.0, "avg_logprob": -0.21730651203383747, "compression_ratio": 1.844, "no_speech_prob": 0.1648114025592804}, {"id": 203, "seek": 127104, "start": 1285.04, "end": 1289.2, "text": " de esto, es tratar de construirme de vuelta, la tabla de traducciones, pero mirando estas", "tokens": [51064, 368, 7433, 11, 785, 42549, 368, 38445, 1398, 368, 41542, 11, 635, 4421, 875, 368, 2479, 1311, 23469, 11, 4768, 3149, 1806, 13897, 51272], "temperature": 0.0, "avg_logprob": -0.21730651203383747, "compression_ratio": 1.844, "no_speech_prob": 0.1648114025592804}, {"id": 204, "seek": 127104, "start": 1289.2, "end": 1294.48, "text": " nuevas salinaciones que hay. Entonces, lo que voy a construir es una tabla que tiene", "tokens": [51272, 42817, 1845, 259, 9188, 631, 4842, 13, 15097, 11, 450, 631, 7552, 257, 38445, 785, 2002, 4421, 875, 631, 7066, 51536], "temperature": 0.0, "avg_logprob": -0.21730651203383747, "compression_ratio": 1.844, "no_speech_prob": 0.1648114025592804}, {"id": 205, "seek": 129448, "start": 1294.48, "end": 1312.64, "text": " todas las palabras de las diferencias y en el mes\u00f3n blu, blu, blu, blu, blu, blu, blu, blu, blu.", "tokens": [50364, 10906, 2439, 35240, 368, 2439, 18959, 12046, 288, 465, 806, 3813, 1801, 888, 84, 11, 888, 84, 11, 888, 84, 11, 888, 84, 11, 888, 84, 11, 888, 84, 11, 888, 84, 11, 888, 84, 11, 888, 84, 13, 51272], "temperature": 0.0, "avg_logprob": -0.4696731964747111, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.06468157470226288}, {"id": 206, "seek": 129448, "start": 1312.64, "end": 1317.32, "text": " Y para llenar, esta nueva tabla es lo que tengo que hacer es iterar sobre las salinaciones,", "tokens": [51272, 398, 1690, 4849, 268, 289, 11, 5283, 28963, 4421, 875, 785, 450, 631, 13989, 631, 6720, 785, 17138, 289, 5473, 2439, 1845, 259, 9188, 11, 51506], "temperature": 0.0, "avg_logprob": -0.4696731964747111, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.06468157470226288}, {"id": 207, "seek": 129448, "start": 1317.32, "end": 1320.96, "text": " mirar cada una de las palabras, cuantas veces est\u00e1 linear con las otras y contar, o sea,", "tokens": [51506, 3149, 289, 8411, 2002, 368, 2439, 35240, 11, 2702, 394, 296, 17054, 3192, 1622, 289, 416, 2439, 20244, 288, 27045, 11, 277, 4158, 11, 51688], "temperature": 0.0, "avg_logprob": -0.4696731964747111, "compression_ratio": 1.690909090909091, "no_speech_prob": 0.06468157470226288}, {"id": 208, "seek": 132096, "start": 1320.96, "end": 1327.44, "text": " y sumar los peso de cabunas de las salinaciones. Entonces, la lineaci\u00f3n entre la y de", "tokens": [50364, 288, 2408, 289, 1750, 28149, 368, 5487, 409, 296, 368, 2439, 1845, 259, 9188, 13, 15097, 11, 635, 1622, 3482, 3962, 635, 288, 368, 50688], "temperature": 0.0, "avg_logprob": -0.33775066028941764, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.07364147156476974}, {"id": 209, "seek": 132096, "start": 1327.44, "end": 1331.54, "text": " en total, mirando ese ejemplo de corpus, cuanto me dar\u00eda de agua, cual ser\u00eda el peso de", "tokens": [50688, 465, 3217, 11, 3149, 1806, 10167, 13358, 368, 1181, 31624, 11, 2702, 5857, 385, 4072, 2686, 368, 19330, 11, 10911, 23679, 806, 28149, 368, 50893], "temperature": 0.0, "avg_logprob": -0.33775066028941764, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.07364147156476974}, {"id": 210, "seek": 132096, "start": 1331.54, "end": 1339.18, "text": " salinaci\u00f3n. Para verlo, en realidad lo que hago es contar, miro cu\u00e1ntas veces la y de est\u00e1n", "tokens": [50893, 1845, 259, 3482, 13, 11107, 1306, 752, 11, 465, 25635, 450, 631, 38721, 785, 27045, 11, 3149, 78, 44256, 296, 17054, 635, 288, 368, 10368, 51275], "temperature": 0.0, "avg_logprob": -0.33775066028941764, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.07364147156476974}, {"id": 211, "seek": 132096, "start": 1339.18, "end": 1345.98, "text": " lineados. Entonces, tengo 0.5 de peso en la primera, en la segunda tengo 0.293 y en la \u00faltima", "tokens": [51275, 1622, 4181, 13, 15097, 11, 13989, 1958, 13, 20, 368, 28149, 465, 635, 17382, 11, 465, 635, 21978, 13989, 1958, 13, 11871, 18, 288, 465, 635, 28118, 51615], "temperature": 0.0, "avg_logprob": -0.33775066028941764, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.07364147156476974}, {"id": 212, "seek": 134598, "start": 1345.98, "end": 1354.1, "text": " tengo 0.5 de vuelta. As\u00ed que en total tengo como 1.33 de peso entre la y de. Despu\u00e9s,", "tokens": [50364, 13989, 1958, 13, 20, 368, 41542, 13, 17419, 631, 465, 3217, 13989, 2617, 502, 13, 10191, 368, 28149, 3962, 635, 288, 368, 13, 40995, 11, 50770], "temperature": 0.0, "avg_logprob": -0.3089165687561035, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.11842858791351318}, {"id": 213, "seek": 134598, "start": 1354.1, "end": 1360.94, "text": " mira, entre la y j, cuanto peso tengo, cuanta masa de probabilidad tengo. Bueno, tengo 0.5 en la", "tokens": [50770, 30286, 11, 3962, 635, 288, 361, 11, 2702, 5857, 28149, 13989, 11, 2702, 5983, 29216, 368, 31959, 4580, 13989, 13, 16046, 11, 13989, 1958, 13, 20, 465, 635, 51112], "temperature": 0.0, "avg_logprob": -0.3089165687561035, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.11842858791351318}, {"id": 214, "seek": 134598, "start": 1360.94, "end": 1368.18, "text": " primera relaci\u00f3n, 0.103 en la segunda y nada en la tercera. Por lo tanto en total, tengo 0.83", "tokens": [51112, 17382, 37247, 11, 1958, 13, 3279, 18, 465, 635, 21978, 288, 8096, 465, 635, 1796, 41034, 13, 5269, 450, 10331, 465, 3217, 11, 13989, 1958, 13, 31849, 51474], "temperature": 0.0, "avg_logprob": -0.3089165687561035, "compression_ratio": 1.508108108108108, "no_speech_prob": 0.11842858791351318}, {"id": 215, "seek": 136818, "start": 1369.1000000000001, "end": 1375.3, "text": " de probabilidades entre la y j. Despu\u00e9s, mira, entre la y blu, cuanto peso tengo.", "tokens": [50410, 368, 31959, 10284, 3962, 635, 288, 361, 13, 40995, 11, 30286, 11, 3962, 635, 288, 888, 84, 11, 2702, 5857, 28149, 13989, 13, 50720], "temperature": 0.0, "avg_logprob": -0.42303791265378055, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.09455184638500214}, {"id": 216, "seek": 136818, "start": 1379.54, "end": 1385.22, "text": " 0.303, solamente 0.33, s\u00f3lo est\u00e1 en la y entre la y fler, cuanto tengo. No, entre", "tokens": [50932, 1958, 13, 3446, 18, 11, 27814, 1958, 13, 10191, 11, 22885, 3192, 465, 635, 288, 3962, 635, 288, 932, 260, 11, 2702, 5857, 13989, 13, 883, 11, 3962, 51216], "temperature": 0.0, "avg_logprob": -0.42303791265378055, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.09455184638500214}, {"id": 217, "seek": 136818, "start": 1385.22, "end": 1391.22, "text": " la y flavor, cuanto tengo. 0.5, s\u00f3lo aparece en la del final. Bien, como lo tengo la siguiente,", "tokens": [51216, 635, 288, 932, 1924, 11, 2702, 5857, 13989, 13, 1958, 13, 20, 11, 22885, 37863, 465, 635, 1103, 2572, 13, 16956, 11, 2617, 450, 13989, 635, 25666, 11, 51516], "temperature": 0.0, "avg_logprob": -0.42303791265378055, "compression_ratio": 1.6134969325153374, "no_speech_prob": 0.09455184638500214}, {"id": 218, "seek": 139122, "start": 1391.22, "end": 1401.82, "text": " entre ms\u00f3n y de cuanto tendr\u00eda. 0.83, est\u00e1 en la primera y la segunda, entre ms\u00f3n y", "tokens": [50364, 3962, 275, 82, 1801, 288, 368, 2702, 5857, 3928, 37183, 13, 1958, 13, 31849, 11, 3192, 465, 635, 17382, 288, 635, 21978, 11, 3962, 275, 82, 1801, 288, 50894], "temperature": 0.2, "avg_logprob": -0.4389739925578489, "compression_ratio": 1.9135135135135135, "no_speech_prob": 0.038435883820056915}, {"id": 219, "seek": 139122, "start": 1401.82, "end": 1410.8600000000001, "text": " j. En la primera y la segunda, entre ms\u00f3n y j. En la segunda, entre ms\u00f3n y j. Si,", "tokens": [50894, 361, 13, 2193, 635, 17382, 288, 635, 21978, 11, 3962, 275, 82, 1801, 288, 361, 13, 2193, 635, 21978, 11, 3962, 275, 82, 1801, 288, 361, 13, 4909, 11, 51346], "temperature": 0.2, "avg_logprob": -0.4389739925578489, "compression_ratio": 1.9135135135135135, "no_speech_prob": 0.038435883820056915}, {"id": 220, "seek": 139122, "start": 1410.8600000000001, "end": 1415.5, "text": " se ve usted de trepo que aparece en las dos. Bien, entre ms\u00f3n y blu solamente aparece en", "tokens": [51346, 369, 1241, 10467, 368, 2192, 2259, 631, 37863, 465, 2439, 4491, 13, 16956, 11, 3962, 275, 82, 1801, 288, 888, 84, 27814, 37863, 465, 51578], "temperature": 0.2, "avg_logprob": -0.4389739925578489, "compression_ratio": 1.9135135135135135, "no_speech_prob": 0.038435883820056915}, {"id": 221, "seek": 139122, "start": 1415.5, "end": 1420.82, "text": " la segunda, as\u00ed que voy a tener 0.33 y entre ms\u00f3n y flavor, no tengo nada. Despu\u00e9s, entre", "tokens": [51578, 635, 21978, 11, 8582, 631, 7552, 257, 11640, 1958, 13, 10191, 288, 3962, 275, 82, 1801, 288, 932, 1924, 11, 572, 13989, 8096, 13, 40995, 11, 3962, 51844], "temperature": 0.2, "avg_logprob": -0.4389739925578489, "compression_ratio": 1.9135135135135135, "no_speech_prob": 0.038435883820056915}, {"id": 222, "seek": 142082, "start": 1420.82, "end": 1428.06, "text": " blu y de solamente aparece en la segunda, as\u00ed que voy a tener 0.33, entre blu y j. Creo que", "tokens": [50364, 888, 84, 288, 368, 27814, 37863, 465, 635, 21978, 11, 8582, 631, 7552, 257, 11640, 1958, 13, 10191, 11, 3962, 888, 84, 288, 361, 13, 40640, 631, 50726], "temperature": 0.0, "avg_logprob": -0.2770682903898864, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.018116822466254234}, {"id": 223, "seek": 142082, "start": 1428.06, "end": 1433.58, "text": " de vuelta tengo 0.33 y entre blu y blu tambi\u00e9n, 0.33 y no aparece junto con flavor.", "tokens": [50726, 368, 41542, 13989, 1958, 13, 10191, 288, 3962, 888, 84, 288, 888, 84, 6407, 11, 1958, 13, 10191, 288, 572, 37863, 24663, 416, 932, 1924, 13, 51002], "temperature": 0.0, "avg_logprob": -0.2770682903898864, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.018116822466254234}, {"id": 224, "seek": 142082, "start": 1433.58, "end": 1443.98, "text": " Y para despu\u00e9s para flar, tengo 0.5, donde 0.jero con j. 0.5 con flavor. Bien, entonces,", "tokens": [51002, 398, 1690, 15283, 1690, 932, 289, 11, 13989, 1958, 13, 20, 11, 10488, 1958, 13, 73, 2032, 416, 361, 13, 1958, 13, 20, 416, 932, 1924, 13, 16956, 11, 13003, 11, 51522], "temperature": 0.0, "avg_logprob": -0.2770682903898864, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.018116822466254234}, {"id": 225, "seek": 142082, "start": 1443.98, "end": 1448.9399999999998, "text": " y si una pasada por todas las salinaciones y me calcul\u00e9 cu\u00e1les son los peso relativos de cada", "tokens": [51522, 288, 1511, 2002, 1736, 1538, 1515, 10906, 2439, 1845, 259, 9188, 288, 385, 4322, 526, 2702, 842, 904, 1872, 1750, 28149, 21960, 329, 368, 8411, 51770], "temperature": 0.0, "avg_logprob": -0.2770682903898864, "compression_ratio": 1.5851528384279476, "no_speech_prob": 0.018116822466254234}, {"id": 226, "seek": 144894, "start": 1448.94, "end": 1454.14, "text": " una de estos pares. Lo siguiente que hago, como esto va a ser una probabilidad, es normalizar.", "tokens": [50364, 2002, 368, 12585, 2502, 495, 13, 6130, 25666, 631, 38721, 11, 2617, 7433, 2773, 257, 816, 2002, 31959, 4580, 11, 785, 2710, 9736, 13, 50624], "temperature": 0.0, "avg_logprob": -0.3516551494598389, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.08599372208118439}, {"id": 227, "seek": 144894, "start": 1454.14, "end": 1458.74, "text": " Entonces, no voy a construir una tabla, digamos, normalizando por, digamos, voy a sumar en cada", "tokens": [50624, 15097, 11, 572, 7552, 257, 38445, 2002, 4421, 875, 11, 36430, 11, 2710, 590, 1806, 1515, 11, 36430, 11, 7552, 257, 2408, 289, 465, 8411, 50854], "temperature": 0.0, "avg_logprob": -0.3516551494598389, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.08599372208118439}, {"id": 228, "seek": 144894, "start": 1458.74, "end": 1463.66, "text": " fila y voy a adir entre la cantidad que aparece para cada fila, as\u00ed que, igual tambi\u00e9n.", "tokens": [50854, 1387, 64, 288, 7552, 257, 614, 347, 3962, 635, 33757, 631, 37863, 1690, 8411, 1387, 64, 11, 8582, 631, 11, 10953, 6407, 13, 51100], "temperature": 0.0, "avg_logprob": -0.3516551494598389, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.08599372208118439}, {"id": 229, "seek": 146366, "start": 1463.66, "end": 1488.1000000000001, "text": " Entonces, lo que voy a hacer es normalizar, entonces, si yo sumo a estos sacas, creo que me da dos", "tokens": [50364, 15097, 11, 450, 631, 7552, 257, 6720, 785, 2710, 9736, 11, 13003, 11, 1511, 5290, 2408, 78, 257, 12585, 4899, 296, 11, 14336, 631, 385, 1120, 4491, 51586], "temperature": 0.0, "avg_logprob": -0.6149850045481036, "compression_ratio": 1.1951219512195121, "no_speech_prob": 0.3427085876464844}, {"id": 230, "seek": 148810, "start": 1488.1, "end": 1496.3, "text": " centodal, no, tres centodal, tengo los valores ac\u00e1, vamos a tener que hacer los c\u00e1lculos, pero", "tokens": [50364, 1489, 378, 304, 11, 572, 11, 15890, 1489, 378, 304, 11, 13989, 1750, 38790, 23496, 11, 5295, 257, 11640, 631, 6720, 1750, 6476, 75, 32397, 11, 4768, 50774], "temperature": 0.0, "avg_logprob": -0.39011624654134114, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.5476596355438232}, {"id": 231, "seek": 148810, "start": 1496.3, "end": 1502.1599999999999, "text": " s\u00ed, me da tres centodal, entonces lo que pasa cuando yo normalizo es que ac\u00e1 me queda 0.24,", "tokens": [50774, 8600, 11, 385, 1120, 15890, 1489, 378, 304, 11, 13003, 450, 631, 20260, 7767, 5290, 2710, 19055, 785, 631, 23496, 385, 23314, 1958, 13, 7911, 11, 51067], "temperature": 0.0, "avg_logprob": -0.39011624654134114, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.5476596355438232}, {"id": 232, "seek": 148810, "start": 1502.1599999999999, "end": 1510.6999999999998, "text": " ac\u00e1 me queda 0.28, ac\u00e1 me queda 0.12 y ac\u00e1 me queda 0.17, pues el segundo tambi\u00e9n lo normalizo,", "tokens": [51067, 23496, 385, 23314, 1958, 13, 11205, 11, 23496, 385, 23314, 1958, 13, 4762, 288, 23496, 385, 23314, 1958, 13, 7773, 11, 11059, 806, 17954, 6407, 450, 2710, 19055, 11, 51494], "temperature": 0.0, "avg_logprob": -0.39011624654134114, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.5476596355438232}, {"id": 233, "seek": 151070, "start": 1510.7, "end": 1521.54, "text": " es entre 2 y me queda 0.42, 0.42, 0.16, 0, el tercero ya suma 1, as\u00ed que me queda 0.23, 0.23,", "tokens": [50364, 785, 3962, 568, 288, 385, 23314, 1958, 13, 15628, 11, 1958, 13, 15628, 11, 1958, 13, 6866, 11, 1958, 11, 806, 38103, 78, 2478, 2408, 64, 502, 11, 8582, 631, 385, 23314, 1958, 13, 9356, 11, 1958, 13, 9356, 11, 50906], "temperature": 0.0, "avg_logprob": -0.3034163110050154, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.059152353554964066}, {"id": 234, "seek": 151070, "start": 1521.54, "end": 1535.98, "text": " 0.23 y el \u00faltimo tambi\u00e9n queda igual, 0.5, 0, 0, 0, 0.25. Bien, entonces, me constru\u00ed una nueva tabla", "tokens": [50906, 1958, 13, 9356, 288, 806, 21013, 6407, 23314, 10953, 11, 1958, 13, 20, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 13, 6074, 13, 16956, 11, 13003, 11, 385, 12946, 870, 2002, 28963, 4421, 875, 51628], "temperature": 0.0, "avg_logprob": -0.3034163110050154, "compression_ratio": 1.4014084507042253, "no_speech_prob": 0.059152353554964066}, {"id": 235, "seek": 153598, "start": 1535.98, "end": 1541.94, "text": " de probabilidad de traducci\u00f3n dado que ahora la salinaci\u00f3n es serianistas, y no te lo que pas\u00f3", "tokens": [50364, 368, 31959, 4580, 368, 2479, 1311, 5687, 29568, 631, 9923, 635, 1845, 259, 3482, 785, 816, 952, 14858, 11, 288, 572, 535, 450, 631, 41382, 50662], "temperature": 0.0, "avg_logprob": -0.40682708076808766, "compression_ratio": 1.9386792452830188, "no_speech_prob": 0.22816802561283112}, {"id": 236, "seek": 153598, "start": 1541.94, "end": 1552.9, "text": " ac\u00e1, si yo miro la fila correspondiente a la que lo que pasa ahora con esta fila, recuerden que yo", "tokens": [50662, 23496, 11, 1511, 5290, 3149, 78, 635, 1387, 64, 6805, 8413, 257, 635, 631, 450, 631, 20260, 9923, 416, 5283, 1387, 64, 11, 39092, 1556, 631, 5290, 51210], "temperature": 0.0, "avg_logprob": -0.40682708076808766, "compression_ratio": 1.9386792452830188, "no_speech_prob": 0.22816802561283112}, {"id": 237, "seek": 153598, "start": 1552.9, "end": 1557.9, "text": " empec\u00e9 de deniendo todas las salinaciones, todas las traducciones de pronto, todas las probabilidades", "tokens": [51210, 846, 494, 13523, 368, 1441, 7304, 10906, 2439, 1845, 259, 9188, 11, 10906, 2439, 2479, 1311, 23469, 368, 26194, 11, 10906, 2439, 31959, 10284, 51460], "temperature": 0.0, "avg_logprob": -0.40682708076808766, "compression_ratio": 1.9386792452830188, "no_speech_prob": 0.22816802561283112}, {"id": 238, "seek": 153598, "start": 1557.9, "end": 1563.1, "text": " de traducci\u00f3n de equipares de palabras eran equiprobables, si yo ahora miro la fila de la que es lo que pasa,", "tokens": [51460, 368, 2479, 1311, 5687, 368, 5037, 8643, 368, 35240, 32762, 5037, 16614, 2965, 11, 1511, 5290, 9923, 3149, 78, 635, 1387, 64, 368, 635, 631, 785, 450, 631, 20260, 11, 51720], "temperature": 0.0, "avg_logprob": -0.40682708076808766, "compression_ratio": 1.9386792452830188, "no_speech_prob": 0.22816802561283112}, {"id": 239, "seek": 156598, "start": 1565.98, "end": 1579.74, "text": " es acto, aparece claramente que la asociaci\u00f3n entre la idea es m\u00e1s fuerte, tengo un 0.44 de probabilidad de traducir", "tokens": [50364, 785, 605, 78, 11, 37863, 6093, 3439, 631, 635, 382, 78, 537, 3482, 3962, 635, 1558, 785, 3573, 37129, 11, 13989, 517, 1958, 13, 13912, 368, 31959, 4580, 368, 2479, 1311, 347, 51052], "temperature": 0.0, "avg_logprob": -0.3053744820987477, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.04511474445462227}, {"id": 240, "seek": 156598, "start": 1579.74, "end": 1586.22, "text": " la como de y tengo bastante menos en los otros, tengo 0.28, 0.27 y yo hab\u00eda empezado diciendo que eran", "tokens": [51052, 635, 2617, 368, 288, 13989, 14651, 8902, 465, 1750, 16422, 11, 13989, 1958, 13, 11205, 11, 1958, 13, 10076, 288, 5290, 16395, 18730, 1573, 42797, 631, 32762, 51376], "temperature": 0.0, "avg_logprob": -0.3053744820987477, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.04511474445462227}, {"id": 241, "seek": 156598, "start": 1586.22, "end": 1592.5, "text": " equiprobables, entonces yo probablemente ten\u00eda 0.25, 0.25, 0.25, 0.25, 0.25 en cada una, y despu\u00e9s de", "tokens": [51376, 5037, 16614, 2965, 11, 13003, 5290, 21759, 4082, 23718, 1958, 13, 6074, 11, 1958, 13, 6074, 11, 1958, 13, 6074, 11, 1958, 13, 6074, 11, 1958, 13, 6074, 465, 8411, 2002, 11, 288, 15283, 368, 51690], "temperature": 0.0, "avg_logprob": -0.3053744820987477, "compression_ratio": 1.5598086124401913, "no_speech_prob": 0.04511474445462227}, {"id": 242, "seek": 159250, "start": 1592.5, "end": 1600.46, "text": " un paso de la iteraci\u00f3n, descubri\u00f3 que la idea tiene m\u00e1s chance de ser una traducci\u00f3n", "tokens": [50364, 517, 29212, 368, 635, 17138, 3482, 11, 32592, 44802, 631, 635, 1558, 7066, 3573, 2931, 368, 816, 2002, 2479, 1311, 5687, 50762], "temperature": 0.0, "avg_logprob": -0.25633675711495535, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.04872170463204384}, {"id": 243, "seek": 159250, "start": 1600.46, "end": 1606.46, "text": " de la otra, en vez de traducirla como jados o la como bl\u00fa o la como flower, eso pasa en", "tokens": [50762, 368, 635, 13623, 11, 465, 5715, 368, 2479, 1311, 347, 875, 2617, 361, 4181, 277, 635, 2617, 888, 2481, 277, 635, 2617, 8617, 11, 7287, 20260, 465, 51062], "temperature": 0.0, "avg_logprob": -0.25633675711495535, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.04872170463204384}, {"id": 244, "seek": 159250, "start": 1606.46, "end": 1611.66, "text": " el primer paso, en la primera iteraci\u00f3n, el tipo descubre, el algoritmo descubre que la", "tokens": [51062, 806, 12595, 29212, 11, 465, 635, 17382, 17138, 3482, 11, 806, 9746, 32592, 265, 11, 806, 3501, 50017, 3280, 32592, 265, 631, 635, 51322], "temperature": 0.0, "avg_logprob": -0.25633675711495535, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.04872170463204384}, {"id": 245, "seek": 159250, "start": 1611.66, "end": 1618.06, "text": " asociaci\u00f3n entre la idea es bastante m\u00e1s fuerte, como pasa eso, lo que va a pasar es que cuando", "tokens": [51322, 382, 78, 537, 3482, 3962, 635, 1558, 785, 14651, 3573, 37129, 11, 2617, 20260, 7287, 11, 450, 631, 2773, 257, 25344, 785, 631, 7767, 51642], "temperature": 0.0, "avg_logprob": -0.25633675711495535, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.04872170463204384}, {"id": 246, "seek": 161806, "start": 1618.06, "end": 1623.12, "text": " yo reparto de vuelta en las alinaciones, estas l\u00edneas que se corresponden a la asociaci\u00f3n", "tokens": [50364, 5290, 1085, 15864, 368, 41542, 465, 2439, 419, 259, 9188, 11, 13897, 16118, 716, 296, 631, 369, 6805, 268, 257, 635, 382, 78, 537, 3482, 50617], "temperature": 0.0, "avg_logprob": -0.27014730926743125, "compression_ratio": 1.9057377049180328, "no_speech_prob": 0.09157466143369675}, {"id": 247, "seek": 161806, "start": 1623.12, "end": 1628.76, "text": " entre la idea van a estar m\u00e1s fuertes, van a tener un poco m\u00e1s de peso, y como esto es una", "tokens": [50617, 3962, 635, 1558, 3161, 257, 8755, 3573, 8536, 911, 279, 11, 3161, 257, 11640, 517, 10639, 3573, 368, 28149, 11, 288, 2617, 7433, 785, 2002, 50899], "temperature": 0.0, "avg_logprob": -0.27014730926743125, "compression_ratio": 1.9057377049180328, "no_speech_prob": 0.09157466143369675}, {"id": 248, "seek": 161806, "start": 1628.76, "end": 1633.6799999999998, "text": " distribuci\u00f3n de probabilidad es esa masa que gan\u00f3 la asociaci\u00f3n entre la idea, se va a tener", "tokens": [50899, 4400, 30813, 368, 31959, 4580, 785, 11342, 29216, 631, 7574, 812, 635, 382, 78, 537, 3482, 3962, 635, 1558, 11, 369, 2773, 257, 11640, 51145], "temperature": 0.0, "avg_logprob": -0.27014730926743125, "compression_ratio": 1.9057377049180328, "no_speech_prob": 0.09157466143369675}, {"id": 249, "seek": 161806, "start": 1633.6799999999998, "end": 1637.22, "text": " que sacar de otras alinaciones posibles, as\u00ed la asociaci\u00f3n va a con de, entonces no est\u00e1", "tokens": [51145, 631, 43823, 368, 20244, 419, 259, 9188, 1366, 14428, 11, 8582, 635, 382, 78, 537, 3482, 2773, 257, 416, 368, 11, 13003, 572, 3192, 51322], "temperature": 0.0, "avg_logprob": -0.27014730926743125, "compression_ratio": 1.9057377049180328, "no_speech_prob": 0.09157466143369675}, {"id": 250, "seek": 161806, "start": 1637.22, "end": 1643.62, "text": " asociada con las otras que est\u00e1n alrededor, entonces esa masa que se pierde, digamos, o sea", "tokens": [51322, 382, 78, 537, 1538, 416, 2439, 20244, 631, 10368, 43663, 11, 13003, 11342, 29216, 631, 369, 9766, 1479, 11, 36430, 11, 277, 4158, 51642], "temperature": 0.0, "avg_logprob": -0.27014730926743125, "compression_ratio": 1.9057377049180328, "no_speech_prob": 0.09157466143369675}, {"id": 251, "seek": 164362, "start": 1643.62, "end": 1651.0, "text": " que gana en la de, se tiene que repartir en las otras alinaciones posibles, o sea, en las", "tokens": [50364, 631, 7574, 64, 465, 635, 368, 11, 369, 7066, 631, 1085, 446, 347, 465, 2439, 20244, 419, 259, 9188, 1366, 14428, 11, 277, 4158, 11, 465, 2439, 50733], "temperature": 0.0, "avg_logprob": -0.2825263125225178, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.021427830681204796}, {"id": 252, "seek": 164362, "start": 1651.0, "end": 1656.34, "text": " que no son entre la idea, entonces despu\u00e9s de una iteraci\u00f3n la asociaci\u00f3n entre la", "tokens": [50733, 631, 572, 1872, 3962, 635, 1558, 11, 13003, 15283, 368, 2002, 17138, 3482, 635, 382, 78, 537, 3482, 3962, 635, 51000], "temperature": 0.0, "avg_logprob": -0.2825263125225178, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.021427830681204796}, {"id": 253, "seek": 164362, "start": 1656.34, "end": 1663.4199999999998, "text": " idea empieza a ser m\u00e1s fuerte, y como pasa eso, en la siguiente iteraci\u00f3n va a empezar", "tokens": [51000, 1558, 44577, 257, 816, 3573, 37129, 11, 288, 2617, 20260, 7287, 11, 465, 635, 25666, 17138, 3482, 2773, 257, 31168, 51354], "temperature": 0.0, "avg_logprob": -0.2825263125225178, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.021427830681204796}, {"id": 254, "seek": 164362, "start": 1663.4199999999998, "end": 1668.06, "text": " a descubrir que como la estaba alinado con de, entonces me son tiene que estar alinado con jados,", "tokens": [51354, 257, 32592, 10949, 631, 2617, 635, 17544, 419, 259, 1573, 416, 368, 11, 13003, 385, 1872, 7066, 631, 8755, 419, 259, 1573, 416, 361, 4181, 11, 51586], "temperature": 0.0, "avg_logprob": -0.2825263125225178, "compression_ratio": 1.7658536585365854, "no_speech_prob": 0.021427830681204796}, {"id": 255, "seek": 166806, "start": 1668.06, "end": 1675.34, "text": " y como me son estaba alinado con jados, digamos esa esa misma masa de probabilidad se va a", "tokens": [50364, 288, 2617, 385, 1872, 17544, 419, 259, 1573, 416, 361, 4181, 11, 36430, 11342, 11342, 24946, 29216, 368, 31959, 4580, 369, 2773, 257, 50728], "temperature": 0.0, "avg_logprob": -0.29105546092259066, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.07278048992156982}, {"id": 256, "seek": 166806, "start": 1675.34, "end": 1680.74, "text": " traducir a transferir a la segunda, y lo mismo, como le ha estado alinado con de, entonces", "tokens": [50728, 2479, 1311, 347, 257, 5003, 347, 257, 635, 21978, 11, 288, 450, 12461, 11, 2617, 476, 324, 18372, 419, 259, 1573, 416, 368, 11, 13003, 50998], "temperature": 0.0, "avg_logprob": -0.29105546092259066, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.07278048992156982}, {"id": 257, "seek": 166806, "start": 1680.74, "end": 1687.1, "text": " fler tiene que estar alinado con flower, entonces si yo sigo iterando en estos pasos, en cada", "tokens": [50998, 932, 260, 7066, 631, 8755, 419, 259, 1573, 416, 8617, 11, 13003, 1511, 5290, 4556, 78, 17138, 1806, 465, 12585, 1736, 329, 11, 465, 8411, 51316], "temperature": 0.0, "avg_logprob": -0.29105546092259066, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.07278048992156982}, {"id": 258, "seek": 166806, "start": 1687.1, "end": 1690.98, "text": " paso lo que va a pasar es que se va a mover un poco m\u00e1s de probabilidad, hasta que al final", "tokens": [51316, 29212, 450, 631, 2773, 257, 25344, 785, 631, 369, 2773, 257, 39945, 517, 10639, 3573, 368, 31959, 4580, 11, 10764, 631, 419, 2572, 51510], "temperature": 0.0, "avg_logprob": -0.29105546092259066, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.07278048992156982}, {"id": 259, "seek": 166806, "start": 1690.98, "end": 1695.98, "text": " va a terminar descubriendo cu\u00e1l es la alinaci\u00f3n real de las palabras, o sea va a descubrir", "tokens": [51510, 2773, 257, 36246, 32592, 470, 3999, 44318, 785, 635, 419, 259, 3482, 957, 368, 2439, 35240, 11, 277, 4158, 2773, 257, 32592, 10949, 51760], "temperature": 0.0, "avg_logprob": -0.29105546092259066, "compression_ratio": 1.81496062992126, "no_speech_prob": 0.07278048992156982}, {"id": 260, "seek": 169598, "start": 1695.98, "end": 1702.9, "text": " que la va, o sea, con de, me son con jados, luego con blue, luego con flower, como que va descubrir", "tokens": [50364, 631, 635, 2773, 11, 277, 4158, 11, 416, 368, 11, 385, 1872, 416, 361, 4181, 11, 17222, 416, 3344, 11, 17222, 416, 8617, 11, 2617, 631, 2773, 32592, 10949, 50710], "temperature": 0.0, "avg_logprob": -0.4085905318869684, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.1229071319103241}, {"id": 261, "seek": 169598, "start": 1702.9, "end": 1707.06, "text": " eso, porque en cada paso lo que va pasando es que algunas de las asociaciones, como est\u00e1n,", "tokens": [50710, 7287, 11, 4021, 465, 8411, 29212, 450, 631, 2773, 45412, 785, 631, 27316, 368, 2439, 382, 78, 537, 9188, 11, 2617, 10368, 11, 50918], "temperature": 0.0, "avg_logprob": -0.4085905318869684, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.1229071319103241}, {"id": 262, "seek": 169598, "start": 1707.06, "end": 1712.3, "text": " como aparecen co-curren, digamos, en m\u00e1s oraciones, tienen m\u00e1s fuerza que otras, entonces el", "tokens": [50918, 2617, 15004, 13037, 598, 12, 14112, 1095, 11, 36430, 11, 465, 3573, 420, 9188, 11, 12536, 3573, 39730, 631, 20244, 11, 13003, 806, 51180], "temperature": 0.0, "avg_logprob": -0.4085905318869684, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.1229071319103241}, {"id": 263, "seek": 169598, "start": 1712.3, "end": 1717.06, "text": " peso que esas asociaciones ganan lo va sacando otro lado, y eso hace que de otro lado se", "tokens": [51180, 28149, 631, 23388, 382, 78, 537, 9188, 7574, 282, 450, 2773, 4899, 1806, 11921, 11631, 11, 288, 7287, 10032, 631, 368, 11921, 11631, 369, 51418], "temperature": 0.0, "avg_logprob": -0.4085905318869684, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.1229071319103241}, {"id": 264, "seek": 169598, "start": 1717.06, "end": 1724.34, "text": " empieza a generar otras alinaciones diferentes, entonces al final esto termina convergiendo que termina", "tokens": [51418, 44577, 257, 1337, 289, 20244, 419, 259, 9188, 17686, 11, 13003, 419, 2572, 7433, 1433, 1426, 9652, 70, 7304, 631, 1433, 1426, 51782], "temperature": 0.0, "avg_logprob": -0.4085905318869684, "compression_ratio": 1.8565891472868217, "no_speech_prob": 0.1229071319103241}, {"id": 265, "seek": 172434, "start": 1724.34, "end": 1728.74, "text": " revelando lo que es la, la estructura, su yacente de las palabras, y como se alinian unas", "tokens": [50364, 15262, 1806, 450, 631, 785, 635, 11, 635, 43935, 2991, 11, 459, 288, 326, 1576, 368, 2439, 35240, 11, 288, 2617, 369, 419, 259, 952, 25405, 50584], "temperature": 0.0, "avg_logprob": -0.3419358389718192, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.05316515639424324}, {"id": 266, "seek": 172434, "start": 1728.74, "end": 1734.5, "text": " con otras, bueno, bien, a ver que yo termine de hacer esto, puedo agarrar y construir me efectivamente", "tokens": [50584, 416, 20244, 11, 11974, 11, 3610, 11, 257, 1306, 631, 5290, 1433, 533, 368, 6720, 7433, 11, 21612, 623, 2284, 289, 288, 38445, 385, 22565, 23957, 50872], "temperature": 0.0, "avg_logprob": -0.3419358389718192, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.05316515639424324}, {"id": 267, "seek": 172434, "start": 1734.5, "end": 1740.06, "text": " la tabla final de traducciones, que es simplemente busco cada una de las posibles traducciones,", "tokens": [50872, 635, 4421, 875, 2572, 368, 2479, 1311, 23469, 11, 631, 785, 33190, 1255, 1291, 8411, 2002, 368, 2439, 1366, 14428, 2479, 1311, 23469, 11, 51150], "temperature": 0.0, "avg_logprob": -0.3419358389718192, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.05316515639424324}, {"id": 268, "seek": 172434, "start": 1740.06, "end": 1747.4199999999998, "text": " digamos, de los posibles pares y saco las probabilidades, y qu\u00e9 pas\u00f3 ac\u00e1, mientras yo", "tokens": [51150, 36430, 11, 368, 1750, 1366, 14428, 971, 279, 288, 4899, 78, 2439, 31959, 10284, 11, 288, 8057, 41382, 23496, 11, 26010, 5290, 51518], "temperature": 0.0, "avg_logprob": -0.3419358389718192, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.05316515639424324}, {"id": 269, "seek": 172434, "start": 1747.4199999999998, "end": 1752.5, "text": " estaba construyendo mi modelo traducci\u00f3n, mientras yo estaba construyendo la tabla de traducciones", "tokens": [51518, 17544, 12946, 88, 3999, 2752, 27825, 2479, 1311, 5687, 11, 26010, 5290, 17544, 12946, 88, 3999, 635, 4421, 875, 368, 2479, 1311, 23469, 51772], "temperature": 0.0, "avg_logprob": -0.3419358389718192, "compression_ratio": 1.8705882352941177, "no_speech_prob": 0.05316515639424324}, {"id": 270, "seek": 175250, "start": 1752.5, "end": 1758.34, "text": " adem\u00e1s de, como efectos secundarios se construy\u00f3 un corpus alinia, un corpus que est\u00e1 alineado", "tokens": [50364, 21251, 368, 11, 2617, 22565, 329, 907, 997, 9720, 369, 12946, 88, 812, 517, 1181, 31624, 419, 259, 654, 11, 517, 1181, 31624, 631, 3192, 419, 533, 1573, 50656], "temperature": 0.0, "avg_logprob": -0.33393173217773436, "compression_ratio": 1.6534090909090908, "no_speech_prob": 0.0086788609623909}, {"id": 271, "seek": 175250, "start": 1758.34, "end": 1771.5, "text": " nivel de palabras, as\u00ed que bueno, el algoritmo de espectrexi\u00f3n maximizaci\u00f3n, funcionan esa manera,", "tokens": [50656, 24423, 368, 35240, 11, 8582, 631, 11974, 11, 806, 3501, 50017, 3280, 368, 38244, 265, 87, 2560, 5138, 27603, 11, 14186, 282, 11342, 13913, 11, 51314], "temperature": 0.0, "avg_logprob": -0.33393173217773436, "compression_ratio": 1.6534090909090908, "no_speech_prob": 0.0086788609623909}, {"id": 272, "seek": 175250, "start": 1771.5, "end": 1777.42, "text": " tiene siempre dos pasos, un paso de espectrexi\u00f3n y un paso de maximizaci\u00f3n, en este caso,", "tokens": [51314, 7066, 12758, 4491, 1736, 329, 11, 517, 29212, 368, 38244, 265, 87, 2560, 288, 517, 29212, 368, 5138, 27603, 11, 465, 4065, 9666, 11, 51610], "temperature": 0.0, "avg_logprob": -0.33393173217773436, "compression_ratio": 1.6534090909090908, "no_speech_prob": 0.0086788609623909}, {"id": 273, "seek": 177742, "start": 1777.42, "end": 1784.38, "text": " el espectrexi\u00f3n era decir el paso de espectrexi\u00f3n, se trataba de agarrar la tabla de", "tokens": [50364, 806, 38244, 265, 87, 2560, 4249, 10235, 806, 29212, 368, 38244, 265, 87, 2560, 11, 369, 21507, 5509, 368, 623, 2284, 289, 635, 4421, 875, 368, 50712], "temperature": 0.0, "avg_logprob": -0.34909955590172154, "compression_ratio": 1.7, "no_speech_prob": 0.03027229942381382}, {"id": 274, "seek": 177742, "start": 1784.38, "end": 1789.78, "text": " propiedad traducci\u00f3n que tengo, y con eso me damos alinianciones, y despu\u00e9s el de maximizaci\u00f3n", "tokens": [50712, 2365, 1091, 345, 2479, 1311, 5687, 631, 13989, 11, 288, 416, 7287, 385, 274, 2151, 419, 259, 952, 23469, 11, 288, 15283, 806, 368, 5138, 27603, 50982], "temperature": 0.0, "avg_logprob": -0.34909955590172154, "compression_ratio": 1.7, "no_speech_prob": 0.03027229942381382}, {"id": 275, "seek": 177742, "start": 1789.78, "end": 1794.26, "text": " es al rev\u00e9s, agarrar las alinianciones que acabo de construir y me damos una nueva tabla, y voy", "tokens": [50982, 785, 419, 3698, 2191, 11, 623, 2284, 289, 2439, 419, 259, 952, 23469, 631, 13281, 78, 368, 38445, 288, 385, 274, 2151, 2002, 28963, 4421, 875, 11, 288, 7552, 51206], "temperature": 0.0, "avg_logprob": -0.34909955590172154, "compression_ratio": 1.7, "no_speech_prob": 0.03027229942381382}, {"id": 276, "seek": 177742, "start": 1794.26, "end": 1801.66, "text": " alterando todos esos pasos hasta que eventualmente converg, bien, dijimos que eran 5 modelos", "tokens": [51206, 11337, 1806, 6321, 22411, 1736, 329, 10764, 631, 33160, 4082, 9652, 70, 11, 3610, 11, 47709, 8372, 631, 32762, 1025, 2316, 329, 51576], "temperature": 0.0, "avg_logprob": -0.34909955590172154, "compression_ratio": 1.7, "no_speech_prob": 0.03027229942381382}, {"id": 277, "seek": 180166, "start": 1801.66, "end": 1806.6200000000001, "text": " de IBM, nos vamos a ver muy en detr\u00e1s y los otros, o sea, solo mencionar que empiezan a", "tokens": [50364, 368, 23487, 11, 3269, 5295, 257, 1306, 5323, 465, 1141, 17168, 288, 1750, 16422, 11, 277, 4158, 11, 6944, 37030, 289, 631, 4012, 18812, 282, 257, 50612], "temperature": 0.0, "avg_logprob": -0.33645650112267694, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.49524039030075073}, {"id": 278, "seek": 180166, "start": 1806.6200000000001, "end": 1812.42, "text": " agregar complejidad, en este modelo uno hab\u00edamos dicho que todas las alinianciones eran equiprobables,", "tokens": [50612, 4554, 2976, 44424, 73, 4580, 11, 465, 4065, 27825, 8526, 3025, 16275, 27346, 631, 10906, 2439, 419, 259, 952, 23469, 32762, 5037, 16614, 2965, 11, 50902], "temperature": 0.0, "avg_logprob": -0.33645650112267694, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.49524039030075073}, {"id": 279, "seek": 180166, "start": 1812.42, "end": 1816.9, "text": " en el modelo 2 abandonan esa noci\u00f3n y dicen bueno en vez de alinianciones equiprobables, yo voy a", "tokens": [50902, 465, 806, 27825, 568, 9072, 282, 11342, 572, 5687, 288, 33816, 11974, 465, 5715, 368, 419, 259, 952, 23469, 5037, 16614, 2965, 11, 5290, 7552, 257, 51126], "temperature": 0.0, "avg_logprob": -0.33645650112267694, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.49524039030075073}, {"id": 280, "seek": 180166, "start": 1816.9, "end": 1822.18, "text": " tener un modelo de reordinamiento de las palabras para decir bueno, tengo cierta probabilidad de que", "tokens": [51126, 11640, 517, 27825, 368, 319, 765, 259, 16971, 368, 2439, 35240, 1690, 10235, 11974, 11, 13989, 39769, 1328, 31959, 4580, 368, 631, 51390], "temperature": 0.0, "avg_logprob": -0.33645650112267694, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.49524039030075073}, {"id": 281, "seek": 180166, "start": 1822.18, "end": 1826.94, "text": " las palabras que est\u00e1n si yo tengo y palabras en ingl\u00e9s, jota palabras en espa\u00f1ol, tengo cierta", "tokens": [51390, 2439, 35240, 631, 10368, 1511, 5290, 13989, 288, 35240, 465, 49766, 11, 361, 5377, 35240, 465, 31177, 11, 13989, 39769, 1328, 51628], "temperature": 0.0, "avg_logprob": -0.33645650112267694, "compression_ratio": 1.7473309608540926, "no_speech_prob": 0.49524039030075073}, {"id": 282, "seek": 182694, "start": 1826.94, "end": 1832.74, "text": " probabilidad de mover la palabra ah\u00ed y la palabra jota, y bueno ya s\u00ed siguen subiendo en complejidad", "tokens": [50364, 31959, 4580, 368, 39945, 635, 31702, 12571, 288, 635, 31702, 361, 5377, 11, 288, 11974, 2478, 8600, 4556, 7801, 1422, 7304, 465, 44424, 73, 4580, 50654], "temperature": 0.0, "avg_logprob": -0.32027775049209595, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.16376470029354095}, {"id": 283, "seek": 182694, "start": 1832.74, "end": 1838.46, "text": " hasta llegar al modelo 5, que modelos 5 es el que anda mejor, pero de todas maneras estos", "tokens": [50654, 10764, 24892, 419, 27825, 1025, 11, 631, 2316, 329, 1025, 785, 806, 631, 21851, 11479, 11, 4768, 368, 10906, 587, 6985, 12585, 50940], "temperature": 0.0, "avg_logprob": -0.32027775049209595, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.16376470029354095}, {"id": 284, "seek": 182694, "start": 1838.46, "end": 1845.18, "text": " son modelos que ya no se usan, digamos esto es del a\u00f1o 93 y en general se han obtenido mejores", "tokens": [50940, 1872, 2316, 329, 631, 2478, 572, 369, 505, 282, 11, 36430, 7433, 785, 1103, 15984, 28876, 288, 465, 2674, 369, 7276, 28326, 2925, 42284, 51276], "temperature": 0.0, "avg_logprob": -0.32027775049209595, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.16376470029354095}, {"id": 285, "seek": 182694, "start": 1845.18, "end": 1850.14, "text": " resultados abandonando estos modelos, entonces que vamos a pasar a ver a continuaci\u00f3n, es un modelo", "tokens": [51276, 36796, 9072, 1806, 12585, 2316, 329, 11, 13003, 631, 5295, 257, 25344, 257, 1306, 257, 2993, 3482, 11, 785, 517, 27825, 51524], "temperature": 0.0, "avg_logprob": -0.32027775049209595, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.16376470029354095}, {"id": 286, "seek": 182694, "start": 1850.14, "end": 1855.8600000000001, "text": " bastante m\u00e1s moderno que es lo que s\u00ed, si utiliza bien d\u00eda en traductores como los de Google,", "tokens": [51524, 14651, 3573, 4363, 78, 631, 785, 450, 631, 8600, 11, 1511, 4976, 13427, 3610, 12271, 465, 2479, 11130, 2706, 2617, 1750, 368, 3329, 11, 51810], "temperature": 0.0, "avg_logprob": -0.32027775049209595, "compression_ratio": 1.653061224489796, "no_speech_prob": 0.16376470029354095}, {"id": 287, "seek": 185586, "start": 1855.86, "end": 1873.1, "text": " s\u00ed, es que en realidad lo claro, a ver estos modelos est\u00e1 d\u00edcicos no utiliza ning\u00fan tipo de", "tokens": [50364, 8600, 11, 785, 631, 465, 25635, 450, 16742, 11, 257, 1306, 12585, 2316, 329, 3192, 274, 870, 66, 9940, 572, 4976, 13427, 30394, 9746, 368, 51226], "temperature": 0.2, "avg_logprob": -0.6068549909089741, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.02911008521914482}, {"id": 288, "seek": 185586, "start": 1873.1, "end": 1878.1, "text": " analizador un boludo jico, hay otros modelos que s\u00ed lo hacen, no vamos a dar ning\u00fan", "tokens": [51226, 2624, 590, 5409, 517, 8986, 6207, 361, 2789, 11, 4842, 16422, 2316, 329, 631, 8600, 450, 27434, 11, 572, 5295, 257, 4072, 30394, 51476], "temperature": 0.2, "avg_logprob": -0.6068549909089741, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.02911008521914482}, {"id": 289, "seek": 185586, "start": 1878.1, "end": 1882.58, "text": " no en esta clase pero est\u00e1, hay otros modelos que s\u00ed hacen uso de esa informaci\u00f3n, igual", "tokens": [51476, 572, 465, 5283, 44578, 4768, 3192, 11, 4842, 16422, 2316, 329, 631, 8600, 27434, 22728, 368, 11342, 21660, 11, 10953, 51700], "temperature": 0.2, "avg_logprob": -0.6068549909089741, "compression_ratio": 1.6058823529411765, "no_speech_prob": 0.02911008521914482}, {"id": 290, "seek": 188258, "start": 1882.58, "end": 1887.34, "text": " son como un refinamiento, creo que ninguno lo tiene como en la base del modelo, el uso de", "tokens": [50364, 1872, 2617, 517, 44395, 16971, 11, 14336, 631, 17210, 12638, 450, 7066, 2617, 465, 635, 3096, 1103, 27825, 11, 806, 22728, 368, 50602], "temperature": 0.0, "avg_logprob": -0.3628889938880657, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.12670910358428955}, {"id": 291, "seek": 188258, "start": 1887.34, "end": 1893.3799999999999, "text": " partos pitch, pero s\u00ed cuando no sabes una palabra de una palabra que se conocida en realidad", "tokens": [50602, 644, 329, 7293, 11, 4768, 8600, 7767, 572, 37790, 2002, 31702, 368, 2002, 31702, 631, 369, 15871, 2887, 465, 25635, 50904], "temperature": 0.0, "avg_logprob": -0.3628889938880657, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.12670910358428955}, {"id": 292, "seek": 188258, "start": 1893.3799999999999, "end": 1899.5, "text": " utilizar informaci\u00f3n sobre el partos pitch y eso probablemente te ayuda, en esto modelo", "tokens": [50904, 24060, 21660, 5473, 806, 644, 329, 7293, 288, 7287, 21759, 4082, 535, 30737, 11, 465, 7433, 27825, 51210], "temperature": 0.0, "avg_logprob": -0.3628889938880657, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.12670910358428955}, {"id": 293, "seek": 188258, "start": 1899.5, "end": 1904.1, "text": " por lo menos no lo hab\u00edan tenido en cuenta, bien entonces s\u00ed lo que vamos a ver ahora es el modelo", "tokens": [51210, 1515, 450, 8902, 572, 450, 44466, 33104, 465, 17868, 11, 3610, 13003, 8600, 450, 631, 5295, 257, 1306, 9923, 785, 806, 27825, 51440], "temperature": 0.0, "avg_logprob": -0.3628889938880657, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.12670910358428955}, {"id": 294, "seek": 188258, "start": 1904.1, "end": 1909.1399999999999, "text": " de frases que es algo m\u00e1s moderno y o sea el Google Translate o Bing Translate se basan", "tokens": [51440, 368, 431, 1957, 631, 785, 8655, 3573, 4363, 78, 288, 277, 4158, 806, 3329, 6531, 17593, 277, 30755, 6531, 17593, 369, 987, 282, 51692], "temperature": 0.0, "avg_logprob": -0.3628889938880657, "compression_ratio": 1.673913043478261, "no_speech_prob": 0.12670910358428955}, {"id": 295, "seek": 190914, "start": 1909.38, "end": 1913.1000000000001, "text": " el modelo de este estilo, y bueno antes de ver c\u00f3mo se modi\u00f3 el frases volvamos un poco", "tokens": [50376, 806, 27825, 368, 4065, 37470, 11, 288, 11974, 11014, 368, 1306, 12826, 369, 1072, 7138, 806, 431, 1957, 1996, 85, 2151, 517, 10639, 50562], "temperature": 0.0, "avg_logprob": -0.4400715999466052, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.1315063238143921}, {"id": 296, "seek": 190914, "start": 1913.1000000000001, "end": 1917.5, "text": " de lo que era la alineaci\u00f3n entre palabras, yo ten\u00eda estas frases cl\u00e1sicas, no Mar\u00eda no di una", "tokens": [50562, 368, 450, 631, 4249, 635, 419, 533, 3482, 3962, 35240, 11, 5290, 23718, 13897, 431, 1957, 47434, 9150, 11, 572, 48472, 572, 1026, 2002, 50782], "temperature": 0.0, "avg_logprob": -0.4400715999466052, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.1315063238143921}, {"id": 297, "seek": 190914, "start": 1917.5, "end": 1924.6200000000001, "text": " ofretada de la bruja verde, en ingl\u00e9s era Merit is Not Slap Greenwich y una alineaci\u00f3n", "tokens": [50782, 295, 1505, 1538, 368, 635, 25267, 2938, 29653, 11, 465, 49766, 4249, 6124, 270, 307, 1726, 6187, 569, 6969, 9669, 288, 2002, 419, 533, 3482, 51138], "temperature": 0.0, "avg_logprob": -0.4400715999466052, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.1315063238143921}, {"id": 298, "seek": 190914, "start": 1924.6200000000001, "end": 1928.14, "text": " entre esas dos oraciones en realidad se ver\u00eda como algo as\u00ed, yo tengo que Mar\u00eda se alinea con", "tokens": [51138, 3962, 23388, 4491, 420, 9188, 465, 25635, 369, 1306, 2686, 2617, 8655, 8582, 11, 5290, 13989, 631, 48472, 369, 419, 533, 64, 416, 51314], "temperature": 0.0, "avg_logprob": -0.4400715999466052, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.1315063238143921}, {"id": 299, "seek": 190914, "start": 1928.14, "end": 1934.7, "text": " Merit no se alinea con disnot, se alinea con daba una ofretada de se alinea con ala podr\u00eda ser", "tokens": [51314, 6124, 270, 572, 369, 419, 533, 64, 416, 717, 2247, 11, 369, 419, 533, 64, 416, 274, 5509, 2002, 295, 1505, 1538, 368, 369, 419, 533, 64, 416, 419, 64, 27246, 816, 51642], "temperature": 0.0, "avg_logprob": -0.4400715999466052, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.1315063238143921}, {"id": 300, "seek": 193470, "start": 1934.7, "end": 1942.22, "text": " solamente con la y el que no est\u00e9 alineona, grince alinea con verde y bruja con Wedch,", "tokens": [50364, 27814, 416, 635, 288, 806, 631, 572, 34584, 419, 533, 4037, 11, 677, 1236, 419, 533, 64, 416, 29653, 288, 25267, 2938, 416, 9589, 339, 11, 50740], "temperature": 0.0, "avg_logprob": -0.4397179388230847, "compression_ratio": 1.6632124352331605, "no_speech_prob": 0.06077219545841217}, {"id": 301, "seek": 193470, "start": 1942.22, "end": 1946.66, "text": " qu\u00e9 diferencia tiene esto con la otra alineaci\u00f3n que hab\u00edamos visto hoy,", "tokens": [50740, 8057, 38844, 7066, 7433, 416, 635, 13623, 419, 533, 3482, 631, 3025, 16275, 17558, 13775, 11, 50962], "temperature": 0.0, "avg_logprob": -0.4397179388230847, "compression_ratio": 1.6632124352331605, "no_speech_prob": 0.06077219545841217}, {"id": 302, "seek": 193470, "start": 1946.66, "end": 1955.22, "text": " as\u00ed se les ocurre algo distinto que tiene esta alineaci\u00f3n y la que hab\u00edamos visto hoy,", "tokens": [50962, 8582, 369, 1512, 26430, 265, 8655, 1483, 17246, 631, 7066, 5283, 419, 533, 3482, 288, 635, 631, 3025, 16275, 17558, 13775, 11, 51390], "temperature": 0.0, "avg_logprob": -0.4397179388230847, "compression_ratio": 1.6632124352331605, "no_speech_prob": 0.06077219545841217}, {"id": 303, "seek": 193470, "start": 1955.22, "end": 1959.82, "text": " era Not con No, s\u00ed, y que es lo que cambia ac\u00e1 para que pase eso.", "tokens": [51390, 4249, 1726, 416, 883, 11, 8600, 11, 288, 631, 785, 450, 631, 18751, 654, 23496, 1690, 631, 47125, 7287, 13, 51620], "temperature": 0.0, "avg_logprob": -0.4397179388230847, "compression_ratio": 1.6632124352331605, "no_speech_prob": 0.06077219545841217}, {"id": 304, "seek": 196470, "start": 1964.7, "end": 1972.5800000000002, "text": " Lo que estaba pasando hoy era que yo partida de las palabras en espa\u00f1ol y a las palabras", "tokens": [50364, 6130, 631, 17544, 45412, 13775, 4249, 631, 5290, 644, 2887, 368, 2439, 35240, 465, 31177, 288, 257, 2439, 35240, 50758], "temperature": 0.0, "avg_logprob": -0.33318051899949164, "compression_ratio": 2.3041666666666667, "no_speech_prob": 0.20145489275455475}, {"id": 305, "seek": 196470, "start": 1972.5800000000002, "end": 1975.54, "text": " en ingl\u00e9s y yo ten\u00eda una funci\u00f3n que me me ap\u00e9 a las palabras en espa\u00f1ol con las", "tokens": [50758, 465, 49766, 288, 5290, 23718, 2002, 43735, 631, 385, 385, 1882, 526, 257, 2439, 35240, 465, 31177, 416, 2439, 50906], "temperature": 0.0, "avg_logprob": -0.33318051899949164, "compression_ratio": 2.3041666666666667, "no_speech_prob": 0.20145489275455475}, {"id": 306, "seek": 196470, "start": 1975.54, "end": 1979.3400000000001, "text": " palabras en ingl\u00e9s, entonces yo a cada palabra en espa\u00f1ol como m\u00e1ximo le pod\u00eda hacer", "tokens": [50906, 35240, 465, 49766, 11, 13003, 5290, 257, 8411, 31702, 465, 31177, 2617, 38876, 476, 45588, 6720, 51096], "temperature": 0.0, "avg_logprob": -0.33318051899949164, "compression_ratio": 2.3041666666666667, "no_speech_prob": 0.20145489275455475}, {"id": 307, "seek": 196470, "start": 1979.3400000000001, "end": 1984.18, "text": " corresponder una palabra en ingl\u00e9s, entonces me quedaba que yo pod\u00eda expresar cosas como que", "tokens": [51096, 6805, 260, 2002, 31702, 465, 49766, 11, 13003, 385, 13617, 5509, 631, 5290, 45588, 33397, 289, 12218, 2617, 631, 51338], "temperature": 0.0, "avg_logprob": -0.33318051899949164, "compression_ratio": 2.3041666666666667, "no_speech_prob": 0.20145489275455475}, {"id": 308, "seek": 196470, "start": 1984.18, "end": 1989.74, "text": " daba una ofretada daba esta ofretada a Slap una, esta ofretada, esta ofretada, esta ofretada,", "tokens": [51338, 274, 5509, 2002, 295, 1505, 1538, 274, 5509, 5283, 295, 1505, 1538, 257, 6187, 569, 2002, 11, 5283, 295, 1505, 1538, 11, 5283, 295, 1505, 1538, 11, 5283, 295, 1505, 1538, 11, 51616], "temperature": 0.0, "avg_logprob": -0.33318051899949164, "compression_ratio": 2.3041666666666667, "no_speech_prob": 0.20145489275455475}, {"id": 309, "seek": 196470, "start": 1989.74, "end": 1994.26, "text": " esa ofretada, eso le pod\u00eda expresar, pero no pod\u00eda expresar algo como esto, que no, esta ofretada", "tokens": [51616, 11342, 295, 1505, 1538, 11, 7287, 476, 45588, 33397, 289, 11, 4768, 572, 45588, 33397, 289, 8655, 2617, 7433, 11, 631, 572, 11, 5283, 295, 1505, 1538, 51842], "temperature": 0.0, "avg_logprob": -0.33318051899949164, "compression_ratio": 2.3041666666666667, "no_speech_prob": 0.20145489275455475}, {"id": 310, "seek": 199426, "start": 1994.26, "end": 1998.34, "text": " es Not porque no ser\u00eda una funci\u00f3n, yo no puedo asociar uno de los valores de la funci\u00f3n", "tokens": [50364, 785, 1726, 4021, 572, 23679, 2002, 43735, 11, 5290, 572, 21612, 382, 78, 537, 289, 8526, 368, 1750, 1323, 2706, 368, 635, 43735, 50568], "temperature": 0.0, "avg_logprob": -0.2780248604568781, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0449095256626606}, {"id": 311, "seek": 199426, "start": 1998.34, "end": 2005.42, "text": " con dos cosas de la olcodom\u00ednio y ac\u00e1 en realidad no puedo hacerlo ni en este sentido ni", "tokens": [50568, 416, 4491, 12218, 368, 635, 2545, 66, 378, 298, 10973, 1004, 288, 23496, 465, 25635, 572, 21612, 32039, 3867, 465, 4065, 19850, 3867, 50922], "temperature": 0.0, "avg_logprob": -0.2780248604568781, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0449095256626606}, {"id": 312, "seek": 199426, "start": 2005.42, "end": 2008.62, "text": " en el otro sentido, con una funci\u00f3n no me sirve porque de vuelta me pasa que Slap est\u00e1", "tokens": [50922, 465, 806, 11921, 19850, 11, 416, 2002, 43735, 572, 385, 4735, 303, 4021, 368, 41542, 385, 20260, 631, 6187, 569, 3192, 51082], "temperature": 0.0, "avg_logprob": -0.2780248604568781, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0449095256626606}, {"id": 313, "seek": 199426, "start": 2008.62, "end": 2012.98, "text": " asociado tres cosas, entonces con una funci\u00f3n de alineaci\u00f3n yo no puedo construir este tipo", "tokens": [51082, 382, 78, 537, 1573, 15890, 12218, 11, 13003, 416, 2002, 43735, 368, 419, 533, 3482, 5290, 572, 21612, 38445, 4065, 9746, 51300], "temperature": 0.0, "avg_logprob": -0.2780248604568781, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0449095256626606}, {"id": 314, "seek": 199426, "start": 2012.98, "end": 2019.42, "text": " de expresiones, en realidad necesito algo como un poco m\u00e1s poderoso, esto es lo que dec\u00edamos,", "tokens": [51300, 368, 33397, 5411, 11, 465, 25635, 11909, 3528, 8655, 2617, 517, 10639, 3573, 8152, 9869, 11, 7433, 785, 450, 631, 979, 16275, 11, 51622], "temperature": 0.0, "avg_logprob": -0.2780248604568781, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0449095256626606}, {"id": 315, "seek": 199426, "start": 2019.42, "end": 2023.98, "text": " los modelos dbms siempre usan un mapeo de uno a muchos, usan en una funci\u00f3n de alineaci\u00f3n,", "tokens": [51622, 1750, 2316, 329, 274, 65, 2592, 12758, 505, 282, 517, 463, 494, 78, 368, 8526, 257, 17061, 11, 505, 282, 465, 2002, 43735, 368, 419, 533, 3482, 11, 51850], "temperature": 0.0, "avg_logprob": -0.2780248604568781, "compression_ratio": 1.937062937062937, "no_speech_prob": 0.0449095256626606}, {"id": 316, "seek": 202398, "start": 2023.98, "end": 2027.42, "text": " mapeo de uno a muchos, pero en realidad lo que necesito para poder capturar realmente", "tokens": [50364, 463, 494, 78, 368, 8526, 257, 17061, 11, 4768, 465, 25635, 450, 631, 11909, 3528, 1690, 8152, 3770, 28586, 14446, 50536], "temperature": 0.0, "avg_logprob": -0.2675094863995403, "compression_ratio": 1.9591078066914498, "no_speech_prob": 0.05543301999568939}, {"id": 317, "seek": 202398, "start": 2027.42, "end": 2031.9, "text": " vamos a funcionar en el lenguaje es mapeo de muchos a muchos, yo voy a tener que un conjunto", "tokens": [50536, 5295, 257, 14186, 289, 465, 806, 35044, 84, 11153, 785, 463, 494, 78, 368, 17061, 257, 17061, 11, 5290, 7552, 257, 11640, 631, 517, 37776, 50760], "temperature": 0.0, "avg_logprob": -0.2675094863995403, "compression_ratio": 1.9591078066914498, "no_speech_prob": 0.05543301999568939}, {"id": 318, "seek": 202398, "start": 2031.9, "end": 2036.22, "text": " de palabra se va a traducir en otro conjunto de palabras, definitiva lo que pasa es que", "tokens": [50760, 368, 31702, 369, 2773, 257, 2479, 1311, 347, 465, 11921, 37776, 368, 35240, 11, 28781, 5931, 450, 631, 20260, 785, 631, 50976], "temperature": 0.0, "avg_logprob": -0.2675094863995403, "compression_ratio": 1.9591078066914498, "no_speech_prob": 0.05543301999568939}, {"id": 319, "seek": 202398, "start": 2036.22, "end": 2040.46, "text": " peque\u00f1as frases se traduce en como otras peque\u00f1as frases, por eso necesito un mapeo de", "tokens": [50976, 19132, 32448, 431, 1957, 369, 2479, 4176, 465, 2617, 20244, 19132, 32448, 431, 1957, 11, 1515, 7287, 11909, 3528, 517, 463, 494, 78, 368, 51188], "temperature": 0.0, "avg_logprob": -0.2675094863995403, "compression_ratio": 1.9591078066914498, "no_speech_prob": 0.05543301999568939}, {"id": 320, "seek": 202398, "start": 2040.46, "end": 2046.46, "text": " muchos a muchos, entonces bueno hay algoritmos que agarran estos mapeos que como", "tokens": [51188, 17061, 257, 17061, 11, 13003, 11974, 4842, 3501, 50017, 3415, 631, 623, 2284, 282, 12585, 463, 494, 329, 631, 2617, 51488], "temperature": 0.0, "avg_logprob": -0.2675094863995403, "compression_ratio": 1.9591078066914498, "no_speech_prob": 0.05543301999568939}, {"id": 321, "seek": 202398, "start": 2046.46, "end": 2051.94, "text": " el construimos reci\u00e9n el mapeo de uno a muchos en los dos, en las dos direcciones digamos", "tokens": [51488, 806, 12946, 8372, 4214, 3516, 806, 463, 494, 78, 368, 8526, 257, 17061, 465, 1750, 4491, 11, 465, 2439, 4491, 1264, 35560, 36430, 51762], "temperature": 0.0, "avg_logprob": -0.2675094863995403, "compression_ratio": 1.9591078066914498, "no_speech_prob": 0.05543301999568939}, {"id": 322, "seek": 205194, "start": 2051.94, "end": 2056.66, "text": " y a partir de eso construyen este mapeo de muchos a muchos, por ejemplo el algoritmo de", "tokens": [50364, 288, 257, 13906, 368, 7287, 12946, 16580, 4065, 463, 494, 78, 368, 17061, 257, 17061, 11, 1515, 13358, 806, 3501, 50017, 3280, 368, 50600], "temperature": 0.0, "avg_logprob": -0.28454469534067006, "compression_ratio": 1.7665369649805447, "no_speech_prob": 0.10688398778438568}, {"id": 323, "seek": 205194, "start": 2056.66, "end": 2060.82, "text": " la herramienta quiz\u00e1s m\u00e1s, lo que hace decir bueno yo tengo un corpus en ingl\u00e9s en espa\u00f1ol", "tokens": [50600, 635, 38271, 64, 15450, 2490, 3573, 11, 450, 631, 10032, 10235, 11974, 5290, 13989, 517, 1181, 31624, 465, 49766, 465, 31177, 50808], "temperature": 0.0, "avg_logprob": -0.28454469534067006, "compression_ratio": 1.7665369649805447, "no_speech_prob": 0.10688398778438568}, {"id": 324, "seek": 205194, "start": 2060.82, "end": 2067.9, "text": " alineo utilizando los modelos dbms, voy alineo por un lado de ingl\u00e9s en espa\u00f1ol, por", "tokens": [50808, 419, 533, 78, 19906, 1806, 1750, 2316, 329, 274, 65, 2592, 11, 7552, 419, 533, 78, 1515, 517, 11631, 368, 49766, 465, 31177, 11, 1515, 51162], "temperature": 0.0, "avg_logprob": -0.28454469534067006, "compression_ratio": 1.7665369649805447, "no_speech_prob": 0.10688398778438568}, {"id": 325, "seek": 205194, "start": 2067.9, "end": 2073.14, "text": " otro lado de espa\u00f1ol en ingl\u00e9s, y ac\u00e1 me quedan dos mapeos de uno a n y vamos dos mapeos", "tokens": [51162, 11921, 11631, 368, 31177, 465, 49766, 11, 288, 23496, 385, 13617, 282, 4491, 463, 494, 329, 368, 8526, 257, 297, 288, 5295, 4491, 463, 494, 329, 51424], "temperature": 0.0, "avg_logprob": -0.28454469534067006, "compression_ratio": 1.7665369649805447, "no_speech_prob": 0.10688398778438568}, {"id": 326, "seek": 205194, "start": 2073.14, "end": 2077.98, "text": " con funciones, y despu\u00e9s lo que hago es interceptar esos dos esa dosa de alineaci\u00f3n que me", "tokens": [51424, 416, 1019, 23469, 11, 288, 15283, 450, 631, 38721, 785, 24700, 289, 22411, 4491, 11342, 4491, 64, 368, 419, 533, 3482, 631, 385, 51666], "temperature": 0.0, "avg_logprob": -0.28454469534067006, "compression_ratio": 1.7665369649805447, "no_speech_prob": 0.10688398778438568}, {"id": 327, "seek": 207798, "start": 2077.98, "end": 2086.5, "text": " caron y unirlas, cuando la intercepto o tengo lo que se conoce como puntos de alta confianza no", "tokens": [50364, 1032, 266, 288, 517, 347, 7743, 11, 7767, 635, 24700, 78, 277, 13989, 450, 631, 369, 33029, 384, 2617, 34375, 368, 26495, 49081, 2394, 572, 50790], "temperature": 0.0, "avg_logprob": -0.28925452645369404, "compression_ratio": 2.0, "no_speech_prob": 0.35561755299568176}, {"id": 328, "seek": 207798, "start": 2086.5, "end": 2090.54, "text": " se llegan a ver bien, los puntos negros son los puntos de alta confianza que son los", "tokens": [50790, 369, 11234, 282, 257, 1306, 3610, 11, 1750, 34375, 408, 861, 329, 1872, 1750, 34375, 368, 26495, 49081, 2394, 631, 1872, 1750, 50992], "temperature": 0.0, "avg_logprob": -0.28925452645369404, "compression_ratio": 2.0, "no_speech_prob": 0.35561755299568176}, {"id": 329, "seek": 207798, "start": 2090.54, "end": 2094.78, "text": " de la intersecci\u00f3n y los puntos grises son lo que est\u00e1n en la uni\u00f3n, o sea los que", "tokens": [50992, 368, 635, 728, 8159, 5687, 288, 1750, 34375, 677, 3598, 1872, 450, 631, 10368, 465, 635, 517, 2560, 11, 277, 4158, 1750, 631, 51204], "temperature": 0.0, "avg_logprob": -0.28925452645369404, "compression_ratio": 2.0, "no_speech_prob": 0.35561755299568176}, {"id": 330, "seek": 207798, "start": 2094.78, "end": 2098.38, "text": " pertenec\u00edan algunos de los modelos, entonces la herramienta lo que hace es decir bueno una", "tokens": [51204, 680, 1147, 3045, 11084, 21078, 368, 1750, 2316, 329, 11, 13003, 635, 38271, 64, 450, 631, 10032, 785, 10235, 11974, 2002, 51384], "temperature": 0.0, "avg_logprob": -0.28925452645369404, "compression_ratio": 2.0, "no_speech_prob": 0.35561755299568176}, {"id": 331, "seek": 207798, "start": 2098.38, "end": 2103.34, "text": " vez que yo tengo la intersecci\u00f3n y la uni\u00f3n hago crecer los puntos que est\u00e1n en la intersecci\u00f3n", "tokens": [51384, 5715, 631, 5290, 13989, 635, 728, 8159, 5687, 288, 635, 517, 2560, 38721, 1197, 1776, 1750, 34375, 631, 10368, 465, 635, 728, 8159, 5687, 51632], "temperature": 0.0, "avg_logprob": -0.28925452645369404, "compression_ratio": 2.0, "no_speech_prob": 0.35561755299568176}, {"id": 332, "seek": 210334, "start": 2103.34, "end": 2107.38, "text": " coeleonizando otros puntos que est\u00e1n en la uni\u00f3n, hasta que al final termin\u00f3 completando", "tokens": [50364, 598, 16884, 266, 590, 1806, 16422, 34375, 631, 10368, 465, 635, 517, 2560, 11, 10764, 631, 419, 2572, 10761, 812, 1557, 1806, 50566], "temperature": 0.0, "avg_logprob": -0.4355855449553459, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.1192113533616066}, {"id": 333, "seek": 210334, "start": 2107.38, "end": 2111.78, "text": " digamos todo el imagen, este punto que qued\u00f3 solito ah\u00ed no ser\u00eda parte de la alineaci\u00f3n", "tokens": [50566, 36430, 5149, 806, 40652, 11, 4065, 14326, 631, 13617, 812, 1404, 3528, 12571, 572, 23679, 6975, 368, 635, 419, 533, 3482, 50786], "temperature": 0.0, "avg_logprob": -0.4355855449553459, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.1192113533616066}, {"id": 334, "seek": 210334, "start": 2111.78, "end": 2120.7400000000002, "text": " al final, solo los que puede llegar moviendo de otra vez de puntos ya conocidos, entonces bueno,", "tokens": [50786, 419, 2572, 11, 6944, 1750, 631, 8919, 24892, 2402, 7304, 368, 13623, 5715, 368, 34375, 2478, 15871, 7895, 11, 13003, 11974, 11, 51234], "temperature": 0.0, "avg_logprob": -0.4355855449553459, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.1192113533616066}, {"id": 335, "seek": 210334, "start": 2120.7400000000002, "end": 2127.38, "text": " eso es una forma que utiliza, se llama el algoritmo de ojinei, que partiendo de alineaciones", "tokens": [51234, 7287, 785, 2002, 8366, 631, 4976, 13427, 11, 369, 23272, 806, 3501, 50017, 3280, 368, 277, 73, 533, 72, 11, 631, 644, 7304, 368, 419, 533, 9188, 51566], "temperature": 0.0, "avg_logprob": -0.4355855449553459, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.1192113533616066}, {"id": 336, "seek": 210334, "start": 2127.38, "end": 2131.42, "text": " uniraccionales y vamos me permite construir una alineaci\u00f3n completa, muchos a muchos entre", "tokens": [51566, 517, 347, 8476, 1966, 279, 288, 5295, 385, 31105, 38445, 2002, 419, 533, 3482, 46822, 11, 17061, 257, 17061, 3962, 51768], "temperature": 0.0, "avg_logprob": -0.4355855449553459, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.1192113533616066}, {"id": 337, "seek": 213142, "start": 2131.42, "end": 2136.98, "text": " las palabras, bien, eso le quer\u00eda mencionar acerca de las alineaciones de palabras y ahora", "tokens": [50364, 2439, 35240, 11, 3610, 11, 7287, 476, 37869, 37030, 289, 46321, 368, 2439, 419, 533, 9188, 368, 35240, 288, 9923, 50642], "temperature": 0.0, "avg_logprob": -0.43027923847066946, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.10116104781627655}, {"id": 338, "seek": 213142, "start": 2136.98, "end": 2141.94, "text": " s\u00ed vamos a ver c\u00f3mo funciona un modelo basado en frases, un modelo basado en frases tiene", "tokens": [50642, 8600, 5295, 257, 1306, 12826, 26210, 517, 27825, 987, 1573, 465, 431, 1957, 11, 517, 27825, 987, 1573, 465, 431, 1957, 7066, 50890], "temperature": 0.0, "avg_logprob": -0.43027923847066946, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.10116104781627655}, {"id": 339, "seek": 213142, "start": 2141.94, "end": 2147.46, "text": " cierto semejanza con el modelo anterior que hay hemos visto, pero es un poco m\u00e1s expresivo", "tokens": [50890, 28558, 369, 1398, 73, 20030, 416, 806, 27825, 22272, 631, 4842, 15396, 17558, 11, 4768, 785, 517, 10639, 3573, 33397, 6340, 51166], "temperature": 0.0, "avg_logprob": -0.43027923847066946, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.10116104781627655}, {"id": 340, "seek": 213142, "start": 2147.46, "end": 2151.3, "text": " en realidad yo parte de una oraci\u00f3n, por ejemplo en Aleman que dec\u00eda Morgan Flick y que", "tokens": [51166, 465, 25635, 5290, 6975, 368, 2002, 420, 3482, 11, 1515, 13358, 465, 9366, 1601, 631, 37599, 16724, 3235, 618, 288, 631, 51358], "temperature": 0.0, "avg_logprob": -0.43027923847066946, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.10116104781627655}, {"id": 341, "seek": 213142, "start": 2151.3, "end": 2156.26, "text": " las canas de sus conference, lo primero que hace el modelo cuando quiere traducir, digamos", "tokens": [51358, 2439, 393, 296, 368, 3291, 7586, 11, 450, 21289, 631, 10032, 806, 27825, 7767, 23877, 2479, 1311, 347, 11, 36430, 51606], "temperature": 0.0, "avg_logprob": -0.43027923847066946, "compression_ratio": 1.6703296703296704, "no_speech_prob": 0.10116104781627655}, {"id": 342, "seek": 215626, "start": 2156.26, "end": 2161.78, "text": " en este caso es decir bueno, yo voy a segmentar esa oraci\u00f3n de origen en cierta cantidad", "tokens": [50364, 465, 4065, 9666, 785, 10235, 11974, 11, 5290, 7552, 257, 9469, 289, 11342, 420, 3482, 368, 2349, 268, 465, 39769, 1328, 33757, 50640], "temperature": 0.0, "avg_logprob": -0.23135705190162137, "compression_ratio": 2.0694980694980694, "no_speech_prob": 0.0965171530842781}, {"id": 343, "seek": 215626, "start": 2161.78, "end": 2166.82, "text": " de frases, despu\u00e9s voy a traducir cada una de esas frases usando una tabla de traducci\u00f3n", "tokens": [50640, 368, 431, 1957, 11, 15283, 7552, 257, 2479, 1311, 347, 8411, 2002, 368, 23388, 431, 1957, 29798, 2002, 4421, 875, 368, 2479, 1311, 5687, 50892], "temperature": 0.0, "avg_logprob": -0.23135705190162137, "compression_ratio": 2.0694980694980694, "no_speech_prob": 0.0965171530842781}, {"id": 344, "seek": 215626, "start": 2166.82, "end": 2169.82, "text": " y esta vez no es una tabla de traducci\u00f3n de palabras sino que es una tabla de traducci\u00f3n", "tokens": [50892, 288, 5283, 5715, 572, 785, 2002, 4421, 875, 368, 2479, 1311, 5687, 368, 35240, 18108, 631, 785, 2002, 4421, 875, 368, 2479, 1311, 5687, 51042], "temperature": 0.0, "avg_logprob": -0.23135705190162137, "compression_ratio": 2.0694980694980694, "no_speech_prob": 0.0965171530842781}, {"id": 345, "seek": 215626, "start": 2169.82, "end": 2175.0600000000004, "text": " de frases que me dice para cada frase con que otra frase corresponde, y una vez que", "tokens": [51042, 368, 431, 1957, 631, 385, 10313, 1690, 8411, 38406, 416, 631, 13623, 38406, 6805, 68, 11, 288, 2002, 5715, 631, 51304], "temperature": 0.0, "avg_logprob": -0.23135705190162137, "compression_ratio": 2.0694980694980694, "no_speech_prob": 0.0965171530842781}, {"id": 346, "seek": 215626, "start": 2175.0600000000004, "end": 2179.6200000000003, "text": " es otra duje cada una de esas frases la voy a ordenar de alguna manera buscando que suena", "tokens": [51304, 785, 13623, 1581, 2884, 8411, 2002, 368, 23388, 431, 1957, 635, 7552, 257, 28615, 289, 368, 20651, 13913, 46804, 631, 459, 4118, 51532], "temperature": 0.0, "avg_logprob": -0.23135705190162137, "compression_ratio": 2.0694980694980694, "no_speech_prob": 0.0965171530842781}, {"id": 347, "seek": 215626, "start": 2179.6200000000003, "end": 2185.1000000000004, "text": " el humanatural posible, buscando aumentar la fluidez de esa oraci\u00f3n, entonces como que la", "tokens": [51532, 806, 1952, 267, 1807, 26644, 11, 46804, 43504, 635, 5029, 45170, 368, 11342, 420, 3482, 11, 13003, 2617, 631, 635, 51806], "temperature": 0.0, "avg_logprob": -0.23135705190162137, "compression_ratio": 2.0694980694980694, "no_speech_prob": 0.0965171530842781}, {"id": 348, "seek": 218510, "start": 2185.1, "end": 2188.02, "text": " historia de generaci\u00f3n es un poco m\u00e1s simple que la otra, no ten\u00eda que ir sorteando", "tokens": [50364, 18385, 368, 1337, 3482, 785, 517, 10639, 3573, 2199, 631, 635, 13623, 11, 572, 23718, 631, 3418, 25559, 1806, 50510], "temperature": 0.0, "avg_logprob": -0.36871722009446883, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.11714361608028412}, {"id": 349, "seek": 218510, "start": 2188.02, "end": 2195.2999999999997, "text": " cosas, simplemente digo separo mi oraci\u00f3n en segmentos que le voy a llamar frases,", "tokens": [50510, 12218, 11, 33190, 22990, 3128, 78, 2752, 420, 3482, 465, 9469, 329, 631, 476, 7552, 257, 16848, 289, 431, 1957, 11, 50874], "temperature": 0.0, "avg_logprob": -0.36871722009446883, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.11714361608028412}, {"id": 350, "seek": 218510, "start": 2195.2999999999997, "end": 2201.14, "text": " los traducos y los reordenos, esa segmentaci\u00f3n en frases no tiene por que tener una", "tokens": [50874, 1750, 2479, 1311, 329, 288, 1750, 319, 19058, 329, 11, 11342, 9469, 3482, 465, 431, 1957, 572, 7066, 1515, 631, 11640, 2002, 51166], "temperature": 0.0, "avg_logprob": -0.36871722009446883, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.11714361608028412}, {"id": 351, "seek": 218510, "start": 2201.14, "end": 2205.42, "text": " un significado ling\u00fc\u00edstico, yo no voy a separarla en grupo nominal, grupo global, grupo", "tokens": [51166, 517, 3350, 1573, 22949, 774, 19512, 2789, 11, 5290, 572, 7552, 257, 3128, 34148, 465, 20190, 41641, 11, 20190, 4338, 11, 20190, 51380], "temperature": 0.0, "avg_logprob": -0.36871722009446883, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.11714361608028412}, {"id": 352, "seek": 218510, "start": 2205.42, "end": 2209.14, "text": " profesional, etc\u00e9tera, no tengo por qu\u00e9, o sea, capas que los segmentos de la frases", "tokens": [51380, 42882, 11, 5183, 526, 23833, 11, 572, 13989, 1515, 8057, 11, 277, 4158, 11, 1410, 296, 631, 1750, 9469, 329, 368, 635, 431, 1957, 51566], "temperature": 0.0, "avg_logprob": -0.36871722009446883, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.11714361608028412}, {"id": 353, "seek": 218510, "start": 2209.14, "end": 2214.2599999999998, "text": " y justo me queda un grupo preposicional capaz que no, lo \u00fanico que tiene que pasar es que", "tokens": [51566, 288, 40534, 385, 23314, 517, 20190, 2666, 329, 299, 1966, 35453, 631, 572, 11, 450, 26113, 631, 7066, 631, 25344, 785, 631, 51822], "temperature": 0.0, "avg_logprob": -0.36871722009446883, "compression_ratio": 1.8286713286713288, "no_speech_prob": 0.11714361608028412}, {"id": 354, "seek": 221426, "start": 2214.34, "end": 2218.46, "text": " estos segmentos que yo construyo tienen que estar en mitad de traducci\u00f3n de frases, alcanza", "tokens": [50368, 12585, 9469, 329, 631, 5290, 12946, 8308, 12536, 631, 8755, 465, 46895, 368, 2479, 1311, 5687, 368, 431, 1957, 11, 419, 7035, 2394, 50574], "temperature": 0.0, "avg_logprob": -0.3268956976421809, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.015676982700824738}, {"id": 355, "seek": 221426, "start": 2218.46, "end": 2221.82, "text": " con eso como para que yo puedo utilizar los en mi traducci\u00f3n, pero no tienen por qu\u00e9", "tokens": [50574, 416, 7287, 2617, 1690, 631, 5290, 21612, 24060, 1750, 465, 2752, 2479, 1311, 5687, 11, 4768, 572, 12536, 1515, 8057, 50742], "temperature": 0.0, "avg_logprob": -0.3268956976421809, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.015676982700824738}, {"id": 356, "seek": 221426, "start": 2221.82, "end": 2228.9, "text": " tener una motivaci\u00f3n ling\u00fc\u00edstica, bueno, entonces un modelo basado en frases tiene", "tokens": [50742, 11640, 2002, 5426, 3482, 22949, 774, 19512, 2262, 11, 11974, 11, 13003, 517, 27825, 987, 1573, 465, 431, 1957, 7066, 51096], "temperature": 0.0, "avg_logprob": -0.3268956976421809, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.015676982700824738}, {"id": 357, "seek": 221426, "start": 2228.9, "end": 2233.6600000000003, "text": " estos componentes, es parecido al anterior porque de vuelta, yo lo que quiero hacer es encontrar", "tokens": [51096, 12585, 6542, 279, 11, 785, 7448, 17994, 419, 22272, 4021, 368, 41542, 11, 5290, 450, 631, 16811, 6720, 785, 17525, 51334], "temperature": 0.0, "avg_logprob": -0.3268956976421809, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.015676982700824738}, {"id": 358, "seek": 221426, "start": 2233.6600000000003, "end": 2239.34, "text": " la probabilidad de ese dado de ambos sigo teniendo la misma ecuaci\u00f3n fundamental de la traducci\u00f3n", "tokens": [51334, 635, 31959, 4580, 368, 10167, 29568, 368, 41425, 4556, 78, 2064, 7304, 635, 24946, 11437, 84, 3482, 8088, 368, 635, 2479, 1311, 5687, 51618], "temperature": 0.0, "avg_logprob": -0.3268956976421809, "compression_ratio": 1.7174721189591078, "no_speech_prob": 0.015676982700824738}, {"id": 359, "seek": 223934, "start": 2239.34, "end": 2245.6600000000003, "text": " autom\u00e1tica estad\u00edstica, la quiero resolver, necesito pdfd y pd, solo que ahora el pdfd lo voy", "tokens": [50364, 3553, 23432, 39160, 19512, 2262, 11, 635, 16811, 34480, 11, 11909, 3528, 280, 45953, 67, 288, 280, 67, 11, 6944, 631, 9923, 806, 280, 45953, 67, 450, 7552, 50680], "temperature": 0.0, "avg_logprob": -0.264451965805172, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.1239766925573349}, {"id": 360, "seek": 223934, "start": 2245.6600000000003, "end": 2249.58, "text": " a calcular una manera extinta, voy a decir que para calcular esto tengo un modelo de traducci\u00f3n", "tokens": [50680, 257, 2104, 17792, 2002, 13913, 1279, 16071, 11, 7552, 257, 10235, 631, 1690, 2104, 17792, 7433, 13989, 517, 27825, 368, 2479, 1311, 5687, 50876], "temperature": 0.0, "avg_logprob": -0.264451965805172, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.1239766925573349}, {"id": 361, "seek": 223934, "start": 2249.58, "end": 2254.26, "text": " de frases y un modelo de ordenamiento, un modelo de una gran tabla de frases que me dice", "tokens": [50876, 368, 431, 1957, 288, 517, 27825, 368, 28615, 16971, 11, 517, 27825, 368, 2002, 9370, 4421, 875, 368, 431, 1957, 631, 385, 10313, 51110], "temperature": 0.0, "avg_logprob": -0.264451965805172, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.1239766925573349}, {"id": 362, "seek": 223934, "start": 2254.26, "end": 2258.98, "text": " cada frase con qu\u00e9 probabilidad la traducci\u00f3n no otra, y despu\u00e9s una forma de decir c\u00f3mo", "tokens": [51110, 8411, 38406, 416, 8057, 31959, 4580, 635, 2479, 1311, 5687, 572, 13623, 11, 288, 15283, 2002, 8366, 368, 10235, 12826, 51346], "temperature": 0.0, "avg_logprob": -0.264451965805172, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.1239766925573349}, {"id": 363, "seek": 223934, "start": 2258.98, "end": 2264.42, "text": " reordenos a frases para tener mejores oraciones, y bueno, como siempre voy a tener otro componente", "tokens": [51346, 319, 19058, 329, 257, 431, 1957, 1690, 11640, 42284, 420, 9188, 11, 288, 11974, 11, 2617, 12758, 7552, 257, 11640, 11921, 4026, 1576, 51618], "temperature": 0.0, "avg_logprob": -0.264451965805172, "compression_ratio": 1.7984790874524714, "no_speech_prob": 0.1239766925573349}, {"id": 364, "seek": 226442, "start": 2264.42, "end": 2272.26, "text": " que es el que mide la fluidez que es el modelo de lenguaje, porque los modelos de frases", "tokens": [50364, 631, 785, 806, 631, 275, 482, 635, 5029, 45170, 631, 785, 806, 27825, 368, 35044, 84, 11153, 11, 4021, 1750, 2316, 329, 368, 431, 1957, 50756], "temperature": 0.0, "avg_logprob": -0.288011764993473, "compression_ratio": 1.714975845410628, "no_speech_prob": 0.21640530228614807}, {"id": 365, "seek": 226442, "start": 2272.26, "end": 2276.58, "text": " funcionan mejor que los modelos basados en palabras, porque las frases ya tienen cierto", "tokens": [50756, 14186, 282, 11479, 631, 1750, 2316, 329, 987, 4181, 465, 35240, 11, 4021, 2439, 431, 1957, 2478, 12536, 28558, 50972], "temperature": 0.0, "avg_logprob": -0.288011764993473, "compression_ratio": 1.714975845410628, "no_speech_prob": 0.21640530228614807}, {"id": 366, "seek": 226442, "start": 2276.58, "end": 2281.54, "text": " contexto, las frases en realidad son como peque\u00f1os grupos de palabras que yo puedo traducir", "tokens": [50972, 47685, 11, 2439, 431, 1957, 465, 25635, 1872, 2617, 19132, 8242, 33758, 368, 35240, 631, 5290, 21612, 2479, 1311, 347, 51220], "temperature": 0.0, "avg_logprob": -0.288011764993473, "compression_ratio": 1.714975845410628, "no_speech_prob": 0.21640530228614807}, {"id": 367, "seek": 226442, "start": 2281.54, "end": 2289.86, "text": " uno en el otro, entonces cosas como dar la mano, dar una ofetada, tomar el pelo, etc.", "tokens": [51220, 8526, 465, 806, 11921, 11, 13003, 12218, 2617, 4072, 635, 18384, 11, 4072, 2002, 295, 302, 1538, 11, 22048, 806, 12167, 11, 5183, 13, 51636], "temperature": 0.0, "avg_logprob": -0.288011764993473, "compression_ratio": 1.714975845410628, "no_speech_prob": 0.21640530228614807}, {"id": 368, "seek": 228986, "start": 2289.94, "end": 2293.9, "text": " esas cosas como expresiones son mucho m\u00e1s f\u00e1cil de traducir si en realidad eso es as\u00ed que", "tokens": [50368, 23388, 12218, 2617, 33397, 5411, 1872, 9824, 3573, 17474, 368, 2479, 1311, 347, 1511, 465, 25635, 7287, 785, 8582, 631, 50566], "temperature": 0.0, "avg_logprob": -0.38458116503729334, "compression_ratio": 1.9927536231884058, "no_speech_prob": 0.39780741930007935}, {"id": 369, "seek": 228986, "start": 2293.9, "end": 2297.3, "text": " esta expresi\u00f3n que son tres cuatro palabras, le puedo traducir en esta otra expresi\u00f3n que son tres", "tokens": [50566, 5283, 33397, 2560, 631, 1872, 15890, 28795, 35240, 11, 476, 21612, 2479, 1311, 347, 465, 5283, 13623, 33397, 2560, 631, 1872, 15890, 50736], "temperature": 0.0, "avg_logprob": -0.38458116503729334, "compression_ratio": 1.9927536231884058, "no_speech_prob": 0.39780741930007935}, {"id": 370, "seek": 228986, "start": 2297.3, "end": 2301.98, "text": " cuatro palabras, y como m\u00e1s expresivo entonces pueda aprender m\u00e1s cosas, y bueno obviamente", "tokens": [50736, 28795, 35240, 11, 288, 2617, 3573, 33397, 6340, 13003, 31907, 24916, 3573, 12218, 11, 288, 11974, 36325, 50970], "temperature": 0.0, "avg_logprob": -0.38458116503729334, "compression_ratio": 1.9927536231884058, "no_speech_prob": 0.39780741930007935}, {"id": 371, "seek": 228986, "start": 2301.98, "end": 2306.2000000000003, "text": " cuanto m\u00e1s tenga, cuanto m\u00e1s largo sea el corpo, que yo tengo yo puedo aprender", "tokens": [50970, 36685, 3573, 36031, 11, 36685, 3573, 31245, 4158, 806, 23257, 11, 631, 5290, 13989, 5290, 21612, 24916, 51181], "temperature": 0.0, "avg_logprob": -0.38458116503729334, "compression_ratio": 1.9927536231884058, "no_speech_prob": 0.39780741930007935}, {"id": 372, "seek": 228986, "start": 2306.2000000000003, "end": 2312.86, "text": " frases m\u00e1s largas, mejores probabilidades, y mejores frases. Bueno, hay un ejemplo de como", "tokens": [51181, 431, 1957, 3573, 1613, 10549, 11, 42284, 31959, 10284, 11, 288, 42284, 431, 1957, 13, 16046, 11, 4842, 517, 13358, 368, 2617, 51514], "temperature": 0.0, "avg_logprob": -0.38458116503729334, "compression_ratio": 1.9927536231884058, "no_speech_prob": 0.39780741930007935}, {"id": 373, "seek": 228986, "start": 2312.86, "end": 2316.58, "text": " ser\u00eda una tabla de traducci\u00f3n de frases, o sea, es parecido la tabla de traducci\u00f3n de", "tokens": [51514, 23679, 2002, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 11, 277, 4158, 11, 785, 7448, 17994, 635, 4421, 875, 368, 2479, 1311, 5687, 368, 51700], "temperature": 0.0, "avg_logprob": -0.38458116503729334, "compression_ratio": 1.9927536231884058, "no_speech_prob": 0.39780741930007935}, {"id": 374, "seek": 231658, "start": 2316.58, "end": 2320.98, "text": " palabras, o lo que ac\u00e1 tengo de enfor\u00e7la, o sea, si yo busco la fila, asociada en", "tokens": [50364, 35240, 11, 277, 450, 631, 23496, 13989, 368, 465, 2994, 1138, 875, 11, 277, 4158, 11, 1511, 5290, 1255, 1291, 635, 1387, 64, 11, 382, 78, 537, 1538, 465, 50584], "temperature": 0.0, "avg_logprob": -0.3756223992456364, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.035085003823041916}, {"id": 375, "seek": 231658, "start": 2320.98, "end": 2324.8199999999997, "text": " for\u00e7la, o sea, encontrar\u00eda todas estas traducciones de proposa, el concediendo de", "tokens": [50584, 337, 1138, 875, 11, 277, 4158, 11, 17525, 2686, 10906, 13897, 2479, 1311, 23469, 368, 2365, 6447, 11, 806, 416, 1232, 7304, 368, 50776], "temperature": 0.0, "avg_logprob": -0.3756223992456364, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.035085003823041916}, {"id": 376, "seek": 231658, "start": 2324.8199999999997, "end": 2329.06, "text": " oposici\u00f3n de broalidad, posesivo proposa, el con 10 por ciento, a proposa, el con", "tokens": [50776, 999, 329, 15534, 368, 2006, 304, 4580, 11, 26059, 6340, 2365, 6447, 11, 806, 416, 1266, 1515, 47361, 11, 257, 2365, 6447, 11, 806, 416, 50988], "temperature": 0.0, "avg_logprob": -0.3756223992456364, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.035085003823041916}, {"id": 377, "seek": 231658, "start": 2329.06, "end": 2335.18, "text": " 3 por ciento, etc. O sea, como ven se traducen frases, en frases. Bueno, y como hago", "tokens": [50988, 805, 1515, 47361, 11, 5183, 13, 422, 4158, 11, 2617, 6138, 369, 2479, 1311, 268, 431, 1957, 11, 465, 431, 1957, 13, 16046, 11, 288, 2617, 38721, 51294], "temperature": 0.0, "avg_logprob": -0.3756223992456364, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.035085003823041916}, {"id": 378, "seek": 231658, "start": 2335.18, "end": 2342.18, "text": " para aprender una tabla de traducci\u00f3n de frases, yo parte de esta alineaci\u00f3n de", "tokens": [51294, 1690, 24916, 2002, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 11, 5290, 6975, 368, 5283, 419, 533, 3482, 368, 51644], "temperature": 0.0, "avg_logprob": -0.3756223992456364, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.035085003823041916}, {"id": 379, "seek": 231658, "start": 2342.18, "end": 2345.42, "text": " palabras, digamos esta alineaci\u00f3n completa, que ya no es una funci\u00f3n, sino que es", "tokens": [51644, 35240, 11, 36430, 5283, 419, 533, 3482, 46822, 11, 631, 2478, 572, 785, 2002, 43735, 11, 18108, 631, 785, 51806], "temperature": 0.0, "avg_logprob": -0.3756223992456364, "compression_ratio": 1.862453531598513, "no_speech_prob": 0.035085003823041916}, {"id": 380, "seek": 234542, "start": 2345.42, "end": 2351.5, "text": " digamos una alineaci\u00f3n de muchos a muchos, y voy a tratar de encontrar todos los todas las", "tokens": [50364, 36430, 2002, 419, 533, 3482, 368, 17061, 257, 17061, 11, 288, 7552, 257, 42549, 368, 17525, 6321, 1750, 10906, 2439, 50668], "temperature": 0.0, "avg_logprob": -0.29319180629050084, "compression_ratio": 2.1421800947867298, "no_speech_prob": 0.16655638813972473}, {"id": 381, "seek": 234542, "start": 2351.5, "end": 2355.86, "text": " frases, todos los pares de frases que son consistentes con la alineaci\u00f3n, a qu\u00e9 me refiero", "tokens": [50668, 431, 1957, 11, 6321, 1750, 2502, 495, 368, 431, 1957, 631, 1872, 4603, 9240, 416, 635, 419, 533, 3482, 11, 257, 8057, 385, 1895, 12030, 50886], "temperature": 0.0, "avg_logprob": -0.29319180629050084, "compression_ratio": 2.1421800947867298, "no_speech_prob": 0.16655638813972473}, {"id": 382, "seek": 234542, "start": 2355.86, "end": 2364.02, "text": " con que son consistentes, a que hay ejemplos, yo quiero decir que mariano y mariano", "tokens": [50886, 416, 631, 1872, 4603, 9240, 11, 257, 631, 4842, 10012, 5895, 329, 11, 5290, 16811, 10235, 631, 1849, 6254, 288, 1849, 6254, 51294], "temperature": 0.0, "avg_logprob": -0.29319180629050084, "compression_ratio": 2.1421800947867298, "no_speech_prob": 0.16655638813972473}, {"id": 383, "seek": 234542, "start": 2364.02, "end": 2370.46, "text": " no son un par de frases que son consistentes con esta alineaci\u00f3n, en cambio, mariano y mariano", "tokens": [51294, 572, 1872, 517, 971, 368, 431, 1957, 631, 1872, 4603, 9240, 416, 5283, 419, 533, 3482, 11, 465, 28731, 11, 1849, 6254, 288, 1849, 6254, 51616], "temperature": 0.0, "avg_logprob": -0.29319180629050084, "compression_ratio": 2.1421800947867298, "no_speech_prob": 0.16655638813972473}, {"id": 384, "seek": 234542, "start": 2370.46, "end": 2375.1, "text": " no son, como es que miro esto, lo que pasa es que cuando yo tengo mariano y mariano, la", "tokens": [51616, 572, 1872, 11, 2617, 785, 631, 2752, 340, 7433, 11, 450, 631, 20260, 785, 631, 7767, 5290, 13989, 1849, 6254, 288, 1849, 6254, 11, 635, 51848], "temperature": 0.0, "avg_logprob": -0.29319180629050084, "compression_ratio": 2.1421800947867298, "no_speech_prob": 0.16655638813972473}, {"id": 385, "seek": 237510, "start": 2375.1, "end": 2381.06, "text": " palabra no esta alinea con 10 knot y el 10 knot, digamos, el knot no pertenece hasta alineaci\u00f3n", "tokens": [50364, 31702, 572, 5283, 419, 533, 64, 416, 1266, 16966, 288, 806, 1266, 16966, 11, 36430, 11, 806, 16966, 572, 680, 1147, 68, 384, 10764, 419, 533, 3482, 50662], "temperature": 0.0, "avg_logprob": -0.3075859578450521, "compression_ratio": 2.1547619047619047, "no_speech_prob": 0.02258153446018696}, {"id": 386, "seek": 237510, "start": 2381.06, "end": 2385.54, "text": " que yo estoy dando decir, entonces digo que es no consistente, lo mismo pasa con si", "tokens": [50662, 631, 5290, 15796, 29854, 10235, 11, 13003, 22990, 631, 785, 572, 4603, 1576, 11, 450, 12461, 20260, 416, 1511, 50886], "temperature": 0.0, "avg_logprob": -0.3075859578450521, "compression_ratio": 2.1547619047619047, "no_speech_prob": 0.02258153446018696}, {"id": 387, "seek": 237510, "start": 2385.54, "end": 2392.02, "text": " yo dado alinear, mariano daba y mariano y mariano, lo que pasa es que daba no est\u00e1, digamos,", "tokens": [50886, 5290, 29568, 419, 533, 289, 11, 1849, 6254, 274, 5509, 288, 1849, 6254, 288, 1849, 6254, 11, 450, 631, 20260, 785, 631, 274, 5509, 572, 3192, 11, 36430, 11, 51210], "temperature": 0.0, "avg_logprob": -0.3075859578450521, "compression_ratio": 2.1547619047619047, "no_speech_prob": 0.02258153446018696}, {"id": 388, "seek": 237510, "start": 2392.02, "end": 2395.02, "text": " los puntos de alineaci\u00f3n de daba, no est\u00e1n dentro de este cuadrante que estoy dando", "tokens": [51210, 1750, 34375, 368, 419, 533, 3482, 368, 274, 5509, 11, 572, 10368, 10856, 368, 4065, 34434, 81, 2879, 631, 15796, 29854, 51360], "temperature": 0.0, "avg_logprob": -0.3075859578450521, "compression_ratio": 2.1547619047619047, "no_speech_prob": 0.02258153446018696}, {"id": 389, "seek": 237510, "start": 2395.02, "end": 2399.7, "text": " a buscar, entonces en definitiva digo que no es consistente, las alineaciones consistentes", "tokens": [51360, 257, 26170, 11, 13003, 465, 28781, 5931, 22990, 631, 572, 785, 4603, 1576, 11, 2439, 419, 533, 9188, 4603, 9240, 51594], "temperature": 0.0, "avg_logprob": -0.3075859578450521, "compression_ratio": 2.1547619047619047, "no_speech_prob": 0.02258153446018696}, {"id": 390, "seek": 237510, "start": 2399.7, "end": 2404.18, "text": " correctas son las que consideran todos los puntos dentro de ese cuadrante, entonces mariano", "tokens": [51594, 3006, 296, 1872, 2439, 631, 1949, 282, 6321, 1750, 34375, 10856, 368, 10167, 34434, 81, 2879, 11, 13003, 1849, 6254, 51818], "temperature": 0.0, "avg_logprob": -0.3075859578450521, "compression_ratio": 2.1547619047619047, "no_speech_prob": 0.02258153446018696}, {"id": 391, "seek": 240418, "start": 2404.18, "end": 2410.18, "text": " est\u00e1 asociado con mariano de knot y esas y es consistente, as\u00ed que como aprendo, frases", "tokens": [50364, 3192, 382, 78, 537, 1573, 416, 1849, 6254, 368, 16966, 288, 23388, 288, 785, 4603, 1576, 11, 8582, 631, 2617, 21003, 78, 11, 431, 1957, 50664], "temperature": 0.0, "avg_logprob": -0.41827776894640567, "compression_ratio": 1.9282700421940928, "no_speech_prob": 0.04252978414297104}, {"id": 392, "seek": 240418, "start": 2410.18, "end": 2417.3799999999997, "text": " consistentes, en piezo por las alineaciones, digamos, el piezo con la alineaci\u00f3n es una palabra,", "tokens": [50664, 4603, 9240, 11, 465, 1730, 4765, 1515, 2439, 419, 533, 9188, 11, 36430, 11, 806, 1730, 4765, 416, 635, 419, 533, 3482, 785, 2002, 31702, 11, 51024], "temperature": 0.0, "avg_logprob": -0.41827776894640567, "compression_ratio": 1.9282700421940928, "no_speech_prob": 0.04252978414297104}, {"id": 393, "seek": 240418, "start": 2417.3799999999997, "end": 2422.4199999999996, "text": " despu\u00e9s busco de una palabra y digo bueno, me quedo con todas esas traduciones de palabras", "tokens": [51024, 15283, 1255, 1291, 368, 2002, 31702, 288, 22990, 11974, 11, 385, 13617, 78, 416, 10906, 23388, 2479, 46649, 368, 35240, 51276], "temperature": 0.0, "avg_logprob": -0.41827776894640567, "compression_ratio": 1.9282700421940928, "no_speech_prob": 0.04252978414297104}, {"id": 394, "seek": 240418, "start": 2422.4199999999996, "end": 2426.8199999999997, "text": " y las pongamitables de frases y despu\u00e9s voy tomando de 2 y me quedo con todas esas otras", "tokens": [51276, 288, 2439, 36164, 335, 270, 2965, 368, 431, 1957, 288, 15283, 7552, 2916, 1806, 368, 568, 288, 385, 13617, 78, 416, 10906, 23388, 20244, 51496], "temperature": 0.0, "avg_logprob": -0.41827776894640567, "compression_ratio": 1.9282700421940928, "no_speech_prob": 0.04252978414297104}, {"id": 395, "seek": 240418, "start": 2426.8199999999997, "end": 2431.58, "text": " frases y la voy agregando, me quedo de frases, despu\u00e9s me puedo avanzar en 1, tomar de", "tokens": [51496, 431, 1957, 288, 635, 7552, 623, 3375, 1806, 11, 385, 13617, 78, 368, 431, 1957, 11, 15283, 385, 21612, 42444, 289, 465, 502, 11, 2916, 289, 368, 51734], "temperature": 0.0, "avg_logprob": -0.41827776894640567, "compression_ratio": 1.9282700421940928, "no_speech_prob": 0.04252978414297104}, {"id": 396, "seek": 243158, "start": 2431.58, "end": 2437.74, "text": " 3, tomar de 4 y llegar a tomar incluso toda la oraci\u00f3n como frases, entonces a partir", "tokens": [50364, 805, 11, 2916, 289, 368, 1017, 288, 24892, 257, 22048, 24018, 11687, 635, 420, 3482, 2617, 431, 1957, 11, 13003, 257, 13906, 50672], "temperature": 0.0, "avg_logprob": -0.35951693552844927, "compression_ratio": 1.5, "no_speech_prob": 0.08973411470651627}, {"id": 397, "seek": 243158, "start": 2437.74, "end": 2443.1, "text": " de estas oraciones que ten\u00edan, no s\u00e9, un 2, 3, 4, 5, 6, 7, 8, no hay palabras, yo termino", "tokens": [50672, 368, 13897, 420, 9188, 631, 47596, 11, 572, 7910, 11, 517, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 11, 1614, 11, 1649, 11, 572, 4842, 35240, 11, 5290, 10761, 78, 50940], "temperature": 0.0, "avg_logprob": -0.35951693552844927, "compression_ratio": 1.5, "no_speech_prob": 0.08973411470651627}, {"id": 398, "seek": 243158, "start": 2443.1, "end": 2450.2999999999997, "text": " aprendiendo como 17 frases, digamos, cada vez m\u00e1s grandes y bueno, hoy voy sacando esto", "tokens": [50940, 21003, 7304, 2617, 3282, 431, 1957, 11, 36430, 11, 8411, 5715, 3573, 16640, 288, 11974, 11, 13775, 7552, 4899, 1806, 7433, 51300], "temperature": 0.0, "avg_logprob": -0.35951693552844927, "compression_ratio": 1.5, "no_speech_prob": 0.08973411470651627}, {"id": 399, "seek": 243158, "start": 2450.2999999999997, "end": 2456.02, "text": " de todo el corpus y calculando mitable de probabilidades, de qu\u00e9 manera, calcula esas", "tokens": [51300, 368, 5149, 806, 1181, 31624, 288, 4322, 1806, 2194, 712, 368, 31959, 10284, 11, 368, 8057, 13913, 11, 4322, 64, 23388, 51586], "temperature": 0.0, "avg_logprob": -0.35951693552844927, "compression_ratio": 1.5, "no_speech_prob": 0.08973411470651627}, {"id": 400, "seek": 245602, "start": 2456.02, "end": 2460.38, "text": " probabilidades, yo lo que puedo hacer es como siempre ver cu\u00e1ntas veces aparecen el corpus", "tokens": [50364, 31959, 10284, 11, 5290, 450, 631, 21612, 6720, 785, 2617, 12758, 1306, 44256, 296, 17054, 15004, 13037, 806, 1181, 31624, 50582], "temperature": 0.0, "avg_logprob": -0.27921197266705267, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.16700443625450134}, {"id": 401, "seek": 245602, "start": 2460.38, "end": 2466.42, "text": " y contar, o si no, si yo ten\u00eda construido el modelo anterior, el modelo de la tabla de", "tokens": [50582, 288, 27045, 11, 277, 1511, 572, 11, 1511, 5290, 23718, 12946, 2925, 806, 27825, 22272, 11, 806, 27825, 368, 635, 4421, 875, 368, 50884], "temperature": 0.0, "avg_logprob": -0.27921197266705267, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.16700443625450134}, {"id": 402, "seek": 245602, "start": 2466.42, "end": 2470.58, "text": " traduciones de palabra palabra, en realidad lo que puedo hacer es aprovechar ese modelo", "tokens": [50884, 2479, 46649, 368, 31702, 31702, 11, 465, 25635, 450, 631, 21612, 6720, 785, 29015, 7374, 10167, 27825, 51092], "temperature": 0.0, "avg_logprob": -0.27921197266705267, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.16700443625450134}, {"id": 403, "seek": 245602, "start": 2470.58, "end": 2475.54, "text": " traducci\u00f3n de palabra palabra y decir bueno, me arma una traducci\u00f3n entre un par de frases", "tokens": [51092, 2479, 1311, 5687, 368, 31702, 31702, 288, 10235, 11974, 11, 385, 46422, 2002, 2479, 1311, 5687, 3962, 517, 971, 368, 431, 1957, 51340], "temperature": 0.0, "avg_logprob": -0.27921197266705267, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.16700443625450134}, {"id": 404, "seek": 245602, "start": 2475.54, "end": 2479.86, "text": " bas\u00e1ndome en las traduciones palabra palabras, son como formas distintas de construirlo y", "tokens": [51340, 987, 18606, 423, 465, 2439, 2479, 46649, 31702, 35240, 11, 1872, 2617, 33463, 31489, 296, 368, 38445, 752, 288, 51556], "temperature": 0.0, "avg_logprob": -0.27921197266705267, "compression_ratio": 1.8791666666666667, "no_speech_prob": 0.16700443625450134}, {"id": 405, "seek": 247986, "start": 2479.86, "end": 2488.5, "text": " a veces hasta complementarias, bien eso fue el modelo de frases, los modelos de frases son", "tokens": [50364, 257, 17054, 10764, 17103, 35027, 11, 3610, 7287, 9248, 806, 27825, 368, 431, 1957, 11, 1750, 2316, 329, 368, 431, 1957, 1872, 50796], "temperature": 0.0, "avg_logprob": -0.30364472815330995, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.1084483414888382}, {"id": 406, "seek": 247986, "start": 2488.5, "end": 2493.1800000000003, "text": " los m\u00e1s usados hoy en d\u00eda en realidad en lo que es la traducci\u00f3n autom\u00e1tica, son los", "tokens": [50796, 1750, 3573, 505, 4181, 13775, 465, 12271, 465, 25635, 465, 450, 631, 785, 635, 2479, 1311, 5687, 3553, 23432, 11, 1872, 1750, 51030], "temperature": 0.0, "avg_logprob": -0.30364472815330995, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.1084483414888382}, {"id": 407, "seek": 247986, "start": 2493.1800000000003, "end": 2499.06, "text": " candados mejor de resultados y bueno, no faltaba una cosa para terminar el toda la imagen", "tokens": [51030, 3955, 4181, 11479, 368, 36796, 288, 11974, 11, 572, 37108, 5509, 2002, 10163, 1690, 36246, 806, 11687, 635, 40652, 51324], "temperature": 0.0, "avg_logprob": -0.30364472815330995, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.1084483414888382}, {"id": 408, "seek": 247986, "start": 2499.06, "end": 2506.2200000000003, "text": " de lo que es la traducci\u00f3n autom\u00e1tica estad\u00edstica que es la decodificaci\u00f3n, entonces", "tokens": [51324, 368, 450, 631, 785, 635, 2479, 1311, 5687, 3553, 23432, 39160, 19512, 2262, 631, 785, 635, 979, 378, 40802, 11, 13003, 51682], "temperature": 0.0, "avg_logprob": -0.30364472815330995, "compression_ratio": 1.7549019607843137, "no_speech_prob": 0.1084483414888382}, {"id": 409, "seek": 250622, "start": 2506.22, "end": 2513.06, "text": " veamos un resumen de lo que ten\u00edamos hasta ahora, hasta ahora yo part\u00ed de yo quer\u00eda resolver", "tokens": [50364, 1241, 2151, 517, 725, 16988, 368, 450, 631, 2064, 16275, 10764, 9923, 11, 10764, 9923, 5290, 644, 870, 368, 5290, 37869, 34480, 50706], "temperature": 0.0, "avg_logprob": -0.2977904343023533, "compression_ratio": 2.017094017094017, "no_speech_prob": 0.24344895780086517}, {"id": 410, "seek": 250622, "start": 2513.06, "end": 2518.4599999999996, "text": " la cocci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica estad\u00edstica y yo ten\u00eda un corpus paralelo", "tokens": [50706, 635, 598, 14735, 8088, 368, 635, 2479, 1311, 5687, 3553, 23432, 39160, 19512, 2262, 288, 5290, 23718, 517, 1181, 31624, 26009, 10590, 50976], "temperature": 0.0, "avg_logprob": -0.2977904343023533, "compression_ratio": 2.017094017094017, "no_speech_prob": 0.24344895780086517}, {"id": 411, "seek": 250622, "start": 2518.4599999999996, "end": 2522.62, "text": " que ten\u00eda texto en el idioma origen y el idioma de estino y a partir de siendo analisis", "tokens": [50976, 631, 23718, 35503, 465, 806, 18014, 6440, 2349, 268, 288, 806, 18014, 6440, 368, 871, 2982, 288, 257, 13906, 368, 31423, 2624, 28436, 51184], "temperature": 0.0, "avg_logprob": -0.2977904343023533, "compression_ratio": 2.017094017094017, "no_speech_prob": 0.24344895780086517}, {"id": 412, "seek": 250622, "start": 2522.62, "end": 2528.58, "text": " estad\u00edstico yo me constru\u00ed un modelo traducci\u00f3n que lo que vimos en esta clase, adem\u00e1s yo", "tokens": [51184, 39160, 19512, 2789, 5290, 385, 12946, 870, 517, 27825, 2479, 1311, 5687, 631, 450, 631, 49266, 465, 5283, 44578, 11, 21251, 5290, 51482], "temperature": 0.0, "avg_logprob": -0.2977904343023533, "compression_ratio": 2.017094017094017, "no_speech_prob": 0.24344895780086517}, {"id": 413, "seek": 250622, "start": 2528.58, "end": 2533.3399999999997, "text": " ten\u00eda cierta cantidad de texto del idioma de estino y a partir de cierto analisis estad\u00edstico", "tokens": [51482, 23718, 39769, 1328, 33757, 368, 35503, 1103, 18014, 6440, 368, 871, 2982, 288, 257, 13906, 368, 28558, 2624, 28436, 39160, 19512, 2789, 51720], "temperature": 0.0, "avg_logprob": -0.2977904343023533, "compression_ratio": 2.017094017094017, "no_speech_prob": 0.24344895780086517}, {"id": 414, "seek": 253334, "start": 2533.34, "end": 2538.2200000000003, "text": " me constru\u00ed un modelo de lenguaje que me dice que tan fluido es una operaci\u00f3n en el lenguaje", "tokens": [50364, 385, 12946, 870, 517, 27825, 368, 35044, 84, 11153, 631, 385, 10313, 631, 7603, 5029, 2925, 785, 2002, 2208, 3482, 465, 806, 35044, 84, 11153, 50608], "temperature": 0.0, "avg_logprob": -0.26965122094890415, "compression_ratio": 1.992619926199262, "no_speech_prob": 0.13637574017047882}, {"id": 415, "seek": 253334, "start": 2538.2200000000003, "end": 2543.7000000000003, "text": " estino, entonces ahora lo que me falta, recuerden que yo lo que ten\u00eda que hacer era", "tokens": [50608, 871, 2982, 11, 13003, 9923, 450, 631, 385, 22111, 11, 39092, 1556, 631, 5290, 450, 631, 23718, 631, 6720, 4249, 50882], "temperature": 0.0, "avg_logprob": -0.26965122094890415, "compression_ratio": 1.992619926199262, "no_speech_prob": 0.13637574017047882}, {"id": 416, "seek": 253334, "start": 2543.7000000000003, "end": 2547.54, "text": " y te era sobre todas las oraciones del lenguaje estino y pasar las a trav\u00e9s del modelo", "tokens": [50882, 288, 535, 4249, 5473, 10906, 2439, 420, 9188, 1103, 35044, 84, 11153, 871, 2982, 288, 25344, 2439, 257, 24463, 1103, 27825, 51074], "temperature": 0.0, "avg_logprob": -0.26965122094890415, "compression_ratio": 1.992619926199262, "no_speech_prob": 0.13637574017047882}, {"id": 417, "seek": 253334, "start": 2547.54, "end": 2552.26, "text": " traducci\u00f3n y del modelo de lenguaje para que me de la probabilidad de esa oraci\u00f3n, bueno", "tokens": [51074, 2479, 1311, 5687, 288, 1103, 27825, 368, 35044, 84, 11153, 1690, 631, 385, 368, 635, 31959, 4580, 368, 11342, 420, 3482, 11, 11974, 51310], "temperature": 0.0, "avg_logprob": -0.26965122094890415, "compression_ratio": 1.992619926199262, "no_speech_prob": 0.13637574017047882}, {"id": 418, "seek": 253334, "start": 2552.26, "end": 2556.98, "text": " lo que me falta es el agorismo de codificaci\u00f3n que en vez de probar con todas las oraciones", "tokens": [51310, 450, 631, 385, 22111, 785, 806, 623, 284, 6882, 368, 17656, 40802, 631, 465, 5715, 368, 1239, 289, 416, 10906, 2439, 420, 9188, 51546], "temperature": 0.0, "avg_logprob": -0.26965122094890415, "compression_ratio": 1.992619926199262, "no_speech_prob": 0.13637574017047882}, {"id": 419, "seek": 253334, "start": 2556.98, "end": 2561.7400000000002, "text": " de lenguaje estinos me va a decir unas cuantas oraciones para probar, porque me dice 150", "tokens": [51546, 368, 35044, 84, 11153, 871, 15220, 385, 2773, 257, 10235, 25405, 2702, 49153, 420, 9188, 1690, 1239, 289, 11, 4021, 385, 10313, 8451, 51784], "temperature": 0.0, "avg_logprob": -0.26965122094890415, "compression_ratio": 1.992619926199262, "no_speech_prob": 0.13637574017047882}, {"id": 420, "seek": 256174, "start": 2561.74, "end": 2566.7, "text": " oraciones para probar sobre las cuales utiliza el modelo traducci\u00f3n en modelo de lenguaje,", "tokens": [50364, 420, 9188, 1690, 1239, 289, 5473, 2439, 46932, 4976, 13427, 806, 27825, 2479, 1311, 5687, 465, 27825, 368, 35044, 84, 11153, 11, 50612], "temperature": 0.0, "avg_logprob": -0.3203852384993174, "compression_ratio": 1.8934010152284264, "no_speech_prob": 0.09906590729951859}, {"id": 421, "seek": 256174, "start": 2566.7, "end": 2572.8599999999997, "text": " entonces esto es como un diagrama de modulos en los cuales el agorismo de codificaci\u00f3n utiliza", "tokens": [50612, 13003, 7433, 785, 2617, 517, 10686, 64, 368, 1072, 425, 329, 465, 1750, 46932, 806, 623, 284, 6882, 368, 17656, 40802, 4976, 13427, 50920], "temperature": 0.0, "avg_logprob": -0.3203852384993174, "compression_ratio": 1.8934010152284264, "no_speech_prob": 0.09906590729951859}, {"id": 422, "seek": 256174, "start": 2572.8599999999997, "end": 2580.7799999999997, "text": " los dos modulos, tanto es la traducci\u00f3n como el lenguaje, bueno, como funciona el agorismo", "tokens": [50920, 1750, 4491, 1072, 425, 329, 11, 10331, 785, 635, 2479, 1311, 5687, 2617, 806, 35044, 84, 11153, 11, 11974, 11, 2617, 26210, 806, 623, 284, 6882, 51316], "temperature": 0.0, "avg_logprob": -0.3203852384993174, "compression_ratio": 1.8934010152284264, "no_speech_prob": 0.09906590729951859}, {"id": 423, "seek": 256174, "start": 2580.7799999999997, "end": 2588.4599999999996, "text": " de codificaci\u00f3n, que vamos a ver es un agorismo de codificaci\u00f3n de tipo bean search y bueno", "tokens": [51316, 368, 17656, 40802, 11, 631, 5295, 257, 1306, 785, 517, 623, 284, 6882, 368, 17656, 40802, 368, 9746, 16230, 3164, 288, 11974, 51700], "temperature": 0.0, "avg_logprob": -0.3203852384993174, "compression_ratio": 1.8934010152284264, "no_speech_prob": 0.09906590729951859}, {"id": 424, "seek": 258846, "start": 2588.46, "end": 2592.9, "text": " la funci\u00f3n de acinde manera, yo tengo la oraci\u00f3n Mar\u00eda no dio una ofetada a la bruja verde", "tokens": [50364, 635, 43735, 368, 696, 8274, 13913, 11, 5290, 13989, 635, 420, 3482, 48472, 572, 31965, 2002, 295, 302, 1538, 257, 635, 25267, 2938, 29653, 50586], "temperature": 0.0, "avg_logprob": -0.3198409578693447, "compression_ratio": 1.9734513274336283, "no_speech_prob": 0.17999018728733063}, {"id": 425, "seek": 258846, "start": 2592.9, "end": 2598.82, "text": " y la quiero traducir al ingl\u00e9s y tengo una tabla de traducci\u00f3n de frases", "tokens": [50586, 288, 635, 16811, 2479, 1311, 347, 419, 49766, 288, 13989, 2002, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 50882], "temperature": 0.0, "avg_logprob": -0.3198409578693447, "compression_ratio": 1.9734513274336283, "no_speech_prob": 0.17999018728733063}, {"id": 426, "seek": 258846, "start": 2598.82, "end": 2604.62, "text": " entonces mi oraci\u00f3n Mar\u00eda no dio una ofetada a la bruja verde, yo busco en la tabla de frases", "tokens": [50882, 13003, 2752, 420, 3482, 48472, 572, 31965, 2002, 295, 302, 1538, 257, 635, 25267, 2938, 29653, 11, 5290, 1255, 1291, 465, 635, 4421, 875, 368, 431, 1957, 51172], "temperature": 0.0, "avg_logprob": -0.3198409578693447, "compression_ratio": 1.9734513274336283, "no_speech_prob": 0.17999018728733063}, {"id": 427, "seek": 258846, "start": 2604.62, "end": 2610.06, "text": " \u00bfCu\u00e1les de esas digamos? \u00bfCu\u00e1les segmento? \u00bfCu\u00e1les subsegmento de esa oraci\u00f3n?", "tokens": [51172, 3841, 35222, 842, 904, 368, 23388, 36430, 30, 3841, 35222, 842, 904, 9469, 78, 30, 3841, 35222, 842, 904, 1422, 405, 10433, 78, 368, 11342, 420, 3482, 30, 51444], "temperature": 0.0, "avg_logprob": -0.3198409578693447, "compression_ratio": 1.9734513274336283, "no_speech_prob": 0.17999018728733063}, {"id": 428, "seek": 258846, "start": 2610.06, "end": 2613.66, "text": " yo puedo encontrar en la tabla de traducci\u00f3n de frases, todo lo que me encanta por ejemplo que", "tokens": [51444, 5290, 21612, 17525, 465, 635, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 11, 5149, 450, 631, 385, 47597, 1515, 13358, 631, 51624], "temperature": 0.0, "avg_logprob": -0.3198409578693447, "compression_ratio": 1.9734513274336283, "no_speech_prob": 0.17999018728733063}, {"id": 429, "seek": 261366, "start": 2613.66, "end": 2618.5, "text": " Mar\u00eda lo pota o s\u00ed como Mary, no lo busco en la tabla y lo pota o s\u00ed como not como", "tokens": [50364, 48472, 450, 1847, 64, 277, 8600, 2617, 6059, 11, 572, 450, 1255, 1291, 465, 635, 4421, 875, 288, 450, 1847, 64, 277, 8600, 2617, 406, 2617, 50606], "temperature": 0.0, "avg_logprob": -0.3398092565402179, "compression_ratio": 2.23, "no_speech_prob": 0.36741408705711365}, {"id": 430, "seek": 261366, "start": 2618.5, "end": 2625.06, "text": " not o como no, dio lo pota o s\u00ed como guir, pero adem\u00e1s no dio esa frase entera, yo le busco", "tokens": [50606, 406, 277, 2617, 572, 11, 31965, 450, 1847, 64, 277, 8600, 2617, 695, 347, 11, 4768, 21251, 572, 31965, 11342, 38406, 3242, 64, 11, 5290, 476, 1255, 1291, 50934], "temperature": 0.0, "avg_logprob": -0.3398092565402179, "compression_ratio": 2.23, "no_speech_prob": 0.36741408705711365}, {"id": 431, "seek": 261366, "start": 2625.06, "end": 2630.22, "text": " en la tabla y me aparece que la pota o s\u00ed como not guir, dio una ofetada a toda esa frase", "tokens": [50934, 465, 635, 4421, 875, 288, 385, 37863, 631, 635, 1847, 64, 277, 8600, 2617, 406, 695, 347, 11, 31965, 2002, 295, 302, 1538, 257, 11687, 11342, 38406, 51192], "temperature": 0.0, "avg_logprob": -0.3398092565402179, "compression_ratio": 2.23, "no_speech_prob": 0.36741408705711365}, {"id": 432, "seek": 261366, "start": 2630.22, "end": 2637.8999999999996, "text": " lo pota o s\u00ed como slape, una ofetada lo pota o s\u00ed como aslape y bueno de otras cosas", "tokens": [51192, 450, 1847, 64, 277, 8600, 2617, 8039, 494, 11, 2002, 295, 302, 1538, 450, 1847, 64, 277, 8600, 2617, 382, 875, 494, 288, 11974, 368, 20244, 12218, 51576], "temperature": 0.0, "avg_logprob": -0.3398092565402179, "compression_ratio": 2.23, "no_speech_prob": 0.36741408705711365}, {"id": 433, "seek": 261366, "start": 2637.8999999999996, "end": 2641.06, "text": " bruja lo pota o s\u00ed como witch, verde como green pero adem\u00e1s en alg\u00fan lado de la tabla", "tokens": [51576, 25267, 2938, 450, 1847, 64, 277, 8600, 2617, 14867, 11, 29653, 2617, 3092, 4768, 21251, 465, 26300, 11631, 368, 635, 4421, 875, 51734], "temperature": 0.0, "avg_logprob": -0.3398092565402179, "compression_ratio": 2.23, "no_speech_prob": 0.36741408705711365}, {"id": 434, "seek": 264106, "start": 2641.06, "end": 2647.7, "text": " tengo que brujar verde lo puedo traducir como green witch y as\u00ed, yo puedo encontrar diferentes", "tokens": [50364, 13989, 631, 738, 4579, 289, 29653, 450, 21612, 2479, 1311, 347, 2617, 3092, 14867, 288, 8582, 11, 5290, 21612, 17525, 17686, 50696], "temperature": 0.0, "avg_logprob": -0.30329073153860203, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.08613716810941696}, {"id": 435, "seek": 264106, "start": 2647.7, "end": 2652.22, "text": " maneras de segmentar la oraci\u00f3n y adem\u00e1s para cada uno de esos segmentos puedo encontrar distintas", "tokens": [50696, 587, 6985, 368, 9469, 289, 635, 420, 3482, 288, 21251, 1690, 8411, 8526, 368, 22411, 9469, 329, 21612, 17525, 31489, 296, 50922], "temperature": 0.0, "avg_logprob": -0.30329073153860203, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.08613716810941696}, {"id": 436, "seek": 264106, "start": 2652.22, "end": 2659.62, "text": " formas de traducirlo en el lenguaje destino con mitable de frases, entonces el algoritmo de", "tokens": [50922, 33463, 368, 2479, 1311, 347, 752, 465, 806, 35044, 84, 11153, 2677, 2982, 416, 2194, 712, 368, 431, 1957, 11, 13003, 806, 3501, 50017, 3280, 368, 51292], "temperature": 0.0, "avg_logprob": -0.30329073153860203, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.08613716810941696}, {"id": 437, "seek": 264106, "start": 2659.62, "end": 2664.06, "text": " codificaci\u00f3n funciona de la siguiente manera, empezamos teniendo en cada paso el algoritmo", "tokens": [51292, 17656, 40802, 26210, 368, 635, 25666, 13913, 11, 18730, 2151, 2064, 7304, 465, 8411, 29212, 806, 3501, 50017, 3280, 51514], "temperature": 0.0, "avg_logprob": -0.30329073153860203, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.08613716810941696}, {"id": 438, "seek": 264106, "start": 2664.06, "end": 2668.82, "text": " vamos a tener un conjunto de hip\u00f3tesis de traducci\u00f3n, se llega a ver ah\u00ed lo que dice a", "tokens": [51514, 5295, 257, 11640, 517, 37776, 368, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 11, 369, 40423, 257, 1306, 12571, 450, 631, 10313, 257, 51752], "temperature": 0.0, "avg_logprob": -0.30329073153860203, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.08613716810941696}, {"id": 439, "seek": 266882, "start": 2668.82, "end": 2683.94, "text": " ojos, m\u00e1s o menos, bien, ac\u00e1 que eran malos, correctes, bueno, en cada uno de los pasos", "tokens": [50364, 39519, 11, 3573, 277, 8902, 11, 3610, 11, 23496, 631, 32762, 2806, 329, 11, 3006, 279, 11, 11974, 11, 465, 8411, 8526, 368, 1750, 1736, 329, 51120], "temperature": 0.0, "avg_logprob": -0.3408551384420956, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.07633580267429352}, {"id": 440, "seek": 266882, "start": 2683.94, "end": 2690.7000000000003, "text": " yo voy a tener un conjunto de hip\u00f3tesis de traducci\u00f3n, al principio el algoritmo voy a empezar", "tokens": [51120, 5290, 7552, 257, 11640, 517, 37776, 368, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 11, 419, 34308, 806, 3501, 50017, 3280, 7552, 257, 31168, 51458], "temperature": 0.0, "avg_logprob": -0.3408551384420956, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.07633580267429352}, {"id": 441, "seek": 266882, "start": 2690.7000000000003, "end": 2696.02, "text": " con una hip\u00f3tesis vac\u00eda, como se le este hip\u00f3tesis dice que lo importante de leer es la parte", "tokens": [51458, 416, 2002, 8103, 812, 7269, 271, 2842, 2686, 11, 2617, 369, 476, 4065, 8103, 812, 7269, 271, 10313, 631, 450, 9416, 368, 34172, 785, 635, 6975, 51724], "temperature": 0.0, "avg_logprob": -0.3408551384420956, "compression_ratio": 1.5635359116022098, "no_speech_prob": 0.07633580267429352}, {"id": 442, "seek": 269602, "start": 2696.02, "end": 2699.22, "text": " de la defe que tiene un mont\u00f3n de guiones, significa que no hay ninguna palabra del espa\u00f1ol", "tokens": [50364, 368, 635, 1060, 68, 631, 7066, 517, 45259, 368, 695, 5411, 11, 19957, 631, 572, 4842, 36073, 31702, 1103, 31177, 50524], "temperature": 0.0, "avg_logprob": -0.2831056154691256, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.24320054054260254}, {"id": 443, "seek": 269602, "start": 2699.22, "end": 2704.58, "text": " cubierta, esas son todas las 9, 9 palabras en espa\u00f1ol, ninguna esta cubierta y esta hip\u00f3tesis", "tokens": [50524, 10057, 811, 1328, 11, 23388, 1872, 10906, 2439, 1722, 11, 1722, 35240, 465, 31177, 11, 36073, 5283, 10057, 811, 1328, 288, 5283, 8103, 812, 7269, 271, 50792], "temperature": 0.0, "avg_logprob": -0.2831056154691256, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.24320054054260254}, {"id": 444, "seek": 269602, "start": 2704.58, "end": 2710.86, "text": " tiene probabilidad 1, entonces en cada paso el algoritmo lo que voy a hacer es elegir un par de", "tokens": [50792, 7066, 31959, 4580, 502, 11, 13003, 465, 8411, 29212, 806, 3501, 50017, 3280, 450, 631, 7552, 257, 6720, 785, 14459, 347, 517, 971, 368, 51106], "temperature": 0.0, "avg_logprob": -0.2831056154691256, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.24320054054260254}, {"id": 445, "seek": 269602, "start": 2710.86, "end": 2715.58, "text": " frases, tal que una traducci\u00f3n de la otra y voy a crear un hip\u00f3tesis nueva a partir de una", "tokens": [51106, 431, 1957, 11, 4023, 631, 2002, 2479, 1311, 5687, 368, 635, 13623, 288, 7552, 257, 31984, 517, 8103, 812, 7269, 271, 28963, 257, 13906, 368, 2002, 51342], "temperature": 0.0, "avg_logprob": -0.2831056154691256, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.24320054054260254}, {"id": 446, "seek": 269602, "start": 2715.58, "end": 2721.02, "text": " que ya tengo, entonces en este paso lo que dice fue decir el hijo, el par de frases Mar\u00eda", "tokens": [51342, 631, 2478, 13989, 11, 13003, 465, 4065, 29212, 450, 631, 10313, 9248, 10235, 806, 38390, 11, 806, 971, 368, 431, 1957, 48472, 51614], "temperature": 0.0, "avg_logprob": -0.2831056154691256, "compression_ratio": 1.7765151515151516, "no_speech_prob": 0.24320054054260254}, {"id": 447, "seek": 272102, "start": 2721.02, "end": 2727.82, "text": " Mary y ah\u00ed me creo, una nueva hip\u00f3tesis que cubre la primera palabra, por eso parece una", "tokens": [50364, 6059, 288, 12571, 385, 14336, 11, 2002, 28963, 8103, 812, 7269, 271, 631, 10057, 265, 635, 17382, 31702, 11, 1515, 7287, 14120, 2002, 50704], "temperature": 0.0, "avg_logprob": -0.34596337805261146, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.2115311324596405}, {"id": 448, "seek": 272102, "start": 2727.82, "end": 2731.82, "text": " cerita en este caso, el hijo, la frase en ingl\u00e9s Mary y ahora tiene una probabilidad", "tokens": [50704, 10146, 2786, 465, 4065, 9666, 11, 806, 38390, 11, 635, 38406, 465, 49766, 6059, 288, 9923, 7066, 2002, 31959, 4580, 50904], "temperature": 0.0, "avg_logprob": -0.34596337805261146, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.2115311324596405}, {"id": 449, "seek": 272102, "start": 2731.82, "end": 2737.18, "text": " de 0.584, ese n\u00famero de esa probabilidad va a servir para guiar un poco en el algoritmo", "tokens": [50904, 368, 1958, 13, 20, 25494, 11, 10167, 14959, 368, 11342, 31959, 4580, 2773, 257, 29463, 1690, 695, 9448, 517, 10639, 465, 806, 3501, 50017, 3280, 51172], "temperature": 0.0, "avg_logprob": -0.34596337805261146, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.2115311324596405}, {"id": 450, "seek": 272102, "start": 2737.18, "end": 2740.42, "text": " pero vamos a ver despu\u00e9s como es que se calcula, porabra que \u00e9l se solamente con el", "tokens": [51172, 4768, 5295, 257, 1306, 15283, 2617, 785, 631, 369, 2104, 66, 3780, 11, 1515, 455, 424, 631, 11810, 369, 27814, 416, 806, 51334], "temperature": 0.0, "avg_logprob": -0.34596337805261146, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.2115311324596405}, {"id": 451, "seek": 272102, "start": 2740.42, "end": 2745.86, "text": " n\u00famero, bien, pero entonces yo ten\u00eda otra opci\u00f3n, en realidad yo pod\u00eda haber elegido", "tokens": [51334, 14959, 11, 3610, 11, 4768, 13003, 5290, 23718, 13623, 999, 5687, 11, 465, 25635, 5290, 45588, 15811, 14459, 2925, 51606], "temperature": 0.0, "avg_logprob": -0.34596337805261146, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.2115311324596405}, {"id": 452, "seek": 272102, "start": 2745.86, "end": 2750.14, "text": " empezar en vez de traducir Mar\u00eda por Mary, pod\u00eda haber elegido empezar por traducir", "tokens": [51606, 31168, 465, 5715, 368, 2479, 1311, 347, 48472, 1515, 6059, 11, 45588, 15811, 14459, 2925, 31168, 1515, 2479, 1311, 347, 51820], "temperature": 0.0, "avg_logprob": -0.34596337805261146, "compression_ratio": 1.730263157894737, "no_speech_prob": 0.2115311324596405}, {"id": 453, "seek": 275014, "start": 2750.14, "end": 2757.8599999999997, "text": " bruja por witch y ah\u00ed me crear\u00eda otra hip\u00f3tesis de traducci\u00f3n donde cubro la pen\u00faltima", "tokens": [50364, 25267, 2938, 1515, 14867, 288, 12571, 385, 1197, 21178, 13623, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 10488, 10057, 340, 635, 3435, 43447, 4775, 50750], "temperature": 0.0, "avg_logprob": -0.2689879307380089, "compression_ratio": 1.5608695652173914, "no_speech_prob": 0.13081826269626617}, {"id": 454, "seek": 275014, "start": 2757.8599999999997, "end": 2764.3399999999997, "text": " de las palabras en espa\u00f1ol agarr\u00f3 la palabra witch, de el hijo de la palabra witch y tiene", "tokens": [50750, 368, 2439, 35240, 465, 31177, 623, 2284, 812, 635, 31702, 14867, 11, 368, 806, 38390, 368, 635, 31702, 14867, 288, 7066, 51074], "temperature": 0.0, "avg_logprob": -0.2689879307380089, "compression_ratio": 1.5608695652173914, "no_speech_prob": 0.13081826269626617}, {"id": 455, "seek": 275014, "start": 2764.3399999999997, "end": 2770.7799999999997, "text": " una probabilidad de 0.882. Entonces, en cada paso el algoritmo lo que hace es elegir una", "tokens": [51074, 2002, 31959, 4580, 368, 1958, 13, 23, 32848, 13, 15097, 11, 465, 8411, 29212, 806, 3501, 50017, 3280, 450, 631, 10032, 785, 14459, 347, 2002, 51396], "temperature": 0.0, "avg_logprob": -0.2689879307380089, "compression_ratio": 1.5608695652173914, "no_speech_prob": 0.13081826269626617}, {"id": 456, "seek": 275014, "start": 2770.7799999999997, "end": 2775.54, "text": " el hip\u00f3tesis que tiene elegir un par de frases y expandir, as\u00ed que lo siguiente que", "tokens": [51396, 806, 8103, 812, 7269, 271, 631, 7066, 14459, 347, 517, 971, 368, 431, 1957, 288, 5268, 347, 11, 8582, 631, 450, 25666, 631, 51634], "temperature": 0.0, "avg_logprob": -0.2689879307380089, "compression_ratio": 1.5608695652173914, "no_speech_prob": 0.13081826269626617}, {"id": 457, "seek": 277554, "start": 2775.54, "end": 2780.2599999999998, "text": " puedo hacer es elegir la frase, dir not, expandirla a partir de la hip\u00f3tesis que ten\u00eda con", "tokens": [50364, 21612, 6720, 785, 14459, 347, 635, 38406, 11, 4746, 406, 11, 5268, 347, 875, 257, 13906, 368, 635, 8103, 812, 7269, 271, 631, 23718, 416, 50600], "temperature": 0.0, "avg_logprob": -0.3594082809808686, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.2352391630411148}, {"id": 458, "seek": 277554, "start": 2780.2599999999998, "end": 2786.62, "text": " Mary y bueno eso me cubre ahora dos palabras en espa\u00f1ol y me tiene medio otra probabilidad", "tokens": [50600, 6059, 288, 11974, 7287, 385, 10057, 265, 9923, 4491, 35240, 465, 31177, 288, 385, 7066, 22123, 13623, 31959, 4580, 50918], "temperature": 0.0, "avg_logprob": -0.3594082809808686, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.2352391630411148}, {"id": 459, "seek": 277554, "start": 2786.62, "end": 2792.46, "text": " y despu\u00e9s, si gobanzando y si gobanzando, hasta que lleg\u00f3 a cubrir en alg\u00fan momento, si", "tokens": [50918, 288, 15283, 11, 1511, 352, 65, 3910, 1806, 288, 1511, 352, 65, 3910, 1806, 11, 10764, 631, 46182, 257, 10057, 10949, 465, 26300, 9333, 11, 1511, 51210], "temperature": 0.0, "avg_logprob": -0.3594082809808686, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.2352391630411148}, {"id": 460, "seek": 277554, "start": 2792.46, "end": 2796.54, "text": " yo sigo avanzando y sigo arregando hip\u00f3tesis, en alg\u00fan momento voy a llegar a cubrir todas", "tokens": [51210, 5290, 4556, 78, 42444, 1806, 288, 4556, 78, 594, 3375, 1806, 8103, 812, 7269, 271, 11, 465, 26300, 9333, 7552, 257, 24892, 257, 10057, 10949, 10906, 51414], "temperature": 0.0, "avg_logprob": -0.3594082809808686, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.2352391630411148}, {"id": 461, "seek": 277554, "start": 2796.54, "end": 2802.46, "text": " las palabras del idioma espa\u00f1ol, todas las palabras de elaboraci\u00f3n en el idioma espa\u00f1ol.", "tokens": [51414, 2439, 35240, 1103, 18014, 6440, 31177, 11, 10906, 2439, 35240, 368, 16298, 3482, 465, 806, 18014, 6440, 31177, 13, 51710], "temperature": 0.0, "avg_logprob": -0.3594082809808686, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.2352391630411148}, {"id": 462, "seek": 280246, "start": 2802.46, "end": 2807.34, "text": " Entonces ah\u00ed una vez que yo cubrito a las palabras digo bueno, esto es una hip\u00f3tesis completa", "tokens": [50364, 15097, 12571, 2002, 5715, 631, 5290, 10057, 81, 3528, 257, 2439, 35240, 22990, 11974, 11, 7433, 785, 2002, 8103, 812, 7269, 271, 46822, 50608], "temperature": 0.0, "avg_logprob": -0.2952030686771168, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.2357455939054489}, {"id": 463, "seek": 280246, "start": 2807.34, "end": 2814.18, "text": " y esto lo devuelvo como un potencial candidata, digamos, una abracci\u00f3n candidata a traducci\u00f3n.", "tokens": [50608, 288, 7433, 450, 1905, 3483, 3080, 2617, 517, 48265, 6268, 3274, 11, 36430, 11, 2002, 410, 12080, 5687, 6268, 3274, 257, 2479, 1311, 5687, 13, 50950], "temperature": 0.0, "avg_logprob": -0.2952030686771168, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.2357455939054489}, {"id": 464, "seek": 280246, "start": 2814.18, "end": 2818.18, "text": " Pero claro, media que yo fie avanzando una cosa que paso es que fui dejando hip\u00f3tesis", "tokens": [50950, 9377, 16742, 11, 3021, 631, 5290, 283, 414, 42444, 1806, 2002, 10163, 631, 29212, 785, 631, 27863, 21259, 1806, 8103, 812, 7269, 271, 51150], "temperature": 0.0, "avg_logprob": -0.2952030686771168, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.2357455939054489}, {"id": 465, "seek": 280246, "start": 2818.18, "end": 2823.54, "text": " colgadas y esas hip\u00f3tesis podr\u00edan tener otras traducciones posible, yo ac\u00e1 lo que devol\u00ed era", "tokens": [51150, 1173, 70, 6872, 288, 23388, 8103, 812, 7269, 271, 15305, 11084, 11640, 20244, 2479, 1311, 23469, 26644, 11, 5290, 23496, 450, 631, 1905, 401, 870, 4249, 51418], "temperature": 0.0, "avg_logprob": -0.2952030686771168, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.2357455939054489}, {"id": 466, "seek": 280246, "start": 2823.54, "end": 2827.3, "text": " una hip\u00f3tesis de traducci\u00f3n, pero a medida que yo ten\u00eda las otras hip\u00f3tesis, si yo hubiera", "tokens": [51418, 2002, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 11, 4768, 257, 32984, 631, 5290, 23718, 2439, 20244, 8103, 812, 7269, 271, 11, 1511, 5290, 11838, 10609, 51606], "temperature": 0.0, "avg_logprob": -0.2952030686771168, "compression_ratio": 1.790874524714829, "no_speech_prob": 0.2357455939054489}, {"id": 467, "seek": 282730, "start": 2827.3, "end": 2833.02, "text": " seguido por las otras hip\u00f3tesis hubiera podido devoler otras cosas. Entonces, yo necesito", "tokens": [50364, 8878, 2925, 1515, 2439, 20244, 8103, 812, 7269, 271, 11838, 10609, 2497, 2925, 1905, 401, 260, 20244, 12218, 13, 15097, 11, 5290, 11909, 3528, 50650], "temperature": 0.0, "avg_logprob": -0.29627057855779476, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.25713735818862915}, {"id": 468, "seek": 282730, "start": 2833.02, "end": 2838.1000000000004, "text": " hacer un backtracking para poder devoler todas las posibilidades, poder volver a ver las hip\u00f3tesis", "tokens": [50650, 6720, 517, 646, 6903, 14134, 1690, 8152, 1905, 401, 260, 10906, 2439, 1366, 11607, 10284, 11, 8152, 33998, 257, 1306, 2439, 8103, 812, 7269, 271, 50904], "temperature": 0.0, "avg_logprob": -0.29627057855779476, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.25713735818862915}, {"id": 469, "seek": 282730, "start": 2838.1000000000004, "end": 2843.3, "text": " a revisitar las hip\u00f3tesis y que hab\u00eda dejado cogeadas y volver a explorar los otros caminos.", "tokens": [50904, 257, 20767, 3981, 2439, 8103, 812, 7269, 271, 288, 631, 16395, 21259, 1573, 598, 432, 6872, 288, 33998, 257, 24765, 289, 1750, 16422, 1945, 15220, 13, 51164], "temperature": 0.0, "avg_logprob": -0.29627057855779476, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.25713735818862915}, {"id": 470, "seek": 282730, "start": 2843.3, "end": 2849.6200000000003, "text": " Entonces, necesitar\u00edas en un backtracking para recorrer las todas. Y si hago un backtracking,", "tokens": [51164, 15097, 11, 11909, 3981, 10025, 465, 517, 646, 6903, 14134, 1690, 850, 284, 9797, 2439, 10906, 13, 398, 1511, 38721, 517, 646, 6903, 14134, 11, 51480], "temperature": 0.0, "avg_logprob": -0.29627057855779476, "compression_ratio": 1.8446601941747574, "no_speech_prob": 0.25713735818862915}, {"id": 471, "seek": 284962, "start": 2849.62, "end": 2856.62, "text": " lo que va a pasar es que voy a va a ocurrir una explosi\u00f3n de exponencial de la espacidad", "tokens": [50364, 450, 631, 2773, 257, 25344, 785, 631, 7552, 257, 2773, 257, 26430, 10949, 2002, 9215, 2560, 368, 12680, 26567, 368, 635, 7089, 326, 4580, 50714], "temperature": 0.0, "avg_logprob": -0.3037442184808686, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.19536790251731873}, {"id": 472, "seek": 284962, "start": 2856.62, "end": 2861.46, "text": " de b\u00fasqueda, porque en realidad todas las posibilidades que se abren son exponenciales", "tokens": [50714, 368, 272, 10227, 358, 8801, 11, 4021, 465, 25635, 10906, 2439, 1366, 11607, 10284, 631, 369, 410, 1095, 1872, 12680, 26567, 279, 50956], "temperature": 0.0, "avg_logprob": -0.3037442184808686, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.19536790251731873}, {"id": 473, "seek": 284962, "start": 2861.46, "end": 2867.74, "text": " y ah\u00ed esto como que se vuelve bastante lento. Entonces, yo quer\u00eda un decodificador para", "tokens": [50956, 288, 12571, 7433, 2617, 631, 369, 20126, 303, 14651, 287, 15467, 13, 15097, 11, 5290, 37869, 517, 979, 378, 1089, 5409, 1690, 51270], "temperature": 0.0, "avg_logprob": -0.3037442184808686, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.19536790251731873}, {"id": 474, "seek": 284962, "start": 2867.74, "end": 2872.06, "text": " volver este problema un problema tratable. En vez de agarrar las infinitas oraciones del idioma,", "tokens": [51270, 33998, 4065, 12395, 517, 12395, 21507, 712, 13, 2193, 5715, 368, 623, 2284, 289, 2439, 7193, 14182, 420, 9188, 1103, 18014, 6440, 11, 51486], "temperature": 0.0, "avg_logprob": -0.3037442184808686, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.19536790251731873}, {"id": 475, "seek": 284962, "start": 2872.06, "end": 2877.06, "text": " me quedo con algunas que sea m\u00e1s probable. Con esta acorrimo de codificaci\u00f3n, logr\u00e9 reducir", "tokens": [51486, 385, 13617, 78, 416, 27316, 631, 4158, 3573, 21759, 13, 2656, 5283, 696, 284, 5565, 78, 368, 17656, 40802, 11, 31013, 526, 2783, 23568, 51736], "temperature": 0.0, "avg_logprob": -0.3037442184808686, "compression_ratio": 1.6690909090909092, "no_speech_prob": 0.19536790251731873}, {"id": 476, "seek": 287706, "start": 2877.06, "end": 2883.66, "text": " de infinito a algo finito, pero a\u00fan as\u00ed es demasiado lento, porque hay una explosi\u00f3n combinaci\u00f3n", "tokens": [50364, 368, 7193, 3528, 257, 8655, 962, 3528, 11, 4768, 31676, 8582, 785, 39820, 287, 15467, 11, 4021, 4842, 2002, 9215, 2560, 38514, 3482, 50694], "temperature": 0.0, "avg_logprob": -0.2740974426269531, "compression_ratio": 1.75, "no_speech_prob": 0.07127844542264938}, {"id": 477, "seek": 287706, "start": 2883.66, "end": 2889.98, "text": " combinatoria de asipotesis y me quedo una cantidad exponencial de hip\u00f3tesis. Entonces,", "tokens": [50694, 38514, 1639, 654, 368, 382, 647, 17251, 271, 288, 385, 13617, 78, 2002, 33757, 12680, 26567, 368, 8103, 812, 7269, 271, 13, 15097, 11, 51010], "temperature": 0.0, "avg_logprob": -0.2740974426269531, "compression_ratio": 1.75, "no_speech_prob": 0.07127844542264938}, {"id": 478, "seek": 287706, "start": 2889.98, "end": 2894.58, "text": " como es tan grande este problema, digamos como la cantidad hip\u00f3tesis de exponencial y este", "tokens": [51010, 2617, 785, 7603, 8883, 4065, 12395, 11, 36430, 2617, 635, 33757, 8103, 812, 7269, 271, 368, 12680, 26567, 288, 4065, 51240], "temperature": 0.0, "avg_logprob": -0.2740974426269531, "compression_ratio": 1.75, "no_speech_prob": 0.07127844542264938}, {"id": 479, "seek": 287706, "start": 2894.58, "end": 2900.86, "text": " es un problema en EP completo, entonces se utilizan t\u00e9cnicas para reducir el espacio de b\u00fasqueda.", "tokens": [51240, 785, 517, 12395, 465, 25330, 40135, 11, 13003, 369, 19906, 282, 25564, 40672, 1690, 2783, 23568, 806, 33845, 368, 272, 10227, 358, 8801, 13, 51554], "temperature": 0.0, "avg_logprob": -0.2740974426269531, "compression_ratio": 1.75, "no_speech_prob": 0.07127844542264938}, {"id": 480, "seek": 287706, "start": 2900.86, "end": 2905.34, "text": " Y hay como dos tipos de t\u00e9cnicas, algunas son con riesgo y otras son sin riesgo. Las t\u00e9cnicas", "tokens": [51554, 398, 4842, 2617, 4491, 37105, 368, 25564, 40672, 11, 27316, 1872, 416, 23932, 1571, 288, 20244, 1872, 3343, 23932, 1571, 13, 10663, 25564, 40672, 51778], "temperature": 0.0, "avg_logprob": -0.2740974426269531, "compression_ratio": 1.75, "no_speech_prob": 0.07127844542264938}, {"id": 481, "seek": 290534, "start": 2905.34, "end": 2910.0, "text": " sin riesgo, lo que quiere decir es que si yo aplico una t\u00e9cnica de reducci\u00f3n de hip\u00f3tesis,", "tokens": [50364, 3343, 23932, 1571, 11, 450, 631, 23877, 10235, 785, 631, 1511, 5290, 18221, 78, 2002, 45411, 368, 2783, 14735, 368, 8103, 812, 7269, 271, 11, 50597], "temperature": 0.0, "avg_logprob": -0.2496800785176238, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.12068323791027069}, {"id": 482, "seek": 290534, "start": 2910.0, "end": 2916.02, "text": " sin riesgo, la soluci\u00f3n ideal que yo ten\u00eda, dentro de mi b\u00fasqueda, no le voy a perder utilizando", "tokens": [50597, 3343, 23932, 1571, 11, 635, 24807, 5687, 7157, 631, 5290, 23718, 11, 10856, 368, 2752, 272, 10227, 358, 8801, 11, 572, 476, 7552, 257, 26971, 19906, 1806, 50898], "temperature": 0.0, "avg_logprob": -0.2496800785176238, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.12068323791027069}, {"id": 483, "seek": 290534, "start": 2916.02, "end": 2920.1400000000003, "text": " una t\u00e9cnica sin riesgo. En cambio en la con riesgo, si yo podr\u00eda llegar a perder la soluci\u00f3n", "tokens": [50898, 2002, 45411, 3343, 23932, 1571, 13, 2193, 28731, 465, 635, 416, 23932, 1571, 11, 1511, 5290, 27246, 24892, 257, 26971, 635, 24807, 5687, 51104], "temperature": 0.0, "avg_logprob": -0.2496800785176238, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.12068323791027069}, {"id": 484, "seek": 290534, "start": 2920.1400000000003, "end": 2926.1000000000004, "text": " \u00f3ptima. Bien, entonces, la t\u00e9cnica sin riesgo que conocemos es la de recombinaci\u00f3n de hip\u00f3tesis,", "tokens": [51104, 11857, 662, 4775, 13, 16956, 11, 13003, 11, 635, 45411, 3343, 23932, 1571, 631, 33029, 38173, 785, 635, 368, 850, 3548, 259, 3482, 368, 8103, 812, 7269, 271, 11, 51402], "temperature": 0.0, "avg_logprob": -0.2496800785176238, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.12068323791027069}, {"id": 485, "seek": 290534, "start": 2926.1000000000004, "end": 2930.3, "text": " que dice que si yo tengo dos hip\u00f3tesis, voy avanzando por dos caminos, dentro del algoritmo", "tokens": [51402, 631, 10313, 631, 1511, 5290, 13989, 4491, 8103, 812, 7269, 271, 11, 7552, 42444, 1806, 1515, 4491, 1945, 15220, 11, 10856, 1103, 3501, 50017, 3280, 51612], "temperature": 0.0, "avg_logprob": -0.2496800785176238, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.12068323791027069}, {"id": 486, "seek": 290534, "start": 2930.3, "end": 2935.06, "text": " y llevo a dos hip\u00f3tesis iguales, por lo menos dos hip\u00f3tesis que cubren las mismas palabras,", "tokens": [51612, 288, 12038, 3080, 257, 4491, 8103, 812, 7269, 271, 10953, 279, 11, 1515, 450, 8902, 4491, 8103, 812, 7269, 271, 631, 10057, 1095, 2439, 23220, 296, 35240, 11, 51850], "temperature": 0.0, "avg_logprob": -0.2496800785176238, "compression_ratio": 1.976027397260274, "no_speech_prob": 0.12068323791027069}, {"id": 487, "seek": 293506, "start": 2935.06, "end": 2940.14, "text": " entonces me puedo quedar con la que tiene mayor probabilidad de las dos y descartar la otra.", "tokens": [50364, 13003, 385, 21612, 39244, 416, 635, 631, 7066, 10120, 31959, 4580, 368, 2439, 4491, 288, 7471, 446, 289, 635, 13623, 13, 50618], "temperature": 0.0, "avg_logprob": -0.2544512950198751, "compression_ratio": 1.8904593639575973, "no_speech_prob": 0.07916773855686188}, {"id": 488, "seek": 293506, "start": 2940.14, "end": 2943.04, "text": " Porque, porque a medida que yo voy a seguir avanzando en el algoritmo, lo que va a pasar", "tokens": [50618, 11287, 11, 4021, 257, 32984, 631, 5290, 7552, 257, 18584, 42444, 1806, 465, 806, 3501, 50017, 3280, 11, 450, 631, 2773, 257, 25344, 50763], "temperature": 0.0, "avg_logprob": -0.2544512950198751, "compression_ratio": 1.8904593639575973, "no_speech_prob": 0.07916773855686188}, {"id": 489, "seek": 293506, "start": 2943.04, "end": 2946.92, "text": " es que van a bajar las probabilidades, digamos, elegiendo m\u00e1s palabras y elegiendo m\u00e1s", "tokens": [50763, 785, 631, 3161, 257, 23589, 289, 2439, 31959, 10284, 11, 36430, 11, 14459, 7304, 3573, 35240, 288, 14459, 7304, 3573, 50957], "temperature": 0.0, "avg_logprob": -0.2544512950198751, "compression_ratio": 1.8904593639575973, "no_speech_prob": 0.07916773855686188}, {"id": 490, "seek": 293506, "start": 2946.92, "end": 2952.62, "text": " frases, me va a bajar la probabilidad y nunca me va a pasar que una de las hip\u00f3tesis que", "tokens": [50957, 431, 1957, 11, 385, 2773, 257, 23589, 289, 635, 31959, 4580, 288, 13768, 385, 2773, 257, 25344, 631, 2002, 368, 2439, 8103, 812, 7269, 271, 631, 51242], "temperature": 0.0, "avg_logprob": -0.2544512950198751, "compression_ratio": 1.8904593639575973, "no_speech_prob": 0.07916773855686188}, {"id": 491, "seek": 293506, "start": 2952.62, "end": 2956.7799999999997, "text": " ten\u00eda menos probabilidad vaya a subir en realidad, siempre va a tener menos. Entonces,", "tokens": [51242, 23718, 8902, 31959, 4580, 47682, 257, 34785, 465, 25635, 11, 12758, 2773, 257, 11640, 8902, 13, 15097, 11, 51450], "temperature": 0.0, "avg_logprob": -0.2544512950198751, "compression_ratio": 1.8904593639575973, "no_speech_prob": 0.07916773855686188}, {"id": 492, "seek": 293506, "start": 2956.7799999999997, "end": 2961.6, "text": " en definitiva, yo puedo conseguir de descartar la que tiene menos probabilidad. Bueno,", "tokens": [51450, 465, 28781, 5931, 11, 5290, 21612, 21229, 368, 7471, 446, 289, 635, 631, 7066, 8902, 31959, 4580, 13, 16046, 11, 51691], "temperature": 0.0, "avg_logprob": -0.2544512950198751, "compression_ratio": 1.8904593639575973, "no_speech_prob": 0.07916773855686188}, {"id": 493, "seek": 296160, "start": 2961.6, "end": 2967.24, "text": " esa es recomendaci\u00f3n de hip\u00f3tesis, pero ni siquiera con eso, alcanza, digamos, para", "tokens": [50364, 11342, 785, 40292, 3482, 368, 8103, 812, 7269, 271, 11, 4768, 3867, 1511, 35134, 416, 7287, 11, 419, 7035, 2394, 11, 36430, 11, 1690, 50646], "temperature": 0.0, "avg_logprob": -0.2959812210827339, "compression_ratio": 1.900709219858156, "no_speech_prob": 0.19694003462791443}, {"id": 494, "seek": 296160, "start": 2967.24, "end": 2971.72, "text": " reducir el espacio de b\u00fasqueda, lo suficiente, a\u00fan queda much\u00edsimas hip\u00f3tesis. Entonces,", "tokens": [50646, 2783, 23568, 806, 33845, 368, 272, 10227, 358, 8801, 11, 450, 33958, 11, 31676, 23314, 29353, 17957, 8103, 812, 7269, 271, 13, 15097, 11, 50870], "temperature": 0.0, "avg_logprob": -0.2959812210827339, "compression_ratio": 1.900709219858156, "no_speech_prob": 0.19694003462791443}, {"id": 495, "seek": 296160, "start": 2971.72, "end": 2976.36, "text": " s\u00f3lo utilizar t\u00e9cnicas de podado con riesgo, la t\u00e9cnica de listo grama, la t\u00e9cnica de", "tokens": [50870, 22885, 24060, 25564, 40672, 368, 2497, 1573, 416, 23932, 1571, 11, 635, 45411, 368, 287, 9334, 677, 2404, 11, 635, 45411, 368, 51102], "temperature": 0.0, "avg_logprob": -0.2959812210827339, "compression_ratio": 1.900709219858156, "no_speech_prob": 0.19694003462791443}, {"id": 496, "seek": 296160, "start": 2976.36, "end": 2980.36, "text": " lumbral, el listo grama significa que, a cada paso, digamos, en cada paso el algoritmo,", "tokens": [51102, 287, 2860, 2155, 11, 806, 287, 9334, 677, 2404, 19957, 631, 11, 257, 8411, 29212, 11, 36430, 11, 465, 8411, 29212, 806, 3501, 50017, 3280, 11, 51302], "temperature": 0.0, "avg_logprob": -0.2959812210827339, "compression_ratio": 1.900709219858156, "no_speech_prob": 0.19694003462791443}, {"id": 497, "seek": 296160, "start": 2980.36, "end": 2984.92, "text": " yo me quedo con los N, las N hip\u00f3tesis de traducci\u00f3n m\u00e1s probable y descart\u00f3 las", "tokens": [51302, 5290, 385, 13617, 78, 416, 1750, 426, 11, 2439, 426, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 3573, 21759, 288, 7471, 446, 812, 2439, 51530], "temperature": 0.0, "avg_logprob": -0.2959812210827339, "compression_ratio": 1.900709219858156, "no_speech_prob": 0.19694003462791443}, {"id": 498, "seek": 296160, "start": 2984.92, "end": 2990.4, "text": " otras. Y la t\u00e9cnica de lumbral dice que, a cada paso el algoritmo, me quedo con la hip\u00f3tesis", "tokens": [51530, 20244, 13, 398, 635, 45411, 368, 287, 2860, 2155, 10313, 631, 11, 257, 8411, 29212, 806, 3501, 50017, 3280, 11, 385, 13617, 78, 416, 635, 8103, 812, 7269, 271, 51804], "temperature": 0.0, "avg_logprob": -0.2959812210827339, "compression_ratio": 1.900709219858156, "no_speech_prob": 0.19694003462791443}, {"id": 499, "seek": 299040, "start": 2990.4, "end": 2995.2000000000003, "text": " de mayor probabilidad y las que est\u00e9n a una distancia alfa m\u00e1xima de esa.", "tokens": [50364, 368, 10120, 31959, 4580, 288, 2439, 631, 871, 3516, 257, 2002, 1483, 22862, 419, 11771, 31031, 64, 368, 11342, 13, 50604], "temperature": 0.0, "avg_logprob": -0.3526744103246881, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.05409056693315506}, {"id": 500, "seek": 299040, "start": 2995.2000000000003, "end": 3002.04, "text": " \u00bfCu\u00e1l es el riesgo de las t\u00e9cnicas de podado? Que si la mejor traducci\u00f3n y la traducci\u00f3n", "tokens": [50604, 3841, 35222, 11447, 785, 806, 23932, 1571, 368, 2439, 25564, 40672, 368, 2497, 1573, 30, 4493, 1511, 635, 11479, 2479, 1311, 5687, 288, 635, 2479, 1311, 5687, 50946], "temperature": 0.0, "avg_logprob": -0.3526744103246881, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.05409056693315506}, {"id": 501, "seek": 299040, "start": 3002.04, "end": 3006.2000000000003, "text": " \u00f3ptima ten\u00eda algunas frases muy poco probable, es al principio, entonces probablemente yo", "tokens": [50946, 11857, 662, 4775, 23718, 27316, 431, 1957, 5323, 10639, 21759, 11, 785, 419, 34308, 11, 13003, 21759, 4082, 5290, 51154], "temperature": 0.0, "avg_logprob": -0.3526744103246881, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.05409056693315506}, {"id": 502, "seek": 299040, "start": 3006.2000000000003, "end": 3011.7200000000003, "text": " descarte esa soluci\u00f3n en los primeros pasos y no lleguen a contar la soluci\u00f3n \u00f3ptima.", "tokens": [51154, 7471, 11026, 11342, 24807, 5687, 465, 1750, 12595, 329, 1736, 329, 288, 572, 11234, 7801, 257, 27045, 635, 24807, 5687, 11857, 662, 4775, 13, 51430], "temperature": 0.0, "avg_logprob": -0.3526744103246881, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.05409056693315506}, {"id": 503, "seek": 299040, "start": 3011.7200000000003, "end": 3018.76, "text": " La p\u00e9rdida, por eso yo haber podado. Sin embargo, bueno, tiene como, como ventaja que en realidad", "tokens": [51430, 2369, 280, 4198, 67, 2887, 11, 1515, 7287, 5290, 15811, 2497, 1573, 13, 11187, 23955, 11, 11974, 11, 7066, 2617, 11, 2617, 6931, 12908, 631, 465, 25635, 51782], "temperature": 0.0, "avg_logprob": -0.3526744103246881, "compression_ratio": 1.5921985815602837, "no_speech_prob": 0.05409056693315506}, {"id": 504, "seek": 301876, "start": 3018.76, "end": 3026.0400000000004, "text": " reducen much\u00edsimo el espacio de b\u00fasqueda y vuelve este problema, un problema tratable.", "tokens": [50364, 2783, 13037, 44722, 806, 33845, 368, 272, 10227, 358, 8801, 288, 20126, 303, 4065, 12395, 11, 517, 12395, 21507, 712, 13, 50728], "temperature": 0.0, "avg_logprob": -0.3311514696538054, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.17671291530132294}, {"id": 505, "seek": 301876, "start": 3026.0400000000004, "end": 3029.5600000000004, "text": " Bueno, y ahora s\u00ed, qu\u00e9 significaba esa probabilidad que estaba viendo en cada una de", "tokens": [50728, 16046, 11, 288, 9923, 8600, 11, 8057, 3350, 5509, 11342, 31959, 4580, 631, 17544, 34506, 465, 8411, 2002, 368, 50904], "temperature": 0.0, "avg_logprob": -0.3311514696538054, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.17671291530132294}, {"id": 506, "seek": 301876, "start": 3029.5600000000004, "end": 3035.0400000000004, "text": " asip\u00f3tesis. O sea, el podado necesita tener las mejores asip\u00f3tesis y bueno, para la", "tokens": [50904, 382, 647, 812, 7269, 271, 13, 422, 4158, 11, 806, 2497, 1573, 45485, 11640, 2439, 42284, 382, 647, 812, 7269, 271, 288, 11974, 11, 1690, 635, 51178], "temperature": 0.0, "avg_logprob": -0.3311514696538054, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.17671291530132294}, {"id": 507, "seek": 301876, "start": 3035.0400000000004, "end": 3039.36, "text": " recomendaci\u00f3n tambi\u00e9n exitos a ver la probabilidad de asip\u00f3tesis. Bueno, la forma de calcular", "tokens": [51178, 40292, 3482, 6407, 11043, 329, 257, 1306, 635, 31959, 4580, 368, 382, 647, 812, 7269, 271, 13, 16046, 11, 635, 8366, 368, 2104, 17792, 51394], "temperature": 0.0, "avg_logprob": -0.3311514696538054, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.17671291530132294}, {"id": 508, "seek": 301876, "start": 3039.36, "end": 3043.32, "text": " la probabilidad de asip\u00f3tesis se divide en dos, digamos, tengo lo que, en contraste al", "tokens": [51394, 635, 31959, 4580, 368, 382, 647, 812, 7269, 271, 369, 9845, 465, 4491, 11, 36430, 11, 13989, 450, 631, 11, 465, 8712, 68, 419, 51592], "temperature": 0.0, "avg_logprob": -0.3311514696538054, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.17671291530132294}, {"id": 509, "seek": 301876, "start": 3043.32, "end": 3047.0800000000004, "text": " momento, el asip\u00f3tesis se va a cuidar a cierta cantidad de palabras. Entonces, para", "tokens": [51592, 9333, 11, 806, 382, 647, 812, 7269, 271, 369, 2773, 257, 20770, 289, 257, 39769, 1328, 33757, 368, 35240, 13, 15097, 11, 1690, 51780], "temperature": 0.0, "avg_logprob": -0.3311514696538054, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.17671291530132294}, {"id": 510, "seek": 304708, "start": 3047.08, "end": 3051.16, "text": " esa cantidad para la verdad, que se llev\u00f3 cubiertas, utilizo los 3 modelos en modelos de", "tokens": [50364, 11342, 33757, 1690, 635, 13692, 11, 631, 369, 27124, 812, 10057, 4859, 296, 11, 4976, 19055, 1750, 805, 2316, 329, 465, 2316, 329, 368, 50568], "temperature": 0.0, "avg_logprob": -0.2765595312832164, "compression_ratio": 2.132295719844358, "no_speech_prob": 0.0723661407828331}, {"id": 511, "seek": 304708, "start": 3051.16, "end": 3055.7599999999998, "text": " traducci\u00f3n, el modelo de rodeonamiento y el modelo de lenguaje, utilizo los 3 modelos para", "tokens": [50568, 2479, 1311, 5687, 11, 806, 27825, 368, 21602, 266, 16971, 288, 806, 27825, 368, 35044, 84, 11153, 11, 4976, 19055, 1750, 805, 2316, 329, 1690, 50798], "temperature": 0.0, "avg_logprob": -0.2765595312832164, "compression_ratio": 2.132295719844358, "no_speech_prob": 0.0723661407828331}, {"id": 512, "seek": 304708, "start": 3055.7599999999998, "end": 3061.36, "text": " calcular la probabilidad de las frases hasta el momento, pero para lo que me falta traducir,", "tokens": [50798, 2104, 17792, 635, 31959, 4580, 368, 2439, 431, 1957, 10764, 806, 9333, 11, 4768, 1690, 450, 631, 385, 22111, 2479, 1311, 347, 11, 51078], "temperature": 0.0, "avg_logprob": -0.2765595312832164, "compression_ratio": 2.132295719844358, "no_speech_prob": 0.0723661407828331}, {"id": 513, "seek": 304708, "start": 3061.36, "end": 3065.44, "text": " yo no puedo utilizar todo porque no tengo toda la informaci\u00f3n de traducci\u00f3n, entonces lo", "tokens": [51078, 5290, 572, 21612, 24060, 5149, 4021, 572, 13989, 11687, 635, 21660, 368, 2479, 1311, 5687, 11, 13003, 450, 51282], "temperature": 0.0, "avg_logprob": -0.2765595312832164, "compression_ratio": 2.132295719844358, "no_speech_prob": 0.0723661407828331}, {"id": 514, "seek": 304708, "start": 3065.44, "end": 3069.44, "text": " que hago es utilizar solamente el modelo de traducci\u00f3n y el modelo de lenguaje. Descarto", "tokens": [51282, 631, 38721, 785, 24060, 27814, 806, 27825, 368, 2479, 1311, 5687, 288, 806, 27825, 368, 35044, 84, 11153, 13, 3885, 66, 15864, 51482], "temperature": 0.0, "avg_logprob": -0.2765595312832164, "compression_ratio": 2.132295719844358, "no_speech_prob": 0.0723661407828331}, {"id": 515, "seek": 304708, "start": 3069.44, "end": 3074.08, "text": " el modelo de rodeonamiento y bueno, entonces algo, calcula una probabilidad que es una parte", "tokens": [51482, 806, 27825, 368, 21602, 266, 16971, 288, 11974, 11, 13003, 8655, 11, 4322, 64, 2002, 31959, 4580, 631, 785, 2002, 6975, 51714], "temperature": 0.0, "avg_logprob": -0.2765595312832164, "compression_ratio": 2.132295719844358, "no_speech_prob": 0.0723661407828331}, {"id": 516, "seek": 307408, "start": 3074.08, "end": 3079.68, "text": " de con todos los 3 modelos y otra parte sin el modelo de rodeonamiento. Bien, este algoritmo", "tokens": [50364, 368, 416, 6321, 1750, 805, 2316, 329, 288, 13623, 6975, 3343, 806, 27825, 368, 21602, 266, 16971, 13, 16956, 11, 4065, 3501, 50017, 3280, 50644], "temperature": 0.0, "avg_logprob": -0.33197836839515743, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.05478670075535774}, {"id": 517, "seek": 307408, "start": 3079.68, "end": 3084.68, "text": " que acabamos de describir que hace esta b\u00fasqueda bas\u00e1ndose en hip\u00f3tesis que utiliza", "tokens": [50644, 631, 13281, 2151, 368, 2189, 10119, 631, 10032, 5283, 272, 10227, 358, 8801, 987, 18606, 541, 465, 8103, 812, 7269, 271, 631, 4976, 13427, 50894], "temperature": 0.0, "avg_logprob": -0.33197836839515743, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.05478670075535774}, {"id": 518, "seek": 307408, "start": 3084.68, "end": 3090.52, "text": " recomendaci\u00f3n y podado hip\u00f3tesis y bueno, calcula las probabilidades de esta manera,", "tokens": [50894, 40292, 3482, 288, 2497, 1573, 8103, 812, 7269, 271, 288, 11974, 11, 4322, 64, 2439, 31959, 10284, 368, 5283, 13913, 11, 51186], "temperature": 0.0, "avg_logprob": -0.33197836839515743, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.05478670075535774}, {"id": 519, "seek": 307408, "start": 3090.52, "end": 3095.68, "text": " se conoce como algoritmo b\u00fasqueda asterico, es un algoritmo de vincers que se usa much\u00edsimo", "tokens": [51186, 369, 33029, 384, 2617, 3501, 50017, 3280, 272, 10227, 358, 8801, 257, 3120, 2789, 11, 785, 517, 3501, 50017, 3280, 368, 371, 4647, 433, 631, 369, 29909, 44722, 51444], "temperature": 0.0, "avg_logprob": -0.33197836839515743, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.05478670075535774}, {"id": 520, "seek": 307408, "start": 3095.68, "end": 3101.6, "text": " en lo que es traducci\u00f3n autom\u00e1tica estad\u00edstica. Por ejemplo, el sistema Moses, ac\u00e1 tenemos", "tokens": [51444, 465, 450, 631, 785, 2479, 1311, 5687, 3553, 23432, 39160, 19512, 2262, 13, 5269, 13358, 11, 806, 13245, 17580, 11, 23496, 9914, 51740], "temperature": 0.0, "avg_logprob": -0.33197836839515743, "compression_ratio": 1.6308243727598566, "no_speech_prob": 0.05478670075535774}, {"id": 521, "seek": 310160, "start": 3101.6, "end": 3108.0, "text": " este ejemplos de herramientas o pensores o gratuitas que siguen para construcci\u00f3n de traducci\u00f3n", "tokens": [50364, 4065, 10012, 5895, 329, 368, 38271, 296, 277, 6099, 2706, 277, 38342, 296, 631, 4556, 7801, 1690, 12946, 14735, 368, 2479, 1311, 5687, 50684], "temperature": 0.0, "avg_logprob": -0.36191994374192604, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.09102693200111389}, {"id": 522, "seek": 310160, "start": 3108.0, "end": 3113.72, "text": " autom\u00e1ticos. Es el sistema Moses, es un sistema o pens\u00f3 para desarrollar este tipo de traducci\u00f3n", "tokens": [50684, 3553, 7656, 9940, 13, 2313, 806, 13245, 17580, 11, 785, 517, 13245, 277, 6099, 812, 1690, 32501, 289, 4065, 9746, 368, 2479, 1311, 5687, 50970], "temperature": 0.0, "avg_logprob": -0.36191994374192604, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.09102693200111389}, {"id": 523, "seek": 310160, "start": 3113.72, "end": 3120.7999999999997, "text": " autom\u00e1ticos estad\u00edsticos y hay implementa este algoritmo de codificaci\u00f3n de b\u00fasqueda asterico.", "tokens": [50970, 3553, 7656, 9940, 39160, 19512, 9940, 288, 4842, 4445, 64, 4065, 3501, 50017, 3280, 368, 17656, 40802, 368, 272, 10227, 358, 8801, 257, 3120, 2789, 13, 51324], "temperature": 0.0, "avg_logprob": -0.36191994374192604, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.09102693200111389}, {"id": 524, "seek": 310160, "start": 3120.7999999999997, "end": 3125.48, "text": " Y bueno, lo que tiene el sistema Moses de Buenio es que en realidad lo que hace adem\u00e1s", "tokens": [51324, 398, 11974, 11, 450, 631, 7066, 806, 13245, 17580, 368, 4078, 268, 1004, 785, 631, 465, 25635, 450, 631, 10032, 21251, 51558], "temperature": 0.0, "avg_logprob": -0.36191994374192604, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.09102693200111389}, {"id": 525, "seek": 310160, "start": 3125.48, "end": 3130.68, "text": " de implementar el de codificadores utiliza a los otros sistemas y los integrar alguna manera.", "tokens": [51558, 368, 4445, 289, 806, 368, 17656, 1089, 11856, 4976, 13427, 257, 1750, 16422, 48720, 288, 1750, 3572, 289, 20651, 13913, 13, 51818], "temperature": 0.0, "avg_logprob": -0.36191994374192604, "compression_ratio": 1.7835820895522387, "no_speech_prob": 0.09102693200111389}, {"id": 526, "seek": 313068, "start": 3130.68, "end": 3135.7999999999997, "text": " Entonces, integra este otro sistema al ERCTLM que es una herramienta para crear modelos", "tokens": [50364, 15097, 11, 3572, 64, 4065, 11921, 13245, 419, 14929, 34, 51, 43, 44, 631, 785, 2002, 38271, 64, 1690, 31984, 2316, 329, 50620], "temperature": 0.0, "avg_logprob": -0.48542622596986834, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.24866001307964325}, {"id": 527, "seek": 313068, "start": 3135.7999999999997, "end": 3140.24, "text": " del lenguaje basados en el gramas y el otro sistema es el quiso m\u00e1s m\u00e1s que lo veo, mencionado", "tokens": [50620, 1103, 35044, 84, 11153, 987, 4181, 465, 806, 677, 19473, 288, 806, 11921, 13245, 785, 806, 421, 19227, 3573, 3573, 631, 450, 41319, 11, 37030, 1573, 50842], "temperature": 0.0, "avg_logprob": -0.48542622596986834, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.24866001307964325}, {"id": 528, "seek": 313068, "start": 3140.24, "end": 3147.3199999999997, "text": " hoy que es el sistema que me permite alinear corpus de operaciones en los distintos", "tokens": [50842, 13775, 631, 785, 806, 13245, 631, 385, 31105, 419, 533, 289, 1181, 31624, 368, 2208, 9188, 465, 1750, 49337, 51196], "temperature": 0.0, "avg_logprob": -0.48542622596986834, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.24866001307964325}, {"id": 529, "seek": 313068, "start": 3147.3199999999997, "end": 3152.68, "text": " sitiomas llegando a los modelos del 1 ad 5 de traducci\u00f3n de BMS. Bueno, entonces, esta", "tokens": [51196, 1394, 72, 7092, 11234, 1806, 257, 1750, 2316, 329, 1103, 502, 614, 1025, 368, 2479, 1311, 5687, 368, 363, 10288, 13, 16046, 11, 13003, 11, 5283, 51464], "temperature": 0.0, "avg_logprob": -0.48542622596986834, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.24866001307964325}, {"id": 530, "seek": 313068, "start": 3152.68, "end": 3156.7599999999998, "text": " tres herramientas, si uno quiere construir un tradutor autom\u00e1tico estad\u00edstico, entre cualquier", "tokens": [51464, 15890, 38271, 296, 11, 1511, 8526, 23877, 38445, 517, 2479, 22163, 3553, 28234, 39160, 19512, 2789, 11, 3962, 21004, 51668], "temperature": 0.0, "avg_logprob": -0.48542622596986834, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.24866001307964325}, {"id": 531, "seek": 315676, "start": 3156.76, "end": 3162.92, "text": " par de idiomas, puede utilizar estas tres herramientas y ten\u00edan un corpus paralelo y un corpus", "tokens": [50364, 971, 368, 18014, 7092, 11, 8919, 24060, 13897, 15890, 38271, 296, 288, 47596, 517, 1181, 31624, 26009, 10590, 288, 517, 1181, 31624, 50672], "temperature": 0.0, "avg_logprob": -0.38441307313980594, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.20615392923355103}, {"id": 532, "seek": 315676, "start": 3162.92, "end": 3168.1200000000003, "text": " monolingue puede construir un tradutor. Pero, bueno, adem\u00e1s, otra cosa que me ense\u00f1amos en la", "tokens": [50672, 1108, 401, 278, 622, 8919, 38445, 517, 2479, 22163, 13, 9377, 11, 11974, 11, 21251, 11, 13623, 10163, 631, 385, 31275, 2151, 465, 635, 50932], "temperature": 0.0, "avg_logprob": -0.38441307313980594, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.20615392923355103}, {"id": 533, "seek": 315676, "start": 3168.1200000000003, "end": 3173.1600000000003, "text": " clase basada, pero eran los sistemas basados en reglas, los sistemas basados en reglas han ca\u00eddo", "tokens": [50932, 44578, 987, 1538, 11, 4768, 32762, 1750, 48720, 987, 4181, 465, 1121, 7743, 11, 1750, 48720, 987, 4181, 465, 1121, 7743, 7276, 1335, 28470, 51184], "temperature": 0.0, "avg_logprob": -0.38441307313980594, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.20615392923355103}, {"id": 534, "seek": 315676, "start": 3173.1600000000003, "end": 3178.6800000000003, "text": " un poco, y a monotiene tanta popularidad como antes. Sin embargo, algunos se siguen usando,", "tokens": [51184, 517, 10639, 11, 288, 257, 1108, 310, 10174, 40864, 3743, 4580, 2617, 11014, 13, 11187, 23955, 11, 21078, 369, 4556, 7801, 29798, 11, 51460], "temperature": 0.0, "avg_logprob": -0.38441307313980594, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.20615392923355103}, {"id": 535, "seek": 315676, "start": 3178.6800000000003, "end": 3182.6800000000003, "text": " y el sistema aperty un sistema o pensor para construir sistema de traducci\u00f3n basados", "tokens": [51460, 288, 806, 13245, 43139, 874, 517, 13245, 277, 6099, 284, 1690, 38445, 13245, 368, 2479, 1311, 5687, 987, 4181, 51660], "temperature": 0.0, "avg_logprob": -0.38441307313980594, "compression_ratio": 1.755639097744361, "no_speech_prob": 0.20615392923355103}, {"id": 536, "seek": 318268, "start": 3182.68, "end": 3188.52, "text": " en reglas, que tienen un mont\u00f3n de pares de lenguajes. Y, bueno, ya anda relativamente bien,", "tokens": [50364, 465, 1121, 7743, 11, 631, 12536, 517, 45259, 368, 2502, 495, 368, 35044, 84, 29362, 13, 398, 11, 11974, 11, 2478, 21851, 21960, 3439, 3610, 11, 50656], "temperature": 0.0, "avg_logprob": -0.3827623407891456, "compression_ratio": 1.6069651741293531, "no_speech_prob": 0.16871792078018188}, {"id": 537, "seek": 318268, "start": 3188.52, "end": 3193.52, "text": " digamos, entonces, se sigue desarrollando esta hoy, entonces, es una alternativa o pensor que", "tokens": [50656, 36430, 11, 13003, 11, 369, 34532, 32501, 1806, 5283, 13775, 11, 13003, 11, 785, 2002, 5400, 18740, 277, 6099, 284, 631, 50906], "temperature": 0.0, "avg_logprob": -0.3827623407891456, "compression_ratio": 1.6069651741293531, "no_speech_prob": 0.16871792078018188}, {"id": 538, "seek": 318268, "start": 3193.52, "end": 3197.8799999999997, "text": " est\u00e1 basada en reglas en vez de estar basado en estas idicas.", "tokens": [50906, 3192, 987, 1538, 465, 1121, 7743, 465, 5715, 368, 8755, 987, 1573, 465, 13897, 4496, 9150, 13, 51124], "temperature": 0.0, "avg_logprob": -0.3827623407891456, "compression_ratio": 1.6069651741293531, "no_speech_prob": 0.16871792078018188}, {"id": 539, "seek": 318268, "start": 3197.8799999999997, "end": 3204.3999999999996, "text": " Y, bueno, esta es un resumen de lo que vimos, as\u00ed que dejamos por ac\u00e1.", "tokens": [51124, 398, 11, 11974, 11, 5283, 785, 517, 725, 16988, 368, 450, 631, 49266, 11, 8582, 631, 21259, 2151, 1515, 23496, 13, 51450], "temperature": 0.0, "avg_logprob": -0.3827623407891456, "compression_ratio": 1.6069651741293531, "no_speech_prob": 0.16871792078018188}], "language": "es"}