{"text": " Una vez que eleg\u00ed en mi, con el paso 1, eleg\u00ed cu\u00e1ntas palabras en espa\u00f1ol e usar, en el paso 2 lo que voy a elegir es una lineaci\u00f3n, una funci\u00f3n de lineaci\u00f3n que me dice cada palabra con cu\u00e1l se va a corresponder, cada palabra al lado del espa\u00f1ol, con qu\u00e9 palabra en ingl\u00e9s se va a corresponder. Este modelo asume de manera muy na\u00ef que todas las lineaciones que yo puedo tener son equiprobables, o sea, asume que yo voy a tener un conjunto de lineaciones posibles y todas van a tener la misma probabilidad. Bien, entonces, la probabilidad de elegir una lineaci\u00f3n en particular, si yo tengo un mont\u00f3n de lineaciones, digamos, la probabilidad de elegir una lineaci\u00f3n en particular va a ser uno sobre la cantidad de lineaciones que tengo, porque en realidad todas van a ser equiprobables. Bien, entonces, \u00bfcu\u00e1ntas lineaciones puedo tener entre dos oraciones, una oraci\u00f3n en ingl\u00e9s que tiene largo y una oraci\u00f3n en espa\u00f1ol que tiene largo jota? \u00bfC\u00f3mo puedo calcular cu\u00e1ntas lineaciones existen? M\u00e1s o menos, s\u00ed, casi de la jota. Recuerden que el lado de ingl\u00e9s yo ten\u00eda ciertas palabras, en ingl\u00e9s ten\u00eda la palabra E1, E2 hasta E sub\u00ed y en espa\u00f1ol ten\u00eda las palabras E1, E2 hasta E subjota. Entonces, yo pod\u00eda trazar l\u00edneas para alinear, pero adem\u00e1s, en ingl\u00e9s yo siempre considerado que ten\u00eda un token null, entonces todas las palabras que no estaban alineadas del lado del espa\u00f1ol y van a parar ah\u00ed. As\u00ed que en ingl\u00e9s en realidad no tengo y posibilidades, tengo una m\u00e1s, tengo y m\u00e1s uno. Entonces, \u00bfcu\u00e1ntas formas tengo yo de mapear estas jota posibilidades en espa\u00f1ol con las E de ingl\u00e9s? Exacto, y m\u00e1s uno de la jota, porque yo tengo y m\u00e1s uno opciones para la primera y m\u00e1s uno opciones para la segunda, etc\u00e9tera, hasta que yo al final. As\u00ed que son y m\u00e1s uno a las jota lineaciones posibles. Ojo, el null es como una pizadita que he oye para alinear cosas que no tienen un correspondiente, o sea, yo ten\u00eda una palabra en espa\u00f1ol que... Estar varias de las CF buenas alineadas de ese null, no importa en qu\u00e9 orden est\u00e1n. Bien, entonces eran y m\u00e1s uno a las jota posibles alineaciones, por lo tanto. La probabilidad de elegir una alineaci\u00f3n A, dada la elaboraci\u00f3n en ingl\u00e9s, la probabilidad de elegir una alineaci\u00f3n cualquiera, dada la oraci\u00f3n en ingl\u00e9s, va a ser el producto de la probabilidad de haber sorteado un valor jota primero, que era Epsilon, por la probabilidad de elegir una alineaci\u00f3n cualquiera para ese jota, que es uno sobre y m\u00e1s uno a la jota. Bien, entonces esto lo resubimos como Epsilon sobre y m\u00e1s uno a la jota. Epsilon sobre y m\u00e1s uno a la jota es la probabilidad de dada una oraci\u00f3n en ingl\u00e9s, elegir cierta alineaci\u00f3n que yo voy a utilizar. Bien, ese fue el segundo paso. El tercer paso es, una vez que ya tengo la alineaci\u00f3n, voy mirando cada palabra de lado en ingl\u00e9s y le voy poniendo una palabra correspondiente de lado espa\u00f1ol. Para ac\u00e1 voy a sumir que yo tengo una tabla de traducci\u00f3n, una tabla de traducci\u00f3n que me dice que tiene de un lado todas las palabras en espa\u00f1ol y de otro lado todas las palabras en ingl\u00e9s, entonces mi tabla va a tener una forma como, por ejemplo, hace una tabla as\u00ed, de lado decir las palabras en espa\u00f1ol como banco, perro, gato y m\u00e1s cosas y de otro lado va a tener las correspondientes en ingl\u00e9s como bank, bench, cat, tree y m\u00e1s cosas. Y entonces esta tabla va a decir la probabilidad de traducir una cosa en la jota, entonces banco probablemente tenga cierta probabilidad para bank y cierta probabilidad para bench, 0.4 y 0.6, 0.06. Y para ac\u00e1 no va a tener ninguna probabilidad y para tree tampoco y despu\u00e9s perro no va a tener nada de esto, pero s\u00ed despu\u00e9s y cat va a ser 0.8 en este caso, etc\u00e9tera. Voy a tener una tabla bastante grande que tiene todas las posibilidades de traducir una palabra como otra. Entonces, si yo tengo esa tabla lo que puedo decir es que la forma de calcular la probabilidad de esa oraci\u00f3n final que yo traduj\u00e9 va a depender de cu\u00e1les son las palabras que yo elija, va a depender de cu\u00e1les son las palabras que yo haya puesto dentro de mi oraci\u00f3n para traducir. Entonces, esa tabla que est\u00e1 ah\u00ed definida le llamamos ac\u00e1 en la slide aparece como tdf sux su y y dice que la probabilidad de traducir la palabra su y como f sux. Entonces, saca de una cosa importante. Si tenemos la oraci\u00f3n en ingl\u00e9s, la oraci\u00f3n en ingl\u00e9s recuerdan que ten\u00eda las palabras, es su 1, es su 2 hasta de su vn, la oraci\u00f3n en espa\u00f1ol ten\u00eda las palabras, es su 1, f su 1, f su 2 hasta f su j, y yo ten\u00eda en el medio una funci\u00f3n de alineaci\u00f3n que me dec\u00eda qu\u00e9 palabras se correspond\u00eda con cu\u00e1l. Entonces, no era de su vn ni f su j, era su y y f su j. Entonces, si yo tengo una palabra cualquiera dentro de la oraci\u00f3n en espa\u00f1ol, tengo un f su j da chica, dentro de la oraci\u00f3n en espa\u00f1ol, esto se va a corresponder con alg\u00fan f su y chica en la oraci\u00f3n en ingl\u00e9s, digamos. Yo s\u00e9 que esto se cumple por la funci\u00f3n de alineaci\u00f3n, porque agarra y mapea todas las palabras que est\u00e1 en espa\u00f1ol con algo que est\u00e1 lado del ingl\u00e9s. Potencialmente con el doque en vac\u00edo null. Bien, entonces, tengo una palabra del lado del espa\u00f1ol que es f su j y una palabra del lado del ingl\u00e9s que es su v. \u00bfCu\u00e1l es la relaci\u00f3n entre ese j y su y? \u00bfC\u00f3mo se relaciona entre s\u00ed? Yo puedo decir que el y es igual a algo de j, de alguna manera. La funci\u00f3n de alineaci\u00f3n, ah\u00ed est\u00e1, o sea, el y es igual a la funci\u00f3n de alineaci\u00f3n aplicada j, como la y, el \u00edndice de ac\u00e1 es igual a la funci\u00f3n de alineaci\u00f3n aplicada j, entonces yo puedo decir que la palabra es su y es igual a la palabra su a su j, as\u00ed que puedo decir que en realidad los que est\u00e1n alineados son la palabra, es f su j est\u00e1 alineado con la palabra e su a su j, y ah\u00ed me saqu\u00e9 el y de encima, digamos. Simplemente, itero sobre las palabras y tir\u00e1ndose la j, puedo establecer la correspondencia entre las dos palabras. Y eso es un poco lo que dice ac\u00e1 para terminar de armar lo que es el modelo de traducci\u00f3n. Para terminar de armar el modelo de traducci\u00f3n dicen que en el tercer paso yo voy a elegir cu\u00e1les son las palabras, entonces lo que voy a hacer es iterar sobre todas las palabras y haciendo el producto de todas las probabilidades, o sea, el producto de dado que hecho ten\u00eda la palabra f su j, dado que yo ten\u00eda la palabra, eso va a su j en ingl\u00e9s, entonces elegir la palabra f su j en espa\u00f1ol, eso a una productoria con todos los valores de las distintas palabras. Bien, entonces ah\u00ed llegu\u00e9 a el \u00faltimo de los valores que quer\u00eda calcular, que es la probabilidad de f dado que conozco a y es igual a la productoria, con j igual 1 hasta j grande, de el valor de la tabla de traducci\u00f3n, que es f su j, t de f su j e suba su j. Bueno, est\u00e1, entonces ah\u00ed tengo como en cada paso fui calculando cosas, este se correspond\u00eda al paso 1 del modelo, paso 1, este se corresponde con el paso 2 del modelo, en realidad, este ya tiene el paso 1 del paso 2 juntos porque ya tengo el epsilon ac\u00e1, y este se corresponde con el paso 3 del modelo, el paso 3 de la historia de generaci\u00f3n. Mi objetivo con todos estos valores que est\u00e1n ac\u00e1 es calcular pd fedadue. \u00bfQu\u00e9 par\u00e1metros introduje? \u00bfQu\u00e9 par\u00e1metros fueron surgiendo a medida que lleva y tirando sobre estos pasos? Bueno, en primer lugar, el epsilon aquel que est\u00e1bamos viendo, este es un valor que yo tendr\u00eda que estimar a partir de mirar en los corpus, como son los largos de las oraciones relativas, y el otro par\u00e1metro importante es aquella tabla all\u00e1, aquella tabla de traducciones que me dice banco, con qu\u00e9 probabilidad lo puedo traducir como banco, que probabilidad lo puedo traducir como bench, etc\u00e9tera, etc\u00e9tera, esa tabla en realidad es un par\u00e1metro del modelo, es un par\u00e1metro del sistema que si yo lo tuviera me alcanzar\u00eda con eso para poder construirme este modelo y calcular la probabilidad de cualquier par de braciones. Bien, y entonces, antes de continuar, vamos a terminar de armar cu\u00e1l es la imagen de esto, que es decir, yo en realidad lo que quer\u00eda calcular era pd fedadue, que eso va a ser mi modelo de traducci\u00f3n, y de hecho va a ser el encargado de medir la adecuaci\u00f3n de una frase. Pd fedadue, lo puedo calcular con esta descomposici\u00f3n de paso, que dice ac\u00e1, en realidad, porque lo hago de la siguiente manera. Yo quiero calcular pd fedadue, y entonces voy a mirar lo que dice ac\u00e1, pd fedadue es igual de la sumatoriana de pd fedadue, que significa eso, que para traducir entre una variaci\u00f3n en espa\u00f1ol y una variaci\u00f3n en ingl\u00e9s, o m\u00e1s bien, para el s\u00ed, bueno, para traducir entre una variaci\u00f3n en ingl\u00e9s y una variaci\u00f3n en espa\u00f1ol, hay muchas formas de alinear las palabras entre el ingl\u00e9s y en espa\u00f1ol, y una vez que yo eleg\u00ed una forma alinear, hay muchas formas de elegir las palabras que vienen despu\u00e9s, digamos, yo miro la traducci\u00f3n y capaz que hay varias maneras de elegir distintas palabras. Entonces, lo que eso significa es que no existe una sola manera de traducir una variaci\u00f3n en ingl\u00e9s a una variaci\u00f3n en espa\u00f1ol. Yo puedo encontrar varias formas de alinear las palabras y de varias formas de elegir las palabras, de manera de que muchas alineaciones son posibles. Entonces, para saber cu\u00e1l es la probabilidad de traducir fd fedadue, entonces yo voy a tener que sumar sobre todo las alineaciones posibles, sobre todo las formas de alinear las dos oraciones fie, voy a tener que iterar sobre eso y para cada una voy a tener que alcular la probabilidad parcial. Entonces, digamos, yo tengo cinco formas alinear las dos oraciones, cinco es un n\u00famero un poco raro, pero digamos, tengo n formas alinear las dos oraciones, voy a tener que mirar, bueno, para la primera alineaci\u00f3n, cu\u00e1l es la probabilidad de encontrar la oraci\u00f3n f, para la segunda alineaci\u00f3n, cu\u00e1l es la probabilidad de encontrar la oraci\u00f3n f, para la tercera alineaci\u00f3n y as\u00ed hasta llegar al final y agarrar el sumo todo eso. Eso lo puedo hacer porque las alineaciones son una descomposici\u00f3n del espacio de probabilidades. En realidad yo puedo descomponer el espacio de probabilidades en pedacitos disjuntos y cada alineaci\u00f3n va a ser uno de ellos. As\u00ed que, digamos que para calcular el modelo de traducci\u00f3n fd fedadue, necesito sumar sobre todo las alineaciones posibles. Ahora, lo que me falta es saber c\u00f3mo calculo este valor ac\u00e1. As\u00ed que lo que estoy diciendo es que la probabilidad de fd fedadue es la suma sobre las alineaciones de la probabilidad de f y esa alineaci\u00f3n dado de. Eso es simplemente lo que dice ah\u00ed en la slide. Lo que me falta a calcular entonces es esta parte de ac\u00e1. Y esa parte de ac\u00e1 la calculo de esta manera. Yo digo que la probabilidad de fdado es igual, ah\u00ed est\u00e1 m\u00e1s o menos al resultado final pero podemos sacar que es lo que tendr\u00eda que poner de este lado. Y ahora s\u00ed me acuerdo bien. Ah, ah\u00ed est\u00e1. Por definici\u00f3n de probabilidad condicional. Eso. Fd fedadue, de verdad, lo har\u00eda manera hacerlo. Pero esto se puede definir como pdf a e sobre pd. \u00bfNo? Por definici\u00f3n de probabilidad condicional. Pero adem\u00e1s esto, si quiero, podr\u00eda llegar a decir esto es lo mismo que pdf a e sobre pd por, podr\u00eda que me faltaba. No, ah\u00ed. Por pd a e sobre pd a e. \u00bfEla esto lo quer\u00eda? S\u00ed, el esto lo quer\u00eda. O sea, yo puedo arrar esta probabilidad que est\u00e1 ac\u00e1 y multiplicarla y dividirla por el mismo n\u00famero que sea que son mayores que cero, eso es la divisi\u00f3n me va a dar uno y ah\u00ed yo puedo tomar y as\u00ed no este con este y este con este en definitiva lo que me queda es si asocio estos dos me va a quedar pdf dado a e y si asocio estos dos de ac\u00e1 me va a quedar pd a dado e qu\u00e9 es lo que dice all\u00e1 la probabilidad de pdf a dado de bueno s\u00ed de los dos de f e a dado e es igual a la probabilidad de f dados a y es por la probabilidad de a dado e bien y estos dos valores que est\u00e1n ac\u00e1 no los elegimos casualidad sino que son los valores que ten\u00eda antes en el modelo o sea yo ten\u00eda que el pd a dado e es igual a epsil\u00f3n sobre y m\u00e1s uno a la j y el otro era la productoria desde j igual 1 hasta j grande de las valores de traducci\u00f3n el f subj y el e suba subj entonces en definitiva puedo calcular pdf a dado e y adem\u00e1s puedo calcular haciendo una suma sobre todas las alineaciones posibles puedo calcular el pdf dado e bien con eso y con todo ese mont\u00f3n de cociones llegamos a construir lo que es un modelo de traducci\u00f3n o sea solamente teniendo una tabla de traducciones que me diga cu\u00e1l es la probabilidad de traducir una palabra como otra palabra yo puedo llegar a definirme cu\u00e1l es la probabilidad de traducir una oraci\u00f3n dada otra oraci\u00f3n bien y hay una cosa m\u00e1s bueno esto ya lo estoy moviendo que aplicamos en cada paso y hay una cosa m\u00e1s que es si yo tuviera las dos oraciones digamos la oraci\u00f3n en ingl\u00e9s y la oraci\u00f3n en espa\u00f1ol y adem\u00e1s tuviera la tabla de esta con todas las probabilidades yo podr\u00eda hacer un algoritmo de programaci\u00f3n din\u00e1mica un algoritmo estilo brit\u00e1nico que vaya recorriendo alineaciones y media cu\u00e1l es la alineaci\u00f3n m\u00e1s probable no vamos a ver los detalles del algoritmo pero hay una forma de decir bueno voy recorriendo las dos oraciones y me voy quedando con las sus secciones m\u00e1s probables y al final me termina devolviendo cu\u00e1l es la alineaci\u00f3n m\u00e1s probable dadas esas oraciones o sea que si yo tuviera ya esa tabla de traducciones esa tabla de probabilidad de traducci\u00f3n podr\u00eda construirme las alineaciones del corpus as\u00ed que bueno hasta el momento dec\u00edamos bueno suponemos que tenemos esta tabla de traducci\u00f3n que me dice para bank si se traduce para bank si se traduce como bank o como bench etc. estaba diciendo que ten\u00eda esa tabla pero en realidad la realidad es que no tengo esa tabla y me gustar\u00eda poder construirlo entonces no gustar\u00eda poder estimar esas probabilidades para poder construir esa tabla si yo tuviera un corpus para el hilo simplemente podr\u00eda ir recorriendo el corpus y contando cu\u00e1ntas veces aparece de banco alineado con bench y cu\u00e1ntas veces aparece alineado con bench y ah\u00ed sacar\u00eda una probabilidad pero no tengo las alineaciones y con lo que vimos digamos reci\u00e9n si yo tuviera la tabla entonces yo adem\u00e1s podr\u00eda ir recorriendo el corpus y construirme las alineaciones as\u00ed que si yo tuviera las alineaciones podr\u00eda contar y sacar la tabla si yo tuviera la tabla podr\u00eda pasarle un algoritmo y construir las alineaciones pero la verdad que no tengo ninguna de las dos cosas entonces se vuelve un problema de huevo y lagallina o sea si yo tuviera las alineaciones construir\u00eda el modelo construir la tabla probabilidades si yo tuviera la tabla probabilidades podr\u00eda construir las alineaciones para este tipo de problemas en los cuales yo tengo como dos variables interdependentes y no conozco exactamente el valor de ninguna de las dos si utiliza lo que se conoce como el algoritmo de expectation maximizaci\u00f3n o maximizaci\u00f3n de la esperanza y bueno es un algoritmo que sirve exactamente para este tipo de problemas en realidad lo que va a hacer el algoritmo es iterar es un algoritmo iterativo que va tratando de converger a una soluci\u00f3n y lo que hace es decir bueno yo no tengo ninguno de los dos valores o sea si yo tuviera mi tabla de probabilidades de traducci\u00f3n me podr\u00eda calcular las alineaciones y tuviera mis alineaciones me podr\u00eda calcular las probabilidades de traducci\u00f3n entonces lo que hace es decir bueno asumo que mi tabla de traducci\u00f3n va a ser uniformes digamos cualquier palabra se puede traducir como cualquier otra palabra con la misma probabilidad a partir de eso calculo alineaciones y a partir de esas nuevas alineaciones calculo otra vez la tabla y de vuelta con esa tabla que calcul\u00e9 vuelvo a medir las alineaciones y de vuelta con esas nuevas alineaciones vuelvo a calcular la tabla entonces aunque no me crean estos despu\u00e9s de muchas iteraciones va convergiendo a algo y parece m\u00e1gico no parece como que tal realidad si yo no tengo ninguno de los valores no deber\u00eda nada deber\u00eda como dar fruta pero voy a tratar de comenzarlos de que en realidad esto s\u00ed funciona con un ejemplito bien tenemos entonces vamos a construir un sistema que es de traducci\u00f3n entre frances y ingl\u00e9s donde hay un cuerpo muy grande pero bueno vamos a concentrar solo en tres peque\u00f1as oraciones que dicen la mes\u00f3n se traduce como de house la mes\u00f3n blu se traduce como de luz house y la flaus se traduce como de flauer entonces al principio lo que hago es decir bueno todas las traducciones entre todas las palabras son equiprobables as\u00ed que lo que me va a quedar es cuando reparta entre las alineaciones todas van a tener el mismo peso entre la y mes\u00f3n la probabilidad de que la se traduja como de o que se traduja como house va a ser la misma en realidad porque todas las alineaciones son equiprobables en la mes\u00f3n blu tambi\u00e9n va a ser lo mismo la probabilidad de traducirla como de como blu o como house va a ser la misma y en la flauer pasa igual entonces eso es la primera el primer paso digamos en el primer paso yo voy a tener todas las alineaciones equiprobables y todas las los valores de las palabras iguales entonces en mi algoritmo yo empec\u00e9 con una tabla de traducci\u00f3n que era toda uniforme digamos yo ten\u00eda la probabilidad de traducir cualquier palabra en cualquier otra era la misma a partir de eso yo me constru\u00ed estas alineaciones que tambi\u00e9n parece que son todas equiprobables y parece que no tienen como mucha informaci\u00f3n entonces lo que voy a hacer ahora a partir de esto es tratar de construirme de vuelta la tabla de traducciones pero mirando estas nuevas alineaciones que hay entonces lo que voy a construir es una tabla que tiene todas las palabras de la de franc\u00e9s tiene la mes\u00f3n blu flado y de house blu flado y para llenar esta nueva tabla lo que tengo que hacer es iterar sobre las alineaciones mirar cada una de las palabras cuantas veces esta alinear con las otras y contar o sea y digamos y sumar los pesos de cada una de las alineaciones entonces la alineaci\u00f3n entre la y de en total mirando ese ejemplo de corpus cu\u00e1nto me dar\u00eda de c\u00f3mo cu\u00e1l ser\u00eda el peso de esa alineaci\u00f3n para verlo en realidad lo que hago es contar miro cu\u00e1ntas veces la y de est\u00e1n alineados entonces tengo 0.5 de peso en la primera en la segunda tengo 0.33 y en la \u00faltima tengo 0.5 de vuelta as\u00ed que en total tengo como 1.33 de peso entre la y de despu\u00e9s miro entre la y house cuantos peso tengo cu\u00e1nta masa de probabilidad tengo bueno tengo 0.5 en la primera relaci\u00f3n 0.33 en la segunda y nada en la tercera por lo tanto en total tengo 0.83 de probabilidades entre la y house despu\u00e9s miro entre la y blu cuantos peso tengo 0.33 solo solamente 0.33 solo est\u00e1 en la del med y entre la y flero cu\u00e1nto tengo no entre la y flower cu\u00e1nto tengo 0.5 solo aparecen la del final bien como tenemos la siguiente entre emes\u00f3n y de cu\u00e1nto tendr\u00eda 0.83 est\u00e1 en la primera en la segunda entre emes\u00f3n y house entre emes\u00f3n y house s\u00ed 0.83 porque aparecen en las dos bien entre emes\u00f3n y blu solamente aparecen la segunda as\u00ed que voy a tener 0.33 y entre emes\u00f3n y flower no tengo nada despu\u00e9s entre blu y de solamente aparece en la segunda as\u00ed que voy a tener 0.33 entre blu y house creo que de vuelta tengo 0.33 y entre blu y blu tambi\u00e9n 0.33 y no aparece junto con flower y para despu\u00e9s para flower tengo 0.5 con de 0 con house 0.5 con flower bien entonces hice una pasada por todas las alineaciones y me calcul\u00e9 cu\u00e1les son los pesos relativos de cada uno de estos pares lo siguiente que hago como esto va a ser una probabilidad es normalizar entonces me voy a construir una tabla digamos normalizando por digamos voy a sumar en cada fila y voy a dividir entre la cantidad que aparece para cada fila as\u00ed que de vuelta tambi\u00e9n construye la tabla que me queda la me son blu y de este lado de la house ac\u00e1 de house blu flower y lo que voy a hacer normalizar entonces si yo sumo estos de ac\u00e1 creo que me da 2 en total no 3 en total tengo los valores ac\u00e1 no tiene que hacer los c\u00e1lculos pero s\u00ed me da 3 en total entonces lo que pasa cuando yo normalizo es que ac\u00e1 me queda 0.44 ac\u00e1 me queda 0.28 ac\u00e1 me queda 0.11 y ac\u00e1 me queda 0.17 pues el segundo tambi\u00e9n lo normalizo esta vez entre dos y me queda 0.42 0.42 0.16 0 el tercero ya suma 1 as\u00ed que me queda 0.23 0.33 0.33 0 y el \u00faltimo tambi\u00e9n queda igual 0.5 0 0 0.5 bien entonces me constru\u00ed una nueva tabla de probabilidad de traducci\u00f3n dado que ahora las alineaciones ser\u00edan estas y no tenlo que pas\u00f3 ac\u00e1 si yo miro la fila correspondiente a la que es lo que pasa ahora con esta fila recuerde que yo empec\u00e9 teniendo todas las alineaciones todas las traducciones pero todas las probabilidades de traducci\u00f3n de que parecen palabras eran equiprobables si yo ahora miro la fila de la que es lo que pasa exacto aparece claramente que la asociaci\u00f3n entre la id es m\u00e1s fuerte tengo un 0.44 de probabilidad de traducirla como de y tengo bastante menos en los otros tengo 0.28 0.11 0.17 y yo hab\u00eda empezado diciendo que eran equiprobables entonces yo probablemente ten\u00eda 0.25 0.25 0.25 0.25 cada una y despu\u00e9s de un paso de la iteraci\u00f3n descubri\u00f3 que la id tienen m\u00e1s chance de ser una traducci\u00f3n de la otra en vez de traducirla como chaos o la como blue o la como flower eso pasa en el primer paso en la primera iteraci\u00f3n el tipo descubre el algoritmo descubre que la asociaci\u00f3n entre la id es bastante m\u00e1s fuerte como pasa eso lo que va a pasar es que cuando yo reparta de vuelta en las alineaciones estas l\u00edneas que se corresponden a la asociaci\u00f3n entre la id van a estar m\u00e1s fuertes van a tener un poco m\u00e1s de peso y como esto es una distribuci\u00f3n de probabilidades esa masa que gan\u00f3 la asociaci\u00f3n entre la id se va a tener que sacar de otras alineaciones posibles o sea si la est\u00e1 asociada con de entonces no est\u00e1 asociada con las otras que est\u00e1n alrededor entonces esa masa que se pierde digamos o sea que que gana en la de se tiene que repartir en las otras alineaciones posibles o sea en las que no son entre la id entonces despu\u00e9s de una iteraci\u00f3n la asociaci\u00f3n entre la id empieza a ser m\u00e1s fuerte y como pasa eso en la siguiente iteraci\u00f3n va a empezar a descubrir que como la estaba alineado con de entonces me son tiene que estar alineado con haus y como me son esta alineado con haus digamos esa esa misma masa de probabilidades se va a traducir a transferir a la segunda y lo mismo como le ha estado alineado con de entonces flea tiene que estar alineado con flauer entonces si yo sigo iterando en estos pasos en cada paso lo que va a pasar es que se va a mover un poco m\u00e1s de probabilidad hasta que al final va a terminar descubriendo cu\u00e1l es la alineaci\u00f3n real de las palabras o sea va a descubrir que la va a social con con de me son con haus lu con blu la ver con flauer como que va a descubrir eso porque en cada paso lo que va pasando es que alguna de las asociaciones como est\u00e1n como aparecen como ocurren digamos en m\u00e1s oraciones tiene m\u00e1s fuerza que otras entonces el peso que esas asociaciones ganan lo va sacando otro lado y eso hace que de otro lado se empiecen a generar otras alineaciones diferentes entonces al final esto termina convergiendo y termina revelando lo que es la estructura su yasente de las palabras y c\u00f3mo se alinean unas con otras bueno y una vez que yo termine de hacer esto puedo agarrar y construirme efectivamente la tabla final de traducciones que es simplemente busco cada una de las posibles traducciones digamos de los posibles pares y saco las probabilidades y qu\u00e9 pas\u00f3 ac\u00e1 mientras yo estaba construyendo mi modelo de traducci\u00f3n mientras yo estaba construyendo la tabla de traducciones adem\u00e1s de como efecto secundario se construyo un corbuz alineado un corbuz que est\u00e1 alineado a nivel de palabra as\u00ed que bueno el algoritmo de expectations maximizaciones funciona de esa manera tiene siempre dos pasos un paso de expectations y un paso de maximizaciones en este caso la expectation era decir el paso de expectations es tratado de agarrar la tabla de probabilidad de traducci\u00f3n que tengo y con eso me arm\u00f3 alineaciones y despu\u00e9s el de maximizaci\u00f3n es al rev\u00e9s agarr\u00f3 las alineaciones que acabo de construir y me arm\u00f3 una nueva tabla y voy iterando todos esos pasos hasta que eventualmente converge bien dijimos que eran cinco modelos dvm no vamos a ver muy en detalle los otros o sea solo mencionar que empiezan a agregar complejidad en este modelo uno hab\u00edamos dicho que todas las alineaciones eran equiprobables en el modelo dos abandonan esa noci\u00f3n y dicen bueno en vez alineaciones equiprobables yo voy a tener un modelo de reordenamiento de las palabras para decir bueno tengo cierta probabilidad de que las palabras que est\u00e1n si yo tengo y palabras en ingl\u00e9s jada palabras en espa\u00f1ol tengo cierta probabilidad de mover la palabra ah\u00ed y la palabra jota y bueno y as\u00ed siguen subiendo en complejidad hasta llegar al modelo 5 que el modelo 5 es el que anda mejor pero de todas maneras estos modelos que ya no se usan digamos esto es del a\u00f1o 93 y en general se han obtenido mejor resultados abandonando estos modelos entonces el que vamos a pasar a ver a continuaci\u00f3n es un modelo bastante m\u00e1s moderno que es lo que s\u00ed se utiliza hoy en d\u00eda en traductores como los de google es que en realidad lo claro a ver estos modelos estad\u00edsticos no utilizan ning\u00fan tipo de analizador morfuelo o ego\u00ed nada para sacarlo hay otros modelos que s\u00ed lo hacen no vamos a dar ninguno en esta clase de brota hay dos modelos que s\u00ed hacen uso de esa informaci\u00f3n igual son como un refinamiento creo que ninguno lo tiene como en la base del modelo el uso de de parto speech pero pero s\u00ed cuando vos no sabes una palabra de una palabra que es desconocida en realidad a utilizar informaci\u00f3n sobre parto speech y eso probablemente te ayude en estos modelos por\u00f3menos no lo hab\u00edan tenido en cuenta bien entonces si lo que vamos a ver ahora es el modelo de frases que es algo m\u00e1s moderno y es o sea el google translate o bin translate se basan en modelos de este estilo y bueno y antes de ver c\u00f3mo se modelo de frases voluamos un poco a lo que era la alineaci\u00f3n entre palabras yo ten\u00eda esta frase cl\u00e1sica no mariano de una ofitada la bruja verde en ingles era mary de not slap green witch y una alineaci\u00f3n entre esas dos oraciones en realidad se ver\u00eda como algo as\u00ed yo tengo que mar\u00eda se alinea con mary no se alinea con this not slap se alinea con da una ofitada de se alinea con ala podr\u00eda ser solamente con la y el a que no est\u00e9 alineado nada green se alinea con verde y bruja con witch qu\u00e9 diferencia tiene esto con la la otra alineaci\u00f3n que hab\u00edamos visto hoy a ver si se les ocurre algo distinto que tiene esta alineaci\u00f3n y la que hab\u00edamos visto hoy era not con no s\u00ed y qu\u00e9 es lo que cambia ac\u00e1 para que pase eso lo que estaba pasando hoy era que yo partida de las palabras en espa\u00f1ol iba a las palabras en ingl\u00e9s y yo ten\u00eda una funci\u00f3n que me mapeaba las palabras en espa\u00f1ol con las palabras en ingl\u00e9s entonces yo a cada palabra en espa\u00f1ol como m\u00e1ximo le pod\u00eda hacer corresponder una palabra en ingl\u00e9s entonces me quedaba que yo pod\u00eda expresar cosas como que daba una ofitada daba esta asociado a slap una esta asociado slap bofeta esta asociado slap eso lo pod\u00eda expresar pero no pod\u00eda expresar algo como esto que no esta asociado did not porque no ser\u00eda una funci\u00f3n yo no puedo asociar uno de los valores de la funci\u00f3n con dos cosas del lado del codominio y ac\u00e1 en realidad no puedo hacerlo ni en este sentido ni en el otro sentido con una funci\u00f3n no me sirve porque de vuelta me pasa que slap esta asociado tres cosas entonces con una funci\u00f3n de alineaci\u00f3n yo no puedo construir este tipo de expresiones en realidad necesito algo como un poco m\u00e1s poderoso esto es lo que dec\u00edamos los modelos dvm siempre usan un mapeo de uno a muchos usan a una funci\u00f3n de alineaci\u00f3n mapeo uno a muchos pero en realidad lo que necesito para poder capturar realmente con funci\u00f3n en el en el lenguaje es mapeo de muchos a muchos yo voy a tener que un conjunto de palabras se va a traducir en otro conjunto de palabras definitiva lo que pasa es que peque\u00f1as frases se traducen como otras peque\u00f1as frases por eso necesito un mapeo de muchos a muchos entonces bueno hay algoritmos que agarran estos mapeos que como el construimos reci\u00e9n el mapeo de uno a muchos en los dos en las dos direcciones digamos y a partir de eso construyen este mapeo de muchos a muchos por ejemplo el algoritmo de la herramienta guisamas m\u00e1s lo que hace es decir bueno yo tengo un corpus en ingl\u00e9s en espa\u00f1ol alineo utilizando los modelos dvm digamos voy alineo por un lado de ingl\u00e9s espa\u00f1ol y por otro lado de espa\u00f1ol ingl\u00e9s y ac\u00e1 me quedan dos mapeos de uno a n digamos dos mapeos con funciones y despu\u00e9s lo que hago es intersectar esos dos esas dos alineaciones que me quedaron y unirlas cuando las intersecto obtengo lo que se conoce como puntos de alta confianza los puntos negros son los puntos de alta confianza que son los de la intersecci\u00f3n y los puntos grises son lo que est\u00e1n en la uni\u00f3n o sea los que pertenec\u00edan algunos de los modelos entonces la herramienta lo que hace es decir bueno una vez que yo tengo la intersecci\u00f3n y la uni\u00f3n hago crecer los puntos que est\u00e1n en la intersecci\u00f3n colonizando otros puntos que est\u00e9n en la uni\u00f3n hasta que al final termin\u00f3 completando digamos toda la imagen este punto que qued\u00f3 solito ah\u00ed ese no ser\u00eda parte de la alineaci\u00f3n al final s\u00f3lo los que puede llegar movi\u00e9ndote a trav\u00e9s de puntos ya conocidos entonces bueno eso es una forma que utiliza se llama el algoritmo de ox y ney que partiendo alineaciones unidireccionales digamos me permite construir una alineaci\u00f3n completa muchos a muchos entre las palabras bien eso le quer\u00eda mencionar acerca de las alineaciones en de palabras y ahora s\u00ed vamos a ver c\u00f3mo funciona un modelo basado en frases un modelo basado en frases tiene cierta semejanza con el modelo anterior que hayamos visto pero es un poco m\u00e1s expresivo en realidad yo parte de una oraci\u00f3n por ejemplo en alem\u00e1n que dec\u00eda Morgan Fliggs ganas ganas de su conference lo primero que hace el modelo cuando quiere traducir digamos en este caso es decir bueno yo voy a segmentar esa oraci\u00f3n de origen en cierta cantidad de frases despu\u00e9s voy a traducir cada una de esas frases usando una tabla de traducci\u00f3n y esta vez no es una tal de traducci\u00f3n de palabras sino que es una tal de traducci\u00f3n de frases que me dice para cada frase con que otra frase se corresponde y una vez que yo traduje cada una esa frase las voy a reordenar de alguna manera buscando que suene lo manatural posible buscando aumentar la fluidez de esa oraci\u00f3n entonces como que la historia de generaci\u00f3n es un poco m\u00e1s simple que la otra no ten\u00eda que ir sortiendo cosas simplemente digo separo mi oraci\u00f3n en segmentos que les voy a llamar frases los traducos y los reordenos esa segmentaci\u00f3n en frases no tiene porque tener una un significado ling\u00fc\u00edstico yo no voy a separarla sin grupo nominal grupo verbal grupo profesional etc\u00e9tera no tengo por qu\u00e9 o sea capaz que yo segmento las frases y justo me queda un grupo proposicional capaz que no lo \u00fanico que tiene que pasar es que estos segmentos que yo construyo tienen que estar en mi tabla de traducci\u00f3n de frases alcanza con eso como para que yo puedo utilizarlos en mi traducci\u00f3n pero no tienen por qu\u00e9 tener una motivaci\u00f3n ling\u00fc\u00edstica bueno entonces un modelo bastante frases tiene estos componentes parecido la anterior porque de vuelta yo lo que quiero hacer es encontrar la probabilidad de fdb digamos sigo teniendo la misma ecuaci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica estad\u00edstica la quiero resolver necesito pdf db y pd s\u00f3lo que ahora el pdf db lo voy a calcular una manera distinta voy a decir que para calcular esto tengo un modelo de traducci\u00f3n de frases y un modelo de ordenamiento un modelo de una gran tabla de frases que me dice cada frase con qu\u00e9 probabil\u00eda la traducci\u00f3n otra y despu\u00e9s una forma de decir c\u00f3mo reorden\u00f3 esa frase para tener mejores oraciones y bueno como siempre voy a tener otro componente que es el que mide la la fluidez que es el modelo del lenguaje porque los modelos de frases funcionan mejor que los modelos basados en palabras porque la frase ya tienen cierto contexto las frases en realidad son como peque\u00f1os grupos de palabras que yo puedo traducir uno en el otro entonces cosas como dar la mano dar una ofetada a tomar el pelo etc\u00e9tera todas esas cosas como expresiones son mucho m\u00e1s f\u00e1ciles de traducir si en realidad yo ya s\u00e9 que esta presi\u00f3n que son tres cuatro palabras le puedo traducir en esta otra expresi\u00f3n que son tres cuatro palabras es como m\u00e1s expresivo entonces puede aprender m\u00e1s cosas y bueno obviamente cuanto m\u00e1s cuanto m\u00e1s atostenga cuanto m\u00e1s largo sea el corpo que yo tengo yo puedo aprender la frase m\u00e1s largas mejores probabilidades y mejores frases bueno ac\u00e1 hay un ejemplo de c\u00f3mo ser\u00eda una tabla de traducci\u00f3n de frases o sea es parecido la tabla de traducci\u00f3n de palabras o lo que ac\u00e1 tengo de en borslac o sea si yo busco la fila asociado en borslac o sea encontrar\u00eda todas estas traducciones de prop\u00f3sal con sesenta dos por ciento de probabilidad posesivo prop\u00f3sal con diez por ciento a prop\u00f3sal con tres por ciento etc\u00e9tera o sea como vence traducen frases en frases bueno y c\u00f3mo hago para aprender una tabla de traducci\u00f3n de frases yo parto de esta alineaci\u00f3n de palabras digamos esta alineaci\u00f3n completa que ya no es una funci\u00f3n sino que es digamos una alineaci\u00f3n de muchos a muchos y voy a tratar de encontrar todos los todas las frases todos los pares de frases que son consistentes con la alineaci\u00f3n a que me refiero con que son consistentes ac\u00e1 hay ejemplos yo quiero decir que mariano y mar\u00eda did not son son un par de frases que son consistentes con esta alineaci\u00f3n en cambio mariano y mar\u00eda did no lo son c\u00f3mo es que miro esto lo que pasa es que cuando yo tengo mariano y mar\u00eda did la palabra no est\u00e1 alineada con did not y el did not digamos el not no pertenece hasta alineaci\u00f3n que yo estoy tratando de decir entonces digo que es no consistente lo mismo pasa con si yo dato alinear mariano daba y mar\u00eda did not lo que pasa ah\u00ed es que daba no est\u00e1 digamos los puntos alineaci\u00f3n de daba no est\u00e1n dentro de este cuadrante que estoy tratando de buscar entonces en definitiva digo que no es consistente las alineaciones consistentes correctas son las que consideran todos los puntos dentro de ese cuadrante entonces mariano est\u00e1 asociado con mariano did not y es as\u00ed es consistente as\u00ed que como aprendo frases consistentes empiezo por las alineaciones digamos empiezo con la alineaci\u00f3n de palabra despu\u00e9s busco de alguna palabra y digo bueno me quedo con todas esas traducciones de palabras y las pongo mi tabla de frases y despu\u00e9s voy tomando de dos y me quedo con todas esas otras frases y las voy agregando mi tabla de frases despu\u00e9s me puedo avanzar en uno tomada tres tomada de cuatro y llegar a tomar incluso toda la elaboraci\u00f3n como frases entonces a partir de estas oraciones que ten\u00edan no s\u00e9 este 1 2 3 4 5 6 7 8 no es palabras yo termin\u00f3 aprendiendo como 17 frases digamos cada vez m\u00e1s grandes y bueno vi hoy sacando esto de todo el corpus y calculando mi tabla de probabilidades de qu\u00e9 manera calculo esas probabilidades yo lo que puedo hacer es como siempre ver cu\u00e1nta vez aparece en el corpus y contar o si no si yo ten\u00eda construido el modelo anterior el modelo de la tabla de traducciones de palabra a palabra en realidad lo que puedo hacer es aprovechar ese modelo de traducci\u00f3n de palabra a palabra y decir bueno me armo una traducci\u00f3n entre un par de frases bas\u00e1ndome en las traducciones palabra a palabra son como dos formas distintas de construirlo y a veces hasta complementarias bien eso fue el modelo de frases los modelos de frases son los m\u00e1s usados hoy en d\u00eda en realidad en lo que es la traducci\u00f3n autom\u00e1tica son los que han dado mejores resultados y bueno y no faltaba una cosa para terminar toda la imagen de lo que es la traducci\u00f3n autom\u00e1tica estad\u00edstica que es la decodificaci\u00f3n entonces vamos un resumen de lo que ten\u00edamos hasta ahora hasta ahora yo part\u00ed de yo quer\u00eda resolver la cocci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica estad\u00edstica y yo ten\u00eda un corpus para el hilo que ten\u00eda texto en el idioma origen y el idioma destino y a partir de haciendo an\u00e1lisis estad\u00edstico yo me constru\u00ed un modelo de traducci\u00f3n que lo que vimos en esta clase adem\u00e1s yo ten\u00eda cierta cantidad de texto en el idioma destino y a partir de cierto an\u00e1lisis estad\u00edstico me constru\u00ed un modelo de lenguaje que me dice que tan fluido es una oraci\u00f3n en el lenguaje destino entonces ahora lo que me falta recuerden que yo lo que ten\u00eda que hacer era iterar sobre todas las oraciones el lenguaje destino y pasar las atraves del modelo de traducci\u00f3n y del modelo de lenguaje para que me d\u00e9 la probabilidad de esa oraci\u00f3n bueno lo que me falta es el algoritmo de codificaci\u00f3n que en vez de probar con toda la oraci\u00f3n del lenguaje destino me va a decir unas cuantas oraciones para probar capaz que me dice 150 oraciones para probar sobre las cuales utilizar el modelo de traducci\u00f3n y el modelo de lenguaje entonces esto es como un diagrama de de m\u00f3dulos en los cuales el algoritmo de codificaci\u00f3n utiliza los dos m\u00f3dulos tanto el de traducci\u00f3n como el de lenguaje bueno c\u00f3mo funciona el algoritmo de codificaci\u00f3n y que vamos a ver es un algoritmo de codificaci\u00f3n de tipo beam search y bueno funciona de as\u00ed de manera yo tengo la oraci\u00f3n Mar\u00eda no dio una ofetada a la bruja verde y la quiero traducir al ingl\u00e9s y tengo una tabla de traducci\u00f3n de frases entonces mi oraci\u00f3n Mar\u00eda no dio una ofetada a la bruja verde yo busco en la tabla de frases cuales de esas digamos cuales segmentos cuales sus segmentos de esa oraci\u00f3n yo puedo encontrar en la tabla de traducci\u00f3n de frases entonces voy a encontrar por ejemplo que Mar\u00eda lo puedo traducir como Mary no lo busco en la tabla y lo puedo traducir como Not como Did Not o como No dio lo puedo traducir como Guid pero adem\u00e1s no dio esa frase entera yo lo busco en la tabla y me aparece que lo puedo traducir como Did Not Guid dio una ofetada a toda esa frase lo puedo traducir como Slap una ofetada lo puedo decir como a Slap y bueno otras cosas bruja lo puedo decir como Witch verde como Green pero adem\u00e1s en alg\u00fan lado de la tabla tengo que bruja verde lo puedo traducir como Green Witch y as\u00ed digamos yo puedo encontrar tengo diferentes maneras de segmentar la oraci\u00f3n y adem\u00e1s para cada uno de esos segmentos pueden encontrar distintas formas de traducirlo en el lenguaje destino con mi tabla de frases entonces el algoritmo de codificaci\u00f3n funciona de la siguiente manera empezamos teniendo en cada paso de la algoritmo vamos a tener un conjunto de hip\u00f3tesis de traducci\u00f3n se llega a ver ah\u00ed lo que dice de ojos m\u00e1s o menos ac\u00e1 quedaron mal los correditos bueno en cada uno de los pasos yo voy a tener un conjunto de hip\u00f3tesis de traducci\u00f3n al principio el algoritmo voy a empezar con una hip\u00f3tesis vac\u00eda como se le potecis dice que lo importante de leer es la parte de la F que tiene un mont\u00f3n de guiones significa que no hay ninguna palabra del espa\u00f1ol cubierta esas son todas las nueve creo nueve palabras en espa\u00f1ol ninguna esta cubierta y esta hip\u00f3tesis tiene probabilidad uno entonces en cada paso del algoritmo lo que voy a hacer es elegir un par de frases tal que una traducci\u00f3n de la otra y voy a crear una hip\u00f3tesis nueva a partir de una que ya tengo entonces en este paso lo que hice fue decir el hijo el par de frases Mar\u00eda Mary y ah\u00ed me creo una nueva hip\u00f3tesis que cubre la primera palabra por eso parece una serie con este caso elige la frase en ingl\u00e9s Mary y ahora tiene una probabilidad de 0 punto 534 ese n\u00famero de esa probabilidad va a servir para guiar un poco en el algoritmo pero vamos a ver despu\u00e9s como es que se calcula por ahora que se es solamente con el n\u00famero bien pero entonces yo ten\u00eda otra opci\u00f3n en realidad yo pod\u00eda haber elegido empezar en vez de traducir Mar\u00eda por Mary pod\u00eda haber elegido empezar por traducir brujo por witch y ah\u00ed me crear\u00eda otra hip\u00f3tesis de traducci\u00f3n donde cubro la pen\u00faltima de las de las palabras en espa\u00f1ol agarr\u00f3 la palabra witch del hijo de la palabra witch y tiene una probabilidad de 0 punto 182 entonces en cada paso del algoritmo lo que hace es elegir una hip\u00f3tesis que tiene elegir un par de frases y expandir as\u00ed que lo siguiente que puedo hacer es elegir la frase did not expandirla a partir de la hip\u00f3tesis que ten\u00eda con Mary y bueno eso me cubre ahora dos palabras en espa\u00f1ol y me tiene medio otra probabilidad y despu\u00e9s sigo avanzando y sigo avanzando hasta que llevo a cubrir en alg\u00fan momento si yo sigo avanzando y sigo arregando hip\u00f3tesis en alg\u00fan momento voy a llegar a cubrir todas las palabras del idioma espa\u00f1ol todas las palabras de la abrasi\u00f3n en espa\u00f1ol entonces ah\u00ed una vez que yo cubri todas las palabras digo bueno esto es una hip\u00f3tesis completa y esto lo devuelvo como una potencial candidata digamos una oraci\u00f3n candidata a traducci\u00f3n pero claro a medida que yo fui avanzando una cosa que pas\u00f3 es que fui dejando hip\u00f3tesis colgadas y esas hip\u00f3tesis podr\u00edan tener otras traducciones posibles yo ac\u00e1 lo que devol\u00ed era una posible traducci\u00f3n pero a medida que yo ten\u00eda las otras hip\u00f3tesis si yo hubiera seguido por las otras hip\u00f3tesis hubiera podido devolver otras cosas entonces yo necesito hacer un backtracking para poder devolver todas las posibilidades poder volver a ver las hip\u00f3tesis a revisitar las hip\u00f3tesis y cabilladas y volver a explorar los otros caminos entonces necesitar\u00eda ser un backtracking para recorrerlas todas y si hago un backtracking lo que va a pasar es que voy a ocurrir una explosi\u00f3n de exponencial del espacio de b\u00fasqueda porque en realidad todas las posibilidades que se abren son exponenciales y ah\u00ed esto como que se vuelve bastante lento entonces yo quer\u00eda un decodificador para volver este problema un problema tratable en vez de agarrar las infinitas oraciones del idioma me quedo con algunas que sean m\u00e1s probables con este acorimo de codificaci\u00f3n logr\u00e9 reducir de infinito a algo finito pero aun as\u00ed es demasiado lento porque hay una explosi\u00f3n combina explosi\u00f3n combinatoria digamos de hip\u00f3tesis y me queda una cantidad exponencial de hip\u00f3tesis entonces como es tan grande este problema digamos como la cantidad de hip\u00f3tesis es ponencial y este es un problema en el completo entonces se utilizan t\u00e9cnicas para reducir el espacio de b\u00fasqueda y hay como dos tipos de t\u00e9cnicas algunas son con riesgo y otras son sin riesgo las t\u00e9cnicas sin riesgo lo que quiere decir es que si yo aplico una t\u00e9cnica de reducci\u00f3n de hip\u00f3tesis sin riesgo la soluci\u00f3n ideal que yo ten\u00eda dentro de mi b\u00fasqueda no la voy a perder utilizando una t\u00e9cnica sin riesgo en cambio en la con riesgo si yo podr\u00eda llegar a perder la soluci\u00f3n \u00f3ptima bien entonces la t\u00e9cnica sin riesgo que conocemos es la de recombinaci\u00f3n de hip\u00f3tesis que dice que si yo tengo dos hip\u00f3tesis voy avanzando por dos caminos dentro del acorimo y llevo a dos hip\u00f3tesis iguales por lo menos dos hip\u00f3tesis que cubren las mismas palabras entonces me pudo quedar con la que tiene mayor probabilidad de las dos y descartar la otra porque porque a medida que yo voy a seguir avanzando en el acorimo lo que va a pasar es que van a bajar las probabilidades digamos eligiendo m\u00e1s palabras y eligiendo m\u00e1s frases me va a bajar la probabilidad y nunca me va a pasar que una de las hip\u00f3tesis que ten\u00eda menos probabilidad vaya a subir en realidad siempre va a tener menos entonces en definitiva yo puedo con seguridad descartar la que tiene menos probabilidad bueno esa es recombinaci\u00f3n de hip\u00f3tesis pero ni siquiera con eso alcanza digamos para la reducci\u00f3n del espacio de b\u00fasqueda lo suficiente a\u00fan queda much\u00edsimas hip\u00f3tesis entonces se suele utilizar t\u00e9cnicas de podado con riesgo la t\u00e9cnica de histograma la t\u00e9cnica de lumbral el histograma significa que a cada paso digamos en cada paso del acorimo yo me quedo con los n las n hip\u00f3tesis de traducci\u00f3n m\u00e1s probable y descarto las otras y la t\u00e9cnica con humbral dice que a cada paso del acorimo me quedo con la hip\u00f3tesis de mayor probabilidad y las que est\u00e9n a una distancia alpha m\u00e1ximo de esa cu\u00e1l es el riesgo de las t\u00e9cnicas de podado que si la mejor traducci\u00f3n y la traducci\u00f3n \u00f3ptima ten\u00eda algunas frases muy poco probables al principio entonces probablemente yo descarte esa soluci\u00f3n de en los primeros pasos y no llegan a contar la soluci\u00f3n \u00f3ptima digamos la perd\u00ed por el hecho de arpodado sin embargo bueno tiene como ventaja que en realidad reduce much\u00edsimo el espacio de b\u00fasqueda y vuelve este problema un problema tratable bueno y ahora s\u00ed qu\u00e9 significaba esa probabilidad que estaba viendo en cada una de las hip\u00f3tesis o sea el podado necesita tener las mejores hip\u00f3tesis y bueno para la recomendaci\u00f3n tambi\u00e9n necesito saber la probabilidad de la hip\u00f3tesis bueno la forma de calcular la probabilidad de la hip\u00f3tesis se divide en dos digamos tengo lo que encontr\u00e9 hasta el momento la hip\u00f3tesis lleva cubierta a cierta cantidad de palabras entonces para esa cantidad palabra que ya llevo cubiertas utilizo los tres modelos el modelo de traducci\u00f3n el modelo de ordenamiento del modelo de lenguaje utilizo los tres modelos para calcular la probabilidad de la frase hasta el momento pero para lo que me falta traducir yo no puedo utilizar todo porque no tengo toda la informaci\u00f3n de traducci\u00f3n entonces lo que hago es utilizar solamente el modelo de traducci\u00f3n y el modelo de lenguaje descarto el modelo de reordenamiento y bueno entonces hago calcula una probabilidad que es una parte con todos los tres modelos y otra parte s\u00ednimo del modelo de reordenamiento bien este algoritmo que acabamos de describir que hace esta b\u00fasqueda bas\u00e1ndose hip\u00f3tesis que utiliza recomendaci\u00f3n hip\u00f3tesis y bueno el calcula de las probabilidades de esta manera se conoce como algoritmo b\u00fasqueda esterisco es un algoritmo de vincers que se usa much\u00edsimo en lo que es traducci\u00f3n autom\u00e1tica estad\u00edstica por ejemplo el sistema Moses ac\u00e1 tenemos este ejemplo de herramientas open source o gratuita que sirven para construcci\u00f3n de traducciones autom\u00e1ticos el sistema Moses es un sistema open source para desarrollar este tipo de traducciones autom\u00e1ticos estad\u00edsticos y implementa este algoritmo de codificaci\u00f3n b\u00fasqueda a esterisco y bueno lo que tiene el sistema Moses de bueno es que en realidad lo que hace adem\u00e1s de implementar el codificadores utiliza a los otros sistemas y los integra de alguna manera entonces integra este otro sistema el ircdlm que es una herramienta para crear modelos de lenguaje basados en en enegramas y el otro sistema se guiza m\u00e1s m\u00e1s que lo vi\u00f3 mencionado hoy que es el sistema que me permite alinear corpus de variaciones en los distintos idiomas llegando los modelos del 1 al 5 de traducci\u00f3n de IBM bueno entonces estas tres herramientas sirven si uno quiere construir un traducador autom\u00e1tico estad\u00edstico entre cualquier par de diomas puede utilizar estas tres herramientas y teniendo un corpus para el hilo y un corpus monolingue puede construirse un traducador pero bueno adem\u00e1s otra cosa que mencionamos en la clase pasada pero eran los sistemas basados en reglas los sistemas basados en reglas han ca\u00eddo un poco este digamos no tienen tanta popularidad como antes sin embargo algunos es inusando y el sistema apertym es un sistema open source para construir sistema de traducci\u00f3n basados en reglas que tienen con un mont\u00f3n de pares de lenguajes y bueno ya anda relativamente bien digamos entonces sigue desarrollando hasta hoy entonces es una alternativa open source que est\u00e1 basado en reglas en vez de estar basado en estad\u00edsticas y bueno esto es un resumen de lo que vimos as\u00ed que dejamos por ac\u00e1", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 23.56, "text": " Una vez que eleg\u00ed en mi, con el paso 1, eleg\u00ed cu\u00e1ntas palabras en espa\u00f1ol e usar,", "tokens": [50364, 15491, 5715, 631, 14459, 870, 465, 2752, 11, 416, 806, 29212, 502, 11, 14459, 870, 44256, 296, 35240, 465, 31177, 308, 14745, 11, 51542], "temperature": 0.0, "avg_logprob": -0.2719460450685941, "compression_ratio": 1.375, "no_speech_prob": 0.10625599324703217}, {"id": 1, "seek": 0, "start": 23.56, "end": 27.76, "text": " en el paso 2 lo que voy a elegir es una lineaci\u00f3n, una funci\u00f3n de lineaci\u00f3n que me dice", "tokens": [51542, 465, 806, 29212, 568, 450, 631, 7552, 257, 14459, 347, 785, 2002, 1622, 3482, 11, 2002, 43735, 368, 1622, 3482, 631, 385, 10313, 51752], "temperature": 0.0, "avg_logprob": -0.2719460450685941, "compression_ratio": 1.375, "no_speech_prob": 0.10625599324703217}, {"id": 2, "seek": 2776, "start": 27.76, "end": 31.6, "text": " cada palabra con cu\u00e1l se va a corresponder, cada palabra al lado del espa\u00f1ol, con qu\u00e9 palabra", "tokens": [50364, 8411, 31702, 416, 44318, 369, 2773, 257, 6805, 260, 11, 8411, 31702, 419, 11631, 1103, 31177, 11, 416, 8057, 31702, 50556], "temperature": 0.0, "avg_logprob": -0.23892373305100661, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.13770198822021484}, {"id": 3, "seek": 2776, "start": 31.6, "end": 37.36, "text": " en ingl\u00e9s se va a corresponder. Este modelo asume de manera muy na\u00ef que todas las", "tokens": [50556, 465, 49766, 369, 2773, 257, 6805, 260, 13, 16105, 27825, 382, 2540, 368, 13913, 5323, 1667, 15487, 631, 10906, 2439, 50844], "temperature": 0.0, "avg_logprob": -0.23892373305100661, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.13770198822021484}, {"id": 4, "seek": 2776, "start": 37.36, "end": 44.64, "text": " lineaciones que yo puedo tener son equiprobables, o sea, asume que yo voy a tener un conjunto", "tokens": [50844, 1622, 9188, 631, 5290, 21612, 11640, 1872, 5037, 16614, 2965, 11, 277, 4158, 11, 382, 2540, 631, 5290, 7552, 257, 11640, 517, 37776, 51208], "temperature": 0.0, "avg_logprob": -0.23892373305100661, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.13770198822021484}, {"id": 5, "seek": 2776, "start": 44.64, "end": 49.68000000000001, "text": " de lineaciones posibles y todas van a tener la misma probabilidad. Bien, entonces, la probabilidad", "tokens": [51208, 368, 1622, 9188, 1366, 14428, 288, 10906, 3161, 257, 11640, 635, 24946, 31959, 4580, 13, 16956, 11, 13003, 11, 635, 31959, 4580, 51460], "temperature": 0.0, "avg_logprob": -0.23892373305100661, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.13770198822021484}, {"id": 6, "seek": 2776, "start": 49.68000000000001, "end": 55.120000000000005, "text": " de elegir una lineaci\u00f3n en particular, si yo tengo un mont\u00f3n de lineaciones, digamos,", "tokens": [51460, 368, 14459, 347, 2002, 1622, 3482, 465, 1729, 11, 1511, 5290, 13989, 517, 45259, 368, 1622, 9188, 11, 36430, 11, 51732], "temperature": 0.0, "avg_logprob": -0.23892373305100661, "compression_ratio": 1.793774319066148, "no_speech_prob": 0.13770198822021484}, {"id": 7, "seek": 5512, "start": 55.12, "end": 60.14, "text": " la probabilidad de elegir una lineaci\u00f3n en particular va a ser uno sobre la cantidad de", "tokens": [50364, 635, 31959, 4580, 368, 14459, 347, 2002, 1622, 3482, 465, 1729, 2773, 257, 816, 8526, 5473, 635, 33757, 368, 50615], "temperature": 0.0, "avg_logprob": -0.2536815257554644, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0023059905506670475}, {"id": 8, "seek": 5512, "start": 60.14, "end": 65.0, "text": " lineaciones que tengo, porque en realidad todas van a ser equiprobables. Bien, entonces,", "tokens": [50615, 1622, 9188, 631, 13989, 11, 4021, 465, 25635, 10906, 3161, 257, 816, 5037, 16614, 2965, 13, 16956, 11, 13003, 11, 50858], "temperature": 0.0, "avg_logprob": -0.2536815257554644, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0023059905506670475}, {"id": 9, "seek": 5512, "start": 65.0, "end": 69.58, "text": " \u00bfcu\u00e1ntas lineaciones puedo tener entre dos oraciones, una oraci\u00f3n en ingl\u00e9s que tiene", "tokens": [50858, 3841, 12032, 27525, 296, 1622, 9188, 21612, 11640, 3962, 4491, 420, 9188, 11, 2002, 420, 3482, 465, 49766, 631, 7066, 51087], "temperature": 0.0, "avg_logprob": -0.2536815257554644, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0023059905506670475}, {"id": 10, "seek": 5512, "start": 69.58, "end": 73.22, "text": " largo y una oraci\u00f3n en espa\u00f1ol que tiene largo jota? \u00bfC\u00f3mo puedo calcular cu\u00e1ntas", "tokens": [51087, 31245, 288, 2002, 420, 3482, 465, 31177, 631, 7066, 31245, 361, 5377, 30, 3841, 28342, 21612, 2104, 17792, 44256, 296, 51269], "temperature": 0.0, "avg_logprob": -0.2536815257554644, "compression_ratio": 1.7184466019417475, "no_speech_prob": 0.0023059905506670475}, {"id": 11, "seek": 7322, "start": 73.22, "end": 86.62, "text": " lineaciones existen? M\u00e1s o menos, s\u00ed, casi de la jota. Recuerden que el lado de ingl\u00e9s", "tokens": [50364, 1622, 9188, 2514, 268, 30, 376, 2490, 277, 8902, 11, 8600, 11, 22567, 368, 635, 361, 5377, 13, 9647, 5486, 1556, 631, 806, 11631, 368, 49766, 51034], "temperature": 0.0, "avg_logprob": -0.3008889285000888, "compression_ratio": 1.2657342657342658, "no_speech_prob": 0.15522927045822144}, {"id": 12, "seek": 7322, "start": 86.62, "end": 100.62, "text": " yo ten\u00eda ciertas palabras, en ingl\u00e9s ten\u00eda la palabra E1, E2 hasta E sub\u00ed y en espa\u00f1ol", "tokens": [51034, 5290, 23718, 49252, 296, 35240, 11, 465, 49766, 23718, 635, 31702, 462, 16, 11, 462, 17, 10764, 462, 1422, 870, 288, 465, 31177, 51734], "temperature": 0.0, "avg_logprob": -0.3008889285000888, "compression_ratio": 1.2657342657342658, "no_speech_prob": 0.15522927045822144}, {"id": 13, "seek": 10062, "start": 100.62, "end": 109.02000000000001, "text": " ten\u00eda las palabras E1, E2 hasta E subjota. Entonces, yo pod\u00eda trazar l\u00edneas para", "tokens": [50364, 23718, 2439, 35240, 462, 16, 11, 462, 17, 10764, 462, 1422, 73, 5377, 13, 15097, 11, 5290, 45588, 944, 26236, 16118, 716, 296, 1690, 50784], "temperature": 0.0, "avg_logprob": -0.25306764582997743, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.020647279918193817}, {"id": 14, "seek": 10062, "start": 109.02000000000001, "end": 115.94, "text": " alinear, pero adem\u00e1s, en ingl\u00e9s yo siempre considerado que ten\u00eda un token null, entonces", "tokens": [50784, 419, 533, 289, 11, 4768, 21251, 11, 465, 49766, 5290, 12758, 1949, 1573, 631, 23718, 517, 14862, 18184, 11, 13003, 51130], "temperature": 0.0, "avg_logprob": -0.25306764582997743, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.020647279918193817}, {"id": 15, "seek": 10062, "start": 115.94, "end": 121.78, "text": " todas las palabras que no estaban alineadas del lado del espa\u00f1ol y van a parar ah\u00ed. As\u00ed", "tokens": [51130, 10906, 2439, 35240, 631, 572, 36713, 419, 533, 6872, 1103, 11631, 1103, 31177, 288, 3161, 257, 37193, 12571, 13, 17419, 51422], "temperature": 0.0, "avg_logprob": -0.25306764582997743, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.020647279918193817}, {"id": 16, "seek": 10062, "start": 121.78, "end": 126.4, "text": " que en ingl\u00e9s en realidad no tengo y posibilidades, tengo una m\u00e1s, tengo y m\u00e1s uno. Entonces,", "tokens": [51422, 631, 465, 49766, 465, 25635, 572, 13989, 288, 1366, 11607, 10284, 11, 13989, 2002, 3573, 11, 13989, 288, 3573, 8526, 13, 15097, 11, 51653], "temperature": 0.0, "avg_logprob": -0.25306764582997743, "compression_ratio": 1.6205357142857142, "no_speech_prob": 0.020647279918193817}, {"id": 17, "seek": 12640, "start": 126.4, "end": 130.6, "text": " \u00bfcu\u00e1ntas formas tengo yo de mapear estas jota posibilidades en espa\u00f1ol con las E de", "tokens": [50364, 3841, 12032, 27525, 296, 33463, 13989, 5290, 368, 463, 494, 289, 13897, 361, 5377, 1366, 11607, 10284, 465, 31177, 416, 2439, 462, 368, 50574], "temperature": 0.0, "avg_logprob": -0.27886686220273865, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.13242444396018982}, {"id": 18, "seek": 12640, "start": 130.6, "end": 136.52, "text": " ingl\u00e9s? Exacto, y m\u00e1s uno de la jota, porque yo tengo y m\u00e1s uno opciones para la primera", "tokens": [50574, 49766, 30, 7199, 78, 11, 288, 3573, 8526, 368, 635, 361, 5377, 11, 4021, 5290, 13989, 288, 3573, 8526, 999, 23469, 1690, 635, 17382, 50870], "temperature": 0.0, "avg_logprob": -0.27886686220273865, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.13242444396018982}, {"id": 19, "seek": 12640, "start": 136.52, "end": 142.12, "text": " y m\u00e1s uno opciones para la segunda, etc\u00e9tera, hasta que yo al final. As\u00ed que son y m\u00e1s uno", "tokens": [50870, 288, 3573, 8526, 999, 23469, 1690, 635, 21978, 11, 5183, 526, 23833, 11, 10764, 631, 5290, 419, 2572, 13, 17419, 631, 1872, 288, 3573, 8526, 51150], "temperature": 0.0, "avg_logprob": -0.27886686220273865, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.13242444396018982}, {"id": 20, "seek": 12640, "start": 142.12, "end": 148.12, "text": " a las jota lineaciones posibles.", "tokens": [51150, 257, 2439, 361, 5377, 1622, 9188, 1366, 14428, 13, 51450], "temperature": 0.0, "avg_logprob": -0.27886686220273865, "compression_ratio": 1.577319587628866, "no_speech_prob": 0.13242444396018982}, {"id": 21, "seek": 14812, "start": 148.12, "end": 163.4, "text": " Ojo, el null es como una pizadita que he oye para alinear cosas que no tienen un correspondiente,", "tokens": [50364, 422, 5134, 11, 806, 18184, 785, 2617, 2002, 280, 590, 345, 2786, 631, 415, 277, 1200, 1690, 419, 533, 289, 12218, 631, 572, 12536, 517, 6805, 8413, 11, 51128], "temperature": 0.0, "avg_logprob": -0.5140220531518909, "compression_ratio": 1.3832335329341316, "no_speech_prob": 0.17435897886753082}, {"id": 22, "seek": 14812, "start": 163.4, "end": 166.88, "text": " o sea, yo ten\u00eda una palabra en espa\u00f1ol que...", "tokens": [51128, 277, 4158, 11, 5290, 23718, 2002, 31702, 465, 31177, 631, 485, 51302], "temperature": 0.0, "avg_logprob": -0.5140220531518909, "compression_ratio": 1.3832335329341316, "no_speech_prob": 0.17435897886753082}, {"id": 23, "seek": 14812, "start": 166.88, "end": 175.8, "text": " Estar varias de las CF buenas alineadas de ese null, no importa en qu\u00e9 orden est\u00e1n.", "tokens": [51302, 4410, 289, 37496, 368, 2439, 383, 37, 43852, 419, 533, 6872, 368, 10167, 18184, 11, 572, 33218, 465, 8057, 28615, 10368, 13, 51748], "temperature": 0.0, "avg_logprob": -0.5140220531518909, "compression_ratio": 1.3832335329341316, "no_speech_prob": 0.17435897886753082}, {"id": 24, "seek": 17580, "start": 176.8, "end": 185.52, "text": " Bien, entonces eran y m\u00e1s uno a las jota posibles alineaciones, por lo tanto. La probabilidad", "tokens": [50414, 16956, 11, 13003, 32762, 288, 3573, 8526, 257, 2439, 361, 5377, 1366, 14428, 419, 533, 9188, 11, 1515, 450, 10331, 13, 2369, 31959, 4580, 50850], "temperature": 0.0, "avg_logprob": -0.20361991240599445, "compression_ratio": 1.9947916666666667, "no_speech_prob": 0.05162585526704788}, {"id": 25, "seek": 17580, "start": 185.52, "end": 191.76000000000002, "text": " de elegir una alineaci\u00f3n A, dada la elaboraci\u00f3n en ingl\u00e9s, la probabilidad de elegir una alineaci\u00f3n", "tokens": [50850, 368, 14459, 347, 2002, 419, 533, 3482, 316, 11, 274, 1538, 635, 16298, 3482, 465, 49766, 11, 635, 31959, 4580, 368, 14459, 347, 2002, 419, 533, 3482, 51162], "temperature": 0.0, "avg_logprob": -0.20361991240599445, "compression_ratio": 1.9947916666666667, "no_speech_prob": 0.05162585526704788}, {"id": 26, "seek": 17580, "start": 191.76000000000002, "end": 198.38000000000002, "text": " cualquiera, dada la oraci\u00f3n en ingl\u00e9s, va a ser el producto de la probabilidad de haber", "tokens": [51162, 10911, 35134, 11, 274, 1538, 635, 420, 3482, 465, 49766, 11, 2773, 257, 816, 806, 47583, 368, 635, 31959, 4580, 368, 15811, 51493], "temperature": 0.0, "avg_logprob": -0.20361991240599445, "compression_ratio": 1.9947916666666667, "no_speech_prob": 0.05162585526704788}, {"id": 27, "seek": 17580, "start": 198.38000000000002, "end": 204.32000000000002, "text": " sorteado un valor jota primero, que era Epsilon, por la probabilidad de elegir una alineaci\u00f3n", "tokens": [51493, 25559, 1573, 517, 15367, 361, 5377, 21289, 11, 631, 4249, 462, 16592, 11, 1515, 635, 31959, 4580, 368, 14459, 347, 2002, 419, 533, 3482, 51790], "temperature": 0.0, "avg_logprob": -0.20361991240599445, "compression_ratio": 1.9947916666666667, "no_speech_prob": 0.05162585526704788}, {"id": 28, "seek": 20432, "start": 204.35999999999999, "end": 210.32, "text": " cualquiera para ese jota, que es uno sobre y m\u00e1s uno a la jota.", "tokens": [50366, 10911, 35134, 1690, 10167, 361, 5377, 11, 631, 785, 8526, 5473, 288, 3573, 8526, 257, 635, 361, 5377, 13, 50664], "temperature": 0.0, "avg_logprob": -0.21207108036164316, "compression_ratio": 1.6758241758241759, "no_speech_prob": 0.0030770492739975452}, {"id": 29, "seek": 20432, "start": 210.32, "end": 215.32, "text": " Bien, entonces esto lo resubimos como Epsilon sobre y m\u00e1s uno a la jota.", "tokens": [50664, 16956, 11, 13003, 7433, 450, 725, 836, 8372, 2617, 462, 16592, 5473, 288, 3573, 8526, 257, 635, 361, 5377, 13, 50914], "temperature": 0.0, "avg_logprob": -0.21207108036164316, "compression_ratio": 1.6758241758241759, "no_speech_prob": 0.0030770492739975452}, {"id": 30, "seek": 20432, "start": 220.32, "end": 226.32, "text": " Epsilon sobre y m\u00e1s uno a la jota es la probabilidad de dada una oraci\u00f3n en ingl\u00e9s,", "tokens": [51164, 462, 16592, 5473, 288, 3573, 8526, 257, 635, 361, 5377, 785, 635, 31959, 4580, 368, 274, 1538, 2002, 420, 3482, 465, 49766, 11, 51464], "temperature": 0.0, "avg_logprob": -0.21207108036164316, "compression_ratio": 1.6758241758241759, "no_speech_prob": 0.0030770492739975452}, {"id": 31, "seek": 20432, "start": 226.32, "end": 232.32, "text": " elegir cierta alineaci\u00f3n que yo voy a utilizar. Bien, ese fue el segundo paso.", "tokens": [51464, 14459, 347, 39769, 1328, 419, 533, 3482, 631, 5290, 7552, 257, 24060, 13, 16956, 11, 10167, 9248, 806, 17954, 29212, 13, 51764], "temperature": 0.0, "avg_logprob": -0.21207108036164316, "compression_ratio": 1.6758241758241759, "no_speech_prob": 0.0030770492739975452}, {"id": 32, "seek": 23232, "start": 232.32, "end": 238.32, "text": " El tercer paso es, una vez que ya tengo la alineaci\u00f3n, voy mirando cada palabra de lado", "tokens": [50364, 2699, 38103, 29212, 785, 11, 2002, 5715, 631, 2478, 13989, 635, 419, 533, 3482, 11, 7552, 3149, 1806, 8411, 31702, 368, 11631, 50664], "temperature": 0.0, "avg_logprob": -0.16763877868652344, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.0028555055614560843}, {"id": 33, "seek": 23232, "start": 238.32, "end": 241.32, "text": " en ingl\u00e9s y le voy poniendo una palabra correspondiente de lado espa\u00f1ol.", "tokens": [50664, 465, 49766, 288, 476, 7552, 9224, 7304, 2002, 31702, 6805, 8413, 368, 11631, 31177, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16763877868652344, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.0028555055614560843}, {"id": 34, "seek": 23232, "start": 243.32, "end": 247.32, "text": " Para ac\u00e1 voy a sumir que yo tengo una tabla de traducci\u00f3n, una tabla de traducci\u00f3n que me dice", "tokens": [50914, 11107, 23496, 7552, 257, 2408, 347, 631, 5290, 13989, 2002, 4421, 875, 368, 2479, 1311, 5687, 11, 2002, 4421, 875, 368, 2479, 1311, 5687, 631, 385, 10313, 51114], "temperature": 0.0, "avg_logprob": -0.16763877868652344, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.0028555055614560843}, {"id": 35, "seek": 23232, "start": 247.32, "end": 251.32, "text": " que tiene de un lado todas las palabras en espa\u00f1ol y de otro lado todas las palabras en ingl\u00e9s,", "tokens": [51114, 631, 7066, 368, 517, 11631, 10906, 2439, 35240, 465, 31177, 288, 368, 11921, 11631, 10906, 2439, 35240, 465, 49766, 11, 51314], "temperature": 0.0, "avg_logprob": -0.16763877868652344, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.0028555055614560843}, {"id": 36, "seek": 23232, "start": 251.32, "end": 258.32, "text": " entonces mi tabla va a tener una forma como, por ejemplo, hace una tabla as\u00ed,", "tokens": [51314, 13003, 2752, 4421, 875, 2773, 257, 11640, 2002, 8366, 2617, 11, 1515, 13358, 11, 10032, 2002, 4421, 875, 8582, 11, 51664], "temperature": 0.0, "avg_logprob": -0.16763877868652344, "compression_ratio": 1.8326359832635983, "no_speech_prob": 0.0028555055614560843}, {"id": 37, "seek": 25832, "start": 258.32, "end": 267.32, "text": " de lado decir las palabras en espa\u00f1ol como banco, perro, gato y m\u00e1s cosas y de otro lado", "tokens": [50364, 368, 11631, 10235, 2439, 35240, 465, 31177, 2617, 45498, 11, 680, 340, 11, 290, 2513, 288, 3573, 12218, 288, 368, 11921, 11631, 50814], "temperature": 0.0, "avg_logprob": -0.2293068055183657, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.03557644784450531}, {"id": 38, "seek": 25832, "start": 267.32, "end": 274.32, "text": " va a tener las correspondientes en ingl\u00e9s como bank, bench, cat, tree y m\u00e1s cosas.", "tokens": [50814, 2773, 257, 11640, 2439, 6805, 20135, 465, 49766, 2617, 3765, 11, 10638, 11, 3857, 11, 4230, 288, 3573, 12218, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2293068055183657, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.03557644784450531}, {"id": 39, "seek": 25832, "start": 276.32, "end": 279.32, "text": " Y entonces esta tabla va a decir la probabilidad de traducir una cosa en la jota,", "tokens": [51264, 398, 13003, 5283, 4421, 875, 2773, 257, 10235, 635, 31959, 4580, 368, 2479, 1311, 347, 2002, 10163, 465, 635, 361, 5377, 11, 51414], "temperature": 0.0, "avg_logprob": -0.2293068055183657, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.03557644784450531}, {"id": 40, "seek": 25832, "start": 279.32, "end": 283.32, "text": " entonces banco probablemente tenga cierta probabilidad para bank y cierta probabilidad para bench,", "tokens": [51414, 13003, 45498, 21759, 4082, 36031, 39769, 1328, 31959, 4580, 1690, 3765, 288, 39769, 1328, 31959, 4580, 1690, 10638, 11, 51614], "temperature": 0.0, "avg_logprob": -0.2293068055183657, "compression_ratio": 1.7889447236180904, "no_speech_prob": 0.03557644784450531}, {"id": 41, "seek": 28332, "start": 283.32, "end": 294.32, "text": " 0.4 y 0.6, 0.06. Y para ac\u00e1 no va a tener ninguna probabilidad y para tree tampoco y despu\u00e9s perro", "tokens": [50364, 1958, 13, 19, 288, 1958, 13, 21, 11, 1958, 13, 12791, 13, 398, 1690, 23496, 572, 2773, 257, 11640, 36073, 31959, 4580, 288, 1690, 4230, 36838, 288, 15283, 680, 340, 50914], "temperature": 0.0, "avg_logprob": -0.18755149841308594, "compression_ratio": 1.5, "no_speech_prob": 0.013134926557540894}, {"id": 42, "seek": 28332, "start": 294.32, "end": 300.32, "text": " no va a tener nada de esto, pero s\u00ed despu\u00e9s y cat va a ser 0.8 en este caso, etc\u00e9tera.", "tokens": [50914, 572, 2773, 257, 11640, 8096, 368, 7433, 11, 4768, 8600, 15283, 288, 3857, 2773, 257, 816, 1958, 13, 23, 465, 4065, 9666, 11, 5183, 526, 23833, 13, 51214], "temperature": 0.0, "avg_logprob": -0.18755149841308594, "compression_ratio": 1.5, "no_speech_prob": 0.013134926557540894}, {"id": 43, "seek": 28332, "start": 300.32, "end": 305.32, "text": " Voy a tener una tabla bastante grande que tiene todas las posibilidades de traducir una palabra como otra.", "tokens": [51214, 25563, 257, 11640, 2002, 4421, 875, 14651, 8883, 631, 7066, 10906, 2439, 1366, 11607, 10284, 368, 2479, 1311, 347, 2002, 31702, 2617, 13623, 13, 51464], "temperature": 0.0, "avg_logprob": -0.18755149841308594, "compression_ratio": 1.5, "no_speech_prob": 0.013134926557540894}, {"id": 44, "seek": 30532, "start": 306.32, "end": 316.32, "text": " Entonces, si yo tengo esa tabla lo que puedo decir es que la forma de calcular la probabilidad", "tokens": [50414, 15097, 11, 1511, 5290, 13989, 11342, 4421, 875, 450, 631, 21612, 10235, 785, 631, 635, 8366, 368, 2104, 17792, 635, 31959, 4580, 50914], "temperature": 0.0, "avg_logprob": -0.1124694730028694, "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.012636635452508926}, {"id": 45, "seek": 30532, "start": 316.32, "end": 321.32, "text": " de esa oraci\u00f3n final que yo traduj\u00e9 va a depender de cu\u00e1les son las palabras que yo elija,", "tokens": [50914, 368, 11342, 420, 3482, 2572, 631, 5290, 2479, 4579, 526, 2773, 257, 1367, 3216, 368, 2702, 842, 904, 1872, 2439, 35240, 631, 5290, 806, 20642, 11, 51164], "temperature": 0.0, "avg_logprob": -0.1124694730028694, "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.012636635452508926}, {"id": 46, "seek": 30532, "start": 321.32, "end": 327.32, "text": " va a depender de cu\u00e1les son las palabras que yo haya puesto dentro de mi oraci\u00f3n para traducir.", "tokens": [51164, 2773, 257, 1367, 3216, 368, 2702, 842, 904, 1872, 2439, 35240, 631, 5290, 24693, 35136, 10856, 368, 2752, 420, 3482, 1690, 2479, 1311, 347, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1124694730028694, "compression_ratio": 1.7228915662650603, "no_speech_prob": 0.012636635452508926}, {"id": 47, "seek": 32732, "start": 328.32, "end": 336.32, "text": " Entonces, esa tabla que est\u00e1 ah\u00ed definida le llamamos ac\u00e1 en la slide aparece como tdf sux su y", "tokens": [50414, 15097, 11, 11342, 4421, 875, 631, 3192, 12571, 1561, 2887, 476, 16848, 2151, 23496, 465, 635, 4137, 37863, 2617, 256, 45953, 459, 87, 459, 288, 50814], "temperature": 0.0, "avg_logprob": -0.30470142594302996, "compression_ratio": 1.546875, "no_speech_prob": 0.03231346607208252}, {"id": 48, "seek": 32732, "start": 336.32, "end": 341.32, "text": " y dice que la probabilidad de traducir la palabra su y como f sux.", "tokens": [50814, 288, 10313, 631, 635, 31959, 4580, 368, 2479, 1311, 347, 635, 31702, 459, 288, 2617, 283, 459, 87, 13, 51064], "temperature": 0.0, "avg_logprob": -0.30470142594302996, "compression_ratio": 1.546875, "no_speech_prob": 0.03231346607208252}, {"id": 49, "seek": 32732, "start": 343.32, "end": 345.32, "text": " Entonces, saca de una cosa importante.", "tokens": [51164, 15097, 11, 4899, 64, 368, 2002, 10163, 9416, 13, 51264], "temperature": 0.0, "avg_logprob": -0.30470142594302996, "compression_ratio": 1.546875, "no_speech_prob": 0.03231346607208252}, {"id": 50, "seek": 32732, "start": 347.32, "end": 355.32, "text": " Si tenemos la oraci\u00f3n en ingl\u00e9s, la oraci\u00f3n en ingl\u00e9s recuerdan que ten\u00eda las palabras,", "tokens": [51364, 4909, 9914, 635, 420, 3482, 465, 49766, 11, 635, 420, 3482, 465, 49766, 39092, 10312, 631, 23718, 2439, 35240, 11, 51764], "temperature": 0.0, "avg_logprob": -0.30470142594302996, "compression_ratio": 1.546875, "no_speech_prob": 0.03231346607208252}, {"id": 51, "seek": 35532, "start": 355.32, "end": 365.32, "text": " es su 1, es su 2 hasta de su vn, la oraci\u00f3n en espa\u00f1ol ten\u00eda las palabras, es su 1, f su 1, f su 2 hasta f su j,", "tokens": [50364, 785, 459, 502, 11, 785, 459, 568, 10764, 368, 459, 371, 77, 11, 635, 420, 3482, 465, 31177, 23718, 2439, 35240, 11, 785, 459, 502, 11, 283, 459, 502, 11, 283, 459, 568, 10764, 283, 459, 361, 11, 50864], "temperature": 0.0, "avg_logprob": -0.2608322355482313, "compression_ratio": 1.622093023255814, "no_speech_prob": 0.007538800127804279}, {"id": 52, "seek": 35532, "start": 365.32, "end": 372.32, "text": " y yo ten\u00eda en el medio una funci\u00f3n de alineaci\u00f3n que me dec\u00eda qu\u00e9 palabras se correspond\u00eda con cu\u00e1l.", "tokens": [50864, 288, 5290, 23718, 465, 806, 22123, 2002, 43735, 368, 419, 533, 3482, 631, 385, 37599, 8057, 35240, 369, 6805, 2686, 416, 44318, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2608322355482313, "compression_ratio": 1.622093023255814, "no_speech_prob": 0.007538800127804279}, {"id": 53, "seek": 35532, "start": 374.32, "end": 383.32, "text": " Entonces, no era de su vn ni f su j, era su y y f su j.", "tokens": [51314, 15097, 11, 572, 4249, 368, 459, 371, 77, 3867, 283, 459, 361, 11, 4249, 459, 288, 288, 283, 459, 361, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2608322355482313, "compression_ratio": 1.622093023255814, "no_speech_prob": 0.007538800127804279}, {"id": 54, "seek": 38332, "start": 384.32, "end": 398.32, "text": " Entonces, si yo tengo una palabra cualquiera dentro de la oraci\u00f3n en espa\u00f1ol, tengo un f su j da chica,", "tokens": [50414, 15097, 11, 1511, 5290, 13989, 2002, 31702, 10911, 35134, 10856, 368, 635, 420, 3482, 465, 31177, 11, 13989, 517, 283, 459, 361, 1120, 417, 2262, 11, 51114], "temperature": 0.0, "avg_logprob": -0.25322871941786546, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.01250412780791521}, {"id": 55, "seek": 38332, "start": 398.32, "end": 405.32, "text": " dentro de la oraci\u00f3n en espa\u00f1ol, esto se va a corresponder con alg\u00fan f su y chica en la oraci\u00f3n en ingl\u00e9s,", "tokens": [51114, 10856, 368, 635, 420, 3482, 465, 31177, 11, 7433, 369, 2773, 257, 6805, 260, 416, 26300, 283, 459, 288, 417, 2262, 465, 635, 420, 3482, 465, 49766, 11, 51464], "temperature": 0.0, "avg_logprob": -0.25322871941786546, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.01250412780791521}, {"id": 56, "seek": 38332, "start": 405.32, "end": 406.32, "text": " digamos.", "tokens": [51464, 36430, 13, 51514], "temperature": 0.0, "avg_logprob": -0.25322871941786546, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.01250412780791521}, {"id": 57, "seek": 38332, "start": 406.32, "end": 412.32, "text": " Yo s\u00e9 que esto se cumple por la funci\u00f3n de alineaci\u00f3n, porque agarra y mapea todas las palabras que est\u00e1 en espa\u00f1ol con algo que est\u00e1 lado del ingl\u00e9s.", "tokens": [51514, 7616, 7910, 631, 7433, 369, 12713, 781, 1515, 635, 43735, 368, 419, 533, 3482, 11, 4021, 623, 289, 424, 288, 463, 494, 64, 10906, 2439, 35240, 631, 3192, 465, 31177, 416, 8655, 631, 3192, 11631, 1103, 49766, 13, 51814], "temperature": 0.0, "avg_logprob": -0.25322871941786546, "compression_ratio": 1.819905213270142, "no_speech_prob": 0.01250412780791521}, {"id": 58, "seek": 41232, "start": 413.32, "end": 415.32, "text": " Potencialmente con el doque en vac\u00edo null.", "tokens": [50414, 9145, 26567, 4082, 416, 806, 360, 1077, 465, 2842, 20492, 18184, 13, 50514], "temperature": 0.0, "avg_logprob": -0.28732224873134066, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.016110243275761604}, {"id": 59, "seek": 41232, "start": 417.32, "end": 423.32, "text": " Bien, entonces, tengo una palabra del lado del espa\u00f1ol que es f su j y una palabra del lado del ingl\u00e9s que es su v.", "tokens": [50614, 16956, 11, 13003, 11, 13989, 2002, 31702, 1103, 11631, 1103, 31177, 631, 785, 283, 459, 361, 288, 2002, 31702, 1103, 11631, 1103, 49766, 631, 785, 459, 371, 13, 50914], "temperature": 0.0, "avg_logprob": -0.28732224873134066, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.016110243275761604}, {"id": 60, "seek": 41232, "start": 423.32, "end": 428.32, "text": " \u00bfCu\u00e1l es la relaci\u00f3n entre ese j y su y? \u00bfC\u00f3mo se relaciona entre s\u00ed?", "tokens": [50914, 3841, 35222, 11447, 785, 635, 37247, 3962, 10167, 361, 288, 459, 288, 30, 3841, 28342, 369, 27189, 64, 3962, 8600, 30, 51164], "temperature": 0.0, "avg_logprob": -0.28732224873134066, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.016110243275761604}, {"id": 61, "seek": 41232, "start": 431.32, "end": 434.32, "text": " Yo puedo decir que el y es igual a algo de j,", "tokens": [51314, 7616, 21612, 10235, 631, 806, 288, 785, 10953, 257, 8655, 368, 361, 11, 51464], "temperature": 0.0, "avg_logprob": -0.28732224873134066, "compression_ratio": 1.5297297297297296, "no_speech_prob": 0.016110243275761604}, {"id": 62, "seek": 43432, "start": 434.32, "end": 440.32, "text": " de alguna manera.", "tokens": [50364, 368, 20651, 13913, 13, 50664], "temperature": 0.0, "avg_logprob": -0.23284663851298984, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.02762456052005291}, {"id": 63, "seek": 43432, "start": 443.32, "end": 448.32, "text": " La funci\u00f3n de alineaci\u00f3n, ah\u00ed est\u00e1, o sea, el y es igual a la funci\u00f3n de alineaci\u00f3n aplicada j,", "tokens": [50814, 2369, 43735, 368, 419, 533, 3482, 11, 12571, 3192, 11, 277, 4158, 11, 806, 288, 785, 10953, 257, 635, 43735, 368, 419, 533, 3482, 18221, 1538, 361, 11, 51064], "temperature": 0.0, "avg_logprob": -0.23284663851298984, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.02762456052005291}, {"id": 64, "seek": 43432, "start": 451.32, "end": 455.32, "text": " como la y, el \u00edndice de ac\u00e1 es igual a la funci\u00f3n de alineaci\u00f3n aplicada j,", "tokens": [51214, 2617, 635, 288, 11, 806, 18645, 273, 573, 368, 23496, 785, 10953, 257, 635, 43735, 368, 419, 533, 3482, 18221, 1538, 361, 11, 51414], "temperature": 0.0, "avg_logprob": -0.23284663851298984, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.02762456052005291}, {"id": 65, "seek": 45532, "start": 455.32, "end": 464.32, "text": " entonces yo puedo decir que la palabra es su y es igual a la palabra su a su j,", "tokens": [50364, 13003, 5290, 21612, 10235, 631, 635, 31702, 785, 459, 288, 785, 10953, 257, 635, 31702, 459, 257, 459, 361, 11, 50814], "temperature": 0.0, "avg_logprob": -0.22411288306826638, "compression_ratio": 1.7740384615384615, "no_speech_prob": 0.04591600224375725}, {"id": 66, "seek": 45532, "start": 464.32, "end": 468.32, "text": " as\u00ed que puedo decir que en realidad los que est\u00e1n alineados son la palabra,", "tokens": [50814, 8582, 631, 21612, 10235, 631, 465, 25635, 1750, 631, 10368, 419, 533, 4181, 1872, 635, 31702, 11, 51014], "temperature": 0.0, "avg_logprob": -0.22411288306826638, "compression_ratio": 1.7740384615384615, "no_speech_prob": 0.04591600224375725}, {"id": 67, "seek": 45532, "start": 468.32, "end": 472.32, "text": " es f su j est\u00e1 alineado con la palabra e su a su j,", "tokens": [51014, 785, 283, 459, 361, 3192, 419, 533, 1573, 416, 635, 31702, 308, 459, 257, 459, 361, 11, 51214], "temperature": 0.0, "avg_logprob": -0.22411288306826638, "compression_ratio": 1.7740384615384615, "no_speech_prob": 0.04591600224375725}, {"id": 68, "seek": 45532, "start": 472.32, "end": 475.32, "text": " y ah\u00ed me saqu\u00e9 el y de encima, digamos.", "tokens": [51214, 288, 12571, 385, 601, 16412, 806, 288, 368, 40265, 11, 36430, 13, 51364], "temperature": 0.0, "avg_logprob": -0.22411288306826638, "compression_ratio": 1.7740384615384615, "no_speech_prob": 0.04591600224375725}, {"id": 69, "seek": 45532, "start": 475.32, "end": 479.32, "text": " Simplemente, itero sobre las palabras y tir\u00e1ndose la j,", "tokens": [51364, 21532, 4082, 11, 309, 2032, 5473, 2439, 35240, 288, 13807, 18606, 541, 635, 361, 11, 51564], "temperature": 0.0, "avg_logprob": -0.22411288306826638, "compression_ratio": 1.7740384615384615, "no_speech_prob": 0.04591600224375725}, {"id": 70, "seek": 45532, "start": 479.32, "end": 482.32, "text": " puedo establecer la correspondencia entre las dos palabras.", "tokens": [51564, 21612, 37444, 1776, 635, 6805, 10974, 3962, 2439, 4491, 35240, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22411288306826638, "compression_ratio": 1.7740384615384615, "no_speech_prob": 0.04591600224375725}, {"id": 71, "seek": 48532, "start": 485.32, "end": 491.32, "text": " Y eso es un poco lo que dice ac\u00e1 para terminar de armar lo que es el modelo de traducci\u00f3n.", "tokens": [50364, 398, 7287, 785, 517, 10639, 450, 631, 10313, 23496, 1690, 36246, 368, 3726, 289, 450, 631, 785, 806, 27825, 368, 2479, 1311, 5687, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13489642510047326, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.015517981722950935}, {"id": 72, "seek": 48532, "start": 491.32, "end": 494.32, "text": " Para terminar de armar el modelo de traducci\u00f3n dicen que en el tercer paso", "tokens": [50664, 11107, 36246, 368, 3726, 289, 806, 27825, 368, 2479, 1311, 5687, 33816, 631, 465, 806, 38103, 29212, 50814], "temperature": 0.0, "avg_logprob": -0.13489642510047326, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.015517981722950935}, {"id": 73, "seek": 48532, "start": 494.32, "end": 496.32, "text": " yo voy a elegir cu\u00e1les son las palabras,", "tokens": [50814, 5290, 7552, 257, 14459, 347, 2702, 842, 904, 1872, 2439, 35240, 11, 50914], "temperature": 0.0, "avg_logprob": -0.13489642510047326, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.015517981722950935}, {"id": 74, "seek": 48532, "start": 496.32, "end": 504.32, "text": " entonces lo que voy a hacer es iterar sobre todas las palabras y haciendo el producto de todas las probabilidades,", "tokens": [50914, 13003, 450, 631, 7552, 257, 6720, 785, 17138, 289, 5473, 10906, 2439, 35240, 288, 20509, 806, 47583, 368, 10906, 2439, 31959, 10284, 11, 51314], "temperature": 0.0, "avg_logprob": -0.13489642510047326, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.015517981722950935}, {"id": 75, "seek": 48532, "start": 504.32, "end": 509.32, "text": " o sea, el producto de dado que hecho ten\u00eda la palabra f su j,", "tokens": [51314, 277, 4158, 11, 806, 47583, 368, 29568, 631, 13064, 23718, 635, 31702, 283, 459, 361, 11, 51564], "temperature": 0.0, "avg_logprob": -0.13489642510047326, "compression_ratio": 1.830188679245283, "no_speech_prob": 0.015517981722950935}, {"id": 76, "seek": 50932, "start": 510.32, "end": 512.3199999999999, "text": " dado que yo ten\u00eda la palabra, eso va a su j en ingl\u00e9s,", "tokens": [50414, 29568, 631, 5290, 23718, 635, 31702, 11, 7287, 2773, 257, 459, 361, 465, 49766, 11, 50514], "temperature": 0.0, "avg_logprob": -0.15673876332712697, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.013501771725714207}, {"id": 77, "seek": 50932, "start": 512.3199999999999, "end": 515.3199999999999, "text": " entonces elegir la palabra f su j en espa\u00f1ol,", "tokens": [50514, 13003, 14459, 347, 635, 31702, 283, 459, 361, 465, 31177, 11, 50664], "temperature": 0.0, "avg_logprob": -0.15673876332712697, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.013501771725714207}, {"id": 78, "seek": 50932, "start": 516.3199999999999, "end": 521.3199999999999, "text": " eso a una productoria con todos los valores de las distintas palabras.", "tokens": [50714, 7287, 257, 2002, 1674, 8172, 416, 6321, 1750, 38790, 368, 2439, 31489, 296, 35240, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15673876332712697, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.013501771725714207}, {"id": 79, "seek": 50932, "start": 523.3199999999999, "end": 530.3199999999999, "text": " Bien, entonces ah\u00ed llegu\u00e9 a el \u00faltimo de los valores que quer\u00eda calcular,", "tokens": [51064, 16956, 11, 13003, 12571, 11234, 42423, 257, 806, 21013, 368, 1750, 38790, 631, 37869, 2104, 17792, 11, 51414], "temperature": 0.0, "avg_logprob": -0.15673876332712697, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.013501771725714207}, {"id": 80, "seek": 50932, "start": 530.3199999999999, "end": 538.3199999999999, "text": " que es la probabilidad de f dado que conozco a y es igual a la productoria,", "tokens": [51414, 631, 785, 635, 31959, 4580, 368, 283, 29568, 631, 416, 15151, 1291, 257, 288, 785, 10953, 257, 635, 1674, 8172, 11, 51814], "temperature": 0.0, "avg_logprob": -0.15673876332712697, "compression_ratio": 1.6237623762376239, "no_speech_prob": 0.013501771725714207}, {"id": 81, "seek": 53832, "start": 538.32, "end": 545.32, "text": " con j igual 1 hasta j grande, de el valor de la tabla de traducci\u00f3n,", "tokens": [50364, 416, 361, 10953, 502, 10764, 361, 8883, 11, 368, 806, 15367, 368, 635, 4421, 875, 368, 2479, 1311, 5687, 11, 50714], "temperature": 0.0, "avg_logprob": -0.24209436617399516, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0023857520427554846}, {"id": 82, "seek": 53832, "start": 545.32, "end": 552.32, "text": " que es f su j, t de f su j e suba su j.", "tokens": [50714, 631, 785, 283, 459, 361, 11, 256, 368, 283, 459, 361, 308, 1422, 64, 459, 361, 13, 51064], "temperature": 0.0, "avg_logprob": -0.24209436617399516, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0023857520427554846}, {"id": 83, "seek": 53832, "start": 556.32, "end": 561.32, "text": " Bueno, est\u00e1, entonces ah\u00ed tengo como en cada paso fui calculando cosas,", "tokens": [51264, 16046, 11, 3192, 11, 13003, 12571, 13989, 2617, 465, 8411, 29212, 27863, 4322, 1806, 12218, 11, 51514], "temperature": 0.0, "avg_logprob": -0.24209436617399516, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0023857520427554846}, {"id": 84, "seek": 53832, "start": 561.32, "end": 565.32, "text": " este se correspond\u00eda al paso 1 del modelo, paso 1,", "tokens": [51514, 4065, 369, 6805, 2686, 419, 29212, 502, 1103, 27825, 11, 29212, 502, 11, 51714], "temperature": 0.0, "avg_logprob": -0.24209436617399516, "compression_ratio": 1.4417177914110428, "no_speech_prob": 0.0023857520427554846}, {"id": 85, "seek": 56532, "start": 565.32, "end": 568.32, "text": " este se corresponde con el paso 2 del modelo, en realidad,", "tokens": [50364, 4065, 369, 6805, 68, 416, 806, 29212, 568, 1103, 27825, 11, 465, 25635, 11, 50514], "temperature": 0.0, "avg_logprob": -0.2175952267934041, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.00377432769164443}, {"id": 86, "seek": 56532, "start": 568.32, "end": 571.32, "text": " este ya tiene el paso 1 del paso 2 juntos porque ya tengo el epsilon ac\u00e1,", "tokens": [50514, 4065, 2478, 7066, 806, 29212, 502, 1103, 29212, 568, 33868, 4021, 2478, 13989, 806, 17889, 23496, 11, 50664], "temperature": 0.0, "avg_logprob": -0.2175952267934041, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.00377432769164443}, {"id": 87, "seek": 56532, "start": 571.32, "end": 574.32, "text": " y este se corresponde con el paso 3 del modelo,", "tokens": [50664, 288, 4065, 369, 6805, 68, 416, 806, 29212, 805, 1103, 27825, 11, 50814], "temperature": 0.0, "avg_logprob": -0.2175952267934041, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.00377432769164443}, {"id": 88, "seek": 56532, "start": 574.32, "end": 577.32, "text": " el paso 3 de la historia de generaci\u00f3n.", "tokens": [50814, 806, 29212, 805, 368, 635, 18385, 368, 1337, 3482, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2175952267934041, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.00377432769164443}, {"id": 89, "seek": 56532, "start": 579.32, "end": 586.32, "text": " Mi objetivo con todos estos valores que est\u00e1n ac\u00e1 es calcular pd fedadue.", "tokens": [51064, 10204, 29809, 416, 6321, 12585, 38790, 631, 10368, 23496, 785, 2104, 17792, 280, 67, 4636, 345, 622, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2175952267934041, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.00377432769164443}, {"id": 90, "seek": 58632, "start": 586.32, "end": 595.32, "text": " \u00bfQu\u00e9 par\u00e1metros introduje? \u00bfQu\u00e9 par\u00e1metros fueron surgiendo a medida que lleva", "tokens": [50364, 3841, 15137, 971, 842, 29570, 2814, 2884, 30, 3841, 15137, 971, 842, 29570, 28739, 19560, 7304, 257, 32984, 631, 37681, 50814], "temperature": 0.0, "avg_logprob": -0.19000525841346153, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.01564267836511135}, {"id": 91, "seek": 58632, "start": 595.32, "end": 599.32, "text": " y tirando sobre estos pasos? Bueno, en primer lugar, el epsilon aquel que est\u00e1bamos viendo,", "tokens": [50814, 288, 13807, 1806, 5473, 12585, 1736, 329, 30, 16046, 11, 465, 12595, 11467, 11, 806, 17889, 2373, 338, 631, 3192, 65, 2151, 34506, 11, 51014], "temperature": 0.0, "avg_logprob": -0.19000525841346153, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.01564267836511135}, {"id": 92, "seek": 58632, "start": 599.32, "end": 602.32, "text": " este es un valor que yo tendr\u00eda que estimar a partir de mirar en los corpus,", "tokens": [51014, 4065, 785, 517, 15367, 631, 5290, 3928, 37183, 631, 8017, 289, 257, 13906, 368, 3149, 289, 465, 1750, 1181, 31624, 11, 51164], "temperature": 0.0, "avg_logprob": -0.19000525841346153, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.01564267836511135}, {"id": 93, "seek": 58632, "start": 602.32, "end": 606.32, "text": " como son los largos de las oraciones relativas,", "tokens": [51164, 2617, 1872, 1750, 11034, 329, 368, 2439, 420, 9188, 21960, 296, 11, 51364], "temperature": 0.0, "avg_logprob": -0.19000525841346153, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.01564267836511135}, {"id": 94, "seek": 58632, "start": 606.32, "end": 609.32, "text": " y el otro par\u00e1metro importante es aquella tabla all\u00e1,", "tokens": [51364, 288, 806, 11921, 971, 842, 45400, 9416, 785, 2373, 9885, 4421, 875, 30642, 11, 51514], "temperature": 0.0, "avg_logprob": -0.19000525841346153, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.01564267836511135}, {"id": 95, "seek": 58632, "start": 609.32, "end": 613.32, "text": " aquella tabla de traducciones que me dice banco, con qu\u00e9 probabilidad lo puedo traducir como banco,", "tokens": [51514, 2373, 9885, 4421, 875, 368, 2479, 1311, 23469, 631, 385, 10313, 45498, 11, 416, 8057, 31959, 4580, 450, 21612, 2479, 1311, 347, 2617, 45498, 11, 51714], "temperature": 0.0, "avg_logprob": -0.19000525841346153, "compression_ratio": 1.6974169741697418, "no_speech_prob": 0.01564267836511135}, {"id": 96, "seek": 61332, "start": 613.32, "end": 616.32, "text": " que probabilidad lo puedo traducir como bench, etc\u00e9tera, etc\u00e9tera,", "tokens": [50364, 631, 31959, 4580, 450, 21612, 2479, 1311, 347, 2617, 10638, 11, 5183, 526, 23833, 11, 5183, 526, 23833, 11, 50514], "temperature": 0.0, "avg_logprob": -0.15213536459302146, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.001704780850559473}, {"id": 97, "seek": 61332, "start": 616.32, "end": 619.32, "text": " esa tabla en realidad es un par\u00e1metro del modelo,", "tokens": [50514, 11342, 4421, 875, 465, 25635, 785, 517, 971, 842, 45400, 1103, 27825, 11, 50664], "temperature": 0.0, "avg_logprob": -0.15213536459302146, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.001704780850559473}, {"id": 98, "seek": 61332, "start": 619.32, "end": 622.32, "text": " es un par\u00e1metro del sistema que si yo lo tuviera me alcanzar\u00eda con eso", "tokens": [50664, 785, 517, 971, 842, 45400, 1103, 13245, 631, 1511, 5290, 450, 38177, 10609, 385, 50200, 21178, 416, 7287, 50814], "temperature": 0.0, "avg_logprob": -0.15213536459302146, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.001704780850559473}, {"id": 99, "seek": 61332, "start": 622.32, "end": 627.32, "text": " para poder construirme este modelo y calcular la probabilidad de cualquier par de braciones.", "tokens": [50814, 1690, 8152, 38445, 1398, 4065, 27825, 288, 2104, 17792, 635, 31959, 4580, 368, 21004, 971, 368, 1548, 23469, 13, 51064], "temperature": 0.0, "avg_logprob": -0.15213536459302146, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.001704780850559473}, {"id": 100, "seek": 61332, "start": 632.32, "end": 634.32, "text": " Bien, y entonces, antes de continuar,", "tokens": [51314, 16956, 11, 288, 13003, 11, 11014, 368, 29980, 11, 51414], "temperature": 0.0, "avg_logprob": -0.15213536459302146, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.001704780850559473}, {"id": 101, "seek": 61332, "start": 634.32, "end": 638.32, "text": " vamos a terminar de armar cu\u00e1l es la imagen de esto,", "tokens": [51414, 5295, 257, 36246, 368, 3726, 289, 44318, 785, 635, 40652, 368, 7433, 11, 51614], "temperature": 0.0, "avg_logprob": -0.15213536459302146, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.001704780850559473}, {"id": 102, "seek": 61332, "start": 638.32, "end": 642.32, "text": " que es decir, yo en realidad lo que quer\u00eda calcular era pd fedadue,", "tokens": [51614, 631, 785, 10235, 11, 5290, 465, 25635, 450, 631, 37869, 2104, 17792, 4249, 280, 67, 4636, 345, 622, 11, 51814], "temperature": 0.0, "avg_logprob": -0.15213536459302146, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.001704780850559473}, {"id": 103, "seek": 64232, "start": 642.32, "end": 646.32, "text": " que eso va a ser mi modelo de traducci\u00f3n,", "tokens": [50364, 631, 7287, 2773, 257, 816, 2752, 27825, 368, 2479, 1311, 5687, 11, 50564], "temperature": 0.0, "avg_logprob": -0.1468503291790302, "compression_ratio": 1.4698795180722892, "no_speech_prob": 0.001988401170819998}, {"id": 104, "seek": 64232, "start": 646.32, "end": 650.32, "text": " y de hecho va a ser el encargado de medir la adecuaci\u00f3n de una frase.", "tokens": [50564, 288, 368, 13064, 2773, 257, 816, 806, 2058, 289, 30135, 368, 1205, 347, 635, 614, 3045, 84, 3482, 368, 2002, 38406, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1468503291790302, "compression_ratio": 1.4698795180722892, "no_speech_prob": 0.001988401170819998}, {"id": 105, "seek": 64232, "start": 650.32, "end": 654.32, "text": " Pd fedadue, lo puedo calcular con esta descomposici\u00f3n de paso,", "tokens": [50764, 430, 67, 4636, 345, 622, 11, 450, 21612, 2104, 17792, 416, 5283, 730, 21541, 329, 15534, 368, 29212, 11, 50964], "temperature": 0.0, "avg_logprob": -0.1468503291790302, "compression_ratio": 1.4698795180722892, "no_speech_prob": 0.001988401170819998}, {"id": 106, "seek": 64232, "start": 654.32, "end": 658.32, "text": " que dice ac\u00e1, en realidad, porque lo hago de la siguiente manera.", "tokens": [50964, 631, 10313, 23496, 11, 465, 25635, 11, 4021, 450, 38721, 368, 635, 25666, 13913, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1468503291790302, "compression_ratio": 1.4698795180722892, "no_speech_prob": 0.001988401170819998}, {"id": 107, "seek": 65832, "start": 658.32, "end": 668.32, "text": " Yo quiero calcular pd fedadue,", "tokens": [50364, 7616, 16811, 2104, 17792, 280, 67, 4636, 345, 622, 11, 50864], "temperature": 0.0, "avg_logprob": -0.22110019127527872, "compression_ratio": 1.305263157894737, "no_speech_prob": 0.009727866388857365}, {"id": 108, "seek": 65832, "start": 668.32, "end": 679.32, "text": " y entonces voy a mirar lo que dice ac\u00e1,", "tokens": [50864, 288, 13003, 7552, 257, 3149, 289, 450, 631, 10313, 23496, 11, 51414], "temperature": 0.0, "avg_logprob": -0.22110019127527872, "compression_ratio": 1.305263157894737, "no_speech_prob": 0.009727866388857365}, {"id": 109, "seek": 65832, "start": 679.32, "end": 684.32, "text": " pd fedadue es igual de la sumatoriana de pd fedadue,", "tokens": [51414, 280, 67, 4636, 345, 622, 785, 10953, 368, 635, 2408, 1639, 8497, 368, 280, 67, 4636, 345, 622, 11, 51664], "temperature": 0.0, "avg_logprob": -0.22110019127527872, "compression_ratio": 1.305263157894737, "no_speech_prob": 0.009727866388857365}, {"id": 110, "seek": 68432, "start": 684.32, "end": 691.32, "text": " que significa eso, que para traducir entre una variaci\u00f3n en espa\u00f1ol y una variaci\u00f3n en ingl\u00e9s,", "tokens": [50364, 631, 19957, 7287, 11, 631, 1690, 2479, 1311, 347, 3962, 2002, 3034, 3482, 465, 31177, 288, 2002, 3034, 3482, 465, 49766, 11, 50714], "temperature": 0.0, "avg_logprob": -0.18616149181456076, "compression_ratio": 2.1101321585903086, "no_speech_prob": 0.16798093914985657}, {"id": 111, "seek": 68432, "start": 691.32, "end": 695.32, "text": " o m\u00e1s bien, para el s\u00ed, bueno, para traducir entre una variaci\u00f3n en ingl\u00e9s y una variaci\u00f3n en espa\u00f1ol,", "tokens": [50714, 277, 3573, 3610, 11, 1690, 806, 8600, 11, 11974, 11, 1690, 2479, 1311, 347, 3962, 2002, 3034, 3482, 465, 49766, 288, 2002, 3034, 3482, 465, 31177, 11, 50914], "temperature": 0.0, "avg_logprob": -0.18616149181456076, "compression_ratio": 2.1101321585903086, "no_speech_prob": 0.16798093914985657}, {"id": 112, "seek": 68432, "start": 695.32, "end": 699.32, "text": " hay muchas formas de alinear las palabras entre el ingl\u00e9s y en espa\u00f1ol,", "tokens": [50914, 4842, 16072, 33463, 368, 419, 533, 289, 2439, 35240, 3962, 806, 49766, 288, 465, 31177, 11, 51114], "temperature": 0.0, "avg_logprob": -0.18616149181456076, "compression_ratio": 2.1101321585903086, "no_speech_prob": 0.16798093914985657}, {"id": 113, "seek": 68432, "start": 699.32, "end": 701.32, "text": " y una vez que yo eleg\u00ed una forma alinear,", "tokens": [51114, 288, 2002, 5715, 631, 5290, 14459, 870, 2002, 8366, 419, 533, 289, 11, 51214], "temperature": 0.0, "avg_logprob": -0.18616149181456076, "compression_ratio": 2.1101321585903086, "no_speech_prob": 0.16798093914985657}, {"id": 114, "seek": 68432, "start": 701.32, "end": 704.32, "text": " hay muchas formas de elegir las palabras que vienen despu\u00e9s,", "tokens": [51214, 4842, 16072, 33463, 368, 14459, 347, 2439, 35240, 631, 49298, 15283, 11, 51364], "temperature": 0.0, "avg_logprob": -0.18616149181456076, "compression_ratio": 2.1101321585903086, "no_speech_prob": 0.16798093914985657}, {"id": 115, "seek": 68432, "start": 704.32, "end": 709.32, "text": " digamos, yo miro la traducci\u00f3n y capaz que hay varias maneras de elegir distintas palabras.", "tokens": [51364, 36430, 11, 5290, 2752, 340, 635, 2479, 1311, 5687, 288, 35453, 631, 4842, 37496, 587, 6985, 368, 14459, 347, 31489, 296, 35240, 13, 51614], "temperature": 0.0, "avg_logprob": -0.18616149181456076, "compression_ratio": 2.1101321585903086, "no_speech_prob": 0.16798093914985657}, {"id": 116, "seek": 70932, "start": 710.32, "end": 716.32, "text": " Entonces, lo que eso significa es que no existe una sola manera de traducir una variaci\u00f3n en ingl\u00e9s a una variaci\u00f3n en espa\u00f1ol.", "tokens": [50414, 15097, 11, 450, 631, 7287, 19957, 785, 631, 572, 16304, 2002, 34162, 13913, 368, 2479, 1311, 347, 2002, 3034, 3482, 465, 49766, 257, 2002, 3034, 3482, 465, 31177, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13732513668030266, "compression_ratio": 1.92, "no_speech_prob": 0.12435036152601242}, {"id": 117, "seek": 70932, "start": 716.32, "end": 720.32, "text": " Yo puedo encontrar varias formas de alinear las palabras y de varias formas de elegir las palabras,", "tokens": [50714, 7616, 21612, 17525, 37496, 33463, 368, 419, 533, 289, 2439, 35240, 288, 368, 37496, 33463, 368, 14459, 347, 2439, 35240, 11, 50914], "temperature": 0.0, "avg_logprob": -0.13732513668030266, "compression_ratio": 1.92, "no_speech_prob": 0.12435036152601242}, {"id": 118, "seek": 70932, "start": 720.32, "end": 723.32, "text": " de manera de que muchas alineaciones son posibles.", "tokens": [50914, 368, 13913, 368, 631, 16072, 419, 533, 9188, 1872, 1366, 14428, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13732513668030266, "compression_ratio": 1.92, "no_speech_prob": 0.12435036152601242}, {"id": 119, "seek": 70932, "start": 723.32, "end": 730.32, "text": " Entonces, para saber cu\u00e1l es la probabilidad de traducir fd fedadue,", "tokens": [51064, 15097, 11, 1690, 12489, 44318, 785, 635, 31959, 4580, 368, 2479, 1311, 347, 283, 67, 4636, 345, 622, 11, 51414], "temperature": 0.0, "avg_logprob": -0.13732513668030266, "compression_ratio": 1.92, "no_speech_prob": 0.12435036152601242}, {"id": 120, "seek": 70932, "start": 730.32, "end": 733.32, "text": " entonces yo voy a tener que sumar sobre todo las alineaciones posibles,", "tokens": [51414, 13003, 5290, 7552, 257, 11640, 631, 2408, 289, 5473, 5149, 2439, 419, 533, 9188, 1366, 14428, 11, 51564], "temperature": 0.0, "avg_logprob": -0.13732513668030266, "compression_ratio": 1.92, "no_speech_prob": 0.12435036152601242}, {"id": 121, "seek": 70932, "start": 733.32, "end": 737.32, "text": " sobre todo las formas de alinear las dos oraciones fie,", "tokens": [51564, 5473, 5149, 2439, 33463, 368, 419, 533, 289, 2439, 4491, 420, 9188, 283, 414, 11, 51764], "temperature": 0.0, "avg_logprob": -0.13732513668030266, "compression_ratio": 1.92, "no_speech_prob": 0.12435036152601242}, {"id": 122, "seek": 73732, "start": 737.32, "end": 742.32, "text": " voy a tener que iterar sobre eso y para cada una voy a tener que alcular la probabilidad parcial.", "tokens": [50364, 7552, 257, 11640, 631, 17138, 289, 5473, 7287, 288, 1690, 8411, 2002, 7552, 257, 11640, 631, 419, 2444, 289, 635, 31959, 4580, 971, 1013, 13, 50614], "temperature": 0.0, "avg_logprob": -0.16762728373209634, "compression_ratio": 2.153225806451613, "no_speech_prob": 0.0003088471421506256}, {"id": 123, "seek": 73732, "start": 742.32, "end": 747.32, "text": " Entonces, digamos, yo tengo cinco formas alinear las dos oraciones,", "tokens": [50614, 15097, 11, 36430, 11, 5290, 13989, 21350, 33463, 419, 533, 289, 2439, 4491, 420, 9188, 11, 50864], "temperature": 0.0, "avg_logprob": -0.16762728373209634, "compression_ratio": 2.153225806451613, "no_speech_prob": 0.0003088471421506256}, {"id": 124, "seek": 73732, "start": 747.32, "end": 751.32, "text": " cinco es un n\u00famero un poco raro, pero digamos, tengo n formas alinear las dos oraciones,", "tokens": [50864, 21350, 785, 517, 14959, 517, 10639, 367, 9708, 11, 4768, 36430, 11, 13989, 297, 33463, 419, 533, 289, 2439, 4491, 420, 9188, 11, 51064], "temperature": 0.0, "avg_logprob": -0.16762728373209634, "compression_ratio": 2.153225806451613, "no_speech_prob": 0.0003088471421506256}, {"id": 125, "seek": 73732, "start": 751.32, "end": 754.32, "text": " voy a tener que mirar, bueno, para la primera alineaci\u00f3n,", "tokens": [51064, 7552, 257, 11640, 631, 3149, 289, 11, 11974, 11, 1690, 635, 17382, 419, 533, 3482, 11, 51214], "temperature": 0.0, "avg_logprob": -0.16762728373209634, "compression_ratio": 2.153225806451613, "no_speech_prob": 0.0003088471421506256}, {"id": 126, "seek": 73732, "start": 754.32, "end": 758.32, "text": " cu\u00e1l es la probabilidad de encontrar la oraci\u00f3n f,", "tokens": [51214, 44318, 785, 635, 31959, 4580, 368, 17525, 635, 420, 3482, 283, 11, 51414], "temperature": 0.0, "avg_logprob": -0.16762728373209634, "compression_ratio": 2.153225806451613, "no_speech_prob": 0.0003088471421506256}, {"id": 127, "seek": 73732, "start": 758.32, "end": 761.32, "text": " para la segunda alineaci\u00f3n, cu\u00e1l es la probabilidad de encontrar la oraci\u00f3n f,", "tokens": [51414, 1690, 635, 21978, 419, 533, 3482, 11, 44318, 785, 635, 31959, 4580, 368, 17525, 635, 420, 3482, 283, 11, 51564], "temperature": 0.0, "avg_logprob": -0.16762728373209634, "compression_ratio": 2.153225806451613, "no_speech_prob": 0.0003088471421506256}, {"id": 128, "seek": 73732, "start": 761.32, "end": 765.32, "text": " para la tercera alineaci\u00f3n y as\u00ed hasta llegar al final y agarrar el sumo todo eso.", "tokens": [51564, 1690, 635, 1796, 41034, 419, 533, 3482, 288, 8582, 10764, 24892, 419, 2572, 288, 623, 2284, 289, 806, 2408, 78, 5149, 7287, 13, 51764], "temperature": 0.0, "avg_logprob": -0.16762728373209634, "compression_ratio": 2.153225806451613, "no_speech_prob": 0.0003088471421506256}, {"id": 129, "seek": 76532, "start": 765.32, "end": 770.32, "text": " Eso lo puedo hacer porque las alineaciones son una descomposici\u00f3n del espacio de probabilidades.", "tokens": [50364, 27795, 450, 21612, 6720, 4021, 2439, 419, 533, 9188, 1872, 2002, 730, 21541, 329, 15534, 1103, 33845, 368, 31959, 10284, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1296558656554291, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.001280081458389759}, {"id": 130, "seek": 76532, "start": 770.32, "end": 773.32, "text": " En realidad yo puedo descomponer el espacio de probabilidades en pedacitos disjuntos", "tokens": [50614, 2193, 25635, 5290, 21612, 730, 21541, 32949, 806, 33845, 368, 31959, 10284, 465, 5670, 326, 11343, 717, 73, 2760, 329, 50764], "temperature": 0.0, "avg_logprob": -0.1296558656554291, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.001280081458389759}, {"id": 131, "seek": 76532, "start": 773.32, "end": 775.32, "text": " y cada alineaci\u00f3n va a ser uno de ellos.", "tokens": [50764, 288, 8411, 419, 533, 3482, 2773, 257, 816, 8526, 368, 16353, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1296558656554291, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.001280081458389759}, {"id": 132, "seek": 76532, "start": 775.32, "end": 780.32, "text": " As\u00ed que, digamos que para calcular el modelo de traducci\u00f3n fd fedadue,", "tokens": [50864, 17419, 631, 11, 36430, 631, 1690, 2104, 17792, 806, 27825, 368, 2479, 1311, 5687, 283, 67, 4636, 345, 622, 11, 51114], "temperature": 0.0, "avg_logprob": -0.1296558656554291, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.001280081458389759}, {"id": 133, "seek": 76532, "start": 780.32, "end": 783.32, "text": " necesito sumar sobre todo las alineaciones posibles.", "tokens": [51114, 11909, 3528, 2408, 289, 5473, 5149, 2439, 419, 533, 9188, 1366, 14428, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1296558656554291, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.001280081458389759}, {"id": 134, "seek": 76532, "start": 783.32, "end": 788.32, "text": " Ahora, lo que me falta es saber c\u00f3mo calculo este valor ac\u00e1.", "tokens": [51264, 18840, 11, 450, 631, 385, 22111, 785, 12489, 12826, 4322, 78, 4065, 15367, 23496, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1296558656554291, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.001280081458389759}, {"id": 135, "seek": 76532, "start": 788.32, "end": 793.32, "text": " As\u00ed que lo que estoy diciendo es que la probabilidad de fd fedadue es la suma", "tokens": [51514, 17419, 631, 450, 631, 15796, 42797, 785, 631, 635, 31959, 4580, 368, 283, 67, 4636, 345, 622, 785, 635, 2408, 64, 51764], "temperature": 0.0, "avg_logprob": -0.1296558656554291, "compression_ratio": 1.8566037735849057, "no_speech_prob": 0.001280081458389759}, {"id": 136, "seek": 79332, "start": 793.32, "end": 798.32, "text": " sobre las alineaciones de la probabilidad de f y esa alineaci\u00f3n dado de.", "tokens": [50364, 5473, 2439, 419, 533, 9188, 368, 635, 31959, 4580, 368, 283, 288, 11342, 419, 533, 3482, 274, 1573, 368, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2166539113455956, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004052399657666683}, {"id": 137, "seek": 79332, "start": 798.32, "end": 801.32, "text": " Eso es simplemente lo que dice ah\u00ed en la slide.", "tokens": [50614, 27795, 785, 33190, 450, 631, 10313, 12571, 465, 635, 4137, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2166539113455956, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004052399657666683}, {"id": 138, "seek": 79332, "start": 801.32, "end": 804.32, "text": " Lo que me falta a calcular entonces es esta parte de ac\u00e1.", "tokens": [50764, 6130, 631, 385, 22111, 257, 2104, 17792, 13003, 785, 5283, 6975, 368, 23496, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2166539113455956, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004052399657666683}, {"id": 139, "seek": 79332, "start": 804.32, "end": 806.32, "text": " Y esa parte de ac\u00e1 la calculo de esta manera.", "tokens": [50914, 398, 11342, 6975, 368, 23496, 635, 4322, 78, 368, 5283, 13913, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2166539113455956, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004052399657666683}, {"id": 140, "seek": 79332, "start": 806.32, "end": 810.32, "text": " Yo digo que la probabilidad de fdado es igual,", "tokens": [51014, 7616, 22990, 631, 635, 31959, 4580, 368, 283, 67, 1573, 785, 10953, 11, 51214], "temperature": 0.0, "avg_logprob": -0.2166539113455956, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004052399657666683}, {"id": 141, "seek": 79332, "start": 810.32, "end": 815.32, "text": " ah\u00ed est\u00e1 m\u00e1s o menos al resultado final pero podemos sacar", "tokens": [51214, 12571, 3192, 3573, 277, 8902, 419, 28047, 2572, 4768, 12234, 43823, 51464], "temperature": 0.0, "avg_logprob": -0.2166539113455956, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004052399657666683}, {"id": 142, "seek": 79332, "start": 815.32, "end": 820.32, "text": " que es lo que tendr\u00eda que poner de este lado.", "tokens": [51464, 631, 785, 450, 631, 3928, 37183, 631, 19149, 368, 4065, 11631, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2166539113455956, "compression_ratio": 1.6916299559471366, "no_speech_prob": 0.004052399657666683}, {"id": 143, "seek": 82032, "start": 820.32, "end": 823.32, "text": " Y ahora s\u00ed me acuerdo bien.", "tokens": [50364, 398, 9923, 8600, 385, 28113, 3610, 13, 50514], "temperature": 0.0, "avg_logprob": -0.40308210584852433, "compression_ratio": 1.267515923566879, "no_speech_prob": 0.0014357692562043667}, {"id": 144, "seek": 82032, "start": 832.32, "end": 833.32, "text": " Ah, ah\u00ed est\u00e1.", "tokens": [50964, 2438, 11, 12571, 3192, 13, 51014], "temperature": 0.0, "avg_logprob": -0.40308210584852433, "compression_ratio": 1.267515923566879, "no_speech_prob": 0.0014357692562043667}, {"id": 145, "seek": 82032, "start": 833.32, "end": 835.32, "text": " Por definici\u00f3n de probabilidad condicional.", "tokens": [51014, 5269, 1561, 15534, 368, 31959, 4580, 2224, 33010, 13, 51114], "temperature": 0.0, "avg_logprob": -0.40308210584852433, "compression_ratio": 1.267515923566879, "no_speech_prob": 0.0014357692562043667}, {"id": 146, "seek": 82032, "start": 835.32, "end": 838.32, "text": " Eso.", "tokens": [51114, 27795, 13, 51264], "temperature": 0.0, "avg_logprob": -0.40308210584852433, "compression_ratio": 1.267515923566879, "no_speech_prob": 0.0014357692562043667}, {"id": 147, "seek": 82032, "start": 838.32, "end": 841.32, "text": " Fd fedadue, de verdad, lo har\u00eda manera hacerlo.", "tokens": [51264, 479, 67, 4636, 345, 622, 11, 368, 13692, 11, 450, 2233, 2686, 13913, 32039, 13, 51414], "temperature": 0.0, "avg_logprob": -0.40308210584852433, "compression_ratio": 1.267515923566879, "no_speech_prob": 0.0014357692562043667}, {"id": 148, "seek": 82032, "start": 841.32, "end": 846.32, "text": " Pero esto se puede definir como pdf a e sobre pd.", "tokens": [51414, 9377, 7433, 369, 8919, 1561, 347, 2617, 280, 45953, 257, 308, 5473, 280, 67, 13, 51664], "temperature": 0.0, "avg_logprob": -0.40308210584852433, "compression_ratio": 1.267515923566879, "no_speech_prob": 0.0014357692562043667}, {"id": 149, "seek": 82032, "start": 846.32, "end": 848.32, "text": " \u00bfNo?", "tokens": [51664, 3841, 4540, 30, 51764], "temperature": 0.0, "avg_logprob": -0.40308210584852433, "compression_ratio": 1.267515923566879, "no_speech_prob": 0.0014357692562043667}, {"id": 150, "seek": 84832, "start": 848.32, "end": 851.32, "text": " Por definici\u00f3n de probabilidad condicional.", "tokens": [50364, 5269, 1561, 15534, 368, 31959, 4580, 2224, 33010, 13, 50514], "temperature": 0.0, "avg_logprob": -0.23590171337127686, "compression_ratio": 1.206896551724138, "no_speech_prob": 0.002210762118920684}, {"id": 151, "seek": 84832, "start": 851.32, "end": 862.32, "text": " Pero adem\u00e1s esto, si quiero, podr\u00eda llegar a decir esto es lo mismo que pdf a e sobre pd por,", "tokens": [50514, 9377, 21251, 7433, 11, 1511, 16811, 11, 27246, 24892, 257, 10235, 7433, 785, 450, 12461, 631, 280, 45953, 257, 308, 5473, 280, 67, 1515, 11, 51064], "temperature": 0.0, "avg_logprob": -0.23590171337127686, "compression_ratio": 1.206896551724138, "no_speech_prob": 0.002210762118920684}, {"id": 152, "seek": 86232, "start": 862.32, "end": 865.32, "text": " podr\u00eda que me faltaba.", "tokens": [50364, 27246, 631, 385, 37108, 5509, 13, 50514], "temperature": 0.6000000000000001, "avg_logprob": -0.4514072642606847, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.0025737688411027193}, {"id": 153, "seek": 86232, "start": 865.32, "end": 871.32, "text": " No, ah\u00ed.", "tokens": [50514, 883, 11, 12571, 13, 50814], "temperature": 0.6000000000000001, "avg_logprob": -0.4514072642606847, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.0025737688411027193}, {"id": 154, "seek": 86232, "start": 871.32, "end": 878.32, "text": " Por pd a e sobre pd a e.", "tokens": [50814, 5269, 280, 67, 257, 308, 5473, 280, 67, 257, 308, 13, 51164], "temperature": 0.6000000000000001, "avg_logprob": -0.4514072642606847, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.0025737688411027193}, {"id": 155, "seek": 86232, "start": 878.32, "end": 882.32, "text": " \u00bfEla esto lo quer\u00eda?", "tokens": [51164, 3841, 36, 875, 7433, 450, 37869, 30, 51364], "temperature": 0.6000000000000001, "avg_logprob": -0.4514072642606847, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.0025737688411027193}, {"id": 156, "seek": 86232, "start": 882.32, "end": 884.32, "text": " S\u00ed, el esto lo quer\u00eda.", "tokens": [51364, 12375, 11, 806, 7433, 450, 37869, 13, 51464], "temperature": 0.6000000000000001, "avg_logprob": -0.4514072642606847, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.0025737688411027193}, {"id": 157, "seek": 86232, "start": 884.32, "end": 887.32, "text": " O sea, yo puedo arrar esta probabilidad que est\u00e1 ac\u00e1 y multiplicarla", "tokens": [51464, 422, 4158, 11, 5290, 21612, 594, 5352, 5283, 31959, 4580, 631, 3192, 23496, 288, 17596, 34148, 51614], "temperature": 0.6000000000000001, "avg_logprob": -0.4514072642606847, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.0025737688411027193}, {"id": 158, "seek": 86232, "start": 887.32, "end": 890.32, "text": " y dividirla por el mismo n\u00famero que sea que son mayores que cero,", "tokens": [51614, 288, 4996, 347, 875, 1515, 806, 12461, 14959, 631, 4158, 631, 1872, 815, 2706, 631, 269, 2032, 11, 51764], "temperature": 0.6000000000000001, "avg_logprob": -0.4514072642606847, "compression_ratio": 1.4186046511627908, "no_speech_prob": 0.0025737688411027193}, {"id": 159, "seek": 89032, "start": 890.32, "end": 896.6, "text": " eso es la divisi\u00f3n me va a dar uno y ah\u00ed yo puedo tomar y as\u00ed no este con este y este con este", "tokens": [50364, 7287, 785, 635, 25974, 2560, 385, 2773, 257, 4072, 8526, 288, 12571, 5290, 21612, 22048, 288, 8582, 572, 4065, 416, 4065, 288, 4065, 416, 4065, 50678], "temperature": 0.0, "avg_logprob": -0.33077387376265094, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.28668418526649475}, {"id": 160, "seek": 89032, "start": 896.6, "end": 908.0, "text": " en definitiva lo que me queda es si asocio estos dos me va a quedar pdf dado a e y si asocio", "tokens": [50678, 465, 28781, 5931, 450, 631, 385, 23314, 785, 1511, 382, 78, 8529, 12585, 4491, 385, 2773, 257, 39244, 280, 45953, 29568, 257, 308, 288, 1511, 382, 78, 8529, 51248], "temperature": 0.0, "avg_logprob": -0.33077387376265094, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.28668418526649475}, {"id": 161, "seek": 89032, "start": 908.0, "end": 917.5600000000001, "text": " estos dos de ac\u00e1 me va a quedar pd a dado e qu\u00e9 es lo que dice all\u00e1 la probabilidad de pdf a dado", "tokens": [51248, 12585, 4491, 368, 23496, 385, 2773, 257, 39244, 280, 67, 257, 29568, 308, 8057, 785, 450, 631, 10313, 30642, 635, 31959, 4580, 368, 280, 45953, 257, 29568, 51726], "temperature": 0.0, "avg_logprob": -0.33077387376265094, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.28668418526649475}, {"id": 162, "seek": 91756, "start": 917.56, "end": 924.16, "text": " de bueno s\u00ed de los dos de f e a dado e es igual a la probabilidad de f dados a y es por la", "tokens": [50364, 368, 11974, 8600, 368, 1750, 4491, 368, 283, 308, 257, 29568, 308, 785, 10953, 257, 635, 31959, 4580, 368, 283, 39915, 257, 288, 785, 1515, 635, 50694], "temperature": 0.0, "avg_logprob": -0.2680104726768402, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.002549423137679696}, {"id": 163, "seek": 91756, "start": 924.16, "end": 930.3199999999999, "text": " probabilidad de a dado e bien y estos dos valores que est\u00e1n ac\u00e1 no los elegimos casualidad sino que", "tokens": [50694, 31959, 4580, 368, 257, 29568, 308, 3610, 288, 12585, 4491, 38790, 631, 10368, 23496, 572, 1750, 14459, 8372, 13052, 4580, 18108, 631, 51002], "temperature": 0.0, "avg_logprob": -0.2680104726768402, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.002549423137679696}, {"id": 164, "seek": 91756, "start": 930.3199999999999, "end": 937.8, "text": " son los valores que ten\u00eda antes en el modelo o sea yo ten\u00eda que el pd a dado e es igual a epsil\u00f3n", "tokens": [51002, 1872, 1750, 38790, 631, 23718, 11014, 465, 806, 27825, 277, 4158, 5290, 23718, 631, 806, 280, 67, 257, 29568, 308, 785, 10953, 257, 308, 1878, 388, 1801, 51376], "temperature": 0.0, "avg_logprob": -0.2680104726768402, "compression_ratio": 1.7818181818181817, "no_speech_prob": 0.002549423137679696}, {"id": 165, "seek": 93780, "start": 937.8, "end": 949.3599999999999, "text": " sobre y m\u00e1s uno a la j y el otro era la productoria desde j igual 1 hasta j grande de las valores", "tokens": [50364, 5473, 288, 3573, 8526, 257, 635, 361, 288, 806, 11921, 4249, 635, 1674, 8172, 10188, 361, 10953, 502, 10764, 361, 8883, 368, 2439, 38790, 50942], "temperature": 0.0, "avg_logprob": -0.2258086786037538, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.002295862417668104}, {"id": 166, "seek": 93780, "start": 949.3599999999999, "end": 958.0799999999999, "text": " de traducci\u00f3n el f subj y el e suba subj entonces en definitiva puedo calcular pdf a dado e y", "tokens": [50942, 368, 2479, 1311, 5687, 806, 283, 1422, 73, 288, 806, 308, 1422, 64, 1422, 73, 13003, 465, 28781, 5931, 21612, 2104, 17792, 280, 45953, 257, 29568, 308, 288, 51378], "temperature": 0.0, "avg_logprob": -0.2258086786037538, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.002295862417668104}, {"id": 167, "seek": 93780, "start": 958.0799999999999, "end": 963.56, "text": " adem\u00e1s puedo calcular haciendo una suma sobre todas las alineaciones posibles puedo calcular el pdf", "tokens": [51378, 21251, 21612, 2104, 17792, 20509, 2002, 2408, 64, 5473, 10906, 2439, 419, 533, 9188, 1366, 14428, 21612, 2104, 17792, 806, 280, 45953, 51652], "temperature": 0.0, "avg_logprob": -0.2258086786037538, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.002295862417668104}, {"id": 168, "seek": 96356, "start": 963.56, "end": 971.9599999999999, "text": " dado e bien con eso y con todo ese mont\u00f3n de cociones llegamos a construir lo que es un modelo de", "tokens": [50364, 29568, 308, 3610, 416, 7287, 288, 416, 5149, 10167, 45259, 368, 598, 23469, 11234, 2151, 257, 38445, 450, 631, 785, 517, 27825, 368, 50784], "temperature": 0.0, "avg_logprob": -0.16550726156968337, "compression_ratio": 1.8952380952380952, "no_speech_prob": 0.010100042447447777}, {"id": 169, "seek": 96356, "start": 971.9599999999999, "end": 977.1199999999999, "text": " traducci\u00f3n o sea solamente teniendo una tabla de traducciones que me diga cu\u00e1l es la probabilidad de", "tokens": [50784, 2479, 1311, 5687, 277, 4158, 27814, 2064, 7304, 2002, 4421, 875, 368, 2479, 1311, 23469, 631, 385, 2528, 64, 44318, 785, 635, 31959, 4580, 368, 51042], "temperature": 0.0, "avg_logprob": -0.16550726156968337, "compression_ratio": 1.8952380952380952, "no_speech_prob": 0.010100042447447777}, {"id": 170, "seek": 96356, "start": 977.1199999999999, "end": 983.5999999999999, "text": " traducir una palabra como otra palabra yo puedo llegar a definirme cu\u00e1l es la probabilidad de traducir", "tokens": [51042, 2479, 1311, 347, 2002, 31702, 2617, 13623, 31702, 5290, 21612, 24892, 257, 1561, 347, 1398, 44318, 785, 635, 31959, 4580, 368, 2479, 1311, 347, 51366], "temperature": 0.0, "avg_logprob": -0.16550726156968337, "compression_ratio": 1.8952380952380952, "no_speech_prob": 0.010100042447447777}, {"id": 171, "seek": 96356, "start": 983.5999999999999, "end": 992.0, "text": " una oraci\u00f3n dada otra oraci\u00f3n bien y hay una cosa m\u00e1s bueno esto ya lo estoy moviendo que", "tokens": [51366, 2002, 420, 3482, 274, 1538, 13623, 420, 3482, 3610, 288, 4842, 2002, 10163, 3573, 11974, 7433, 2478, 450, 15796, 2402, 7304, 631, 51786], "temperature": 0.0, "avg_logprob": -0.16550726156968337, "compression_ratio": 1.8952380952380952, "no_speech_prob": 0.010100042447447777}, {"id": 172, "seek": 99200, "start": 992.0, "end": 1001.08, "text": " aplicamos en cada paso y hay una cosa m\u00e1s que es si yo tuviera las dos oraciones digamos la", "tokens": [50364, 18221, 2151, 465, 8411, 29212, 288, 4842, 2002, 10163, 3573, 631, 785, 1511, 5290, 38177, 10609, 2439, 4491, 420, 9188, 36430, 635, 50818], "temperature": 0.0, "avg_logprob": -0.22251340044223197, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.010885841213166714}, {"id": 173, "seek": 99200, "start": 1001.08, "end": 1004.72, "text": " oraci\u00f3n en ingl\u00e9s y la oraci\u00f3n en espa\u00f1ol y adem\u00e1s tuviera la tabla de esta con todas las", "tokens": [50818, 420, 3482, 465, 49766, 288, 635, 420, 3482, 465, 31177, 288, 21251, 38177, 10609, 635, 4421, 875, 368, 5283, 416, 10906, 2439, 51000], "temperature": 0.0, "avg_logprob": -0.22251340044223197, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.010885841213166714}, {"id": 174, "seek": 99200, "start": 1004.72, "end": 1009.24, "text": " probabilidades yo podr\u00eda hacer un algoritmo de programaci\u00f3n din\u00e1mica un algoritmo estilo", "tokens": [51000, 31959, 10284, 5290, 27246, 6720, 517, 3501, 50017, 3280, 368, 1461, 3482, 3791, 19524, 2262, 517, 3501, 50017, 3280, 37470, 51226], "temperature": 0.0, "avg_logprob": -0.22251340044223197, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.010885841213166714}, {"id": 175, "seek": 99200, "start": 1009.24, "end": 1013.84, "text": " brit\u00e1nico que vaya recorriendo alineaciones y media cu\u00e1l es la alineaci\u00f3n m\u00e1s probable no vamos", "tokens": [51226, 38389, 7200, 2789, 631, 47682, 850, 284, 470, 3999, 419, 533, 9188, 288, 3021, 44318, 785, 635, 419, 533, 3482, 3573, 21759, 572, 5295, 51456], "temperature": 0.0, "avg_logprob": -0.22251340044223197, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.010885841213166714}, {"id": 176, "seek": 99200, "start": 1013.84, "end": 1018.24, "text": " a ver los detalles del algoritmo pero hay una forma de decir bueno voy recorriendo las dos", "tokens": [51456, 257, 1306, 1750, 1141, 37927, 1103, 3501, 50017, 3280, 4768, 4842, 2002, 8366, 368, 10235, 11974, 7552, 850, 284, 470, 3999, 2439, 4491, 51676], "temperature": 0.0, "avg_logprob": -0.22251340044223197, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.010885841213166714}, {"id": 177, "seek": 101824, "start": 1018.24, "end": 1023.96, "text": " oraciones y me voy quedando con las sus secciones m\u00e1s probables y al final me termina devolviendo", "tokens": [50364, 420, 9188, 288, 385, 7552, 13617, 1806, 416, 2439, 3291, 907, 23469, 3573, 1239, 2965, 288, 419, 2572, 385, 1433, 1426, 1905, 401, 85, 7304, 50650], "temperature": 0.0, "avg_logprob": -0.15268538338797433, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.004702772945165634}, {"id": 178, "seek": 101824, "start": 1023.96, "end": 1030.16, "text": " cu\u00e1l es la alineaci\u00f3n m\u00e1s probable dadas esas oraciones o sea que si yo tuviera ya esa tabla de", "tokens": [50650, 44318, 785, 635, 419, 533, 3482, 3573, 21759, 3546, 296, 23388, 420, 9188, 277, 4158, 631, 1511, 5290, 38177, 10609, 2478, 11342, 4421, 875, 368, 50960], "temperature": 0.0, "avg_logprob": -0.15268538338797433, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.004702772945165634}, {"id": 179, "seek": 101824, "start": 1030.16, "end": 1034.92, "text": " traducciones esa tabla de probabilidad de traducci\u00f3n podr\u00eda construirme las alineaciones del corpus", "tokens": [50960, 2479, 1311, 23469, 11342, 4421, 875, 368, 31959, 4580, 368, 2479, 1311, 5687, 27246, 38445, 1398, 2439, 419, 533, 9188, 1103, 1181, 31624, 51198], "temperature": 0.0, "avg_logprob": -0.15268538338797433, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.004702772945165634}, {"id": 180, "seek": 101824, "start": 1037.68, "end": 1043.4, "text": " as\u00ed que bueno hasta el momento dec\u00edamos bueno suponemos que tenemos esta tabla de traducci\u00f3n que", "tokens": [51336, 8582, 631, 11974, 10764, 806, 9333, 979, 16275, 11974, 9331, 266, 4485, 631, 9914, 5283, 4421, 875, 368, 2479, 1311, 5687, 631, 51622], "temperature": 0.0, "avg_logprob": -0.15268538338797433, "compression_ratio": 1.8387096774193548, "no_speech_prob": 0.004702772945165634}, {"id": 181, "seek": 104340, "start": 1043.4, "end": 1049.52, "text": " me dice para bank si se traduce para bank si se traduce como bank o como bench etc. estaba", "tokens": [50364, 385, 10313, 1690, 3765, 1511, 369, 2479, 4176, 1690, 3765, 1511, 369, 2479, 4176, 2617, 3765, 277, 2617, 10638, 5183, 13, 17544, 50670], "temperature": 0.0, "avg_logprob": -0.21822957095936832, "compression_ratio": 2.0560344827586206, "no_speech_prob": 0.011394641362130642}, {"id": 182, "seek": 104340, "start": 1049.52, "end": 1055.0800000000002, "text": " diciendo que ten\u00eda esa tabla pero en realidad la realidad es que no tengo esa tabla y me gustar\u00eda", "tokens": [50670, 42797, 631, 23718, 11342, 4421, 875, 4768, 465, 25635, 635, 25635, 785, 631, 572, 13989, 11342, 4421, 875, 288, 385, 45896, 50948], "temperature": 0.0, "avg_logprob": -0.21822957095936832, "compression_ratio": 2.0560344827586206, "no_speech_prob": 0.011394641362130642}, {"id": 183, "seek": 104340, "start": 1055.0800000000002, "end": 1060.76, "text": " poder construirlo entonces no gustar\u00eda poder estimar esas probabilidades para poder construir", "tokens": [50948, 8152, 38445, 752, 13003, 572, 45896, 8152, 8017, 289, 23388, 31959, 10284, 1690, 8152, 38445, 51232], "temperature": 0.0, "avg_logprob": -0.21822957095936832, "compression_ratio": 2.0560344827586206, "no_speech_prob": 0.011394641362130642}, {"id": 184, "seek": 104340, "start": 1060.76, "end": 1064.8000000000002, "text": " esa tabla si yo tuviera un corpus para el hilo simplemente podr\u00eda ir recorriendo el corpus y", "tokens": [51232, 11342, 4421, 875, 1511, 5290, 38177, 10609, 517, 1181, 31624, 1690, 806, 276, 10720, 33190, 27246, 3418, 850, 284, 470, 3999, 806, 1181, 31624, 288, 51434], "temperature": 0.0, "avg_logprob": -0.21822957095936832, "compression_ratio": 2.0560344827586206, "no_speech_prob": 0.011394641362130642}, {"id": 185, "seek": 104340, "start": 1064.8000000000002, "end": 1069.0400000000002, "text": " contando cu\u00e1ntas veces aparece de banco alineado con bench y cu\u00e1ntas veces aparece alineado con", "tokens": [51434, 660, 1806, 44256, 296, 17054, 37863, 368, 45498, 419, 533, 1573, 416, 10638, 288, 44256, 296, 17054, 37863, 419, 533, 1573, 416, 51646], "temperature": 0.0, "avg_logprob": -0.21822957095936832, "compression_ratio": 2.0560344827586206, "no_speech_prob": 0.011394641362130642}, {"id": 186, "seek": 106904, "start": 1069.04, "end": 1077.3999999999999, "text": " bench y ah\u00ed sacar\u00eda una probabilidad pero no tengo las alineaciones y con lo que vimos digamos", "tokens": [50364, 10638, 288, 12571, 4899, 21178, 2002, 31959, 4580, 4768, 572, 13989, 2439, 419, 533, 9188, 288, 416, 450, 631, 49266, 36430, 50782], "temperature": 0.0, "avg_logprob": -0.1825106181795635, "compression_ratio": 1.987603305785124, "no_speech_prob": 0.002961569931358099}, {"id": 187, "seek": 106904, "start": 1077.3999999999999, "end": 1082.2, "text": " reci\u00e9n si yo tuviera la tabla entonces yo adem\u00e1s podr\u00eda ir recorriendo el corpus y construirme", "tokens": [50782, 4214, 3516, 1511, 5290, 38177, 10609, 635, 4421, 875, 13003, 5290, 21251, 27246, 3418, 850, 284, 470, 3999, 806, 1181, 31624, 288, 38445, 1398, 51022], "temperature": 0.0, "avg_logprob": -0.1825106181795635, "compression_ratio": 1.987603305785124, "no_speech_prob": 0.002961569931358099}, {"id": 188, "seek": 106904, "start": 1082.2, "end": 1086.8, "text": " las alineaciones as\u00ed que si yo tuviera las alineaciones podr\u00eda contar y sacar la tabla si yo", "tokens": [51022, 2439, 419, 533, 9188, 8582, 631, 1511, 5290, 38177, 10609, 2439, 419, 533, 9188, 27246, 27045, 288, 43823, 635, 4421, 875, 1511, 5290, 51252], "temperature": 0.0, "avg_logprob": -0.1825106181795635, "compression_ratio": 1.987603305785124, "no_speech_prob": 0.002961569931358099}, {"id": 189, "seek": 106904, "start": 1086.8, "end": 1092.6, "text": " tuviera la tabla podr\u00eda pasarle un algoritmo y construir las alineaciones pero la verdad que no", "tokens": [51252, 38177, 10609, 635, 4421, 875, 27246, 25344, 306, 517, 3501, 50017, 3280, 288, 38445, 2439, 419, 533, 9188, 4768, 635, 13692, 631, 572, 51542], "temperature": 0.0, "avg_logprob": -0.1825106181795635, "compression_ratio": 1.987603305785124, "no_speech_prob": 0.002961569931358099}, {"id": 190, "seek": 106904, "start": 1092.6, "end": 1097.24, "text": " tengo ninguna de las dos cosas entonces se vuelve un problema de huevo y lagallina o sea si yo", "tokens": [51542, 13989, 36073, 368, 2439, 4491, 12218, 13003, 369, 20126, 303, 517, 12395, 368, 24967, 3080, 288, 8953, 336, 1426, 277, 4158, 1511, 5290, 51774], "temperature": 0.0, "avg_logprob": -0.1825106181795635, "compression_ratio": 1.987603305785124, "no_speech_prob": 0.002961569931358099}, {"id": 191, "seek": 109724, "start": 1097.24, "end": 1101.8, "text": " tuviera las alineaciones construir\u00eda el modelo construir la tabla probabilidades si yo tuviera la", "tokens": [50364, 38177, 10609, 2439, 419, 533, 9188, 38445, 2686, 806, 27825, 38445, 635, 4421, 875, 31959, 10284, 1511, 5290, 38177, 10609, 635, 50592], "temperature": 0.0, "avg_logprob": -0.13829637206761183, "compression_ratio": 2.029535864978903, "no_speech_prob": 0.0024791061878204346}, {"id": 192, "seek": 109724, "start": 1101.8, "end": 1108.4, "text": " tabla probabilidades podr\u00eda construir las alineaciones para este tipo de problemas en los cuales yo", "tokens": [50592, 4421, 875, 31959, 10284, 27246, 38445, 2439, 419, 533, 9188, 1690, 4065, 9746, 368, 20720, 465, 1750, 46932, 5290, 50922], "temperature": 0.0, "avg_logprob": -0.13829637206761183, "compression_ratio": 2.029535864978903, "no_speech_prob": 0.0024791061878204346}, {"id": 193, "seek": 109724, "start": 1108.4, "end": 1112.68, "text": " tengo como dos variables interdependentes y no conozco exactamente el valor de ninguna de las", "tokens": [50922, 13989, 2617, 4491, 9102, 728, 36763, 9240, 288, 572, 416, 15151, 1291, 48686, 806, 15367, 368, 36073, 368, 2439, 51136], "temperature": 0.0, "avg_logprob": -0.13829637206761183, "compression_ratio": 2.029535864978903, "no_speech_prob": 0.0024791061878204346}, {"id": 194, "seek": 109724, "start": 1112.68, "end": 1117.92, "text": " dos si utiliza lo que se conoce como el algoritmo de expectation maximizaci\u00f3n o maximizaci\u00f3n de", "tokens": [51136, 4491, 1511, 4976, 13427, 450, 631, 369, 33029, 384, 2617, 806, 3501, 50017, 3280, 368, 14334, 5138, 27603, 277, 5138, 27603, 368, 51398], "temperature": 0.0, "avg_logprob": -0.13829637206761183, "compression_ratio": 2.029535864978903, "no_speech_prob": 0.0024791061878204346}, {"id": 195, "seek": 109724, "start": 1117.92, "end": 1124.48, "text": " la esperanza y bueno es un algoritmo que sirve exactamente para este tipo de problemas en", "tokens": [51398, 635, 10045, 20030, 288, 11974, 785, 517, 3501, 50017, 3280, 631, 4735, 303, 48686, 1690, 4065, 9746, 368, 20720, 465, 51726], "temperature": 0.0, "avg_logprob": -0.13829637206761183, "compression_ratio": 2.029535864978903, "no_speech_prob": 0.0024791061878204346}, {"id": 196, "seek": 112448, "start": 1124.48, "end": 1128.2, "text": " realidad lo que va a hacer el algoritmo es iterar es un algoritmo iterativo que va tratando de", "tokens": [50364, 25635, 450, 631, 2773, 257, 6720, 806, 3501, 50017, 3280, 785, 17138, 289, 785, 517, 3501, 50017, 3280, 17138, 18586, 631, 2773, 21507, 1806, 368, 50550], "temperature": 0.0, "avg_logprob": -0.15375584411621093, "compression_ratio": 2.092511013215859, "no_speech_prob": 0.002441472141072154}, {"id": 197, "seek": 112448, "start": 1128.2, "end": 1134.4, "text": " converger a una soluci\u00f3n y lo que hace es decir bueno yo no tengo ninguno de los dos valores o", "tokens": [50550, 9652, 1321, 257, 2002, 24807, 5687, 288, 450, 631, 10032, 785, 10235, 11974, 5290, 572, 13989, 17210, 12638, 368, 1750, 4491, 38790, 277, 50860], "temperature": 0.0, "avg_logprob": -0.15375584411621093, "compression_ratio": 2.092511013215859, "no_speech_prob": 0.002441472141072154}, {"id": 198, "seek": 112448, "start": 1134.4, "end": 1141.68, "text": " sea si yo tuviera mi tabla de probabilidades de traducci\u00f3n me podr\u00eda calcular las alineaciones y", "tokens": [50860, 4158, 1511, 5290, 38177, 10609, 2752, 4421, 875, 368, 31959, 10284, 368, 2479, 1311, 5687, 385, 27246, 2104, 17792, 2439, 419, 533, 9188, 288, 51224], "temperature": 0.0, "avg_logprob": -0.15375584411621093, "compression_ratio": 2.092511013215859, "no_speech_prob": 0.002441472141072154}, {"id": 199, "seek": 112448, "start": 1141.68, "end": 1146.32, "text": " tuviera mis alineaciones me podr\u00eda calcular las probabilidades de traducci\u00f3n entonces lo que", "tokens": [51224, 38177, 10609, 3346, 419, 533, 9188, 385, 27246, 2104, 17792, 2439, 31959, 10284, 368, 2479, 1311, 5687, 13003, 450, 631, 51456], "temperature": 0.0, "avg_logprob": -0.15375584411621093, "compression_ratio": 2.092511013215859, "no_speech_prob": 0.002441472141072154}, {"id": 200, "seek": 112448, "start": 1146.32, "end": 1151.48, "text": " hace es decir bueno asumo que mi tabla de traducci\u00f3n va a ser uniformes digamos cualquier", "tokens": [51456, 10032, 785, 10235, 11974, 382, 40904, 631, 2752, 4421, 875, 368, 2479, 1311, 5687, 2773, 257, 816, 9452, 279, 36430, 21004, 51714], "temperature": 0.0, "avg_logprob": -0.15375584411621093, "compression_ratio": 2.092511013215859, "no_speech_prob": 0.002441472141072154}, {"id": 201, "seek": 115148, "start": 1151.48, "end": 1155.46, "text": " palabra se puede traducir como cualquier otra palabra con la misma probabilidad a partir de eso", "tokens": [50364, 31702, 369, 8919, 2479, 1311, 347, 2617, 21004, 13623, 31702, 416, 635, 24946, 31959, 4580, 257, 13906, 368, 7287, 50563], "temperature": 0.0, "avg_logprob": -0.1743419329325358, "compression_ratio": 2.017316017316017, "no_speech_prob": 0.0006354342913255095}, {"id": 202, "seek": 115148, "start": 1155.46, "end": 1159.4, "text": " calculo alineaciones y a partir de esas nuevas alineaciones calculo otra vez la tabla", "tokens": [50563, 4322, 78, 419, 533, 9188, 288, 257, 13906, 368, 23388, 42817, 419, 533, 9188, 4322, 78, 13623, 5715, 635, 4421, 875, 50760], "temperature": 0.0, "avg_logprob": -0.1743419329325358, "compression_ratio": 2.017316017316017, "no_speech_prob": 0.0006354342913255095}, {"id": 203, "seek": 115148, "start": 1162.2, "end": 1168.24, "text": " y de vuelta con esa tabla que calcul\u00e9 vuelvo a medir las alineaciones y de vuelta con esas nuevas", "tokens": [50900, 288, 368, 41542, 416, 11342, 4421, 875, 631, 4322, 526, 20126, 3080, 257, 1205, 347, 2439, 419, 533, 9188, 288, 368, 41542, 416, 23388, 42817, 51202], "temperature": 0.0, "avg_logprob": -0.1743419329325358, "compression_ratio": 2.017316017316017, "no_speech_prob": 0.0006354342913255095}, {"id": 204, "seek": 115148, "start": 1168.24, "end": 1173.64, "text": " alineaciones vuelvo a calcular la tabla entonces aunque no me crean estos despu\u00e9s de muchas", "tokens": [51202, 419, 533, 9188, 20126, 3080, 257, 2104, 17792, 635, 4421, 875, 13003, 21962, 572, 385, 1197, 282, 12585, 15283, 368, 16072, 51472], "temperature": 0.0, "avg_logprob": -0.1743419329325358, "compression_ratio": 2.017316017316017, "no_speech_prob": 0.0006354342913255095}, {"id": 205, "seek": 115148, "start": 1173.64, "end": 1178.84, "text": " iteraciones va convergiendo a algo y parece m\u00e1gico no parece como que tal realidad si yo no", "tokens": [51472, 17138, 9188, 2773, 9652, 70, 7304, 257, 8655, 288, 14120, 12228, 70, 2789, 572, 14120, 2617, 631, 4023, 25635, 1511, 5290, 572, 51732], "temperature": 0.0, "avg_logprob": -0.1743419329325358, "compression_ratio": 2.017316017316017, "no_speech_prob": 0.0006354342913255095}, {"id": 206, "seek": 117884, "start": 1178.84, "end": 1185.4399999999998, "text": " tengo ninguno de los valores no deber\u00eda nada deber\u00eda como dar fruta pero voy a tratar de", "tokens": [50364, 13989, 17210, 12638, 368, 1750, 38790, 572, 29671, 2686, 8096, 29671, 2686, 2617, 4072, 431, 12093, 4768, 7552, 257, 42549, 368, 50694], "temperature": 0.0, "avg_logprob": -0.28495241248089337, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.00351634225808084}, {"id": 207, "seek": 117884, "start": 1185.4399999999998, "end": 1194.4399999999998, "text": " comenzarlos de que en realidad esto s\u00ed funciona con un ejemplito bien tenemos entonces vamos a", "tokens": [50694, 29564, 39734, 368, 631, 465, 25635, 7433, 8600, 26210, 416, 517, 10012, 5895, 3528, 3610, 9914, 13003, 5295, 257, 51144], "temperature": 0.0, "avg_logprob": -0.28495241248089337, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.00351634225808084}, {"id": 208, "seek": 117884, "start": 1194.4399999999998, "end": 1200.08, "text": " construir un sistema que es de traducci\u00f3n entre frances y ingl\u00e9s donde hay un cuerpo muy grande", "tokens": [51144, 38445, 517, 13245, 631, 785, 368, 2479, 1311, 5687, 3962, 431, 2676, 288, 49766, 10488, 4842, 517, 20264, 5323, 8883, 51426], "temperature": 0.0, "avg_logprob": -0.28495241248089337, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.00351634225808084}, {"id": 209, "seek": 117884, "start": 1200.08, "end": 1204.6799999999998, "text": " pero bueno vamos a concentrar solo en tres peque\u00f1as oraciones que dicen la mes\u00f3n se traduce como", "tokens": [51426, 4768, 11974, 5295, 257, 5512, 5352, 6944, 465, 15890, 19132, 32448, 420, 9188, 631, 33816, 635, 3813, 1801, 369, 2479, 4176, 2617, 51656], "temperature": 0.0, "avg_logprob": -0.28495241248089337, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.00351634225808084}, {"id": 210, "seek": 120468, "start": 1204.68, "end": 1208.72, "text": " de house la mes\u00f3n blu se traduce como de luz house y la flaus se traduce como de flauer", "tokens": [50364, 368, 1782, 635, 3813, 1801, 888, 84, 369, 2479, 4176, 2617, 368, 20671, 1782, 288, 635, 46338, 301, 369, 2479, 4176, 2617, 368, 46338, 5486, 50566], "temperature": 0.0, "avg_logprob": -0.25153591106464335, "compression_ratio": 2.1908396946564888, "no_speech_prob": 0.019897308200597763}, {"id": 211, "seek": 120468, "start": 1209.8400000000001, "end": 1213.92, "text": " entonces al principio lo que hago es decir bueno todas las traducciones entre todas las palabras", "tokens": [50622, 13003, 419, 34308, 450, 631, 38721, 785, 10235, 11974, 10906, 2439, 2479, 1311, 23469, 3962, 10906, 2439, 35240, 50826], "temperature": 0.0, "avg_logprob": -0.25153591106464335, "compression_ratio": 2.1908396946564888, "no_speech_prob": 0.019897308200597763}, {"id": 212, "seek": 120468, "start": 1213.92, "end": 1219.5600000000002, "text": " son equiprobables as\u00ed que lo que me va a quedar es cuando reparta entre las alineaciones todas", "tokens": [50826, 1872, 5037, 16614, 2965, 8582, 631, 450, 631, 385, 2773, 257, 39244, 785, 7767, 1085, 19061, 3962, 2439, 419, 533, 9188, 10906, 51108], "temperature": 0.0, "avg_logprob": -0.25153591106464335, "compression_ratio": 2.1908396946564888, "no_speech_prob": 0.019897308200597763}, {"id": 213, "seek": 120468, "start": 1219.5600000000002, "end": 1224.5800000000002, "text": " van a tener el mismo peso entre la y mes\u00f3n la probabilidad de que la se traduja como de o que se", "tokens": [51108, 3161, 257, 11640, 806, 12461, 28149, 3962, 635, 288, 3813, 1801, 635, 31959, 4580, 368, 631, 635, 369, 2479, 84, 2938, 2617, 368, 277, 631, 369, 51359], "temperature": 0.0, "avg_logprob": -0.25153591106464335, "compression_ratio": 2.1908396946564888, "no_speech_prob": 0.019897308200597763}, {"id": 214, "seek": 120468, "start": 1224.5800000000002, "end": 1228.4, "text": " traduja como house va a ser la misma en realidad porque todas las alineaciones son equiprobables", "tokens": [51359, 2479, 84, 2938, 2617, 1782, 2773, 257, 816, 635, 24946, 465, 25635, 4021, 10906, 2439, 419, 533, 9188, 1872, 5037, 16614, 2965, 51550], "temperature": 0.0, "avg_logprob": -0.25153591106464335, "compression_ratio": 2.1908396946564888, "no_speech_prob": 0.019897308200597763}, {"id": 215, "seek": 120468, "start": 1228.4, "end": 1233.5800000000002, "text": " en la mes\u00f3n blu tambi\u00e9n va a ser lo mismo la probabilidad de traducirla como de como blu o como", "tokens": [51550, 465, 635, 3813, 1801, 888, 84, 6407, 2773, 257, 816, 450, 12461, 635, 31959, 4580, 368, 2479, 1311, 347, 875, 2617, 368, 2617, 888, 84, 277, 2617, 51809], "temperature": 0.0, "avg_logprob": -0.25153591106464335, "compression_ratio": 2.1908396946564888, "no_speech_prob": 0.019897308200597763}, {"id": 216, "seek": 123358, "start": 1233.58, "end": 1239.3, "text": " house va a ser la misma y en la flauer pasa igual entonces eso es la primera", "tokens": [50364, 1782, 2773, 257, 816, 635, 24946, 288, 465, 635, 46338, 5486, 20260, 10953, 13003, 7287, 785, 635, 17382, 50650], "temperature": 0.0, "avg_logprob": -0.230516603257921, "compression_ratio": 1.5, "no_speech_prob": 0.0009854400996118784}, {"id": 217, "seek": 123358, "start": 1243.22, "end": 1248.1, "text": " el primer paso digamos en el primer paso yo voy a tener todas las alineaciones equiprobables y", "tokens": [50846, 806, 12595, 29212, 36430, 465, 806, 12595, 29212, 5290, 7552, 257, 11640, 10906, 2439, 419, 533, 9188, 5037, 16614, 2965, 288, 51090], "temperature": 0.0, "avg_logprob": -0.230516603257921, "compression_ratio": 1.5, "no_speech_prob": 0.0009854400996118784}, {"id": 218, "seek": 124810, "start": 1248.1, "end": 1250.62, "text": " todas las los valores de las palabras iguales", "tokens": [50364, 10906, 2439, 1750, 38790, 368, 2439, 35240, 10953, 279, 50490], "temperature": 0.0, "avg_logprob": -0.3033892209412622, "compression_ratio": 1.5222929936305734, "no_speech_prob": 0.0018960003508254886}, {"id": 219, "seek": 124810, "start": 1263.58, "end": 1271.1799999999998, "text": " entonces en mi algoritmo yo empec\u00e9 con una tabla de traducci\u00f3n que era toda uniforme digamos", "tokens": [51138, 13003, 465, 2752, 3501, 50017, 3280, 5290, 846, 494, 13523, 416, 2002, 4421, 875, 368, 2479, 1311, 5687, 631, 4249, 11687, 9452, 68, 36430, 51518], "temperature": 0.0, "avg_logprob": -0.3033892209412622, "compression_ratio": 1.5222929936305734, "no_speech_prob": 0.0018960003508254886}, {"id": 220, "seek": 124810, "start": 1271.1799999999998, "end": 1276.9399999999998, "text": " yo ten\u00eda la probabilidad de traducir cualquier palabra en cualquier otra era la misma a partir de", "tokens": [51518, 5290, 23718, 635, 31959, 4580, 368, 2479, 1311, 347, 21004, 31702, 465, 21004, 13623, 4249, 635, 24946, 257, 13906, 368, 51806], "temperature": 0.0, "avg_logprob": -0.3033892209412622, "compression_ratio": 1.5222929936305734, "no_speech_prob": 0.0018960003508254886}, {"id": 221, "seek": 127694, "start": 1276.94, "end": 1281.38, "text": " eso yo me constru\u00ed estas alineaciones que tambi\u00e9n parece que son todas equiprobables y parece", "tokens": [50364, 7287, 5290, 385, 12946, 870, 13897, 419, 533, 9188, 631, 6407, 14120, 631, 1872, 10906, 5037, 16614, 2965, 288, 14120, 50586], "temperature": 0.0, "avg_logprob": -0.1752391483472741, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.004941937513649464}, {"id": 222, "seek": 127694, "start": 1281.38, "end": 1286.02, "text": " que no tienen como mucha informaci\u00f3n entonces lo que voy a hacer ahora a partir de esto es tratar", "tokens": [50586, 631, 572, 12536, 2617, 25248, 21660, 13003, 450, 631, 7552, 257, 6720, 9923, 257, 13906, 368, 7433, 785, 42549, 50818], "temperature": 0.0, "avg_logprob": -0.1752391483472741, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.004941937513649464}, {"id": 223, "seek": 127694, "start": 1286.02, "end": 1290.26, "text": " de construirme de vuelta la tabla de traducciones pero mirando estas nuevas alineaciones que hay", "tokens": [50818, 368, 38445, 1398, 368, 41542, 635, 4421, 875, 368, 2479, 1311, 23469, 4768, 3149, 1806, 13897, 42817, 419, 533, 9188, 631, 4842, 51030], "temperature": 0.0, "avg_logprob": -0.1752391483472741, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.004941937513649464}, {"id": 224, "seek": 127694, "start": 1290.26, "end": 1296.5800000000002, "text": " entonces lo que voy a construir es una tabla que tiene todas las palabras de la de franc\u00e9s tiene", "tokens": [51030, 13003, 450, 631, 7552, 257, 38445, 785, 2002, 4421, 875, 631, 7066, 10906, 2439, 35240, 368, 635, 368, 30514, 2191, 7066, 51346], "temperature": 0.0, "avg_logprob": -0.1752391483472741, "compression_ratio": 1.8093023255813954, "no_speech_prob": 0.004941937513649464}, {"id": 225, "seek": 129658, "start": 1296.58, "end": 1314.6999999999998, "text": " la mes\u00f3n blu flado y de house blu flado y para llenar esta nueva tabla lo que tengo que hacer es", "tokens": [50364, 635, 3813, 1801, 888, 84, 932, 1573, 288, 368, 1782, 888, 84, 932, 1573, 288, 1690, 4849, 268, 289, 5283, 28963, 4421, 875, 450, 631, 13989, 631, 6720, 785, 51270], "temperature": 0.0, "avg_logprob": -0.26419814614688647, "compression_ratio": 1.7738095238095237, "no_speech_prob": 0.0057069952599704266}, {"id": 226, "seek": 129658, "start": 1314.6999999999998, "end": 1319.6599999999999, "text": " iterar sobre las alineaciones mirar cada una de las palabras cuantas veces esta alinear con las", "tokens": [51270, 17138, 289, 5473, 2439, 419, 533, 9188, 3149, 289, 8411, 2002, 368, 2439, 35240, 2702, 49153, 17054, 5283, 419, 533, 289, 416, 2439, 51518], "temperature": 0.0, "avg_logprob": -0.26419814614688647, "compression_ratio": 1.7738095238095237, "no_speech_prob": 0.0057069952599704266}, {"id": 227, "seek": 129658, "start": 1319.6599999999999, "end": 1325.8999999999999, "text": " otras y contar o sea y digamos y sumar los pesos de cada una de las alineaciones entonces la alineaci\u00f3n", "tokens": [51518, 20244, 288, 27045, 277, 4158, 288, 36430, 288, 2408, 289, 1750, 33204, 368, 8411, 2002, 368, 2439, 419, 533, 9188, 13003, 635, 419, 533, 3482, 51830], "temperature": 0.0, "avg_logprob": -0.26419814614688647, "compression_ratio": 1.7738095238095237, "no_speech_prob": 0.0057069952599704266}, {"id": 228, "seek": 132590, "start": 1325.9, "end": 1331.26, "text": " entre la y de en total mirando ese ejemplo de corpus cu\u00e1nto me dar\u00eda de c\u00f3mo cu\u00e1l ser\u00eda el", "tokens": [50364, 3962, 635, 288, 368, 465, 3217, 3149, 1806, 10167, 13358, 368, 1181, 31624, 44256, 78, 385, 4072, 2686, 368, 12826, 44318, 23679, 806, 50632], "temperature": 0.0, "avg_logprob": -0.18464736938476561, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.004239868838340044}, {"id": 229, "seek": 132590, "start": 1331.26, "end": 1338.7, "text": " peso de esa alineaci\u00f3n para verlo en realidad lo que hago es contar miro cu\u00e1ntas veces la y de", "tokens": [50632, 28149, 368, 11342, 419, 533, 3482, 1690, 1306, 752, 465, 25635, 450, 631, 38721, 785, 27045, 2752, 340, 44256, 296, 17054, 635, 288, 368, 51004], "temperature": 0.0, "avg_logprob": -0.18464736938476561, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.004239868838340044}, {"id": 230, "seek": 132590, "start": 1338.7, "end": 1346.18, "text": " est\u00e1n alineados entonces tengo 0.5 de peso en la primera en la segunda tengo 0.33 y en la \u00faltima tengo", "tokens": [51004, 10368, 419, 533, 4181, 13003, 13989, 1958, 13, 20, 368, 28149, 465, 635, 17382, 465, 635, 21978, 13989, 1958, 13, 10191, 288, 465, 635, 28118, 13989, 51378], "temperature": 0.0, "avg_logprob": -0.18464736938476561, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.004239868838340044}, {"id": 231, "seek": 132590, "start": 1346.18, "end": 1355.26, "text": " 0.5 de vuelta as\u00ed que en total tengo como 1.33 de peso entre la y de despu\u00e9s miro entre la y house", "tokens": [51378, 1958, 13, 20, 368, 41542, 8582, 631, 465, 3217, 13989, 2617, 502, 13, 10191, 368, 28149, 3962, 635, 288, 368, 15283, 2752, 340, 3962, 635, 288, 1782, 51832], "temperature": 0.0, "avg_logprob": -0.18464736938476561, "compression_ratio": 1.7767857142857142, "no_speech_prob": 0.004239868838340044}, {"id": 232, "seek": 135526, "start": 1355.26, "end": 1363.66, "text": " cuantos peso tengo cu\u00e1nta masa de probabilidad tengo bueno tengo 0.5 en la primera relaci\u00f3n 0.33 en", "tokens": [50364, 2702, 394, 329, 28149, 13989, 44256, 64, 29216, 368, 31959, 4580, 13989, 11974, 13989, 1958, 13, 20, 465, 635, 17382, 37247, 1958, 13, 10191, 465, 50784], "temperature": 0.0, "avg_logprob": -0.2644239970615932, "compression_ratio": 1.6168831168831168, "no_speech_prob": 0.0031312548089772463}, {"id": 233, "seek": 135526, "start": 1363.66, "end": 1371.22, "text": " la segunda y nada en la tercera por lo tanto en total tengo 0.83 de probabilidades entre la y house", "tokens": [50784, 635, 21978, 288, 8096, 465, 635, 1796, 41034, 1515, 450, 10331, 465, 3217, 13989, 1958, 13, 31849, 368, 31959, 10284, 3962, 635, 288, 1782, 51162], "temperature": 0.0, "avg_logprob": -0.2644239970615932, "compression_ratio": 1.6168831168831168, "no_speech_prob": 0.0031312548089772463}, {"id": 234, "seek": 135526, "start": 1371.22, "end": 1375.3, "text": " despu\u00e9s miro entre la y blu cuantos peso tengo", "tokens": [51162, 15283, 2752, 340, 3962, 635, 288, 888, 84, 2702, 394, 329, 28149, 13989, 51366], "temperature": 0.0, "avg_logprob": -0.2644239970615932, "compression_ratio": 1.6168831168831168, "no_speech_prob": 0.0031312548089772463}, {"id": 235, "seek": 137530, "start": 1375.3, "end": 1385.34, "text": " 0.33 solo solamente 0.33 solo est\u00e1 en la del med y entre la y flero cu\u00e1nto tengo no entre la", "tokens": [50364, 1958, 13, 10191, 6944, 27814, 1958, 13, 10191, 6944, 3192, 465, 635, 1103, 1205, 288, 3962, 635, 288, 932, 2032, 44256, 78, 13989, 572, 3962, 635, 50866], "temperature": 0.0, "avg_logprob": -0.39289283752441406, "compression_ratio": 1.5289256198347108, "no_speech_prob": 0.026217449456453323}, {"id": 236, "seek": 137530, "start": 1385.34, "end": 1391.54, "text": " y flower cu\u00e1nto tengo 0.5 solo aparecen la del final bien como tenemos la siguiente entre", "tokens": [50866, 288, 932, 968, 44256, 78, 13989, 1958, 13, 20, 6944, 15004, 13037, 635, 1103, 2572, 3610, 2617, 9914, 635, 25666, 3962, 51176], "temperature": 0.0, "avg_logprob": -0.39289283752441406, "compression_ratio": 1.5289256198347108, "no_speech_prob": 0.026217449456453323}, {"id": 237, "seek": 139154, "start": 1391.54, "end": 1401.8999999999999, "text": " emes\u00f3n y de cu\u00e1nto tendr\u00eda 0.83 est\u00e1 en la primera en la segunda entre emes\u00f3n y", "tokens": [50364, 846, 279, 1801, 288, 368, 44256, 78, 3928, 37183, 1958, 13, 31849, 3192, 465, 635, 17382, 465, 635, 21978, 3962, 846, 279, 1801, 288, 50882], "temperature": 0.0, "avg_logprob": -0.2579986060537943, "compression_ratio": 1.7098765432098766, "no_speech_prob": 0.01776689849793911}, {"id": 238, "seek": 139154, "start": 1401.8999999999999, "end": 1415.26, "text": " house entre emes\u00f3n y house s\u00ed 0.83 porque aparecen en las dos bien entre emes\u00f3n y blu solamente", "tokens": [50882, 1782, 3962, 846, 279, 1801, 288, 1782, 8600, 1958, 13, 31849, 4021, 15004, 13037, 465, 2439, 4491, 3610, 3962, 846, 279, 1801, 288, 888, 84, 27814, 51550], "temperature": 0.0, "avg_logprob": -0.2579986060537943, "compression_ratio": 1.7098765432098766, "no_speech_prob": 0.01776689849793911}, {"id": 239, "seek": 139154, "start": 1415.26, "end": 1420.62, "text": " aparecen la segunda as\u00ed que voy a tener 0.33 y entre emes\u00f3n y flower no tengo nada despu\u00e9s", "tokens": [51550, 15004, 13037, 635, 21978, 8582, 631, 7552, 257, 11640, 1958, 13, 10191, 288, 3962, 846, 279, 1801, 288, 932, 968, 572, 13989, 8096, 15283, 51818], "temperature": 0.0, "avg_logprob": -0.2579986060537943, "compression_ratio": 1.7098765432098766, "no_speech_prob": 0.01776689849793911}, {"id": 240, "seek": 142062, "start": 1420.62, "end": 1428.06, "text": " entre blu y de solamente aparece en la segunda as\u00ed que voy a tener 0.33 entre blu y house creo que", "tokens": [50364, 3962, 888, 84, 288, 368, 27814, 37863, 465, 635, 21978, 8582, 631, 7552, 257, 11640, 1958, 13, 10191, 3962, 888, 84, 288, 1782, 14336, 631, 50736], "temperature": 0.0, "avg_logprob": -0.19694615506577764, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.004064050503075123}, {"id": 241, "seek": 142062, "start": 1428.06, "end": 1434.82, "text": " de vuelta tengo 0.33 y entre blu y blu tambi\u00e9n 0.33 y no aparece junto con flower y para despu\u00e9s para", "tokens": [50736, 368, 41542, 13989, 1958, 13, 10191, 288, 3962, 888, 84, 288, 888, 84, 6407, 1958, 13, 10191, 288, 572, 37863, 24663, 416, 932, 968, 288, 1690, 15283, 1690, 51074], "temperature": 0.0, "avg_logprob": -0.19694615506577764, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.004064050503075123}, {"id": 242, "seek": 142062, "start": 1434.82, "end": 1445.4199999999998, "text": " flower tengo 0.5 con de 0 con house 0.5 con flower bien entonces hice una pasada por todas las", "tokens": [51074, 932, 968, 13989, 1958, 13, 20, 416, 368, 1958, 416, 1782, 1958, 13, 20, 416, 932, 968, 3610, 13003, 50026, 2002, 1736, 1538, 1515, 10906, 2439, 51604], "temperature": 0.0, "avg_logprob": -0.19694615506577764, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.004064050503075123}, {"id": 243, "seek": 144542, "start": 1445.42, "end": 1450.8600000000001, "text": " alineaciones y me calcul\u00e9 cu\u00e1les son los pesos relativos de cada uno de estos pares lo", "tokens": [50364, 419, 533, 9188, 288, 385, 4322, 526, 2702, 842, 904, 1872, 1750, 33204, 21960, 329, 368, 8411, 8526, 368, 12585, 2502, 495, 450, 50636], "temperature": 0.0, "avg_logprob": -0.22157524571274267, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.08654559403657913}, {"id": 244, "seek": 144542, "start": 1450.8600000000001, "end": 1454.98, "text": " siguiente que hago como esto va a ser una probabilidad es normalizar entonces me voy a construir una", "tokens": [50636, 25666, 631, 38721, 2617, 7433, 2773, 257, 816, 2002, 31959, 4580, 785, 2710, 9736, 13003, 385, 7552, 257, 38445, 2002, 50842], "temperature": 0.0, "avg_logprob": -0.22157524571274267, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.08654559403657913}, {"id": 245, "seek": 144542, "start": 1454.98, "end": 1460.66, "text": " tabla digamos normalizando por digamos voy a sumar en cada fila y voy a dividir entre la cantidad", "tokens": [50842, 4421, 875, 36430, 2710, 590, 1806, 1515, 36430, 7552, 257, 2408, 289, 465, 8411, 1387, 64, 288, 7552, 257, 4996, 347, 3962, 635, 33757, 51126], "temperature": 0.0, "avg_logprob": -0.22157524571274267, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.08654559403657913}, {"id": 246, "seek": 144542, "start": 1460.66, "end": 1466.6200000000001, "text": " que aparece para cada fila as\u00ed que de vuelta tambi\u00e9n construye la tabla que me queda la me son", "tokens": [51126, 631, 37863, 1690, 8411, 1387, 64, 8582, 631, 368, 41542, 6407, 12946, 1200, 635, 4421, 875, 631, 385, 23314, 635, 385, 1872, 51424], "temperature": 0.0, "avg_logprob": -0.22157524571274267, "compression_ratio": 1.7219730941704037, "no_speech_prob": 0.08654559403657913}, {"id": 247, "seek": 146662, "start": 1466.62, "end": 1485.6999999999998, "text": " blu y de este lado de la house ac\u00e1 de house blu flower y lo que voy a hacer normalizar entonces", "tokens": [50364, 888, 84, 288, 368, 4065, 11631, 368, 635, 1782, 23496, 368, 1782, 888, 84, 932, 968, 288, 450, 631, 7552, 257, 6720, 2710, 9736, 13003, 51318], "temperature": 0.0, "avg_logprob": -0.25655323604367813, "compression_ratio": 1.4488188976377954, "no_speech_prob": 0.0022709735203534365}, {"id": 248, "seek": 146662, "start": 1485.6999999999998, "end": 1493.02, "text": " si yo sumo estos de ac\u00e1 creo que me da 2 en total no 3 en total tengo los valores ac\u00e1", "tokens": [51318, 1511, 5290, 2408, 78, 12585, 368, 23496, 14336, 631, 385, 1120, 568, 465, 3217, 572, 805, 465, 3217, 13989, 1750, 38790, 23496, 51684], "temperature": 0.0, "avg_logprob": -0.25655323604367813, "compression_ratio": 1.4488188976377954, "no_speech_prob": 0.0022709735203534365}, {"id": 249, "seek": 149302, "start": 1493.02, "end": 1499.22, "text": " no tiene que hacer los c\u00e1lculos pero s\u00ed me da 3 en total entonces lo que pasa cuando yo normalizo", "tokens": [50364, 572, 7066, 631, 6720, 1750, 6476, 75, 32397, 4768, 8600, 385, 1120, 805, 465, 3217, 13003, 450, 631, 20260, 7767, 5290, 2710, 19055, 50674], "temperature": 0.0, "avg_logprob": -0.20210463501686274, "compression_ratio": 1.6845238095238095, "no_speech_prob": 0.004295019898563623}, {"id": 250, "seek": 149302, "start": 1499.22, "end": 1509.06, "text": " es que ac\u00e1 me queda 0.44 ac\u00e1 me queda 0.28 ac\u00e1 me queda 0.11 y ac\u00e1 me queda 0.17 pues el", "tokens": [50674, 785, 631, 23496, 385, 23314, 1958, 13, 13912, 23496, 385, 23314, 1958, 13, 11205, 23496, 385, 23314, 1958, 13, 5348, 288, 23496, 385, 23314, 1958, 13, 7773, 11059, 806, 51166], "temperature": 0.0, "avg_logprob": -0.20210463501686274, "compression_ratio": 1.6845238095238095, "no_speech_prob": 0.004295019898563623}, {"id": 251, "seek": 149302, "start": 1509.06, "end": 1517.98, "text": " segundo tambi\u00e9n lo normalizo esta vez entre dos y me queda 0.42 0.42 0.16 0 el tercero ya", "tokens": [51166, 17954, 6407, 450, 2710, 19055, 5283, 5715, 3962, 4491, 288, 385, 23314, 1958, 13, 15628, 1958, 13, 15628, 1958, 13, 6866, 1958, 806, 38103, 78, 2478, 51612], "temperature": 0.0, "avg_logprob": -0.20210463501686274, "compression_ratio": 1.6845238095238095, "no_speech_prob": 0.004295019898563623}, {"id": 252, "seek": 151798, "start": 1517.98, "end": 1531.18, "text": " suma 1 as\u00ed que me queda 0.23 0.33 0.33 0 y el \u00faltimo tambi\u00e9n queda igual 0.5 0 0 0.5 bien", "tokens": [50364, 2408, 64, 502, 8582, 631, 385, 23314, 1958, 13, 9356, 1958, 13, 10191, 1958, 13, 10191, 1958, 288, 806, 21013, 6407, 23314, 10953, 1958, 13, 20, 1958, 1958, 1958, 13, 20, 3610, 51024], "temperature": 0.0, "avg_logprob": -0.2121417166172773, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.0025298106484115124}, {"id": 253, "seek": 151798, "start": 1531.18, "end": 1539.74, "text": " entonces me constru\u00ed una nueva tabla de probabilidad de traducci\u00f3n dado que ahora las alineaciones", "tokens": [51024, 13003, 385, 12946, 870, 2002, 28963, 4421, 875, 368, 31959, 4580, 368, 2479, 1311, 5687, 29568, 631, 9923, 2439, 419, 533, 9188, 51452], "temperature": 0.0, "avg_logprob": -0.2121417166172773, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.0025298106484115124}, {"id": 254, "seek": 151798, "start": 1539.74, "end": 1545.8600000000001, "text": " ser\u00edan estas y no tenlo que pas\u00f3 ac\u00e1 si yo miro la fila correspondiente a la que es lo que", "tokens": [51452, 816, 11084, 13897, 288, 572, 2064, 752, 631, 41382, 23496, 1511, 5290, 3149, 78, 635, 1387, 64, 6805, 8413, 257, 635, 631, 785, 450, 631, 51758], "temperature": 0.0, "avg_logprob": -0.2121417166172773, "compression_ratio": 1.4568527918781726, "no_speech_prob": 0.0025298106484115124}, {"id": 255, "seek": 154586, "start": 1545.86, "end": 1556.2199999999998, "text": " pasa ahora con esta fila recuerde que yo empec\u00e9 teniendo todas las alineaciones todas las", "tokens": [50364, 20260, 9923, 416, 5283, 1387, 64, 39092, 1479, 631, 5290, 846, 494, 13523, 2064, 7304, 10906, 2439, 419, 533, 9188, 10906, 2439, 50882], "temperature": 0.0, "avg_logprob": -0.33153918295195606, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.011309586465358734}, {"id": 256, "seek": 154586, "start": 1556.2199999999998, "end": 1559.4199999999998, "text": " traducciones pero todas las probabilidades de traducci\u00f3n de que parecen palabras eran", "tokens": [50882, 2479, 1311, 23469, 4768, 10906, 2439, 31959, 10284, 368, 2479, 1311, 5687, 368, 631, 7448, 13037, 35240, 32762, 51042], "temperature": 0.0, "avg_logprob": -0.33153918295195606, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.011309586465358734}, {"id": 257, "seek": 154586, "start": 1559.4199999999998, "end": 1563.2199999999998, "text": " equiprobables si yo ahora miro la fila de la que es lo que pasa", "tokens": [51042, 5037, 16614, 2965, 1511, 5290, 9923, 3149, 78, 635, 1387, 64, 368, 635, 631, 785, 450, 631, 20260, 51232], "temperature": 0.0, "avg_logprob": -0.33153918295195606, "compression_ratio": 1.6620689655172414, "no_speech_prob": 0.011309586465358734}, {"id": 258, "seek": 156322, "start": 1563.22, "end": 1579.26, "text": " exacto aparece claramente que la asociaci\u00f3n entre la id es m\u00e1s fuerte tengo un 0.44 de probabilidad de", "tokens": [50364, 1900, 78, 37863, 6093, 3439, 631, 635, 382, 78, 537, 3482, 3962, 635, 4496, 785, 3573, 37129, 13989, 517, 1958, 13, 13912, 368, 31959, 4580, 368, 51166], "temperature": 0.0, "avg_logprob": -0.1860292210298426, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.07176174968481064}, {"id": 259, "seek": 156322, "start": 1579.26, "end": 1585.14, "text": " traducirla como de y tengo bastante menos en los otros tengo 0.28 0.11 0.17 y yo hab\u00eda", "tokens": [51166, 2479, 1311, 347, 875, 2617, 368, 288, 13989, 14651, 8902, 465, 1750, 16422, 13989, 1958, 13, 11205, 1958, 13, 5348, 1958, 13, 7773, 288, 5290, 16395, 51460], "temperature": 0.0, "avg_logprob": -0.1860292210298426, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.07176174968481064}, {"id": 260, "seek": 156322, "start": 1585.14, "end": 1589.78, "text": " empezado diciendo que eran equiprobables entonces yo probablemente ten\u00eda 0.25 0.25 0.25 0.25", "tokens": [51460, 18730, 1573, 42797, 631, 32762, 5037, 16614, 2965, 13003, 5290, 21759, 4082, 23718, 1958, 13, 6074, 1958, 13, 6074, 1958, 13, 6074, 1958, 13, 6074, 51692], "temperature": 0.0, "avg_logprob": -0.1860292210298426, "compression_ratio": 1.5132275132275133, "no_speech_prob": 0.07176174968481064}, {"id": 261, "seek": 158978, "start": 1590.34, "end": 1600.02, "text": " cada una y despu\u00e9s de un paso de la iteraci\u00f3n descubri\u00f3 que la id tienen m\u00e1s chance de ser una", "tokens": [50392, 8411, 2002, 288, 15283, 368, 517, 29212, 368, 635, 17138, 3482, 32592, 44802, 631, 635, 4496, 12536, 3573, 2931, 368, 816, 2002, 50876], "temperature": 0.0, "avg_logprob": -0.18881869316101074, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.005155446007847786}, {"id": 262, "seek": 158978, "start": 1600.02, "end": 1606.42, "text": " traducci\u00f3n de la otra en vez de traducirla como chaos o la como blue o la como flower eso pasa en", "tokens": [50876, 2479, 1311, 5687, 368, 635, 13623, 465, 5715, 368, 2479, 1311, 347, 875, 2617, 14158, 277, 635, 2617, 3344, 277, 635, 2617, 8617, 7287, 20260, 465, 51196], "temperature": 0.0, "avg_logprob": -0.18881869316101074, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.005155446007847786}, {"id": 263, "seek": 158978, "start": 1606.42, "end": 1612.02, "text": " el primer paso en la primera iteraci\u00f3n el tipo descubre el algoritmo descubre que la asociaci\u00f3n", "tokens": [51196, 806, 12595, 29212, 465, 635, 17382, 17138, 3482, 806, 9746, 32592, 265, 806, 3501, 50017, 3280, 32592, 265, 631, 635, 382, 78, 537, 3482, 51476], "temperature": 0.0, "avg_logprob": -0.18881869316101074, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.005155446007847786}, {"id": 264, "seek": 158978, "start": 1612.02, "end": 1619.02, "text": " entre la id es bastante m\u00e1s fuerte como pasa eso lo que va a pasar es que cuando yo reparta de", "tokens": [51476, 3962, 635, 4496, 785, 14651, 3573, 37129, 2617, 20260, 7287, 450, 631, 2773, 257, 25344, 785, 631, 7767, 5290, 1085, 19061, 368, 51826], "temperature": 0.0, "avg_logprob": -0.18881869316101074, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.005155446007847786}, {"id": 265, "seek": 161902, "start": 1619.02, "end": 1624.06, "text": " vuelta en las alineaciones estas l\u00edneas que se corresponden a la asociaci\u00f3n entre la id van a", "tokens": [50364, 41542, 465, 2439, 419, 533, 9188, 13897, 16118, 716, 296, 631, 369, 6805, 268, 257, 635, 382, 78, 537, 3482, 3962, 635, 4496, 3161, 257, 50616], "temperature": 0.0, "avg_logprob": -0.15651722808382404, "compression_ratio": 1.983739837398374, "no_speech_prob": 0.002213660627603531}, {"id": 266, "seek": 161902, "start": 1624.06, "end": 1629.66, "text": " estar m\u00e1s fuertes van a tener un poco m\u00e1s de peso y como esto es una distribuci\u00f3n de probabilidades", "tokens": [50616, 8755, 3573, 8536, 911, 279, 3161, 257, 11640, 517, 10639, 3573, 368, 28149, 288, 2617, 7433, 785, 2002, 4400, 30813, 368, 31959, 10284, 50896], "temperature": 0.0, "avg_logprob": -0.15651722808382404, "compression_ratio": 1.983739837398374, "no_speech_prob": 0.002213660627603531}, {"id": 267, "seek": 161902, "start": 1629.66, "end": 1634.98, "text": " esa masa que gan\u00f3 la asociaci\u00f3n entre la id se va a tener que sacar de otras alineaciones", "tokens": [50896, 11342, 29216, 631, 7574, 812, 635, 382, 78, 537, 3482, 3962, 635, 4496, 369, 2773, 257, 11640, 631, 43823, 368, 20244, 419, 533, 9188, 51162], "temperature": 0.0, "avg_logprob": -0.15651722808382404, "compression_ratio": 1.983739837398374, "no_speech_prob": 0.002213660627603531}, {"id": 268, "seek": 161902, "start": 1634.98, "end": 1638.98, "text": " posibles o sea si la est\u00e1 asociada con de entonces no est\u00e1 asociada con las otras que est\u00e1n", "tokens": [51162, 1366, 14428, 277, 4158, 1511, 635, 3192, 382, 78, 537, 1538, 416, 368, 13003, 572, 3192, 382, 78, 537, 1538, 416, 2439, 20244, 631, 10368, 51362], "temperature": 0.0, "avg_logprob": -0.15651722808382404, "compression_ratio": 1.983739837398374, "no_speech_prob": 0.002213660627603531}, {"id": 269, "seek": 161902, "start": 1638.98, "end": 1647.18, "text": " alrededor entonces esa masa que se pierde digamos o sea que que gana en la de se tiene que repartir en", "tokens": [51362, 43663, 13003, 11342, 29216, 631, 369, 9766, 1479, 36430, 277, 4158, 631, 631, 290, 2095, 465, 635, 368, 369, 7066, 631, 1085, 446, 347, 465, 51772], "temperature": 0.0, "avg_logprob": -0.15651722808382404, "compression_ratio": 1.983739837398374, "no_speech_prob": 0.002213660627603531}, {"id": 270, "seek": 164718, "start": 1647.18, "end": 1654.1000000000001, "text": " las otras alineaciones posibles o sea en las que no son entre la id entonces despu\u00e9s de una", "tokens": [50364, 2439, 20244, 419, 533, 9188, 1366, 14428, 277, 4158, 465, 2439, 631, 572, 1872, 3962, 635, 4496, 13003, 15283, 368, 2002, 50710], "temperature": 0.0, "avg_logprob": -0.20613927180224126, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.0009255260229110718}, {"id": 271, "seek": 164718, "start": 1654.1000000000001, "end": 1662.0600000000002, "text": " iteraci\u00f3n la asociaci\u00f3n entre la id empieza a ser m\u00e1s fuerte y como pasa eso en la siguiente", "tokens": [50710, 17138, 3482, 635, 382, 78, 537, 3482, 3962, 635, 4496, 44577, 257, 816, 3573, 37129, 288, 2617, 20260, 7287, 465, 635, 25666, 51108], "temperature": 0.0, "avg_logprob": -0.20613927180224126, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.0009255260229110718}, {"id": 272, "seek": 164718, "start": 1662.0600000000002, "end": 1666.94, "text": " iteraci\u00f3n va a empezar a descubrir que como la estaba alineado con de entonces me son tiene que", "tokens": [51108, 17138, 3482, 2773, 257, 31168, 257, 32592, 10949, 631, 2617, 635, 17544, 419, 533, 1573, 416, 368, 13003, 385, 1872, 7066, 631, 51352], "temperature": 0.0, "avg_logprob": -0.20613927180224126, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.0009255260229110718}, {"id": 273, "seek": 164718, "start": 1666.94, "end": 1674.94, "text": " estar alineado con haus y como me son esta alineado con haus digamos esa esa misma masa de probabilidades", "tokens": [51352, 8755, 419, 533, 1573, 416, 324, 301, 288, 2617, 385, 1872, 5283, 419, 533, 1573, 416, 324, 301, 36430, 11342, 11342, 24946, 29216, 368, 31959, 10284, 51752], "temperature": 0.0, "avg_logprob": -0.20613927180224126, "compression_ratio": 1.8708133971291867, "no_speech_prob": 0.0009255260229110718}, {"id": 274, "seek": 167494, "start": 1674.94, "end": 1680.7, "text": " se va a traducir a transferir a la segunda y lo mismo como le ha estado alineado con de entonces", "tokens": [50364, 369, 2773, 257, 2479, 1311, 347, 257, 5003, 347, 257, 635, 21978, 288, 450, 12461, 2617, 476, 324, 18372, 419, 533, 1573, 416, 368, 13003, 50652], "temperature": 0.0, "avg_logprob": -0.27278636483585134, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.006860236171633005}, {"id": 275, "seek": 167494, "start": 1680.7, "end": 1687.26, "text": " flea tiene que estar alineado con flauer entonces si yo sigo iterando en estos pasos en cada paso", "tokens": [50652, 7025, 64, 7066, 631, 8755, 419, 533, 1573, 416, 46338, 5486, 13003, 1511, 5290, 4556, 78, 17138, 1806, 465, 12585, 1736, 329, 465, 8411, 29212, 50980], "temperature": 0.0, "avg_logprob": -0.27278636483585134, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.006860236171633005}, {"id": 276, "seek": 167494, "start": 1687.26, "end": 1691.5, "text": " lo que va a pasar es que se va a mover un poco m\u00e1s de probabilidad hasta que al final va a terminar", "tokens": [50980, 450, 631, 2773, 257, 25344, 785, 631, 369, 2773, 257, 39945, 517, 10639, 3573, 368, 31959, 4580, 10764, 631, 419, 2572, 2773, 257, 36246, 51192], "temperature": 0.0, "avg_logprob": -0.27278636483585134, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.006860236171633005}, {"id": 277, "seek": 167494, "start": 1691.5, "end": 1697.18, "text": " descubriendo cu\u00e1l es la alineaci\u00f3n real de las palabras o sea va a descubrir que la va a", "tokens": [51192, 32592, 470, 3999, 44318, 785, 635, 419, 533, 3482, 957, 368, 2439, 35240, 277, 4158, 2773, 257, 32592, 10949, 631, 635, 2773, 257, 51476], "temperature": 0.0, "avg_logprob": -0.27278636483585134, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.006860236171633005}, {"id": 278, "seek": 167494, "start": 1697.18, "end": 1703.5, "text": " social con con de me son con haus lu con blu la ver con flauer como que va a descubrir eso porque en", "tokens": [51476, 2093, 416, 416, 368, 385, 1872, 416, 324, 301, 10438, 416, 888, 84, 635, 1306, 416, 46338, 5486, 2617, 631, 2773, 257, 32592, 10949, 7287, 4021, 465, 51792], "temperature": 0.0, "avg_logprob": -0.27278636483585134, "compression_ratio": 1.8875968992248062, "no_speech_prob": 0.006860236171633005}, {"id": 279, "seek": 170350, "start": 1703.5, "end": 1708.86, "text": " cada paso lo que va pasando es que alguna de las asociaciones como est\u00e1n como aparecen como ocurren", "tokens": [50364, 8411, 29212, 450, 631, 2773, 45412, 785, 631, 20651, 368, 2439, 382, 78, 537, 9188, 2617, 10368, 2617, 15004, 13037, 2617, 26430, 1095, 50632], "temperature": 0.0, "avg_logprob": -0.15925823421928825, "compression_ratio": 1.921875, "no_speech_prob": 0.00036850039032287896}, {"id": 280, "seek": 170350, "start": 1708.86, "end": 1713.74, "text": " digamos en m\u00e1s oraciones tiene m\u00e1s fuerza que otras entonces el peso que esas asociaciones ganan", "tokens": [50632, 36430, 465, 3573, 420, 9188, 7066, 3573, 39730, 631, 20244, 13003, 806, 28149, 631, 23388, 382, 78, 537, 9188, 7574, 282, 50876], "temperature": 0.0, "avg_logprob": -0.15925823421928825, "compression_ratio": 1.921875, "no_speech_prob": 0.00036850039032287896}, {"id": 281, "seek": 170350, "start": 1713.74, "end": 1719.34, "text": " lo va sacando otro lado y eso hace que de otro lado se empiecen a generar otras alineaciones", "tokens": [50876, 450, 2773, 4899, 1806, 11921, 11631, 288, 7287, 10032, 631, 368, 11921, 11631, 369, 4012, 414, 13037, 257, 1337, 289, 20244, 419, 533, 9188, 51156], "temperature": 0.0, "avg_logprob": -0.15925823421928825, "compression_ratio": 1.921875, "no_speech_prob": 0.00036850039032287896}, {"id": 282, "seek": 170350, "start": 1719.34, "end": 1726.46, "text": " diferentes entonces al final esto termina convergiendo y termina revelando lo que es la estructura su", "tokens": [51156, 17686, 13003, 419, 2572, 7433, 1433, 1426, 9652, 70, 7304, 288, 1433, 1426, 15262, 1806, 450, 631, 785, 635, 43935, 2991, 459, 51512], "temperature": 0.0, "avg_logprob": -0.15925823421928825, "compression_ratio": 1.921875, "no_speech_prob": 0.00036850039032287896}, {"id": 283, "seek": 170350, "start": 1726.46, "end": 1731.78, "text": " yasente de las palabras y c\u00f3mo se alinean unas con otras bueno y una vez que yo termine de hacer", "tokens": [51512, 288, 296, 1576, 368, 2439, 35240, 288, 12826, 369, 419, 533, 282, 25405, 416, 20244, 11974, 288, 2002, 5715, 631, 5290, 1433, 533, 368, 6720, 51778], "temperature": 0.0, "avg_logprob": -0.15925823421928825, "compression_ratio": 1.921875, "no_speech_prob": 0.00036850039032287896}, {"id": 284, "seek": 173178, "start": 1731.78, "end": 1737.7, "text": " esto puedo agarrar y construirme efectivamente la tabla final de traducciones que es simplemente busco cada", "tokens": [50364, 7433, 21612, 623, 2284, 289, 288, 38445, 1398, 22565, 23957, 635, 4421, 875, 2572, 368, 2479, 1311, 23469, 631, 785, 33190, 1255, 1291, 8411, 50660], "temperature": 0.0, "avg_logprob": -0.17938871837797618, "compression_ratio": 1.8878504672897196, "no_speech_prob": 0.006639114581048489}, {"id": 285, "seek": 173178, "start": 1737.7, "end": 1742.34, "text": " una de las posibles traducciones digamos de los posibles pares y saco las probabilidades", "tokens": [50660, 2002, 368, 2439, 1366, 14428, 2479, 1311, 23469, 36430, 368, 1750, 1366, 14428, 2502, 495, 288, 4899, 78, 2439, 31959, 10284, 50892], "temperature": 0.0, "avg_logprob": -0.17938871837797618, "compression_ratio": 1.8878504672897196, "no_speech_prob": 0.006639114581048489}, {"id": 286, "seek": 173178, "start": 1745.78, "end": 1750.82, "text": " y qu\u00e9 pas\u00f3 ac\u00e1 mientras yo estaba construyendo mi modelo de traducci\u00f3n mientras yo estaba construyendo", "tokens": [51064, 288, 8057, 41382, 23496, 26010, 5290, 17544, 12946, 88, 3999, 2752, 27825, 368, 2479, 1311, 5687, 26010, 5290, 17544, 12946, 88, 3999, 51316], "temperature": 0.0, "avg_logprob": -0.17938871837797618, "compression_ratio": 1.8878504672897196, "no_speech_prob": 0.006639114581048489}, {"id": 287, "seek": 173178, "start": 1750.82, "end": 1757.66, "text": " la tabla de traducciones adem\u00e1s de como efecto secundario se construyo un corbuz alineado un corbuz", "tokens": [51316, 635, 4421, 875, 368, 2479, 1311, 23469, 21251, 368, 2617, 46783, 907, 997, 4912, 369, 12946, 8308, 517, 1181, 65, 3334, 419, 533, 1573, 517, 1181, 65, 3334, 51658], "temperature": 0.0, "avg_logprob": -0.17938871837797618, "compression_ratio": 1.8878504672897196, "no_speech_prob": 0.006639114581048489}, {"id": 288, "seek": 175766, "start": 1757.66, "end": 1766.8600000000001, "text": " que est\u00e1 alineado a nivel de palabra as\u00ed que bueno el algoritmo de expectations maximizaciones", "tokens": [50364, 631, 3192, 419, 533, 1573, 257, 24423, 368, 31702, 8582, 631, 11974, 806, 3501, 50017, 3280, 368, 2066, 763, 5138, 590, 9188, 50824], "temperature": 0.0, "avg_logprob": -0.2553615061442057, "compression_ratio": 1.8074534161490683, "no_speech_prob": 0.0034659963566809893}, {"id": 289, "seek": 175766, "start": 1770.3000000000002, "end": 1774.5, "text": " funciona de esa manera tiene siempre dos pasos un paso de expectations y un paso de maximizaciones", "tokens": [50996, 26210, 368, 11342, 13913, 7066, 12758, 4491, 1736, 329, 517, 29212, 368, 2066, 763, 288, 517, 29212, 368, 5138, 590, 9188, 51206], "temperature": 0.0, "avg_logprob": -0.2553615061442057, "compression_ratio": 1.8074534161490683, "no_speech_prob": 0.0034659963566809893}, {"id": 290, "seek": 175766, "start": 1776.5, "end": 1783.3400000000001, "text": " en este caso la expectation era decir el paso de expectations es tratado de agarrar la tabla de", "tokens": [51306, 465, 4065, 9666, 635, 2066, 399, 4249, 10235, 806, 29212, 368, 2066, 763, 785, 21507, 1573, 368, 623, 2284, 289, 635, 4421, 875, 368, 51648], "temperature": 0.0, "avg_logprob": -0.2553615061442057, "compression_ratio": 1.8074534161490683, "no_speech_prob": 0.0034659963566809893}, {"id": 291, "seek": 178334, "start": 1784.3, "end": 1789.6599999999999, "text": " probabilidad de traducci\u00f3n que tengo y con eso me arm\u00f3 alineaciones y despu\u00e9s el de maximizaci\u00f3n", "tokens": [50412, 31959, 4580, 368, 2479, 1311, 5687, 631, 13989, 288, 416, 7287, 385, 3726, 812, 419, 533, 9188, 288, 15283, 806, 368, 5138, 590, 3482, 50680], "temperature": 0.0, "avg_logprob": -0.20173542998557867, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.002320525236427784}, {"id": 292, "seek": 178334, "start": 1789.6599999999999, "end": 1794.1399999999999, "text": " es al rev\u00e9s agarr\u00f3 las alineaciones que acabo de construir y me arm\u00f3 una nueva tabla y voy", "tokens": [50680, 785, 419, 3698, 2191, 623, 2284, 812, 2439, 419, 533, 9188, 631, 13281, 78, 368, 38445, 288, 385, 3726, 812, 2002, 28963, 4421, 875, 288, 7552, 50904], "temperature": 0.0, "avg_logprob": -0.20173542998557867, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.002320525236427784}, {"id": 293, "seek": 178334, "start": 1794.1399999999999, "end": 1801.6599999999999, "text": " iterando todos esos pasos hasta que eventualmente converge bien dijimos que eran cinco modelos", "tokens": [50904, 17138, 1806, 6321, 22411, 1736, 329, 10764, 631, 33160, 4082, 9652, 432, 3610, 47709, 8372, 631, 32762, 21350, 2316, 329, 51280], "temperature": 0.0, "avg_logprob": -0.20173542998557867, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.002320525236427784}, {"id": 294, "seek": 178334, "start": 1801.6599999999999, "end": 1806.8999999999999, "text": " dvm no vamos a ver muy en detalle los otros o sea solo mencionar que empiezan a agregar", "tokens": [51280, 274, 85, 76, 572, 5295, 257, 1306, 5323, 465, 1141, 11780, 1750, 16422, 277, 4158, 6944, 37030, 289, 631, 4012, 18812, 282, 257, 4554, 2976, 51542], "temperature": 0.0, "avg_logprob": -0.20173542998557867, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.002320525236427784}, {"id": 295, "seek": 178334, "start": 1806.8999999999999, "end": 1812.4599999999998, "text": " complejidad en este modelo uno hab\u00edamos dicho que todas las alineaciones eran equiprobables en el", "tokens": [51542, 44424, 73, 4580, 465, 4065, 27825, 8526, 3025, 16275, 27346, 631, 10906, 2439, 419, 533, 9188, 32762, 5037, 16614, 2965, 465, 806, 51820], "temperature": 0.0, "avg_logprob": -0.20173542998557867, "compression_ratio": 1.7372262773722629, "no_speech_prob": 0.002320525236427784}, {"id": 296, "seek": 181246, "start": 1812.46, "end": 1818.06, "text": " modelo dos abandonan esa noci\u00f3n y dicen bueno en vez alineaciones equiprobables yo voy a tener un", "tokens": [50364, 27825, 4491, 9072, 282, 11342, 572, 5687, 288, 33816, 11974, 465, 5715, 419, 533, 9188, 5037, 16614, 2965, 5290, 7552, 257, 11640, 517, 50644], "temperature": 0.0, "avg_logprob": -0.22098865672054455, "compression_ratio": 1.8745098039215686, "no_speech_prob": 0.005485367029905319}, {"id": 297, "seek": 181246, "start": 1818.06, "end": 1822.3, "text": " modelo de reordenamiento de las palabras para decir bueno tengo cierta probabilidad de que las", "tokens": [50644, 27825, 368, 319, 19058, 16971, 368, 2439, 35240, 1690, 10235, 11974, 13989, 39769, 1328, 31959, 4580, 368, 631, 2439, 50856], "temperature": 0.0, "avg_logprob": -0.22098865672054455, "compression_ratio": 1.8745098039215686, "no_speech_prob": 0.005485367029905319}, {"id": 298, "seek": 181246, "start": 1822.3, "end": 1826.92, "text": " palabras que est\u00e1n si yo tengo y palabras en ingl\u00e9s jada palabras en espa\u00f1ol tengo cierta", "tokens": [50856, 35240, 631, 10368, 1511, 5290, 13989, 288, 35240, 465, 49766, 361, 1538, 35240, 465, 31177, 13989, 39769, 1328, 51087], "temperature": 0.0, "avg_logprob": -0.22098865672054455, "compression_ratio": 1.8745098039215686, "no_speech_prob": 0.005485367029905319}, {"id": 299, "seek": 181246, "start": 1826.92, "end": 1832.66, "text": " probabilidad de mover la palabra ah\u00ed y la palabra jota y bueno y as\u00ed siguen subiendo en complejidad", "tokens": [51087, 31959, 4580, 368, 39945, 635, 31702, 12571, 288, 635, 31702, 361, 5377, 288, 11974, 288, 8582, 4556, 7801, 1422, 7304, 465, 44424, 73, 4580, 51374], "temperature": 0.0, "avg_logprob": -0.22098865672054455, "compression_ratio": 1.8745098039215686, "no_speech_prob": 0.005485367029905319}, {"id": 300, "seek": 181246, "start": 1832.66, "end": 1838.42, "text": " hasta llegar al modelo 5 que el modelo 5 es el que anda mejor pero de todas maneras estos", "tokens": [51374, 10764, 24892, 419, 27825, 1025, 631, 806, 27825, 1025, 785, 806, 631, 293, 64, 11479, 4768, 368, 10906, 587, 6985, 12585, 51662], "temperature": 0.0, "avg_logprob": -0.22098865672054455, "compression_ratio": 1.8745098039215686, "no_speech_prob": 0.005485367029905319}, {"id": 301, "seek": 183842, "start": 1838.42, "end": 1845.14, "text": " modelos que ya no se usan digamos esto es del a\u00f1o 93 y en general se han obtenido mejor", "tokens": [50364, 2316, 329, 631, 2478, 572, 369, 505, 282, 36430, 7433, 785, 1103, 15984, 28876, 288, 465, 2674, 369, 7276, 28326, 2925, 11479, 50700], "temperature": 0.0, "avg_logprob": -0.21986320283677843, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.026058511808514595}, {"id": 302, "seek": 183842, "start": 1845.14, "end": 1849.8600000000001, "text": " resultados abandonando estos modelos entonces el que vamos a pasar a ver a continuaci\u00f3n es un", "tokens": [50700, 36796, 9072, 1806, 12585, 2316, 329, 13003, 806, 631, 5295, 257, 25344, 257, 1306, 257, 2993, 3482, 785, 517, 50936], "temperature": 0.0, "avg_logprob": -0.21986320283677843, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.026058511808514595}, {"id": 303, "seek": 183842, "start": 1849.8600000000001, "end": 1855.66, "text": " modelo bastante m\u00e1s moderno que es lo que s\u00ed se utiliza hoy en d\u00eda en traductores como los de google", "tokens": [50936, 27825, 14651, 3573, 4363, 78, 631, 785, 450, 631, 8600, 369, 4976, 13427, 13775, 465, 12271, 465, 2479, 11130, 2706, 2617, 1750, 368, 20742, 51226], "temperature": 0.0, "avg_logprob": -0.21986320283677843, "compression_ratio": 1.53475935828877, "no_speech_prob": 0.026058511808514595}, {"id": 304, "seek": 186842, "start": 1868.54, "end": 1873.0600000000002, "text": " es que en realidad lo claro a ver estos modelos estad\u00edsticos no utilizan ning\u00fan tipo de", "tokens": [50370, 785, 631, 465, 25635, 450, 16742, 257, 1306, 12585, 2316, 329, 39160, 19512, 9940, 572, 19906, 282, 30394, 9746, 368, 50596], "temperature": 0.0, "avg_logprob": -0.29523937761290997, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.020908264443278313}, {"id": 305, "seek": 186842, "start": 1873.0600000000002, "end": 1877.8600000000001, "text": " analizador morfuelo o ego\u00ed nada para sacarlo hay otros modelos que s\u00ed lo hacen no vamos a dar", "tokens": [50596, 2624, 590, 5409, 1896, 69, 3483, 78, 277, 14495, 870, 8096, 1690, 4899, 19457, 4842, 16422, 2316, 329, 631, 8600, 450, 27434, 572, 5295, 257, 4072, 50836], "temperature": 0.0, "avg_logprob": -0.29523937761290997, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.020908264443278313}, {"id": 306, "seek": 186842, "start": 1877.8600000000001, "end": 1883.14, "text": " ninguno en esta clase de brota hay dos modelos que s\u00ed hacen uso de esa informaci\u00f3n igual son como", "tokens": [50836, 17210, 12638, 465, 5283, 44578, 368, 738, 5377, 4842, 4491, 2316, 329, 631, 8600, 27434, 22728, 368, 11342, 21660, 10953, 1872, 2617, 51100], "temperature": 0.0, "avg_logprob": -0.29523937761290997, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.020908264443278313}, {"id": 307, "seek": 186842, "start": 1883.14, "end": 1887.5800000000002, "text": " un refinamiento creo que ninguno lo tiene como en la base del modelo el uso de de parto", "tokens": [51100, 517, 44395, 16971, 14336, 631, 17210, 12638, 450, 7066, 2617, 465, 635, 3096, 1103, 27825, 806, 22728, 368, 368, 644, 78, 51322], "temperature": 0.0, "avg_logprob": -0.29523937761290997, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.020908264443278313}, {"id": 308, "seek": 186842, "start": 1887.5800000000002, "end": 1893.26, "text": " speech pero pero s\u00ed cuando vos no sabes una palabra de una palabra que es desconocida en realidad", "tokens": [51322, 6218, 4768, 4768, 8600, 7767, 13845, 572, 37790, 2002, 31702, 368, 2002, 31702, 631, 785, 49801, 905, 2887, 465, 25635, 51606], "temperature": 0.0, "avg_logprob": -0.29523937761290997, "compression_ratio": 1.8223938223938223, "no_speech_prob": 0.020908264443278313}, {"id": 309, "seek": 189326, "start": 1893.26, "end": 1899.5, "text": " a utilizar informaci\u00f3n sobre parto speech y eso probablemente te ayude en estos modelos", "tokens": [50364, 257, 24060, 21660, 5473, 644, 78, 6218, 288, 7287, 21759, 4082, 535, 7494, 2303, 465, 12585, 2316, 329, 50676], "temperature": 0.0, "avg_logprob": -0.2742346153884638, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.004360186401754618}, {"id": 310, "seek": 189326, "start": 1899.5, "end": 1904.06, "text": " por\u00f3menos no lo hab\u00edan tenido en cuenta bien entonces si lo que vamos a ver ahora es el modelo", "tokens": [50676, 1515, 812, 2558, 329, 572, 450, 44466, 33104, 465, 17868, 3610, 13003, 1511, 450, 631, 5295, 257, 1306, 9923, 785, 806, 27825, 50904], "temperature": 0.0, "avg_logprob": -0.2742346153884638, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.004360186401754618}, {"id": 311, "seek": 189326, "start": 1904.06, "end": 1908.9, "text": " de frases que es algo m\u00e1s moderno y es o sea el google translate o bin translate se basan", "tokens": [50904, 368, 431, 1957, 631, 785, 8655, 3573, 4363, 78, 288, 785, 277, 4158, 806, 20742, 13799, 277, 5171, 13799, 369, 987, 282, 51146], "temperature": 0.0, "avg_logprob": -0.2742346153884638, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.004360186401754618}, {"id": 312, "seek": 189326, "start": 1908.9, "end": 1913.3799999999999, "text": " en modelos de este estilo y bueno y antes de ver c\u00f3mo se modelo de frases voluamos un poco a lo que", "tokens": [51146, 465, 2316, 329, 368, 4065, 37470, 288, 11974, 288, 11014, 368, 1306, 12826, 369, 2316, 78, 368, 431, 1957, 1996, 84, 2151, 517, 10639, 257, 450, 631, 51370], "temperature": 0.0, "avg_logprob": -0.2742346153884638, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.004360186401754618}, {"id": 313, "seek": 189326, "start": 1913.3799999999999, "end": 1917.82, "text": " era la alineaci\u00f3n entre palabras yo ten\u00eda esta frase cl\u00e1sica no mariano de una ofitada", "tokens": [51370, 4249, 635, 419, 533, 3482, 3962, 35240, 5290, 23718, 5283, 38406, 47434, 2262, 572, 1849, 6254, 368, 2002, 295, 270, 1538, 51592], "temperature": 0.0, "avg_logprob": -0.2742346153884638, "compression_ratio": 1.7622641509433963, "no_speech_prob": 0.004360186401754618}, {"id": 314, "seek": 191782, "start": 1917.82, "end": 1925.02, "text": " la bruja verde en ingles era mary de not slap green witch y una alineaci\u00f3n entre esas", "tokens": [50364, 635, 738, 84, 2938, 29653, 465, 3957, 904, 4249, 275, 822, 368, 406, 21075, 3092, 14867, 288, 2002, 419, 533, 3482, 3962, 23388, 50724], "temperature": 0.0, "avg_logprob": -0.330692669292828, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.10710648447275162}, {"id": 315, "seek": 191782, "start": 1925.02, "end": 1929.1399999999999, "text": " dos oraciones en realidad se ver\u00eda como algo as\u00ed yo tengo que mar\u00eda se alinea con mary no se", "tokens": [50724, 4491, 420, 9188, 465, 25635, 369, 1306, 2686, 2617, 8655, 8582, 5290, 13989, 631, 1849, 2686, 369, 419, 533, 64, 416, 275, 822, 572, 369, 50930], "temperature": 0.0, "avg_logprob": -0.330692669292828, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.10710648447275162}, {"id": 316, "seek": 191782, "start": 1929.1399999999999, "end": 1935.06, "text": " alinea con this not slap se alinea con da una ofitada de se alinea con ala podr\u00eda ser solamente", "tokens": [50930, 419, 533, 64, 416, 341, 406, 21075, 369, 419, 533, 64, 416, 1120, 2002, 295, 270, 1538, 368, 369, 419, 533, 64, 416, 419, 64, 27246, 816, 27814, 51226], "temperature": 0.0, "avg_logprob": -0.330692669292828, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.10710648447275162}, {"id": 317, "seek": 191782, "start": 1935.06, "end": 1939.54, "text": " con la y el a que no est\u00e9 alineado nada green se alinea con verde y bruja con witch", "tokens": [51226, 416, 635, 288, 806, 257, 631, 572, 34584, 419, 533, 1573, 8096, 3092, 369, 419, 533, 64, 416, 29653, 288, 738, 84, 2938, 416, 14867, 51450], "temperature": 0.0, "avg_logprob": -0.330692669292828, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.10710648447275162}, {"id": 318, "seek": 193954, "start": 1939.54, "end": 1945.1399999999999, "text": " qu\u00e9 diferencia tiene esto con la la otra alineaci\u00f3n que hab\u00edamos visto hoy", "tokens": [50364, 8057, 38844, 7066, 7433, 416, 635, 635, 13623, 419, 533, 3482, 631, 3025, 16275, 17558, 13775, 50644], "temperature": 0.0, "avg_logprob": -0.3739319443702698, "compression_ratio": 1.6054421768707483, "no_speech_prob": 0.06074190512299538}, {"id": 319, "seek": 193954, "start": 1946.18, "end": 1950.58, "text": " a ver si se les ocurre algo distinto que tiene esta alineaci\u00f3n y la que hab\u00edamos visto hoy", "tokens": [50696, 257, 1306, 1511, 369, 1512, 26430, 265, 8655, 1483, 17246, 631, 7066, 5283, 419, 533, 3482, 288, 635, 631, 3025, 16275, 17558, 13775, 50916], "temperature": 0.0, "avg_logprob": -0.3739319443702698, "compression_ratio": 1.6054421768707483, "no_speech_prob": 0.06074190512299538}, {"id": 320, "seek": 193954, "start": 1954.82, "end": 1959.78, "text": " era not con no s\u00ed y qu\u00e9 es lo que cambia ac\u00e1 para que pase eso", "tokens": [51128, 4249, 406, 416, 572, 8600, 288, 8057, 785, 450, 631, 18751, 654, 23496, 1690, 631, 47125, 7287, 51376], "temperature": 0.0, "avg_logprob": -0.3739319443702698, "compression_ratio": 1.6054421768707483, "no_speech_prob": 0.06074190512299538}, {"id": 321, "seek": 195978, "start": 1959.78, "end": 1972.62, "text": " lo que estaba pasando hoy era que yo partida de las palabras en espa\u00f1ol iba a las palabras en", "tokens": [50364, 450, 631, 17544, 45412, 13775, 4249, 631, 5290, 644, 2887, 368, 2439, 35240, 465, 31177, 33423, 257, 2439, 35240, 465, 51006], "temperature": 0.0, "avg_logprob": -0.1818128435799245, "compression_ratio": 2.0104712041884816, "no_speech_prob": 0.047026313841342926}, {"id": 322, "seek": 195978, "start": 1972.62, "end": 1976.1399999999999, "text": " ingl\u00e9s y yo ten\u00eda una funci\u00f3n que me mapeaba las palabras en espa\u00f1ol con las palabras en ingl\u00e9s", "tokens": [51006, 49766, 288, 5290, 23718, 2002, 43735, 631, 385, 463, 494, 5509, 2439, 35240, 465, 31177, 416, 2439, 35240, 465, 49766, 51182], "temperature": 0.0, "avg_logprob": -0.1818128435799245, "compression_ratio": 2.0104712041884816, "no_speech_prob": 0.047026313841342926}, {"id": 323, "seek": 195978, "start": 1976.1399999999999, "end": 1980.54, "text": " entonces yo a cada palabra en espa\u00f1ol como m\u00e1ximo le pod\u00eda hacer corresponder una palabra en", "tokens": [51182, 13003, 5290, 257, 8411, 31702, 465, 31177, 2617, 38876, 476, 45588, 6720, 6805, 260, 2002, 31702, 465, 51402], "temperature": 0.0, "avg_logprob": -0.1818128435799245, "compression_ratio": 2.0104712041884816, "no_speech_prob": 0.047026313841342926}, {"id": 324, "seek": 195978, "start": 1980.54, "end": 1985.86, "text": " ingl\u00e9s entonces me quedaba que yo pod\u00eda expresar cosas como que daba una ofitada daba esta", "tokens": [51402, 49766, 13003, 385, 13617, 5509, 631, 5290, 45588, 33397, 289, 12218, 2617, 631, 274, 5509, 2002, 295, 270, 1538, 274, 5509, 5283, 51668], "temperature": 0.0, "avg_logprob": -0.1818128435799245, "compression_ratio": 2.0104712041884816, "no_speech_prob": 0.047026313841342926}, {"id": 325, "seek": 198586, "start": 1985.86, "end": 1991.2199999999998, "text": " asociado a slap una esta asociado slap bofeta esta asociado slap eso lo pod\u00eda expresar pero no", "tokens": [50364, 382, 78, 537, 1573, 257, 21075, 2002, 5283, 382, 78, 537, 1573, 21075, 748, 69, 7664, 5283, 382, 78, 537, 1573, 21075, 7287, 450, 45588, 33397, 289, 4768, 572, 50632], "temperature": 0.0, "avg_logprob": -0.2030814244196965, "compression_ratio": 2.0905172413793105, "no_speech_prob": 0.013332095928490162}, {"id": 326, "seek": 198586, "start": 1991.2199999999998, "end": 1996.34, "text": " pod\u00eda expresar algo como esto que no esta asociado did not porque no ser\u00eda una funci\u00f3n yo no", "tokens": [50632, 45588, 33397, 289, 8655, 2617, 7433, 631, 572, 5283, 382, 78, 537, 1573, 630, 406, 4021, 572, 23679, 2002, 43735, 5290, 572, 50888], "temperature": 0.0, "avg_logprob": -0.2030814244196965, "compression_ratio": 2.0905172413793105, "no_speech_prob": 0.013332095928490162}, {"id": 327, "seek": 198586, "start": 1996.34, "end": 2003.5, "text": " puedo asociar uno de los valores de la funci\u00f3n con dos cosas del lado del codominio y ac\u00e1 en", "tokens": [50888, 21612, 382, 78, 537, 289, 8526, 368, 1750, 38790, 368, 635, 43735, 416, 4491, 12218, 1103, 11631, 1103, 17656, 6981, 1004, 288, 23496, 465, 51246], "temperature": 0.0, "avg_logprob": -0.2030814244196965, "compression_ratio": 2.0905172413793105, "no_speech_prob": 0.013332095928490162}, {"id": 328, "seek": 198586, "start": 2003.5, "end": 2007.06, "text": " realidad no puedo hacerlo ni en este sentido ni en el otro sentido con una funci\u00f3n no me sirve", "tokens": [51246, 25635, 572, 21612, 32039, 3867, 465, 4065, 19850, 3867, 465, 806, 11921, 19850, 416, 2002, 43735, 572, 385, 4735, 303, 51424], "temperature": 0.0, "avg_logprob": -0.2030814244196965, "compression_ratio": 2.0905172413793105, "no_speech_prob": 0.013332095928490162}, {"id": 329, "seek": 198586, "start": 2007.06, "end": 2011.4599999999998, "text": " porque de vuelta me pasa que slap esta asociado tres cosas entonces con una funci\u00f3n de alineaci\u00f3n yo", "tokens": [51424, 4021, 368, 41542, 385, 20260, 631, 21075, 5283, 382, 78, 537, 1573, 15890, 12218, 13003, 416, 2002, 43735, 368, 419, 533, 3482, 5290, 51644], "temperature": 0.0, "avg_logprob": -0.2030814244196965, "compression_ratio": 2.0905172413793105, "no_speech_prob": 0.013332095928490162}, {"id": 330, "seek": 201146, "start": 2011.46, "end": 2016.74, "text": " no puedo construir este tipo de expresiones en realidad necesito algo como un poco m\u00e1s poderoso", "tokens": [50364, 572, 21612, 38445, 4065, 9746, 368, 33397, 5411, 465, 25635, 11909, 3528, 8655, 2617, 517, 10639, 3573, 8152, 9869, 50628], "temperature": 0.0, "avg_logprob": -0.18709637799600917, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.026332028210163116}, {"id": 331, "seek": 201146, "start": 2018.3400000000001, "end": 2023.5, "text": " esto es lo que dec\u00edamos los modelos dvm siempre usan un mapeo de uno a muchos usan a una funci\u00f3n", "tokens": [50708, 7433, 785, 450, 631, 979, 16275, 1750, 2316, 329, 274, 85, 76, 12758, 505, 282, 517, 463, 494, 78, 368, 8526, 257, 17061, 505, 282, 257, 2002, 43735, 50966], "temperature": 0.0, "avg_logprob": -0.18709637799600917, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.026332028210163116}, {"id": 332, "seek": 201146, "start": 2023.5, "end": 2027.5, "text": " de alineaci\u00f3n mapeo uno a muchos pero en realidad lo que necesito para poder capturar realmente", "tokens": [50966, 368, 419, 533, 3482, 463, 494, 78, 8526, 257, 17061, 4768, 465, 25635, 450, 631, 11909, 3528, 1690, 8152, 3770, 28586, 14446, 51166], "temperature": 0.0, "avg_logprob": -0.18709637799600917, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.026332028210163116}, {"id": 333, "seek": 201146, "start": 2027.5, "end": 2031.98, "text": " con funci\u00f3n en el en el lenguaje es mapeo de muchos a muchos yo voy a tener que un conjunto de", "tokens": [51166, 416, 43735, 465, 806, 465, 806, 35044, 84, 11153, 785, 463, 494, 78, 368, 17061, 257, 17061, 5290, 7552, 257, 11640, 631, 517, 37776, 368, 51390], "temperature": 0.0, "avg_logprob": -0.18709637799600917, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.026332028210163116}, {"id": 334, "seek": 201146, "start": 2031.98, "end": 2036.9, "text": " palabras se va a traducir en otro conjunto de palabras definitiva lo que pasa es que peque\u00f1as", "tokens": [51390, 35240, 369, 2773, 257, 2479, 1311, 347, 465, 11921, 37776, 368, 35240, 28781, 5931, 450, 631, 20260, 785, 631, 19132, 32448, 51636], "temperature": 0.0, "avg_logprob": -0.18709637799600917, "compression_ratio": 1.8941176470588235, "no_speech_prob": 0.026332028210163116}, {"id": 335, "seek": 203690, "start": 2036.9, "end": 2041.14, "text": " frases se traducen como otras peque\u00f1as frases por eso necesito un mapeo de muchos a muchos", "tokens": [50364, 431, 1957, 369, 2479, 1311, 268, 2617, 20244, 19132, 32448, 431, 1957, 1515, 7287, 11909, 3528, 517, 463, 494, 78, 368, 17061, 257, 17061, 50576], "temperature": 0.0, "avg_logprob": -0.20109868483109908, "compression_ratio": 1.8433179723502304, "no_speech_prob": 0.0026425975374877453}, {"id": 336, "seek": 203690, "start": 2043.14, "end": 2048.06, "text": " entonces bueno hay algoritmos que agarran estos mapeos que como el construimos reci\u00e9n el mapeo de uno a", "tokens": [50676, 13003, 11974, 4842, 3501, 50017, 3415, 631, 623, 2284, 282, 12585, 463, 494, 329, 631, 2617, 806, 12946, 8372, 4214, 3516, 806, 463, 494, 78, 368, 8526, 257, 50922], "temperature": 0.0, "avg_logprob": -0.20109868483109908, "compression_ratio": 1.8433179723502304, "no_speech_prob": 0.0026425975374877453}, {"id": 337, "seek": 203690, "start": 2048.06, "end": 2054.3, "text": " muchos en los dos en las dos direcciones digamos y a partir de eso construyen este mapeo de muchos a", "tokens": [50922, 17061, 465, 1750, 4491, 465, 2439, 4491, 1264, 35560, 36430, 288, 257, 13906, 368, 7287, 12946, 16580, 4065, 463, 494, 78, 368, 17061, 257, 51234], "temperature": 0.0, "avg_logprob": -0.20109868483109908, "compression_ratio": 1.8433179723502304, "no_speech_prob": 0.0026425975374877453}, {"id": 338, "seek": 203690, "start": 2054.3, "end": 2059.54, "text": " muchos por ejemplo el algoritmo de la herramienta guisamas m\u00e1s lo que hace es decir bueno yo tengo un", "tokens": [51234, 17061, 1515, 13358, 806, 3501, 50017, 3280, 368, 635, 38271, 64, 695, 271, 19473, 3573, 450, 631, 10032, 785, 10235, 11974, 5290, 13989, 517, 51496], "temperature": 0.0, "avg_logprob": -0.20109868483109908, "compression_ratio": 1.8433179723502304, "no_speech_prob": 0.0026425975374877453}, {"id": 339, "seek": 205954, "start": 2059.54, "end": 2066.9, "text": " corpus en ingl\u00e9s en espa\u00f1ol alineo utilizando los modelos dvm digamos voy alineo por un lado de", "tokens": [50364, 1181, 31624, 465, 49766, 465, 31177, 419, 533, 78, 19906, 1806, 1750, 2316, 329, 274, 85, 76, 36430, 7552, 419, 533, 78, 1515, 517, 11631, 368, 50732], "temperature": 0.0, "avg_logprob": -0.21205377141270068, "compression_ratio": 1.8851674641148326, "no_speech_prob": 0.022240426391363144}, {"id": 340, "seek": 205954, "start": 2066.9, "end": 2072.82, "text": " ingl\u00e9s espa\u00f1ol y por otro lado de espa\u00f1ol ingl\u00e9s y ac\u00e1 me quedan dos mapeos de uno a n digamos dos", "tokens": [50732, 49766, 31177, 288, 1515, 11921, 11631, 368, 31177, 49766, 288, 23496, 385, 13617, 282, 4491, 463, 494, 329, 368, 8526, 257, 297, 36430, 4491, 51028], "temperature": 0.0, "avg_logprob": -0.21205377141270068, "compression_ratio": 1.8851674641148326, "no_speech_prob": 0.022240426391363144}, {"id": 341, "seek": 205954, "start": 2072.82, "end": 2077.86, "text": " mapeos con funciones y despu\u00e9s lo que hago es intersectar esos dos esas dos alineaciones que", "tokens": [51028, 463, 494, 329, 416, 1019, 23469, 288, 15283, 450, 631, 38721, 785, 728, 405, 46412, 22411, 4491, 23388, 4491, 419, 533, 9188, 631, 51280], "temperature": 0.0, "avg_logprob": -0.21205377141270068, "compression_ratio": 1.8851674641148326, "no_speech_prob": 0.022240426391363144}, {"id": 342, "seek": 205954, "start": 2077.86, "end": 2084.86, "text": " me quedaron y unirlas cuando las intersecto obtengo lo que se conoce como puntos de alta confianza", "tokens": [51280, 385, 13617, 6372, 288, 517, 1648, 296, 7767, 2439, 728, 405, 349, 78, 28326, 1571, 450, 631, 369, 33029, 384, 2617, 34375, 368, 26495, 49081, 2394, 51630], "temperature": 0.0, "avg_logprob": -0.21205377141270068, "compression_ratio": 1.8851674641148326, "no_speech_prob": 0.022240426391363144}, {"id": 343, "seek": 208486, "start": 2085.86, "end": 2091.7000000000003, "text": " los puntos negros son los puntos de alta confianza que son los de la intersecci\u00f3n y los puntos", "tokens": [50414, 1750, 34375, 408, 861, 329, 1872, 1750, 34375, 368, 26495, 49081, 2394, 631, 1872, 1750, 368, 635, 728, 8159, 5687, 288, 1750, 34375, 50706], "temperature": 0.0, "avg_logprob": -0.17235313543752462, "compression_ratio": 2.0308370044052864, "no_speech_prob": 0.005042410921305418}, {"id": 344, "seek": 208486, "start": 2091.7000000000003, "end": 2096.26, "text": " grises son lo que est\u00e1n en la uni\u00f3n o sea los que pertenec\u00edan algunos de los modelos", "tokens": [50706, 677, 3598, 1872, 450, 631, 10368, 465, 635, 517, 2560, 277, 4158, 1750, 631, 680, 1147, 3045, 11084, 21078, 368, 1750, 2316, 329, 50934], "temperature": 0.0, "avg_logprob": -0.17235313543752462, "compression_ratio": 2.0308370044052864, "no_speech_prob": 0.005042410921305418}, {"id": 345, "seek": 208486, "start": 2096.26, "end": 2100.5, "text": " entonces la herramienta lo que hace es decir bueno una vez que yo tengo la intersecci\u00f3n y la", "tokens": [50934, 13003, 635, 38271, 64, 450, 631, 10032, 785, 10235, 11974, 2002, 5715, 631, 5290, 13989, 635, 728, 8159, 5687, 288, 635, 51146], "temperature": 0.0, "avg_logprob": -0.17235313543752462, "compression_ratio": 2.0308370044052864, "no_speech_prob": 0.005042410921305418}, {"id": 346, "seek": 208486, "start": 2100.5, "end": 2104.84, "text": " uni\u00f3n hago crecer los puntos que est\u00e1n en la intersecci\u00f3n colonizando otros puntos que", "tokens": [51146, 517, 2560, 38721, 1197, 1776, 1750, 34375, 631, 10368, 465, 635, 728, 8159, 5687, 8255, 590, 1806, 16422, 34375, 631, 51363], "temperature": 0.0, "avg_logprob": -0.17235313543752462, "compression_ratio": 2.0308370044052864, "no_speech_prob": 0.005042410921305418}, {"id": 347, "seek": 208486, "start": 2104.84, "end": 2108.94, "text": " est\u00e9n en la uni\u00f3n hasta que al final termin\u00f3 completando digamos toda la imagen este punto", "tokens": [51363, 871, 3516, 465, 635, 517, 2560, 10764, 631, 419, 2572, 10761, 812, 1557, 1806, 36430, 11687, 635, 40652, 4065, 14326, 51568], "temperature": 0.0, "avg_logprob": -0.17235313543752462, "compression_ratio": 2.0308370044052864, "no_speech_prob": 0.005042410921305418}, {"id": 348, "seek": 210894, "start": 2108.94, "end": 2114.58, "text": " que qued\u00f3 solito ah\u00ed ese no ser\u00eda parte de la alineaci\u00f3n al final s\u00f3lo los que puede llegar", "tokens": [50364, 631, 13617, 812, 1404, 3528, 12571, 10167, 572, 23679, 6975, 368, 635, 419, 533, 3482, 419, 2572, 22885, 1750, 631, 8919, 24892, 50646], "temperature": 0.0, "avg_logprob": -0.25387224859120894, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0008504582801833749}, {"id": 349, "seek": 210894, "start": 2115.58, "end": 2123.34, "text": " movi\u00e9ndote a trav\u00e9s de puntos ya conocidos entonces bueno eso es una forma que utiliza se llama", "tokens": [50696, 2402, 72, 34577, 1370, 257, 24463, 368, 34375, 2478, 15871, 7895, 13003, 11974, 7287, 785, 2002, 8366, 631, 4976, 13427, 369, 23272, 51084], "temperature": 0.0, "avg_logprob": -0.25387224859120894, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0008504582801833749}, {"id": 350, "seek": 210894, "start": 2123.34, "end": 2129.6, "text": " el algoritmo de ox y ney que partiendo alineaciones unidireccionales digamos me permite construir una", "tokens": [51084, 806, 3501, 50017, 3280, 368, 5976, 288, 408, 88, 631, 644, 7304, 419, 533, 9188, 517, 327, 621, 1914, 1966, 279, 36430, 385, 31105, 38445, 2002, 51397], "temperature": 0.0, "avg_logprob": -0.25387224859120894, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0008504582801833749}, {"id": 351, "seek": 210894, "start": 2129.6, "end": 2135.2200000000003, "text": " alineaci\u00f3n completa muchos a muchos entre las palabras bien eso le quer\u00eda mencionar acerca de", "tokens": [51397, 419, 533, 3482, 46822, 17061, 257, 17061, 3962, 2439, 35240, 3610, 7287, 476, 37869, 37030, 289, 46321, 368, 51678], "temperature": 0.0, "avg_logprob": -0.25387224859120894, "compression_ratio": 1.6401673640167365, "no_speech_prob": 0.0008504582801833749}, {"id": 352, "seek": 213522, "start": 2135.22, "end": 2139.4199999999996, "text": " las alineaciones en de palabras y ahora s\u00ed vamos a ver c\u00f3mo funciona un modelo basado en frases", "tokens": [50364, 2439, 419, 533, 9188, 465, 368, 35240, 288, 9923, 8600, 5295, 257, 1306, 12826, 26210, 517, 27825, 987, 1573, 465, 431, 1957, 50574], "temperature": 0.0, "avg_logprob": -0.2392598419189453, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.009373703971505165}, {"id": 353, "seek": 213522, "start": 2139.4199999999996, "end": 2146.74, "text": " un modelo basado en frases tiene cierta semejanza con el modelo anterior que hayamos visto pero es un", "tokens": [50574, 517, 27825, 987, 1573, 465, 431, 1957, 7066, 39769, 1328, 369, 1398, 14763, 2394, 416, 806, 27825, 22272, 631, 4842, 2151, 17558, 4768, 785, 517, 50940], "temperature": 0.0, "avg_logprob": -0.2392598419189453, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.009373703971505165}, {"id": 354, "seek": 213522, "start": 2146.74, "end": 2150.74, "text": " poco m\u00e1s expresivo en realidad yo parte de una oraci\u00f3n por ejemplo en alem\u00e1n que dec\u00eda Morgan", "tokens": [50940, 10639, 3573, 33397, 6340, 465, 25635, 5290, 6975, 368, 2002, 420, 3482, 1515, 13358, 465, 6775, 76, 7200, 631, 37599, 16724, 51140], "temperature": 0.0, "avg_logprob": -0.2392598419189453, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.009373703971505165}, {"id": 355, "seek": 213522, "start": 2150.74, "end": 2156.4599999999996, "text": " Fliggs ganas ganas de su conference lo primero que hace el modelo cuando quiere traducir digamos en", "tokens": [51140, 3235, 32555, 7574, 296, 7574, 296, 368, 459, 7586, 450, 21289, 631, 10032, 806, 27825, 7767, 23877, 2479, 1311, 347, 36430, 465, 51426], "temperature": 0.0, "avg_logprob": -0.2392598419189453, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.009373703971505165}, {"id": 356, "seek": 213522, "start": 2156.4599999999996, "end": 2162.2599999999998, "text": " este caso es decir bueno yo voy a segmentar esa oraci\u00f3n de origen en cierta cantidad de frases", "tokens": [51426, 4065, 9666, 785, 10235, 11974, 5290, 7552, 257, 9469, 289, 11342, 420, 3482, 368, 2349, 268, 465, 39769, 1328, 33757, 368, 431, 1957, 51716], "temperature": 0.0, "avg_logprob": -0.2392598419189453, "compression_ratio": 1.7420494699646643, "no_speech_prob": 0.009373703971505165}, {"id": 357, "seek": 216226, "start": 2162.6600000000003, "end": 2167.42, "text": " despu\u00e9s voy a traducir cada una de esas frases usando una tabla de traducci\u00f3n y esta vez no es una", "tokens": [50384, 15283, 7552, 257, 2479, 1311, 347, 8411, 2002, 368, 23388, 431, 1957, 29798, 2002, 4421, 875, 368, 2479, 1311, 5687, 288, 5283, 5715, 572, 785, 2002, 50622], "temperature": 0.0, "avg_logprob": -0.18327604021344865, "compression_ratio": 1.9637096774193548, "no_speech_prob": 0.004304718226194382}, {"id": 358, "seek": 216226, "start": 2167.42, "end": 2171.0200000000004, "text": " tal de traducci\u00f3n de palabras sino que es una tal de traducci\u00f3n de frases que me dice para cada", "tokens": [50622, 4023, 368, 2479, 1311, 5687, 368, 35240, 18108, 631, 785, 2002, 4023, 368, 2479, 1311, 5687, 368, 431, 1957, 631, 385, 10313, 1690, 8411, 50802], "temperature": 0.0, "avg_logprob": -0.18327604021344865, "compression_ratio": 1.9637096774193548, "no_speech_prob": 0.004304718226194382}, {"id": 359, "seek": 216226, "start": 2171.0200000000004, "end": 2176.6600000000003, "text": " frase con que otra frase se corresponde y una vez que yo traduje cada una esa frase las voy a", "tokens": [50802, 38406, 416, 631, 13623, 38406, 369, 6805, 68, 288, 2002, 5715, 631, 5290, 2479, 4579, 68, 8411, 2002, 11342, 38406, 2439, 7552, 257, 51084], "temperature": 0.0, "avg_logprob": -0.18327604021344865, "compression_ratio": 1.9637096774193548, "no_speech_prob": 0.004304718226194382}, {"id": 360, "seek": 216226, "start": 2176.6600000000003, "end": 2182.1000000000004, "text": " reordenar de alguna manera buscando que suene lo manatural posible buscando aumentar la fluidez", "tokens": [51084, 319, 19058, 289, 368, 20651, 13913, 46804, 631, 459, 1450, 450, 587, 267, 1807, 26644, 46804, 43504, 635, 5029, 45170, 51356], "temperature": 0.0, "avg_logprob": -0.18327604021344865, "compression_ratio": 1.9637096774193548, "no_speech_prob": 0.004304718226194382}, {"id": 361, "seek": 216226, "start": 2182.1000000000004, "end": 2187.1000000000004, "text": " de esa oraci\u00f3n entonces como que la historia de generaci\u00f3n es un poco m\u00e1s simple que la otra no", "tokens": [51356, 368, 11342, 420, 3482, 13003, 2617, 631, 635, 18385, 368, 1337, 3482, 785, 517, 10639, 3573, 2199, 631, 635, 13623, 572, 51606], "temperature": 0.0, "avg_logprob": -0.18327604021344865, "compression_ratio": 1.9637096774193548, "no_speech_prob": 0.004304718226194382}, {"id": 362, "seek": 218710, "start": 2187.1, "end": 2193.06, "text": " ten\u00eda que ir sortiendo cosas simplemente digo separo mi oraci\u00f3n en segmentos que les voy a", "tokens": [50364, 23718, 631, 3418, 1333, 7304, 12218, 33190, 22990, 3128, 78, 2752, 420, 3482, 465, 9469, 329, 631, 1512, 7552, 257, 50662], "temperature": 0.0, "avg_logprob": -0.22204562130137387, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.008480516262352467}, {"id": 363, "seek": 218710, "start": 2193.06, "end": 2200.18, "text": " llamar frases los traducos y los reordenos esa segmentaci\u00f3n en frases no tiene porque tener", "tokens": [50662, 16848, 289, 431, 1957, 1750, 2479, 1311, 329, 288, 1750, 319, 19058, 329, 11342, 9469, 3482, 465, 431, 1957, 572, 7066, 4021, 11640, 51018], "temperature": 0.0, "avg_logprob": -0.22204562130137387, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.008480516262352467}, {"id": 364, "seek": 218710, "start": 2200.18, "end": 2205.42, "text": " una un significado ling\u00fc\u00edstico yo no voy a separarla sin grupo nominal grupo verbal grupo", "tokens": [51018, 2002, 517, 3350, 1573, 22949, 774, 19512, 2789, 5290, 572, 7552, 257, 3128, 34148, 3343, 20190, 41641, 20190, 24781, 20190, 51280], "temperature": 0.0, "avg_logprob": -0.22204562130137387, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.008480516262352467}, {"id": 365, "seek": 218710, "start": 2205.42, "end": 2209.7, "text": " profesional etc\u00e9tera no tengo por qu\u00e9 o sea capaz que yo segmento las frases y justo me queda", "tokens": [51280, 42882, 5183, 526, 23833, 572, 13989, 1515, 8057, 277, 4158, 35453, 631, 5290, 9469, 78, 2439, 431, 1957, 288, 40534, 385, 23314, 51494], "temperature": 0.0, "avg_logprob": -0.22204562130137387, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.008480516262352467}, {"id": 366, "seek": 218710, "start": 2209.7, "end": 2215.22, "text": " un grupo proposicional capaz que no lo \u00fanico que tiene que pasar es que estos segmentos que yo", "tokens": [51494, 517, 20190, 7532, 33010, 35453, 631, 572, 450, 26113, 631, 7066, 631, 25344, 785, 631, 12585, 9469, 329, 631, 5290, 51770], "temperature": 0.0, "avg_logprob": -0.22204562130137387, "compression_ratio": 1.8249027237354085, "no_speech_prob": 0.008480516262352467}, {"id": 367, "seek": 221522, "start": 2215.22, "end": 2219.2999999999997, "text": " construyo tienen que estar en mi tabla de traducci\u00f3n de frases alcanza con eso como para que yo", "tokens": [50364, 12946, 8308, 12536, 631, 8755, 465, 2752, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 419, 7035, 2394, 416, 7287, 2617, 1690, 631, 5290, 50568], "temperature": 0.0, "avg_logprob": -0.24506748328774663, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.003405151888728142}, {"id": 368, "seek": 221522, "start": 2219.2999999999997, "end": 2223.7, "text": " puedo utilizarlos en mi traducci\u00f3n pero no tienen por qu\u00e9 tener una motivaci\u00f3n ling\u00fc\u00edstica", "tokens": [50568, 21612, 19906, 39734, 465, 2752, 2479, 1311, 5687, 4768, 572, 12536, 1515, 8057, 11640, 2002, 5426, 3482, 22949, 774, 19512, 2262, 50788], "temperature": 0.0, "avg_logprob": -0.24506748328774663, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.003405151888728142}, {"id": 369, "seek": 221522, "start": 2226.4599999999996, "end": 2231.5, "text": " bueno entonces un modelo bastante frases tiene estos componentes parecido la anterior porque", "tokens": [50926, 11974, 13003, 517, 27825, 14651, 431, 1957, 7066, 12585, 6542, 279, 7448, 17994, 635, 22272, 4021, 51178], "temperature": 0.0, "avg_logprob": -0.24506748328774663, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.003405151888728142}, {"id": 370, "seek": 221522, "start": 2231.5, "end": 2237.4199999999996, "text": " de vuelta yo lo que quiero hacer es encontrar la probabilidad de fdb digamos sigo teniendo la misma", "tokens": [51178, 368, 41542, 5290, 450, 631, 16811, 6720, 785, 17525, 635, 31959, 4580, 368, 283, 67, 65, 36430, 4556, 78, 2064, 7304, 635, 24946, 51474], "temperature": 0.0, "avg_logprob": -0.24506748328774663, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.003405151888728142}, {"id": 371, "seek": 221522, "start": 2237.4199999999996, "end": 2242.06, "text": " ecuaci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica estad\u00edstica la quiero resolver necesito pdf", "tokens": [51474, 11437, 84, 3482, 8088, 368, 635, 2479, 1311, 5687, 3553, 23432, 39160, 19512, 2262, 635, 16811, 34480, 11909, 3528, 280, 45953, 51706], "temperature": 0.0, "avg_logprob": -0.24506748328774663, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.003405151888728142}, {"id": 372, "seek": 224206, "start": 2242.06, "end": 2247.66, "text": " db y pd s\u00f3lo que ahora el pdf db lo voy a calcular una manera distinta voy a decir que para", "tokens": [50364, 274, 65, 288, 280, 67, 22885, 631, 9923, 806, 280, 45953, 274, 65, 450, 7552, 257, 2104, 17792, 2002, 13913, 1483, 16071, 7552, 257, 10235, 631, 1690, 50644], "temperature": 0.0, "avg_logprob": -0.21683991800143024, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.02416663058102131}, {"id": 373, "seek": 224206, "start": 2247.66, "end": 2252.62, "text": " calcular esto tengo un modelo de traducci\u00f3n de frases y un modelo de ordenamiento un modelo de", "tokens": [50644, 2104, 17792, 7433, 13989, 517, 27825, 368, 2479, 1311, 5687, 368, 431, 1957, 288, 517, 27825, 368, 28615, 16971, 517, 27825, 368, 50892], "temperature": 0.0, "avg_logprob": -0.21683991800143024, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.02416663058102131}, {"id": 374, "seek": 224206, "start": 2252.62, "end": 2257.18, "text": " una gran tabla de frases que me dice cada frase con qu\u00e9 probabil\u00eda la traducci\u00f3n otra y despu\u00e9s", "tokens": [50892, 2002, 9370, 4421, 875, 368, 431, 1957, 631, 385, 10313, 8411, 38406, 416, 8057, 31959, 2686, 635, 2479, 1311, 5687, 13623, 288, 15283, 51120], "temperature": 0.0, "avg_logprob": -0.21683991800143024, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.02416663058102131}, {"id": 375, "seek": 224206, "start": 2257.18, "end": 2263.2599999999998, "text": " una forma de decir c\u00f3mo reorden\u00f3 esa frase para tener mejores oraciones y bueno como siempre", "tokens": [51120, 2002, 8366, 368, 10235, 12826, 319, 19058, 812, 11342, 38406, 1690, 11640, 42284, 420, 9188, 288, 11974, 2617, 12758, 51424], "temperature": 0.0, "avg_logprob": -0.21683991800143024, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.02416663058102131}, {"id": 376, "seek": 224206, "start": 2263.2599999999998, "end": 2268.46, "text": " voy a tener otro componente que es el que mide la la fluidez que es el modelo del lenguaje", "tokens": [51424, 7552, 257, 11640, 11921, 4026, 1576, 631, 785, 806, 631, 275, 482, 635, 635, 5029, 45170, 631, 785, 806, 27825, 1103, 35044, 84, 11153, 51684], "temperature": 0.0, "avg_logprob": -0.21683991800143024, "compression_ratio": 1.8661417322834646, "no_speech_prob": 0.02416663058102131}, {"id": 377, "seek": 226846, "start": 2268.46, "end": 2275.62, "text": " porque los modelos de frases funcionan mejor que los modelos basados en palabras porque la", "tokens": [50364, 4021, 1750, 2316, 329, 368, 431, 1957, 14186, 282, 11479, 631, 1750, 2316, 329, 987, 4181, 465, 35240, 4021, 635, 50722], "temperature": 0.0, "avg_logprob": -0.1908397189641403, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.011142358183860779}, {"id": 378, "seek": 226846, "start": 2275.62, "end": 2280.7, "text": " frase ya tienen cierto contexto las frases en realidad son como peque\u00f1os grupos de palabras que", "tokens": [50722, 38406, 2478, 12536, 28558, 47685, 2439, 431, 1957, 465, 25635, 1872, 2617, 19132, 8242, 33758, 368, 35240, 631, 50976], "temperature": 0.0, "avg_logprob": -0.1908397189641403, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.011142358183860779}, {"id": 379, "seek": 226846, "start": 2280.7, "end": 2289.26, "text": " yo puedo traducir uno en el otro entonces cosas como dar la mano dar una ofetada a tomar el pelo", "tokens": [50976, 5290, 21612, 2479, 1311, 347, 8526, 465, 806, 11921, 13003, 12218, 2617, 4072, 635, 18384, 4072, 2002, 295, 302, 1538, 257, 22048, 806, 12167, 51404], "temperature": 0.0, "avg_logprob": -0.1908397189641403, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.011142358183860779}, {"id": 380, "seek": 226846, "start": 2289.26, "end": 2293.18, "text": " etc\u00e9tera todas esas cosas como expresiones son mucho m\u00e1s f\u00e1ciles de traducir si en realidad", "tokens": [51404, 5183, 526, 23833, 10906, 23388, 12218, 2617, 33397, 5411, 1872, 9824, 3573, 17474, 279, 368, 2479, 1311, 347, 1511, 465, 25635, 51600], "temperature": 0.0, "avg_logprob": -0.1908397189641403, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.011142358183860779}, {"id": 381, "seek": 226846, "start": 2293.18, "end": 2297.1, "text": " yo ya s\u00e9 que esta presi\u00f3n que son tres cuatro palabras le puedo traducir en esta otra expresi\u00f3n que", "tokens": [51600, 5290, 2478, 7910, 631, 5283, 1183, 2560, 631, 1872, 15890, 28795, 35240, 476, 21612, 2479, 1311, 347, 465, 5283, 13623, 33397, 2560, 631, 51796], "temperature": 0.0, "avg_logprob": -0.1908397189641403, "compression_ratio": 1.9051383399209487, "no_speech_prob": 0.011142358183860779}, {"id": 382, "seek": 229710, "start": 2297.1, "end": 2301.94, "text": " son tres cuatro palabras es como m\u00e1s expresivo entonces puede aprender m\u00e1s cosas y bueno obviamente", "tokens": [50364, 1872, 15890, 28795, 35240, 785, 2617, 3573, 33397, 6340, 13003, 8919, 24916, 3573, 12218, 288, 11974, 36325, 50606], "temperature": 0.0, "avg_logprob": -0.24647528125393775, "compression_ratio": 1.952, "no_speech_prob": 0.002470600651577115}, {"id": 383, "seek": 229710, "start": 2301.94, "end": 2306.14, "text": " cuanto m\u00e1s cuanto m\u00e1s atostenga cuanto m\u00e1s largo sea el corpo que yo tengo yo puedo aprender", "tokens": [50606, 36685, 3573, 36685, 3573, 412, 555, 31494, 36685, 3573, 31245, 4158, 806, 1181, 2259, 631, 5290, 13989, 5290, 21612, 24916, 50816], "temperature": 0.0, "avg_logprob": -0.24647528125393775, "compression_ratio": 1.952, "no_speech_prob": 0.002470600651577115}, {"id": 384, "seek": 229710, "start": 2306.14, "end": 2312.8199999999997, "text": " la frase m\u00e1s largas mejores probabilidades y mejores frases bueno ac\u00e1 hay un ejemplo de c\u00f3mo", "tokens": [50816, 635, 38406, 3573, 1613, 10549, 42284, 31959, 10284, 288, 42284, 431, 1957, 11974, 23496, 4842, 517, 13358, 368, 12826, 51150], "temperature": 0.0, "avg_logprob": -0.24647528125393775, "compression_ratio": 1.952, "no_speech_prob": 0.002470600651577115}, {"id": 385, "seek": 229710, "start": 2312.8199999999997, "end": 2316.96, "text": " ser\u00eda una tabla de traducci\u00f3n de frases o sea es parecido la tabla de traducci\u00f3n de palabras o", "tokens": [51150, 23679, 2002, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 277, 4158, 785, 7448, 17994, 635, 4421, 875, 368, 2479, 1311, 5687, 368, 35240, 277, 51357], "temperature": 0.0, "avg_logprob": -0.24647528125393775, "compression_ratio": 1.952, "no_speech_prob": 0.002470600651577115}, {"id": 386, "seek": 229710, "start": 2316.96, "end": 2322.66, "text": " lo que ac\u00e1 tengo de en borslac o sea si yo busco la fila asociado en borslac o sea encontrar\u00eda", "tokens": [51357, 450, 631, 23496, 13989, 368, 465, 14828, 10418, 326, 277, 4158, 1511, 5290, 1255, 1291, 635, 1387, 64, 382, 78, 537, 1573, 465, 14828, 10418, 326, 277, 4158, 17525, 2686, 51642], "temperature": 0.0, "avg_logprob": -0.24647528125393775, "compression_ratio": 1.952, "no_speech_prob": 0.002470600651577115}, {"id": 387, "seek": 232266, "start": 2322.66, "end": 2327.66, "text": " todas estas traducciones de prop\u00f3sal con sesenta dos por ciento de probabilidad posesivo prop\u00f3sal con", "tokens": [50364, 10906, 13897, 2479, 1311, 23469, 368, 2365, 12994, 304, 416, 5385, 8938, 4491, 1515, 47361, 368, 31959, 4580, 26059, 6340, 2365, 12994, 304, 416, 50614], "temperature": 0.0, "avg_logprob": -0.23879254305804218, "compression_ratio": 1.9712918660287082, "no_speech_prob": 0.003896634327247739}, {"id": 388, "seek": 232266, "start": 2327.66, "end": 2334.74, "text": " diez por ciento a prop\u00f3sal con tres por ciento etc\u00e9tera o sea como vence traducen frases en frases bueno", "tokens": [50614, 48165, 1515, 47361, 257, 2365, 12994, 304, 416, 15890, 1515, 47361, 5183, 526, 23833, 277, 4158, 2617, 6138, 384, 2479, 1311, 268, 431, 1957, 465, 431, 1957, 11974, 50968], "temperature": 0.0, "avg_logprob": -0.23879254305804218, "compression_ratio": 1.9712918660287082, "no_speech_prob": 0.003896634327247739}, {"id": 389, "seek": 232266, "start": 2334.74, "end": 2342.62, "text": " y c\u00f3mo hago para aprender una tabla de traducci\u00f3n de frases yo parto de esta alineaci\u00f3n de palabras", "tokens": [50968, 288, 12826, 38721, 1690, 24916, 2002, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 5290, 644, 78, 368, 5283, 419, 533, 3482, 368, 35240, 51362], "temperature": 0.0, "avg_logprob": -0.23879254305804218, "compression_ratio": 1.9712918660287082, "no_speech_prob": 0.003896634327247739}, {"id": 390, "seek": 232266, "start": 2342.62, "end": 2346.92, "text": " digamos esta alineaci\u00f3n completa que ya no es una funci\u00f3n sino que es digamos una alineaci\u00f3n de", "tokens": [51362, 36430, 5283, 419, 533, 3482, 46822, 631, 2478, 572, 785, 2002, 43735, 18108, 631, 785, 36430, 2002, 419, 533, 3482, 368, 51577], "temperature": 0.0, "avg_logprob": -0.23879254305804218, "compression_ratio": 1.9712918660287082, "no_speech_prob": 0.003896634327247739}, {"id": 391, "seek": 234692, "start": 2346.92, "end": 2353.0, "text": " muchos a muchos y voy a tratar de encontrar todos los todas las frases todos los pares de frases que", "tokens": [50364, 17061, 257, 17061, 288, 7552, 257, 42549, 368, 17525, 6321, 1750, 10906, 2439, 431, 1957, 6321, 1750, 2502, 495, 368, 431, 1957, 631, 50668], "temperature": 0.0, "avg_logprob": -0.18485450744628906, "compression_ratio": 2.035, "no_speech_prob": 0.025422221049666405}, {"id": 392, "seek": 234692, "start": 2353.0, "end": 2359.6, "text": " son consistentes con la alineaci\u00f3n a que me refiero con que son consistentes ac\u00e1 hay ejemplos yo quiero", "tokens": [50668, 1872, 4603, 9240, 416, 635, 419, 533, 3482, 257, 631, 385, 1895, 12030, 416, 631, 1872, 4603, 9240, 23496, 4842, 10012, 5895, 329, 5290, 16811, 50998], "temperature": 0.0, "avg_logprob": -0.18485450744628906, "compression_ratio": 2.035, "no_speech_prob": 0.025422221049666405}, {"id": 393, "seek": 234692, "start": 2359.6, "end": 2368.0, "text": " decir que mariano y mar\u00eda did not son son un par de frases que son consistentes con esta alineaci\u00f3n", "tokens": [50998, 10235, 631, 1849, 6254, 288, 1849, 2686, 630, 406, 1872, 1872, 517, 971, 368, 431, 1957, 631, 1872, 4603, 9240, 416, 5283, 419, 533, 3482, 51418], "temperature": 0.0, "avg_logprob": -0.18485450744628906, "compression_ratio": 2.035, "no_speech_prob": 0.025422221049666405}, {"id": 394, "seek": 234692, "start": 2368.0, "end": 2373.4, "text": " en cambio mariano y mar\u00eda did no lo son c\u00f3mo es que miro esto lo que pasa es que cuando yo tengo", "tokens": [51418, 465, 28731, 1849, 6254, 288, 1849, 2686, 630, 572, 450, 1872, 12826, 785, 631, 2752, 340, 7433, 450, 631, 20260, 785, 631, 7767, 5290, 13989, 51688], "temperature": 0.0, "avg_logprob": -0.18485450744628906, "compression_ratio": 2.035, "no_speech_prob": 0.025422221049666405}, {"id": 395, "seek": 237340, "start": 2373.4, "end": 2379.4, "text": " mariano y mar\u00eda did la palabra no est\u00e1 alineada con did not y el did not digamos el not no", "tokens": [50364, 1849, 6254, 288, 1849, 2686, 630, 635, 31702, 572, 3192, 419, 533, 1538, 416, 630, 406, 288, 806, 630, 406, 36430, 806, 406, 572, 50664], "temperature": 0.0, "avg_logprob": -0.17879064459549754, "compression_ratio": 2.081967213114754, "no_speech_prob": 0.0033095343969762325}, {"id": 396, "seek": 237340, "start": 2379.4, "end": 2384.4, "text": " pertenece hasta alineaci\u00f3n que yo estoy tratando de decir entonces digo que es no consistente lo mismo", "tokens": [50664, 680, 1147, 68, 384, 10764, 419, 533, 3482, 631, 5290, 15796, 21507, 1806, 368, 10235, 13003, 22990, 631, 785, 572, 4603, 1576, 450, 12461, 50914], "temperature": 0.0, "avg_logprob": -0.17879064459549754, "compression_ratio": 2.081967213114754, "no_speech_prob": 0.0033095343969762325}, {"id": 397, "seek": 237340, "start": 2384.4, "end": 2392.04, "text": " pasa con si yo dato alinear mariano daba y mar\u00eda did not lo que pasa ah\u00ed es que daba no est\u00e1 digamos", "tokens": [50914, 20260, 416, 1511, 5290, 46971, 419, 533, 289, 1849, 6254, 274, 5509, 288, 1849, 2686, 630, 406, 450, 631, 20260, 12571, 785, 631, 274, 5509, 572, 3192, 36430, 51296], "temperature": 0.0, "avg_logprob": -0.17879064459549754, "compression_ratio": 2.081967213114754, "no_speech_prob": 0.0033095343969762325}, {"id": 398, "seek": 237340, "start": 2392.04, "end": 2395.92, "text": " los puntos alineaci\u00f3n de daba no est\u00e1n dentro de este cuadrante que estoy tratando de buscar entonces", "tokens": [51296, 1750, 34375, 419, 533, 3482, 368, 274, 5509, 572, 10368, 10856, 368, 4065, 34434, 81, 2879, 631, 15796, 21507, 1806, 368, 26170, 13003, 51490], "temperature": 0.0, "avg_logprob": -0.17879064459549754, "compression_ratio": 2.081967213114754, "no_speech_prob": 0.0033095343969762325}, {"id": 399, "seek": 237340, "start": 2395.92, "end": 2400.96, "text": " en definitiva digo que no es consistente las alineaciones consistentes correctas son las que consideran", "tokens": [51490, 465, 28781, 5931, 22990, 631, 572, 785, 4603, 1576, 2439, 419, 533, 9188, 4603, 9240, 3006, 296, 1872, 2439, 631, 1949, 282, 51742], "temperature": 0.0, "avg_logprob": -0.17879064459549754, "compression_ratio": 2.081967213114754, "no_speech_prob": 0.0033095343969762325}, {"id": 400, "seek": 240096, "start": 2400.96, "end": 2405.6, "text": " todos los puntos dentro de ese cuadrante entonces mariano est\u00e1 asociado con mariano did not y", "tokens": [50364, 6321, 1750, 34375, 10856, 368, 10167, 34434, 81, 2879, 13003, 1849, 6254, 3192, 382, 78, 537, 1573, 416, 1849, 6254, 630, 406, 288, 50596], "temperature": 0.0, "avg_logprob": -0.236433043960453, "compression_ratio": 1.9834710743801653, "no_speech_prob": 0.0039423322305083275}, {"id": 401, "seek": 240096, "start": 2405.6, "end": 2416.08, "text": " es as\u00ed es consistente as\u00ed que como aprendo frases consistentes empiezo por las alineaciones digamos", "tokens": [50596, 785, 8582, 785, 4603, 1576, 8582, 631, 2617, 21003, 78, 431, 1957, 4603, 9240, 4012, 414, 4765, 1515, 2439, 419, 533, 9188, 36430, 51120], "temperature": 0.0, "avg_logprob": -0.236433043960453, "compression_ratio": 1.9834710743801653, "no_speech_prob": 0.0039423322305083275}, {"id": 402, "seek": 240096, "start": 2416.08, "end": 2419.52, "text": " empiezo con la alineaci\u00f3n de palabra despu\u00e9s busco de alguna palabra y digo bueno me quedo con", "tokens": [51120, 4012, 414, 4765, 416, 635, 419, 533, 3482, 368, 31702, 15283, 1255, 1291, 368, 20651, 31702, 288, 22990, 11974, 385, 13617, 78, 416, 51292], "temperature": 0.0, "avg_logprob": -0.236433043960453, "compression_ratio": 1.9834710743801653, "no_speech_prob": 0.0039423322305083275}, {"id": 403, "seek": 240096, "start": 2419.52, "end": 2424.76, "text": " todas esas traducciones de palabras y las pongo mi tabla de frases y despu\u00e9s voy tomando de", "tokens": [51292, 10906, 23388, 2479, 1311, 23469, 368, 35240, 288, 2439, 280, 25729, 2752, 4421, 875, 368, 431, 1957, 288, 15283, 7552, 2916, 1806, 368, 51554], "temperature": 0.0, "avg_logprob": -0.236433043960453, "compression_ratio": 1.9834710743801653, "no_speech_prob": 0.0039423322305083275}, {"id": 404, "seek": 240096, "start": 2424.76, "end": 2429.52, "text": " dos y me quedo con todas esas otras frases y las voy agregando mi tabla de frases despu\u00e9s me", "tokens": [51554, 4491, 288, 385, 13617, 78, 416, 10906, 23388, 20244, 431, 1957, 288, 2439, 7552, 623, 3375, 1806, 2752, 4421, 875, 368, 431, 1957, 15283, 385, 51792], "temperature": 0.0, "avg_logprob": -0.236433043960453, "compression_ratio": 1.9834710743801653, "no_speech_prob": 0.0039423322305083275}, {"id": 405, "seek": 242952, "start": 2430.08, "end": 2436.36, "text": " puedo avanzar en uno tomada tres tomada de cuatro y llegar a tomar incluso toda la elaboraci\u00f3n como", "tokens": [50392, 21612, 42444, 289, 465, 8526, 2916, 1538, 15890, 2916, 1538, 368, 28795, 288, 24892, 257, 22048, 24018, 11687, 635, 16298, 3482, 2617, 50706], "temperature": 0.0, "avg_logprob": -0.3008816512589602, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0014106163289397955}, {"id": 406, "seek": 242952, "start": 2436.36, "end": 2441.64, "text": " frases entonces a partir de estas oraciones que ten\u00edan no s\u00e9 este 1 2 3 4 5 6 7 8 no es", "tokens": [50706, 431, 1957, 13003, 257, 13906, 368, 13897, 420, 9188, 631, 47596, 572, 7910, 4065, 502, 568, 805, 1017, 1025, 1386, 1614, 1649, 572, 785, 50970], "temperature": 0.0, "avg_logprob": -0.3008816512589602, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0014106163289397955}, {"id": 407, "seek": 242952, "start": 2441.64, "end": 2448.44, "text": " palabras yo termin\u00f3 aprendiendo como 17 frases digamos cada vez m\u00e1s grandes y bueno", "tokens": [50970, 35240, 5290, 10761, 812, 21003, 7304, 2617, 3282, 431, 1957, 36430, 8411, 5715, 3573, 16640, 288, 11974, 51310], "temperature": 0.0, "avg_logprob": -0.3008816512589602, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0014106163289397955}, {"id": 408, "seek": 242952, "start": 2448.44, "end": 2455.6, "text": " vi hoy sacando esto de todo el corpus y calculando mi tabla de probabilidades de qu\u00e9 manera calculo", "tokens": [51310, 1932, 13775, 4899, 1806, 7433, 368, 5149, 806, 1181, 31624, 288, 4322, 1806, 2752, 4421, 875, 368, 31959, 10284, 368, 8057, 13913, 4322, 78, 51668], "temperature": 0.0, "avg_logprob": -0.3008816512589602, "compression_ratio": 1.551440329218107, "no_speech_prob": 0.0014106163289397955}, {"id": 409, "seek": 245560, "start": 2455.6, "end": 2460.48, "text": " esas probabilidades yo lo que puedo hacer es como siempre ver cu\u00e1nta vez aparece en el corpus y", "tokens": [50364, 23388, 31959, 10284, 5290, 450, 631, 21612, 6720, 785, 2617, 12758, 1306, 44256, 64, 5715, 37863, 465, 806, 1181, 31624, 288, 50608], "temperature": 0.0, "avg_logprob": -0.2137476496335839, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.0037043089978396893}, {"id": 410, "seek": 245560, "start": 2460.48, "end": 2467.3199999999997, "text": " contar o si no si yo ten\u00eda construido el modelo anterior el modelo de la tabla de traducciones de", "tokens": [50608, 27045, 277, 1511, 572, 1511, 5290, 23718, 12946, 2925, 806, 27825, 22272, 806, 27825, 368, 635, 4421, 875, 368, 2479, 1311, 23469, 368, 50950], "temperature": 0.0, "avg_logprob": -0.2137476496335839, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.0037043089978396893}, {"id": 411, "seek": 245560, "start": 2467.3199999999997, "end": 2471.4, "text": " palabra a palabra en realidad lo que puedo hacer es aprovechar ese modelo de traducci\u00f3n de palabra", "tokens": [50950, 31702, 257, 31702, 465, 25635, 450, 631, 21612, 6720, 785, 29015, 7374, 10167, 27825, 368, 2479, 1311, 5687, 368, 31702, 51154], "temperature": 0.0, "avg_logprob": -0.2137476496335839, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.0037043089978396893}, {"id": 412, "seek": 245560, "start": 2471.4, "end": 2476.88, "text": " a palabra y decir bueno me armo una traducci\u00f3n entre un par de frases bas\u00e1ndome en las traducciones", "tokens": [51154, 257, 31702, 288, 10235, 11974, 385, 3726, 78, 2002, 2479, 1311, 5687, 3962, 517, 971, 368, 431, 1957, 987, 18606, 423, 465, 2439, 2479, 1311, 23469, 51428], "temperature": 0.0, "avg_logprob": -0.2137476496335839, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.0037043089978396893}, {"id": 413, "seek": 245560, "start": 2476.88, "end": 2481.48, "text": " palabra a palabra son como dos formas distintas de construirlo y a veces hasta complementarias", "tokens": [51428, 31702, 257, 31702, 1872, 2617, 4491, 33463, 31489, 296, 368, 38445, 752, 288, 257, 17054, 10764, 17103, 35027, 51658], "temperature": 0.0, "avg_logprob": -0.2137476496335839, "compression_ratio": 1.9446640316205535, "no_speech_prob": 0.0037043089978396893}, {"id": 414, "seek": 248560, "start": 2485.6, "end": 2490.2, "text": " bien eso fue el modelo de frases los modelos de frases son los m\u00e1s usados hoy en d\u00eda en realidad en", "tokens": [50364, 3610, 7287, 9248, 806, 27825, 368, 431, 1957, 1750, 2316, 329, 368, 431, 1957, 1872, 1750, 3573, 505, 4181, 13775, 465, 12271, 465, 25635, 465, 50594], "temperature": 0.0, "avg_logprob": -0.16308543608360684, "compression_ratio": 1.8737864077669903, "no_speech_prob": 0.007642797194421291}, {"id": 415, "seek": 248560, "start": 2490.2, "end": 2495.24, "text": " lo que es la traducci\u00f3n autom\u00e1tica son los que han dado mejores resultados y bueno y no", "tokens": [50594, 450, 631, 785, 635, 2479, 1311, 5687, 3553, 23432, 1872, 1750, 631, 7276, 29568, 42284, 36796, 288, 11974, 288, 572, 50846], "temperature": 0.0, "avg_logprob": -0.16308543608360684, "compression_ratio": 1.8737864077669903, "no_speech_prob": 0.007642797194421291}, {"id": 416, "seek": 248560, "start": 2495.24, "end": 2500.68, "text": " faltaba una cosa para terminar toda la imagen de lo que es la traducci\u00f3n autom\u00e1tica estad\u00edstica", "tokens": [50846, 37108, 5509, 2002, 10163, 1690, 36246, 11687, 635, 40652, 368, 450, 631, 785, 635, 2479, 1311, 5687, 3553, 23432, 39160, 19512, 2262, 51118], "temperature": 0.0, "avg_logprob": -0.16308543608360684, "compression_ratio": 1.8737864077669903, "no_speech_prob": 0.007642797194421291}, {"id": 417, "seek": 248560, "start": 2500.68, "end": 2510.24, "text": " que es la decodificaci\u00f3n entonces vamos un resumen de lo que ten\u00edamos hasta ahora hasta ahora", "tokens": [51118, 631, 785, 635, 979, 378, 40802, 13003, 5295, 517, 725, 16988, 368, 450, 631, 2064, 16275, 10764, 9923, 10764, 9923, 51596], "temperature": 0.0, "avg_logprob": -0.16308543608360684, "compression_ratio": 1.8737864077669903, "no_speech_prob": 0.007642797194421291}, {"id": 418, "seek": 251024, "start": 2510.24, "end": 2515.3999999999996, "text": " yo part\u00ed de yo quer\u00eda resolver la cocci\u00f3n fundamental de la traducci\u00f3n autom\u00e1tica estad\u00edstica", "tokens": [50364, 5290, 644, 870, 368, 5290, 37869, 34480, 635, 598, 14735, 8088, 368, 635, 2479, 1311, 5687, 3553, 23432, 39160, 19512, 2262, 50622], "temperature": 0.0, "avg_logprob": -0.1819346140301417, "compression_ratio": 2.051063829787234, "no_speech_prob": 0.013201451860368252}, {"id": 419, "seek": 251024, "start": 2516.3999999999996, "end": 2521.64, "text": " y yo ten\u00eda un corpus para el hilo que ten\u00eda texto en el idioma origen y el idioma destino y a", "tokens": [50672, 288, 5290, 23718, 517, 1181, 31624, 1690, 806, 276, 10720, 631, 23718, 35503, 465, 806, 18014, 6440, 2349, 268, 288, 806, 18014, 6440, 2677, 2982, 288, 257, 50934], "temperature": 0.0, "avg_logprob": -0.1819346140301417, "compression_ratio": 2.051063829787234, "no_speech_prob": 0.013201451860368252}, {"id": 420, "seek": 251024, "start": 2521.64, "end": 2525.2, "text": " partir de haciendo an\u00e1lisis estad\u00edstico yo me constru\u00ed un modelo de traducci\u00f3n que lo que", "tokens": [50934, 13906, 368, 20509, 44113, 28436, 39160, 19512, 2789, 5290, 385, 12946, 870, 517, 27825, 368, 2479, 1311, 5687, 631, 450, 631, 51112], "temperature": 0.0, "avg_logprob": -0.1819346140301417, "compression_ratio": 2.051063829787234, "no_speech_prob": 0.013201451860368252}, {"id": 421, "seek": 251024, "start": 2525.2, "end": 2531.8799999999997, "text": " vimos en esta clase adem\u00e1s yo ten\u00eda cierta cantidad de texto en el idioma destino y a partir", "tokens": [51112, 49266, 465, 5283, 44578, 21251, 5290, 23718, 39769, 1328, 33757, 368, 35503, 465, 806, 18014, 6440, 2677, 2982, 288, 257, 13906, 51446], "temperature": 0.0, "avg_logprob": -0.1819346140301417, "compression_ratio": 2.051063829787234, "no_speech_prob": 0.013201451860368252}, {"id": 422, "seek": 251024, "start": 2531.8799999999997, "end": 2536.4399999999996, "text": " de cierto an\u00e1lisis estad\u00edstico me constru\u00ed un modelo de lenguaje que me dice que tan fluido es", "tokens": [51446, 368, 28558, 44113, 28436, 39160, 19512, 2789, 385, 12946, 870, 517, 27825, 368, 35044, 84, 11153, 631, 385, 10313, 631, 7603, 5029, 2925, 785, 51674], "temperature": 0.0, "avg_logprob": -0.1819346140301417, "compression_ratio": 2.051063829787234, "no_speech_prob": 0.013201451860368252}, {"id": 423, "seek": 253644, "start": 2536.44, "end": 2542.76, "text": " una oraci\u00f3n en el lenguaje destino entonces ahora lo que me falta recuerden que yo lo que", "tokens": [50364, 2002, 420, 3482, 465, 806, 35044, 84, 11153, 2677, 2982, 13003, 9923, 450, 631, 385, 22111, 39092, 1556, 631, 5290, 450, 631, 50680], "temperature": 0.0, "avg_logprob": -0.22072314453125, "compression_ratio": 1.9956896551724137, "no_speech_prob": 0.00740646431222558}, {"id": 424, "seek": 253644, "start": 2542.76, "end": 2546.88, "text": " ten\u00eda que hacer era iterar sobre todas las oraciones el lenguaje destino y pasar las", "tokens": [50680, 23718, 631, 6720, 4249, 17138, 289, 5473, 10906, 2439, 420, 9188, 806, 35044, 84, 11153, 2677, 2982, 288, 25344, 2439, 50886], "temperature": 0.0, "avg_logprob": -0.22072314453125, "compression_ratio": 1.9956896551724137, "no_speech_prob": 0.00740646431222558}, {"id": 425, "seek": 253644, "start": 2546.88, "end": 2550.32, "text": " atraves del modelo de traducci\u00f3n y del modelo de lenguaje para que me d\u00e9 la probabilidad de esa", "tokens": [50886, 44192, 977, 1103, 27825, 368, 2479, 1311, 5687, 288, 1103, 27825, 368, 35044, 84, 11153, 1690, 631, 385, 2795, 635, 31959, 4580, 368, 11342, 51058], "temperature": 0.0, "avg_logprob": -0.22072314453125, "compression_ratio": 1.9956896551724137, "no_speech_prob": 0.00740646431222558}, {"id": 426, "seek": 253644, "start": 2550.32, "end": 2556.4, "text": " oraci\u00f3n bueno lo que me falta es el algoritmo de codificaci\u00f3n que en vez de probar con toda", "tokens": [51058, 420, 3482, 11974, 450, 631, 385, 22111, 785, 806, 3501, 50017, 3280, 368, 17656, 40802, 631, 465, 5715, 368, 1239, 289, 416, 11687, 51362], "temperature": 0.0, "avg_logprob": -0.22072314453125, "compression_ratio": 1.9956896551724137, "no_speech_prob": 0.00740646431222558}, {"id": 427, "seek": 253644, "start": 2556.4, "end": 2560.62, "text": " la oraci\u00f3n del lenguaje destino me va a decir unas cuantas oraciones para probar capaz que me", "tokens": [51362, 635, 420, 3482, 1103, 35044, 84, 11153, 2677, 2982, 385, 2773, 257, 10235, 25405, 2702, 49153, 420, 9188, 1690, 1239, 289, 35453, 631, 385, 51573], "temperature": 0.0, "avg_logprob": -0.22072314453125, "compression_ratio": 1.9956896551724137, "no_speech_prob": 0.00740646431222558}, {"id": 428, "seek": 256062, "start": 2560.62, "end": 2566.1, "text": " dice 150 oraciones para probar sobre las cuales utilizar el modelo de traducci\u00f3n y el modelo", "tokens": [50364, 10313, 8451, 420, 9188, 1690, 1239, 289, 5473, 2439, 46932, 24060, 806, 27825, 368, 2479, 1311, 5687, 288, 806, 27825, 50638], "temperature": 0.0, "avg_logprob": -0.17562963485717772, "compression_ratio": 1.8944723618090453, "no_speech_prob": 0.005166310351341963}, {"id": 429, "seek": 256062, "start": 2566.1, "end": 2571.58, "text": " de lenguaje entonces esto es como un diagrama de de m\u00f3dulos en los cuales el algoritmo de codificaci\u00f3n", "tokens": [50638, 368, 35044, 84, 11153, 13003, 7433, 785, 2617, 517, 10686, 64, 368, 368, 275, 17081, 28348, 465, 1750, 46932, 806, 3501, 50017, 3280, 368, 17656, 40802, 50912], "temperature": 0.0, "avg_logprob": -0.17562963485717772, "compression_ratio": 1.8944723618090453, "no_speech_prob": 0.005166310351341963}, {"id": 430, "seek": 256062, "start": 2571.58, "end": 2580.3399999999997, "text": " utiliza los dos m\u00f3dulos tanto el de traducci\u00f3n como el de lenguaje bueno c\u00f3mo funciona el", "tokens": [50912, 4976, 13427, 1750, 4491, 275, 17081, 28348, 10331, 806, 368, 2479, 1311, 5687, 2617, 806, 368, 35044, 84, 11153, 11974, 12826, 26210, 806, 51350], "temperature": 0.0, "avg_logprob": -0.17562963485717772, "compression_ratio": 1.8944723618090453, "no_speech_prob": 0.005166310351341963}, {"id": 431, "seek": 256062, "start": 2580.3399999999997, "end": 2585.18, "text": " algoritmo de codificaci\u00f3n y que vamos a ver es un algoritmo de codificaci\u00f3n de tipo", "tokens": [51350, 3501, 50017, 3280, 368, 17656, 40802, 288, 631, 5295, 257, 1306, 785, 517, 3501, 50017, 3280, 368, 17656, 40802, 368, 9746, 51592], "temperature": 0.0, "avg_logprob": -0.17562963485717772, "compression_ratio": 1.8944723618090453, "no_speech_prob": 0.005166310351341963}, {"id": 432, "seek": 258518, "start": 2585.18, "end": 2591.8999999999996, "text": " beam search y bueno funciona de as\u00ed de manera yo tengo la oraci\u00f3n Mar\u00eda no dio una ofetada", "tokens": [50364, 14269, 3164, 288, 11974, 26210, 368, 8582, 368, 13913, 5290, 13989, 635, 420, 3482, 48472, 572, 31965, 2002, 295, 302, 1538, 50700], "temperature": 0.0, "avg_logprob": -0.25705182670366644, "compression_ratio": 1.9536082474226804, "no_speech_prob": 0.10969400405883789}, {"id": 433, "seek": 258518, "start": 2591.8999999999996, "end": 2596.8199999999997, "text": " a la bruja verde y la quiero traducir al ingl\u00e9s y tengo una tabla de traducci\u00f3n de frases", "tokens": [50700, 257, 635, 25267, 2938, 29653, 288, 635, 16811, 2479, 1311, 347, 419, 49766, 288, 13989, 2002, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 50946], "temperature": 0.0, "avg_logprob": -0.25705182670366644, "compression_ratio": 1.9536082474226804, "no_speech_prob": 0.10969400405883789}, {"id": 434, "seek": 258518, "start": 2598.4199999999996, "end": 2604.3799999999997, "text": " entonces mi oraci\u00f3n Mar\u00eda no dio una ofetada a la bruja verde yo busco en la tabla de frases", "tokens": [51026, 13003, 2752, 420, 3482, 48472, 572, 31965, 2002, 295, 302, 1538, 257, 635, 25267, 2938, 29653, 5290, 1255, 1291, 465, 635, 4421, 875, 368, 431, 1957, 51324], "temperature": 0.0, "avg_logprob": -0.25705182670366644, "compression_ratio": 1.9536082474226804, "no_speech_prob": 0.10969400405883789}, {"id": 435, "seek": 258518, "start": 2604.3799999999997, "end": 2610.8199999999997, "text": " cuales de esas digamos cuales segmentos cuales sus segmentos de esa oraci\u00f3n yo puedo encontrar en", "tokens": [51324, 46932, 368, 23388, 36430, 46932, 9469, 329, 46932, 3291, 9469, 329, 368, 11342, 420, 3482, 5290, 21612, 17525, 465, 51646], "temperature": 0.0, "avg_logprob": -0.25705182670366644, "compression_ratio": 1.9536082474226804, "no_speech_prob": 0.10969400405883789}, {"id": 436, "seek": 261082, "start": 2610.82, "end": 2614.46, "text": " la tabla de traducci\u00f3n de frases entonces voy a encontrar por ejemplo que Mar\u00eda lo puedo", "tokens": [50364, 635, 4421, 875, 368, 2479, 1311, 5687, 368, 431, 1957, 13003, 7552, 257, 17525, 1515, 13358, 631, 48472, 450, 21612, 50546], "temperature": 0.0, "avg_logprob": -0.22950485057400583, "compression_ratio": 2.15962441314554, "no_speech_prob": 0.018106864765286446}, {"id": 437, "seek": 261082, "start": 2614.46, "end": 2619.5, "text": " traducir como Mary no lo busco en la tabla y lo puedo traducir como Not como Did Not o como No", "tokens": [50546, 2479, 1311, 347, 2617, 6059, 572, 450, 1255, 1291, 465, 635, 4421, 875, 288, 450, 21612, 2479, 1311, 347, 2617, 1726, 2617, 2589, 1726, 277, 2617, 883, 50798], "temperature": 0.0, "avg_logprob": -0.22950485057400583, "compression_ratio": 2.15962441314554, "no_speech_prob": 0.018106864765286446}, {"id": 438, "seek": 261082, "start": 2619.5, "end": 2625.46, "text": " dio lo puedo traducir como Guid pero adem\u00e1s no dio esa frase entera yo lo busco en la tabla", "tokens": [50798, 31965, 450, 21612, 2479, 1311, 347, 2617, 2694, 327, 4768, 21251, 572, 31965, 11342, 38406, 948, 1663, 5290, 450, 1255, 1291, 465, 635, 4421, 875, 51096], "temperature": 0.0, "avg_logprob": -0.22950485057400583, "compression_ratio": 2.15962441314554, "no_speech_prob": 0.018106864765286446}, {"id": 439, "seek": 261082, "start": 2625.46, "end": 2630.3, "text": " y me aparece que lo puedo traducir como Did Not Guid dio una ofetada a toda esa frase lo", "tokens": [51096, 288, 385, 37863, 631, 450, 21612, 2479, 1311, 347, 2617, 2589, 1726, 2694, 327, 31965, 2002, 295, 302, 1538, 257, 11687, 11342, 38406, 450, 51338], "temperature": 0.0, "avg_logprob": -0.22950485057400583, "compression_ratio": 2.15962441314554, "no_speech_prob": 0.018106864765286446}, {"id": 440, "seek": 261082, "start": 2630.3, "end": 2638.3, "text": " puedo traducir como Slap una ofetada lo puedo decir como a Slap y bueno otras cosas bruja lo", "tokens": [51338, 21612, 2479, 1311, 347, 2617, 6187, 569, 2002, 295, 302, 1538, 450, 21612, 10235, 2617, 257, 6187, 569, 288, 11974, 20244, 12218, 25267, 2938, 450, 51738], "temperature": 0.0, "avg_logprob": -0.22950485057400583, "compression_ratio": 2.15962441314554, "no_speech_prob": 0.018106864765286446}, {"id": 441, "seek": 263830, "start": 2638.3, "end": 2641.9, "text": " puedo decir como Witch verde como Green pero adem\u00e1s en alg\u00fan lado de la tabla tengo que bruja verde", "tokens": [50364, 21612, 10235, 2617, 23522, 29653, 2617, 6969, 4768, 21251, 465, 26300, 11631, 368, 635, 4421, 875, 13989, 631, 25267, 2938, 29653, 50544], "temperature": 0.0, "avg_logprob": -0.17385304875734472, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.01707150600850582}, {"id": 442, "seek": 263830, "start": 2641.9, "end": 2648.1000000000004, "text": " lo puedo traducir como Green Witch y as\u00ed digamos yo puedo encontrar tengo diferentes maneras de", "tokens": [50544, 450, 21612, 2479, 1311, 347, 2617, 6969, 23522, 288, 8582, 36430, 5290, 21612, 17525, 13989, 17686, 587, 6985, 368, 50854], "temperature": 0.0, "avg_logprob": -0.17385304875734472, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.01707150600850582}, {"id": 443, "seek": 263830, "start": 2648.1000000000004, "end": 2652.5800000000004, "text": " segmentar la oraci\u00f3n y adem\u00e1s para cada uno de esos segmentos pueden encontrar distintas formas de", "tokens": [50854, 9469, 289, 635, 420, 3482, 288, 21251, 1690, 8411, 8526, 368, 22411, 9469, 329, 14714, 17525, 31489, 296, 33463, 368, 51078], "temperature": 0.0, "avg_logprob": -0.17385304875734472, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.01707150600850582}, {"id": 444, "seek": 263830, "start": 2652.5800000000004, "end": 2660.26, "text": " traducirlo en el lenguaje destino con mi tabla de frases entonces el algoritmo de codificaci\u00f3n", "tokens": [51078, 2479, 1311, 347, 752, 465, 806, 35044, 84, 11153, 2677, 2982, 416, 2752, 4421, 875, 368, 431, 1957, 13003, 806, 3501, 50017, 3280, 368, 17656, 40802, 51462], "temperature": 0.0, "avg_logprob": -0.17385304875734472, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.01707150600850582}, {"id": 445, "seek": 263830, "start": 2660.26, "end": 2664.6600000000003, "text": " funciona de la siguiente manera empezamos teniendo en cada paso de la algoritmo vamos a tener un", "tokens": [51462, 26210, 368, 635, 25666, 13913, 18730, 2151, 2064, 7304, 465, 8411, 29212, 368, 635, 3501, 50017, 3280, 5295, 257, 11640, 517, 51682], "temperature": 0.0, "avg_logprob": -0.17385304875734472, "compression_ratio": 1.835820895522388, "no_speech_prob": 0.01707150600850582}, {"id": 446, "seek": 266466, "start": 2664.66, "end": 2670.66, "text": " conjunto de hip\u00f3tesis de traducci\u00f3n se llega a ver ah\u00ed lo que dice de ojos m\u00e1s o menos", "tokens": [50364, 37776, 368, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 369, 40423, 257, 1306, 12571, 450, 631, 10313, 368, 39519, 3573, 277, 8902, 50664], "temperature": 0.0, "avg_logprob": -0.2723665004823266, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.011418227106332779}, {"id": 447, "seek": 266466, "start": 2680.22, "end": 2685.46, "text": " ac\u00e1 quedaron mal los correditos bueno en cada uno de los pasos yo voy a tener un conjunto de hip\u00f3tesis", "tokens": [51142, 23496, 13617, 6372, 2806, 1750, 1181, 986, 11343, 11974, 465, 8411, 8526, 368, 1750, 1736, 329, 5290, 7552, 257, 11640, 517, 37776, 368, 8103, 812, 7269, 271, 51404], "temperature": 0.0, "avg_logprob": -0.2723665004823266, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.011418227106332779}, {"id": 448, "seek": 266466, "start": 2685.46, "end": 2692.7799999999997, "text": " de traducci\u00f3n al principio el algoritmo voy a empezar con una hip\u00f3tesis vac\u00eda como se le", "tokens": [51404, 368, 2479, 1311, 5687, 419, 34308, 806, 3501, 50017, 3280, 7552, 257, 31168, 416, 2002, 8103, 812, 7269, 271, 2842, 2686, 2617, 369, 476, 51770], "temperature": 0.0, "avg_logprob": -0.2723665004823266, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.011418227106332779}, {"id": 449, "seek": 269278, "start": 2692.78, "end": 2697.7400000000002, "text": " potecis dice que lo importante de leer es la parte de la F que tiene un mont\u00f3n de guiones significa", "tokens": [50364, 280, 1370, 66, 271, 10313, 631, 450, 9416, 368, 34172, 785, 635, 6975, 368, 635, 479, 631, 7066, 517, 45259, 368, 695, 5411, 19957, 50612], "temperature": 0.0, "avg_logprob": -0.20710263289804534, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.005486384034156799}, {"id": 450, "seek": 269278, "start": 2697.7400000000002, "end": 2701.98, "text": " que no hay ninguna palabra del espa\u00f1ol cubierta esas son todas las nueve creo nueve palabras en", "tokens": [50612, 631, 572, 4842, 36073, 31702, 1103, 31177, 10057, 811, 1328, 23388, 1872, 10906, 2439, 10412, 303, 14336, 10412, 303, 35240, 465, 50824], "temperature": 0.0, "avg_logprob": -0.20710263289804534, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.005486384034156799}, {"id": 451, "seek": 269278, "start": 2701.98, "end": 2707.82, "text": " espa\u00f1ol ninguna esta cubierta y esta hip\u00f3tesis tiene probabilidad uno entonces en cada paso del", "tokens": [50824, 31177, 36073, 5283, 10057, 811, 1328, 288, 5283, 8103, 812, 7269, 271, 7066, 31959, 4580, 8526, 13003, 465, 8411, 29212, 1103, 51116], "temperature": 0.0, "avg_logprob": -0.20710263289804534, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.005486384034156799}, {"id": 452, "seek": 269278, "start": 2707.82, "end": 2713.6200000000003, "text": " algoritmo lo que voy a hacer es elegir un par de frases tal que una traducci\u00f3n de la otra y", "tokens": [51116, 3501, 50017, 3280, 450, 631, 7552, 257, 6720, 785, 14459, 347, 517, 971, 368, 431, 1957, 4023, 631, 2002, 2479, 1311, 5687, 368, 635, 13623, 288, 51406], "temperature": 0.0, "avg_logprob": -0.20710263289804534, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.005486384034156799}, {"id": 453, "seek": 269278, "start": 2713.6200000000003, "end": 2718.02, "text": " voy a crear una hip\u00f3tesis nueva a partir de una que ya tengo entonces en este paso lo que hice", "tokens": [51406, 7552, 257, 31984, 2002, 8103, 812, 7269, 271, 28963, 257, 13906, 368, 2002, 631, 2478, 13989, 13003, 465, 4065, 29212, 450, 631, 50026, 51626], "temperature": 0.0, "avg_logprob": -0.20710263289804534, "compression_ratio": 1.8544061302681993, "no_speech_prob": 0.005486384034156799}, {"id": 454, "seek": 271802, "start": 2718.02, "end": 2725.78, "text": " fue decir el hijo el par de frases Mar\u00eda Mary y ah\u00ed me creo una nueva hip\u00f3tesis que cubre la", "tokens": [50364, 9248, 10235, 806, 38390, 806, 971, 368, 431, 1957, 48472, 6059, 288, 12571, 385, 14336, 2002, 28963, 8103, 812, 7269, 271, 631, 10057, 265, 635, 50752], "temperature": 0.0, "avg_logprob": -0.22191643915256531, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004277734085917473}, {"id": 455, "seek": 271802, "start": 2725.78, "end": 2730.74, "text": " primera palabra por eso parece una serie con este caso elige la frase en ingl\u00e9s Mary y ahora", "tokens": [50752, 17382, 31702, 1515, 7287, 14120, 2002, 23030, 416, 4065, 9666, 806, 3969, 635, 38406, 465, 49766, 6059, 288, 9923, 51000], "temperature": 0.0, "avg_logprob": -0.22191643915256531, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004277734085917473}, {"id": 456, "seek": 271802, "start": 2730.74, "end": 2736.5, "text": " tiene una probabilidad de 0 punto 534 ese n\u00famero de esa probabilidad va a servir para guiar un", "tokens": [51000, 7066, 2002, 31959, 4580, 368, 1958, 14326, 1025, 12249, 10167, 14959, 368, 11342, 31959, 4580, 2773, 257, 29463, 1690, 695, 9448, 517, 51288], "temperature": 0.0, "avg_logprob": -0.22191643915256531, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004277734085917473}, {"id": 457, "seek": 271802, "start": 2736.5, "end": 2740.1, "text": " poco en el algoritmo pero vamos a ver despu\u00e9s como es que se calcula por ahora que se es solamente", "tokens": [51288, 10639, 465, 806, 3501, 50017, 3280, 4768, 5295, 257, 1306, 15283, 2617, 785, 631, 369, 4322, 64, 1515, 9923, 631, 369, 785, 27814, 51468], "temperature": 0.0, "avg_logprob": -0.22191643915256531, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004277734085917473}, {"id": 458, "seek": 271802, "start": 2740.1, "end": 2745.88, "text": " con el n\u00famero bien pero entonces yo ten\u00eda otra opci\u00f3n en realidad yo pod\u00eda haber elegido", "tokens": [51468, 416, 806, 14959, 3610, 4768, 13003, 5290, 23718, 13623, 999, 5687, 465, 25635, 5290, 45588, 15811, 14459, 2925, 51757], "temperature": 0.0, "avg_logprob": -0.22191643915256531, "compression_ratio": 1.6950354609929077, "no_speech_prob": 0.004277734085917473}, {"id": 459, "seek": 274588, "start": 2745.88, "end": 2750.56, "text": " empezar en vez de traducir Mar\u00eda por Mary pod\u00eda haber elegido empezar por traducir brujo por", "tokens": [50364, 31168, 465, 5715, 368, 2479, 1311, 347, 48472, 1515, 6059, 45588, 15811, 14459, 2925, 31168, 1515, 2479, 1311, 347, 738, 4579, 78, 1515, 50598], "temperature": 0.0, "avg_logprob": -0.17810726165771484, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.0029182888101786375}, {"id": 460, "seek": 274588, "start": 2750.56, "end": 2759.88, "text": " witch y ah\u00ed me crear\u00eda otra hip\u00f3tesis de traducci\u00f3n donde cubro la pen\u00faltima de las de las", "tokens": [50598, 14867, 288, 12571, 385, 1197, 21178, 13623, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 10488, 10057, 340, 635, 3435, 43447, 4775, 368, 2439, 368, 2439, 51064], "temperature": 0.0, "avg_logprob": -0.17810726165771484, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.0029182888101786375}, {"id": 461, "seek": 274588, "start": 2759.88, "end": 2764.88, "text": " palabras en espa\u00f1ol agarr\u00f3 la palabra witch del hijo de la palabra witch y tiene una probabilidad de", "tokens": [51064, 35240, 465, 31177, 623, 2284, 812, 635, 31702, 14867, 1103, 38390, 368, 635, 31702, 14867, 288, 7066, 2002, 31959, 4580, 368, 51314], "temperature": 0.0, "avg_logprob": -0.17810726165771484, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.0029182888101786375}, {"id": 462, "seek": 274588, "start": 2764.88, "end": 2772.28, "text": " 0 punto 182 entonces en cada paso del algoritmo lo que hace es elegir una hip\u00f3tesis que tiene elegir un", "tokens": [51314, 1958, 14326, 2443, 17, 13003, 465, 8411, 29212, 1103, 3501, 50017, 3280, 450, 631, 10032, 785, 14459, 347, 2002, 8103, 812, 7269, 271, 631, 7066, 14459, 347, 517, 51684], "temperature": 0.0, "avg_logprob": -0.17810726165771484, "compression_ratio": 1.7304347826086957, "no_speech_prob": 0.0029182888101786375}, {"id": 463, "seek": 277228, "start": 2772.28, "end": 2777.44, "text": " par de frases y expandir as\u00ed que lo siguiente que puedo hacer es elegir la frase did not", "tokens": [50364, 971, 368, 431, 1957, 288, 5268, 347, 8582, 631, 450, 25666, 631, 21612, 6720, 785, 14459, 347, 635, 38406, 630, 406, 50622], "temperature": 0.0, "avg_logprob": -0.17883180633304627, "compression_ratio": 1.88715953307393, "no_speech_prob": 0.005751417949795723}, {"id": 464, "seek": 277228, "start": 2777.44, "end": 2783.2400000000002, "text": " expandirla a partir de la hip\u00f3tesis que ten\u00eda con Mary y bueno eso me cubre ahora dos palabras en", "tokens": [50622, 5268, 347, 875, 257, 13906, 368, 635, 8103, 812, 7269, 271, 631, 23718, 416, 6059, 288, 11974, 7287, 385, 10057, 265, 9923, 4491, 35240, 465, 50912], "temperature": 0.0, "avg_logprob": -0.17883180633304627, "compression_ratio": 1.88715953307393, "no_speech_prob": 0.005751417949795723}, {"id": 465, "seek": 277228, "start": 2783.2400000000002, "end": 2790.6800000000003, "text": " espa\u00f1ol y me tiene medio otra probabilidad y despu\u00e9s sigo avanzando y sigo avanzando hasta que", "tokens": [50912, 31177, 288, 385, 7066, 22123, 13623, 31959, 4580, 288, 15283, 4556, 78, 42444, 1806, 288, 4556, 78, 42444, 1806, 10764, 631, 51284], "temperature": 0.0, "avg_logprob": -0.17883180633304627, "compression_ratio": 1.88715953307393, "no_speech_prob": 0.005751417949795723}, {"id": 466, "seek": 277228, "start": 2790.6800000000003, "end": 2795.44, "text": " llevo a cubrir en alg\u00fan momento si yo sigo avanzando y sigo arregando hip\u00f3tesis en alg\u00fan momento voy", "tokens": [51284, 12038, 3080, 257, 10057, 10949, 465, 26300, 9333, 1511, 5290, 4556, 78, 42444, 1806, 288, 4556, 78, 594, 3375, 1806, 8103, 812, 7269, 271, 465, 26300, 9333, 7552, 51522], "temperature": 0.0, "avg_logprob": -0.17883180633304627, "compression_ratio": 1.88715953307393, "no_speech_prob": 0.005751417949795723}, {"id": 467, "seek": 277228, "start": 2795.44, "end": 2801.4, "text": " a llegar a cubrir todas las palabras del idioma espa\u00f1ol todas las palabras de la abrasi\u00f3n en", "tokens": [51522, 257, 24892, 257, 10057, 10949, 10906, 2439, 35240, 1103, 18014, 6440, 31177, 10906, 2439, 35240, 368, 635, 37351, 2560, 465, 51820], "temperature": 0.0, "avg_logprob": -0.17883180633304627, "compression_ratio": 1.88715953307393, "no_speech_prob": 0.005751417949795723}, {"id": 468, "seek": 280140, "start": 2801.4, "end": 2806.52, "text": " espa\u00f1ol entonces ah\u00ed una vez que yo cubri todas las palabras digo bueno esto es una hip\u00f3tesis", "tokens": [50364, 31177, 13003, 12571, 2002, 5715, 631, 5290, 10057, 470, 10906, 2439, 35240, 22990, 11974, 7433, 785, 2002, 8103, 812, 7269, 271, 50620], "temperature": 0.0, "avg_logprob": -0.18259150946318214, "compression_ratio": 1.9348659003831417, "no_speech_prob": 0.0017925428692251444}, {"id": 469, "seek": 280140, "start": 2806.52, "end": 2812.84, "text": " completa y esto lo devuelvo como una potencial candidata digamos una oraci\u00f3n candidata a traducci\u00f3n", "tokens": [50620, 46822, 288, 7433, 450, 1905, 3483, 3080, 2617, 2002, 48265, 6268, 3274, 36430, 2002, 420, 3482, 6268, 3274, 257, 2479, 1311, 5687, 50936], "temperature": 0.0, "avg_logprob": -0.18259150946318214, "compression_ratio": 1.9348659003831417, "no_speech_prob": 0.0017925428692251444}, {"id": 470, "seek": 280140, "start": 2812.84, "end": 2818.64, "text": " pero claro a medida que yo fui avanzando una cosa que pas\u00f3 es que fui dejando hip\u00f3tesis colgadas", "tokens": [50936, 4768, 16742, 257, 32984, 631, 5290, 27863, 42444, 1806, 2002, 10163, 631, 41382, 785, 631, 27863, 21259, 1806, 8103, 812, 7269, 271, 1173, 70, 6872, 51226], "temperature": 0.0, "avg_logprob": -0.18259150946318214, "compression_ratio": 1.9348659003831417, "no_speech_prob": 0.0017925428692251444}, {"id": 471, "seek": 280140, "start": 2818.64, "end": 2824.36, "text": " y esas hip\u00f3tesis podr\u00edan tener otras traducciones posibles yo ac\u00e1 lo que devol\u00ed era una posible", "tokens": [51226, 288, 23388, 8103, 812, 7269, 271, 15305, 11084, 11640, 20244, 2479, 1311, 23469, 1366, 14428, 5290, 696, 842, 450, 631, 1905, 401, 870, 4249, 2002, 26644, 51512], "temperature": 0.0, "avg_logprob": -0.18259150946318214, "compression_ratio": 1.9348659003831417, "no_speech_prob": 0.0017925428692251444}, {"id": 472, "seek": 280140, "start": 2824.36, "end": 2828.64, "text": " traducci\u00f3n pero a medida que yo ten\u00eda las otras hip\u00f3tesis si yo hubiera seguido por las otras hip\u00f3tesis", "tokens": [51512, 2479, 1311, 5687, 4768, 257, 32984, 631, 5290, 23718, 2439, 20244, 8103, 812, 7269, 271, 1511, 5290, 11838, 10609, 8878, 2925, 1515, 2439, 20244, 8103, 812, 7269, 271, 51726], "temperature": 0.0, "avg_logprob": -0.18259150946318214, "compression_ratio": 1.9348659003831417, "no_speech_prob": 0.0017925428692251444}, {"id": 473, "seek": 282864, "start": 2829.3599999999997, "end": 2835.2, "text": " hubiera podido devolver otras cosas entonces yo necesito hacer un backtracking para poder devolver todas", "tokens": [50400, 11838, 10609, 2497, 2925, 1905, 401, 331, 20244, 12218, 13003, 5290, 11909, 3528, 6720, 517, 646, 6903, 14134, 1690, 8152, 1905, 401, 331, 10906, 50692], "temperature": 0.0, "avg_logprob": -0.1913561907681552, "compression_ratio": 1.9023255813953488, "no_speech_prob": 0.0036947831977158785}, {"id": 474, "seek": 282864, "start": 2835.2, "end": 2842.3199999999997, "text": " las posibilidades poder volver a ver las hip\u00f3tesis a revisitar las hip\u00f3tesis y cabilladas y volver a explorar", "tokens": [50692, 2439, 1366, 11607, 10284, 8152, 33998, 257, 1306, 2439, 8103, 812, 7269, 271, 257, 20767, 3981, 2439, 8103, 812, 7269, 271, 288, 5487, 373, 6872, 288, 33998, 257, 24765, 289, 51048], "temperature": 0.0, "avg_logprob": -0.1913561907681552, "compression_ratio": 1.9023255813953488, "no_speech_prob": 0.0036947831977158785}, {"id": 475, "seek": 282864, "start": 2842.3199999999997, "end": 2848.7999999999997, "text": " los otros caminos entonces necesitar\u00eda ser un backtracking para recorrerlas todas y si hago un", "tokens": [51048, 1750, 16422, 1945, 15220, 13003, 11909, 3981, 2686, 816, 517, 646, 6903, 14134, 1690, 850, 284, 9797, 7743, 10906, 288, 1511, 38721, 517, 51372], "temperature": 0.0, "avg_logprob": -0.1913561907681552, "compression_ratio": 1.9023255813953488, "no_speech_prob": 0.0036947831977158785}, {"id": 476, "seek": 282864, "start": 2848.7999999999997, "end": 2856.64, "text": " backtracking lo que va a pasar es que voy a ocurrir una explosi\u00f3n de exponencial del espacio de", "tokens": [51372, 646, 6903, 14134, 450, 631, 2773, 257, 25344, 785, 631, 7552, 257, 26430, 10949, 2002, 9215, 2560, 368, 12680, 26567, 1103, 33845, 368, 51764], "temperature": 0.0, "avg_logprob": -0.1913561907681552, "compression_ratio": 1.9023255813953488, "no_speech_prob": 0.0036947831977158785}, {"id": 477, "seek": 285664, "start": 2856.64, "end": 2862.2, "text": " b\u00fasqueda porque en realidad todas las posibilidades que se abren son exponenciales y ah\u00ed esto como", "tokens": [50364, 272, 10227, 358, 8801, 4021, 465, 25635, 10906, 2439, 1366, 11607, 10284, 631, 369, 410, 1095, 1872, 12680, 26567, 279, 288, 12571, 7433, 2617, 50642], "temperature": 0.0, "avg_logprob": -0.20136312159096323, "compression_ratio": 1.781021897810219, "no_speech_prob": 0.002833052771165967}, {"id": 478, "seek": 285664, "start": 2862.2, "end": 2868.8799999999997, "text": " que se vuelve bastante lento entonces yo quer\u00eda un decodificador para volver este problema un", "tokens": [50642, 631, 369, 20126, 303, 14651, 287, 15467, 13003, 5290, 37869, 517, 979, 378, 1089, 5409, 1690, 33998, 4065, 12395, 517, 50976], "temperature": 0.0, "avg_logprob": -0.20136312159096323, "compression_ratio": 1.781021897810219, "no_speech_prob": 0.002833052771165967}, {"id": 479, "seek": 285664, "start": 2868.8799999999997, "end": 2873.4, "text": " problema tratable en vez de agarrar las infinitas oraciones del idioma me quedo con algunas que", "tokens": [50976, 12395, 21507, 712, 465, 5715, 368, 623, 2284, 289, 2439, 7193, 14182, 420, 9188, 1103, 18014, 6440, 385, 13617, 78, 416, 27316, 631, 51202], "temperature": 0.0, "avg_logprob": -0.20136312159096323, "compression_ratio": 1.781021897810219, "no_speech_prob": 0.002833052771165967}, {"id": 480, "seek": 285664, "start": 2873.4, "end": 2879.48, "text": " sean m\u00e1s probables con este acorimo de codificaci\u00f3n logr\u00e9 reducir de infinito a algo finito pero aun", "tokens": [51202, 37670, 3573, 1239, 2965, 416, 4065, 696, 284, 6934, 368, 17656, 40802, 31013, 526, 2783, 23568, 368, 7193, 3528, 257, 8655, 962, 3528, 4768, 15879, 51506], "temperature": 0.0, "avg_logprob": -0.20136312159096323, "compression_ratio": 1.781021897810219, "no_speech_prob": 0.002833052771165967}, {"id": 481, "seek": 285664, "start": 2879.48, "end": 2884.7599999999998, "text": " as\u00ed es demasiado lento porque hay una explosi\u00f3n combina explosi\u00f3n combinatoria digamos de", "tokens": [51506, 8582, 785, 39820, 287, 15467, 4021, 4842, 2002, 9215, 2560, 2512, 1426, 9215, 2560, 2512, 31927, 654, 36430, 368, 51770], "temperature": 0.0, "avg_logprob": -0.20136312159096323, "compression_ratio": 1.781021897810219, "no_speech_prob": 0.002833052771165967}, {"id": 482, "seek": 288476, "start": 2884.88, "end": 2892.0400000000004, "text": " hip\u00f3tesis y me queda una cantidad exponencial de hip\u00f3tesis entonces como es tan grande este problema", "tokens": [50370, 8103, 812, 7269, 271, 288, 385, 23314, 2002, 33757, 12680, 26567, 368, 8103, 812, 7269, 271, 13003, 2617, 785, 7603, 8883, 4065, 12395, 50728], "temperature": 0.0, "avg_logprob": -0.1559825986623764, "compression_ratio": 1.9880478087649402, "no_speech_prob": 0.0018387807067483664}, {"id": 483, "seek": 288476, "start": 2892.0400000000004, "end": 2896.88, "text": " digamos como la cantidad de hip\u00f3tesis es ponencial y este es un problema en el completo entonces se", "tokens": [50728, 36430, 2617, 635, 33757, 368, 8103, 812, 7269, 271, 785, 9224, 26567, 288, 4065, 785, 517, 12395, 465, 806, 40135, 13003, 369, 50970], "temperature": 0.0, "avg_logprob": -0.1559825986623764, "compression_ratio": 1.9880478087649402, "no_speech_prob": 0.0018387807067483664}, {"id": 484, "seek": 288476, "start": 2896.88, "end": 2902.36, "text": " utilizan t\u00e9cnicas para reducir el espacio de b\u00fasqueda y hay como dos tipos de t\u00e9cnicas algunas", "tokens": [50970, 19906, 282, 25564, 40672, 1690, 2783, 23568, 806, 33845, 368, 272, 10227, 358, 8801, 288, 4842, 2617, 4491, 37105, 368, 25564, 40672, 27316, 51244], "temperature": 0.0, "avg_logprob": -0.1559825986623764, "compression_ratio": 1.9880478087649402, "no_speech_prob": 0.0018387807067483664}, {"id": 485, "seek": 288476, "start": 2902.36, "end": 2907.6400000000003, "text": " son con riesgo y otras son sin riesgo las t\u00e9cnicas sin riesgo lo que quiere decir es que si yo", "tokens": [51244, 1872, 416, 23932, 1571, 288, 20244, 1872, 3343, 23932, 1571, 2439, 25564, 40672, 3343, 23932, 1571, 450, 631, 23877, 10235, 785, 631, 1511, 5290, 51508], "temperature": 0.0, "avg_logprob": -0.1559825986623764, "compression_ratio": 1.9880478087649402, "no_speech_prob": 0.0018387807067483664}, {"id": 486, "seek": 288476, "start": 2907.6400000000003, "end": 2913.76, "text": " aplico una t\u00e9cnica de reducci\u00f3n de hip\u00f3tesis sin riesgo la soluci\u00f3n ideal que yo ten\u00eda dentro de", "tokens": [51508, 25522, 2789, 2002, 45411, 368, 2783, 14735, 368, 8103, 812, 7269, 271, 3343, 23932, 1571, 635, 24807, 5687, 7157, 631, 5290, 23718, 10856, 368, 51814], "temperature": 0.0, "avg_logprob": -0.1559825986623764, "compression_ratio": 1.9880478087649402, "no_speech_prob": 0.0018387807067483664}, {"id": 487, "seek": 291376, "start": 2913.76, "end": 2918.5200000000004, "text": " mi b\u00fasqueda no la voy a perder utilizando una t\u00e9cnica sin riesgo en cambio en la con riesgo si yo", "tokens": [50364, 2752, 272, 10227, 358, 8801, 572, 635, 7552, 257, 26971, 19906, 1806, 2002, 45411, 3343, 23932, 1571, 465, 28731, 465, 635, 416, 23932, 1571, 1511, 5290, 50602], "temperature": 0.0, "avg_logprob": -0.12599354255490186, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.0009539544698782265}, {"id": 488, "seek": 291376, "start": 2918.5200000000004, "end": 2923.6400000000003, "text": " podr\u00eda llegar a perder la soluci\u00f3n \u00f3ptima bien entonces la t\u00e9cnica sin riesgo que conocemos es la", "tokens": [50602, 27246, 24892, 257, 26971, 635, 24807, 5687, 11857, 662, 4775, 3610, 13003, 635, 45411, 3343, 23932, 1571, 631, 33029, 38173, 785, 635, 50858], "temperature": 0.0, "avg_logprob": -0.12599354255490186, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.0009539544698782265}, {"id": 489, "seek": 291376, "start": 2923.6400000000003, "end": 2929.0800000000004, "text": " de recombinaci\u00f3n de hip\u00f3tesis que dice que si yo tengo dos hip\u00f3tesis voy avanzando por dos", "tokens": [50858, 368, 850, 3548, 259, 3482, 368, 8103, 812, 7269, 271, 631, 10313, 631, 1511, 5290, 13989, 4491, 8103, 812, 7269, 271, 7552, 42444, 1806, 1515, 4491, 51130], "temperature": 0.0, "avg_logprob": -0.12599354255490186, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.0009539544698782265}, {"id": 490, "seek": 291376, "start": 2929.0800000000004, "end": 2933.6400000000003, "text": " caminos dentro del acorimo y llevo a dos hip\u00f3tesis iguales por lo menos dos hip\u00f3tesis que cubren las", "tokens": [51130, 1945, 15220, 10856, 1103, 696, 284, 6934, 288, 12038, 3080, 257, 4491, 8103, 812, 7269, 271, 10953, 279, 1515, 450, 8902, 4491, 8103, 812, 7269, 271, 631, 10057, 1095, 2439, 51358], "temperature": 0.0, "avg_logprob": -0.12599354255490186, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.0009539544698782265}, {"id": 491, "seek": 291376, "start": 2933.6400000000003, "end": 2939.0800000000004, "text": " mismas palabras entonces me pudo quedar con la que tiene mayor probabilidad de las dos y descartar", "tokens": [51358, 23220, 296, 35240, 13003, 385, 280, 6207, 39244, 416, 635, 631, 7066, 10120, 31959, 4580, 368, 2439, 4491, 288, 7471, 446, 289, 51630], "temperature": 0.0, "avg_logprob": -0.12599354255490186, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.0009539544698782265}, {"id": 492, "seek": 291376, "start": 2939.0800000000004, "end": 2943.0800000000004, "text": " la otra porque porque a medida que yo voy a seguir avanzando en el acorimo lo que va a pasar es", "tokens": [51630, 635, 13623, 4021, 4021, 257, 32984, 631, 5290, 7552, 257, 18584, 42444, 1806, 465, 806, 696, 284, 6934, 450, 631, 2773, 257, 25344, 785, 51830], "temperature": 0.0, "avg_logprob": -0.12599354255490186, "compression_ratio": 1.9253246753246753, "no_speech_prob": 0.0009539544698782265}, {"id": 493, "seek": 294308, "start": 2943.08, "end": 2947.7599999999998, "text": " que van a bajar las probabilidades digamos eligiendo m\u00e1s palabras y eligiendo m\u00e1s frases me", "tokens": [50364, 631, 3161, 257, 23589, 289, 2439, 31959, 10284, 36430, 806, 328, 7304, 3573, 35240, 288, 806, 328, 7304, 3573, 431, 1957, 385, 50598], "temperature": 0.0, "avg_logprob": -0.16421139624810988, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.0006138952448964119}, {"id": 494, "seek": 294308, "start": 2947.7599999999998, "end": 2953.48, "text": " va a bajar la probabilidad y nunca me va a pasar que una de las hip\u00f3tesis que ten\u00eda menos probabilidad", "tokens": [50598, 2773, 257, 23589, 289, 635, 31959, 4580, 288, 13768, 385, 2773, 257, 25344, 631, 2002, 368, 2439, 8103, 812, 7269, 271, 631, 23718, 8902, 31959, 4580, 50884], "temperature": 0.0, "avg_logprob": -0.16421139624810988, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.0006138952448964119}, {"id": 495, "seek": 294308, "start": 2953.48, "end": 2959.24, "text": " vaya a subir en realidad siempre va a tener menos entonces en definitiva yo puedo con seguridad", "tokens": [50884, 47682, 257, 34785, 465, 25635, 12758, 2773, 257, 11640, 8902, 13003, 465, 28781, 5931, 5290, 21612, 416, 35415, 51172], "temperature": 0.0, "avg_logprob": -0.16421139624810988, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.0006138952448964119}, {"id": 496, "seek": 294308, "start": 2959.24, "end": 2965.64, "text": " descartar la que tiene menos probabilidad bueno esa es recombinaci\u00f3n de hip\u00f3tesis pero ni si", "tokens": [51172, 7471, 446, 289, 635, 631, 7066, 8902, 31959, 4580, 11974, 11342, 785, 850, 3548, 259, 3482, 368, 8103, 812, 7269, 271, 4768, 3867, 1511, 51492], "temperature": 0.0, "avg_logprob": -0.16421139624810988, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.0006138952448964119}, {"id": 497, "seek": 294308, "start": 2965.64, "end": 2969.92, "text": "quiera con eso alcanza digamos para la reducci\u00f3n del espacio de b\u00fasqueda lo suficiente a\u00fan queda", "tokens": [51492, 35134, 416, 7287, 419, 7035, 2394, 36430, 1690, 635, 2783, 14735, 1103, 33845, 368, 272, 10227, 358, 8801, 450, 33958, 31676, 23314, 51706], "temperature": 0.0, "avg_logprob": -0.16421139624810988, "compression_ratio": 1.8988326848249026, "no_speech_prob": 0.0006138952448964119}, {"id": 498, "seek": 296992, "start": 2969.92, "end": 2975.4, "text": " much\u00edsimas hip\u00f3tesis entonces se suele utilizar t\u00e9cnicas de podado con riesgo la t\u00e9cnica de", "tokens": [50364, 29353, 17957, 8103, 812, 7269, 271, 13003, 369, 459, 16884, 24060, 25564, 40672, 368, 2497, 1573, 416, 23932, 1571, 635, 45411, 368, 50638], "temperature": 0.0, "avg_logprob": -0.1943294437787005, "compression_ratio": 1.9471544715447155, "no_speech_prob": 0.00457144295796752}, {"id": 499, "seek": 296992, "start": 2975.4, "end": 2979.88, "text": " histograma la t\u00e9cnica de lumbral el histograma significa que a cada paso digamos en cada paso del", "tokens": [50638, 49816, 64, 635, 45411, 368, 287, 2860, 2155, 806, 49816, 64, 19957, 631, 257, 8411, 29212, 36430, 465, 8411, 29212, 1103, 50862], "temperature": 0.0, "avg_logprob": -0.1943294437787005, "compression_ratio": 1.9471544715447155, "no_speech_prob": 0.00457144295796752}, {"id": 500, "seek": 296992, "start": 2979.88, "end": 2985.96, "text": " acorimo yo me quedo con los n las n hip\u00f3tesis de traducci\u00f3n m\u00e1s probable y descarto las otras y", "tokens": [50862, 696, 284, 6934, 5290, 385, 13617, 78, 416, 1750, 297, 2439, 297, 8103, 812, 7269, 271, 368, 2479, 1311, 5687, 3573, 21759, 288, 7471, 15864, 2439, 20244, 288, 51166], "temperature": 0.0, "avg_logprob": -0.1943294437787005, "compression_ratio": 1.9471544715447155, "no_speech_prob": 0.00457144295796752}, {"id": 501, "seek": 296992, "start": 2985.96, "end": 2991.08, "text": " la t\u00e9cnica con humbral dice que a cada paso del acorimo me quedo con la hip\u00f3tesis de mayor", "tokens": [51166, 635, 45411, 416, 276, 2860, 2155, 10313, 631, 257, 8411, 29212, 1103, 696, 284, 6934, 385, 13617, 78, 416, 635, 8103, 812, 7269, 271, 368, 10120, 51422], "temperature": 0.0, "avg_logprob": -0.1943294437787005, "compression_ratio": 1.9471544715447155, "no_speech_prob": 0.00457144295796752}, {"id": 502, "seek": 296992, "start": 2991.08, "end": 2998.52, "text": " probabilidad y las que est\u00e9n a una distancia alpha m\u00e1ximo de esa cu\u00e1l es el riesgo de las", "tokens": [51422, 31959, 4580, 288, 2439, 631, 871, 3516, 257, 2002, 1483, 22862, 8961, 38876, 368, 11342, 44318, 785, 806, 23932, 1571, 368, 2439, 51794], "temperature": 0.0, "avg_logprob": -0.1943294437787005, "compression_ratio": 1.9471544715447155, "no_speech_prob": 0.00457144295796752}, {"id": 503, "seek": 299852, "start": 2998.52, "end": 3003.72, "text": " t\u00e9cnicas de podado que si la mejor traducci\u00f3n y la traducci\u00f3n \u00f3ptima ten\u00eda algunas frases muy", "tokens": [50364, 25564, 40672, 368, 2497, 1573, 631, 1511, 635, 11479, 2479, 1311, 5687, 288, 635, 2479, 1311, 5687, 11857, 662, 4775, 23718, 27316, 431, 1957, 5323, 50624], "temperature": 0.0, "avg_logprob": -0.24350413154153264, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0005526181776076555}, {"id": 504, "seek": 299852, "start": 3003.72, "end": 3010.48, "text": " poco probables al principio entonces probablemente yo descarte esa soluci\u00f3n de en los primeros pasos y", "tokens": [50624, 10639, 1239, 2965, 419, 34308, 13003, 21759, 4082, 5290, 7471, 11026, 11342, 24807, 5687, 368, 465, 1750, 12595, 329, 1736, 329, 288, 50962], "temperature": 0.0, "avg_logprob": -0.24350413154153264, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0005526181776076555}, {"id": 505, "seek": 299852, "start": 3010.48, "end": 3013.7599999999998, "text": " no llegan a contar la soluci\u00f3n \u00f3ptima digamos la perd\u00ed por el hecho de arpodado", "tokens": [50962, 572, 11234, 282, 257, 27045, 635, 24807, 5687, 11857, 662, 4775, 36430, 635, 12611, 870, 1515, 806, 13064, 368, 594, 79, 378, 1573, 51126], "temperature": 0.0, "avg_logprob": -0.24350413154153264, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0005526181776076555}, {"id": 506, "seek": 299852, "start": 3015.48, "end": 3020.52, "text": " sin embargo bueno tiene como ventaja que en realidad reduce much\u00edsimo el espacio de b\u00fasqueda y vuelve", "tokens": [51212, 3343, 23955, 11974, 7066, 2617, 6931, 12908, 631, 465, 25635, 5407, 44722, 806, 33845, 368, 272, 10227, 358, 8801, 288, 20126, 303, 51464], "temperature": 0.0, "avg_logprob": -0.24350413154153264, "compression_ratio": 1.6276150627615062, "no_speech_prob": 0.0005526181776076555}, {"id": 507, "seek": 302052, "start": 3020.52, "end": 3028.8, "text": " este problema un problema tratable bueno y ahora s\u00ed qu\u00e9 significaba esa probabilidad que estaba", "tokens": [50364, 4065, 12395, 517, 12395, 21507, 712, 11974, 288, 9923, 8600, 8057, 3350, 5509, 11342, 31959, 4580, 631, 17544, 50778], "temperature": 0.0, "avg_logprob": -0.1854319532378381, "compression_ratio": 1.948, "no_speech_prob": 0.003549958812072873}, {"id": 508, "seek": 302052, "start": 3028.8, "end": 3034.72, "text": " viendo en cada una de las hip\u00f3tesis o sea el podado necesita tener las mejores hip\u00f3tesis y bueno para", "tokens": [50778, 34506, 465, 8411, 2002, 368, 2439, 8103, 812, 7269, 271, 277, 4158, 806, 2497, 1573, 45485, 11640, 2439, 42284, 8103, 812, 7269, 271, 288, 11974, 1690, 51074], "temperature": 0.0, "avg_logprob": -0.1854319532378381, "compression_ratio": 1.948, "no_speech_prob": 0.003549958812072873}, {"id": 509, "seek": 302052, "start": 3034.72, "end": 3038.64, "text": " la recomendaci\u00f3n tambi\u00e9n necesito saber la probabilidad de la hip\u00f3tesis bueno la forma de", "tokens": [51074, 635, 40292, 3482, 6407, 11909, 3528, 12489, 635, 31959, 4580, 368, 635, 8103, 812, 7269, 271, 11974, 635, 8366, 368, 51270], "temperature": 0.0, "avg_logprob": -0.1854319532378381, "compression_ratio": 1.948, "no_speech_prob": 0.003549958812072873}, {"id": 510, "seek": 302052, "start": 3038.64, "end": 3043.36, "text": " calcular la probabilidad de la hip\u00f3tesis se divide en dos digamos tengo lo que encontr\u00e9 hasta el", "tokens": [51270, 2104, 17792, 635, 31959, 4580, 368, 635, 8103, 812, 7269, 271, 369, 9845, 465, 4491, 36430, 13989, 450, 631, 10176, 10521, 10764, 806, 51506], "temperature": 0.0, "avg_logprob": -0.1854319532378381, "compression_ratio": 1.948, "no_speech_prob": 0.003549958812072873}, {"id": 511, "seek": 302052, "start": 3043.36, "end": 3047.52, "text": " momento la hip\u00f3tesis lleva cubierta a cierta cantidad de palabras entonces para esa cantidad", "tokens": [51506, 9333, 635, 8103, 812, 7269, 271, 37681, 10057, 811, 1328, 257, 39769, 1328, 33757, 368, 35240, 13003, 1690, 11342, 33757, 51714], "temperature": 0.0, "avg_logprob": -0.1854319532378381, "compression_ratio": 1.948, "no_speech_prob": 0.003549958812072873}, {"id": 512, "seek": 304752, "start": 3047.52, "end": 3052.44, "text": " palabra que ya llevo cubiertas utilizo los tres modelos el modelo de traducci\u00f3n el modelo de", "tokens": [50364, 31702, 631, 2478, 12038, 3080, 10057, 4859, 296, 4976, 19055, 1750, 15890, 2316, 329, 806, 27825, 368, 2479, 1311, 5687, 806, 27825, 368, 50610], "temperature": 0.0, "avg_logprob": -0.14014333089192707, "compression_ratio": 2.192825112107623, "no_speech_prob": 0.008663294836878777}, {"id": 513, "seek": 304752, "start": 3052.44, "end": 3057.52, "text": " ordenamiento del modelo de lenguaje utilizo los tres modelos para calcular la probabilidad de la", "tokens": [50610, 28615, 16971, 1103, 27825, 368, 35044, 84, 11153, 4976, 19055, 1750, 15890, 2316, 329, 1690, 2104, 17792, 635, 31959, 4580, 368, 635, 50864], "temperature": 0.0, "avg_logprob": -0.14014333089192707, "compression_ratio": 2.192825112107623, "no_speech_prob": 0.008663294836878777}, {"id": 514, "seek": 304752, "start": 3057.52, "end": 3063.48, "text": " frase hasta el momento pero para lo que me falta traducir yo no puedo utilizar todo porque no tengo", "tokens": [50864, 38406, 10764, 806, 9333, 4768, 1690, 450, 631, 385, 22111, 2479, 1311, 347, 5290, 572, 21612, 24060, 5149, 4021, 572, 13989, 51162], "temperature": 0.0, "avg_logprob": -0.14014333089192707, "compression_ratio": 2.192825112107623, "no_speech_prob": 0.008663294836878777}, {"id": 515, "seek": 304752, "start": 3063.48, "end": 3067.52, "text": " toda la informaci\u00f3n de traducci\u00f3n entonces lo que hago es utilizar solamente el modelo de traducci\u00f3n", "tokens": [51162, 11687, 635, 21660, 368, 2479, 1311, 5687, 13003, 450, 631, 38721, 785, 24060, 27814, 806, 27825, 368, 2479, 1311, 5687, 51364], "temperature": 0.0, "avg_logprob": -0.14014333089192707, "compression_ratio": 2.192825112107623, "no_speech_prob": 0.008663294836878777}, {"id": 516, "seek": 304752, "start": 3067.52, "end": 3072.84, "text": " y el modelo de lenguaje descarto el modelo de reordenamiento y bueno entonces hago calcula una", "tokens": [51364, 288, 806, 27825, 368, 35044, 84, 11153, 7471, 15864, 806, 27825, 368, 319, 19058, 16971, 288, 11974, 13003, 38721, 4322, 64, 2002, 51630], "temperature": 0.0, "avg_logprob": -0.14014333089192707, "compression_ratio": 2.192825112107623, "no_speech_prob": 0.008663294836878777}, {"id": 517, "seek": 307284, "start": 3072.84, "end": 3076.44, "text": " probabilidad que es una parte con todos los tres modelos y otra parte s\u00ednimo del modelo de", "tokens": [50364, 31959, 4580, 631, 785, 2002, 6975, 416, 6321, 1750, 15890, 2316, 329, 288, 13623, 6975, 8600, 77, 6934, 1103, 27825, 368, 50544], "temperature": 0.0, "avg_logprob": -0.2263826325882313, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.006714160088449717}, {"id": 518, "seek": 307284, "start": 3076.44, "end": 3083.52, "text": " reordenamiento bien este algoritmo que acabamos de describir que hace esta b\u00fasqueda bas\u00e1ndose", "tokens": [50544, 319, 19058, 16971, 3610, 4065, 3501, 50017, 3280, 631, 13281, 2151, 368, 2189, 10119, 631, 10032, 5283, 272, 10227, 358, 8801, 987, 18606, 541, 50898], "temperature": 0.0, "avg_logprob": -0.2263826325882313, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.006714160088449717}, {"id": 519, "seek": 307284, "start": 3083.52, "end": 3089.32, "text": " hip\u00f3tesis que utiliza recomendaci\u00f3n hip\u00f3tesis y bueno el calcula de las probabilidades de esta", "tokens": [50898, 8103, 812, 7269, 271, 631, 4976, 13427, 40292, 3482, 8103, 812, 7269, 271, 288, 11974, 806, 4322, 64, 368, 2439, 31959, 10284, 368, 5283, 51188], "temperature": 0.0, "avg_logprob": -0.2263826325882313, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.006714160088449717}, {"id": 520, "seek": 307284, "start": 3089.32, "end": 3095.32, "text": " manera se conoce como algoritmo b\u00fasqueda esterisco es un algoritmo de vincers que se usa much\u00edsimo", "tokens": [51188, 13913, 369, 33029, 384, 2617, 3501, 50017, 3280, 272, 10227, 358, 8801, 871, 260, 8610, 785, 517, 3501, 50017, 3280, 368, 371, 4647, 433, 631, 369, 29909, 44722, 51488], "temperature": 0.0, "avg_logprob": -0.2263826325882313, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.006714160088449717}, {"id": 521, "seek": 307284, "start": 3095.32, "end": 3101.76, "text": " en lo que es traducci\u00f3n autom\u00e1tica estad\u00edstica por ejemplo el sistema Moses ac\u00e1 tenemos este", "tokens": [51488, 465, 450, 631, 785, 2479, 1311, 5687, 3553, 23432, 39160, 19512, 2262, 1515, 13358, 806, 13245, 17580, 23496, 9914, 4065, 51810], "temperature": 0.0, "avg_logprob": -0.2263826325882313, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.006714160088449717}, {"id": 522, "seek": 310176, "start": 3101.76, "end": 3108.6000000000004, "text": " ejemplo de herramientas open source o gratuita que sirven para construcci\u00f3n de traducciones autom\u00e1ticos", "tokens": [50364, 13358, 368, 38271, 296, 1269, 4009, 277, 38342, 64, 631, 4735, 553, 1690, 12946, 14735, 368, 2479, 1311, 23469, 3553, 7656, 9940, 50706], "temperature": 0.0, "avg_logprob": -0.22975612439607318, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0034937853924930096}, {"id": 523, "seek": 310176, "start": 3108.6000000000004, "end": 3114.2400000000002, "text": " el sistema Moses es un sistema open source para desarrollar este tipo de traducciones autom\u00e1ticos", "tokens": [50706, 806, 13245, 17580, 785, 517, 13245, 1269, 4009, 1690, 32501, 289, 4065, 9746, 368, 2479, 1311, 23469, 3553, 7656, 9940, 50988], "temperature": 0.0, "avg_logprob": -0.22975612439607318, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0034937853924930096}, {"id": 524, "seek": 310176, "start": 3114.2400000000002, "end": 3122.28, "text": " estad\u00edsticos y implementa este algoritmo de codificaci\u00f3n b\u00fasqueda a esterisco y bueno lo que", "tokens": [50988, 39160, 19512, 9940, 288, 4445, 64, 4065, 3501, 50017, 3280, 368, 17656, 40802, 272, 10227, 358, 8801, 257, 871, 260, 8610, 288, 11974, 450, 631, 51390], "temperature": 0.0, "avg_logprob": -0.22975612439607318, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0034937853924930096}, {"id": 525, "seek": 310176, "start": 3122.28, "end": 3126.0, "text": " tiene el sistema Moses de bueno es que en realidad lo que hace adem\u00e1s de implementar el", "tokens": [51390, 7066, 806, 13245, 17580, 368, 11974, 785, 631, 465, 25635, 450, 631, 10032, 21251, 368, 4445, 289, 806, 51576], "temperature": 0.0, "avg_logprob": -0.22975612439607318, "compression_ratio": 1.8177570093457944, "no_speech_prob": 0.0034937853924930096}, {"id": 526, "seek": 312600, "start": 3126.04, "end": 3131.32, "text": " codificadores utiliza a los otros sistemas y los integra de alguna manera entonces integra", "tokens": [50366, 17656, 1089, 11856, 4976, 13427, 257, 1750, 16422, 48720, 288, 1750, 16200, 424, 368, 20651, 13913, 13003, 16200, 424, 50630], "temperature": 0.0, "avg_logprob": -0.3045506050916222, "compression_ratio": 1.75, "no_speech_prob": 0.067948117852211}, {"id": 527, "seek": 312600, "start": 3131.32, "end": 3136.6, "text": " este otro sistema el ircdlm que es una herramienta para crear modelos de lenguaje basados en", "tokens": [50630, 4065, 11921, 13245, 806, 3418, 66, 67, 75, 76, 631, 785, 2002, 38271, 64, 1690, 31984, 2316, 329, 368, 35044, 84, 11153, 987, 4181, 465, 50894], "temperature": 0.0, "avg_logprob": -0.3045506050916222, "compression_ratio": 1.75, "no_speech_prob": 0.067948117852211}, {"id": 528, "seek": 312600, "start": 3136.6, "end": 3141.96, "text": " en enegramas y el otro sistema se guiza m\u00e1s m\u00e1s que lo vi\u00f3 mencionado hoy que es el sistema", "tokens": [50894, 465, 465, 1146, 2356, 296, 288, 806, 11921, 13245, 369, 695, 13427, 3573, 3573, 631, 450, 1932, 812, 37030, 1573, 13775, 631, 785, 806, 13245, 51162], "temperature": 0.0, "avg_logprob": -0.3045506050916222, "compression_ratio": 1.75, "no_speech_prob": 0.067948117852211}, {"id": 529, "seek": 312600, "start": 3141.96, "end": 3149.04, "text": " que me permite alinear corpus de variaciones en los distintos idiomas llegando los modelos", "tokens": [51162, 631, 385, 31105, 419, 533, 289, 1181, 31624, 368, 3034, 9188, 465, 1750, 49337, 18014, 7092, 11234, 1806, 1750, 2316, 329, 51516], "temperature": 0.0, "avg_logprob": -0.3045506050916222, "compression_ratio": 1.75, "no_speech_prob": 0.067948117852211}, {"id": 530, "seek": 312600, "start": 3149.04, "end": 3154.04, "text": " del 1 al 5 de traducci\u00f3n de IBM bueno entonces estas tres herramientas sirven si uno quiere", "tokens": [51516, 1103, 502, 419, 1025, 368, 2479, 1311, 5687, 368, 23487, 11974, 13003, 13897, 15890, 38271, 296, 4735, 553, 1511, 8526, 23877, 51766], "temperature": 0.0, "avg_logprob": -0.3045506050916222, "compression_ratio": 1.75, "no_speech_prob": 0.067948117852211}, {"id": 531, "seek": 315404, "start": 3154.04, "end": 3158.16, "text": " construir un traducador autom\u00e1tico estad\u00edstico entre cualquier par de diomas puede utilizar estas", "tokens": [50364, 38445, 517, 2479, 1311, 5409, 3553, 28234, 39160, 19512, 2789, 3962, 21004, 971, 368, 1026, 7092, 8919, 24060, 13897, 50570], "temperature": 0.0, "avg_logprob": -0.23295749028523763, "compression_ratio": 1.870722433460076, "no_speech_prob": 0.003426390001550317}, {"id": 532, "seek": 315404, "start": 3158.16, "end": 3164.4, "text": " tres herramientas y teniendo un corpus para el hilo y un corpus monolingue puede construirse un", "tokens": [50570, 15890, 38271, 296, 288, 2064, 7304, 517, 1181, 31624, 1690, 806, 276, 10720, 288, 517, 1181, 31624, 1108, 401, 278, 622, 8919, 38445, 405, 517, 50882], "temperature": 0.0, "avg_logprob": -0.23295749028523763, "compression_ratio": 1.870722433460076, "no_speech_prob": 0.003426390001550317}, {"id": 533, "seek": 315404, "start": 3164.4, "end": 3170.84, "text": " traducador pero bueno adem\u00e1s otra cosa que mencionamos en la clase pasada pero eran los sistemas", "tokens": [50882, 2479, 1311, 5409, 4768, 11974, 21251, 13623, 10163, 631, 37030, 2151, 465, 635, 44578, 1736, 1538, 4768, 32762, 1750, 48720, 51204], "temperature": 0.0, "avg_logprob": -0.23295749028523763, "compression_ratio": 1.870722433460076, "no_speech_prob": 0.003426390001550317}, {"id": 534, "seek": 315404, "start": 3170.84, "end": 3175.52, "text": " basados en reglas los sistemas basados en reglas han ca\u00eddo un poco este digamos no tienen tanta", "tokens": [51204, 987, 4181, 465, 1121, 7743, 1750, 48720, 987, 4181, 465, 1121, 7743, 7276, 1335, 28470, 517, 10639, 4065, 36430, 572, 12536, 40864, 51438], "temperature": 0.0, "avg_logprob": -0.23295749028523763, "compression_ratio": 1.870722433460076, "no_speech_prob": 0.003426390001550317}, {"id": 535, "seek": 315404, "start": 3175.52, "end": 3180.6, "text": " popularidad como antes sin embargo algunos es inusando y el sistema apertym es un sistema open source", "tokens": [51438, 3743, 4580, 2617, 11014, 3343, 23955, 21078, 785, 294, 301, 1806, 288, 806, 13245, 43139, 874, 76, 785, 517, 13245, 1269, 4009, 51692], "temperature": 0.0, "avg_logprob": -0.23295749028523763, "compression_ratio": 1.870722433460076, "no_speech_prob": 0.003426390001550317}, {"id": 536, "seek": 318060, "start": 3180.6, "end": 3185.2799999999997, "text": " para construir sistema de traducci\u00f3n basados en reglas que tienen con un mont\u00f3n de pares de", "tokens": [50364, 1690, 38445, 13245, 368, 2479, 1311, 5687, 987, 4181, 465, 1121, 7743, 631, 12536, 416, 517, 45259, 368, 2502, 495, 368, 50598], "temperature": 0.0, "avg_logprob": -0.20320701599121094, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.000452569336630404}, {"id": 537, "seek": 318060, "start": 3185.2799999999997, "end": 3190.4, "text": " lenguajes y bueno ya anda relativamente bien digamos entonces sigue desarrollando hasta hoy", "tokens": [50598, 35044, 84, 29362, 288, 11974, 2478, 21851, 21960, 3439, 3610, 36430, 13003, 34532, 32501, 1806, 10764, 13775, 50854], "temperature": 0.0, "avg_logprob": -0.20320701599121094, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.000452569336630404}, {"id": 538, "seek": 318060, "start": 3190.4, "end": 3196.56, "text": " entonces es una alternativa open source que est\u00e1 basado en reglas en vez de estar basado en estad\u00edsticas", "tokens": [50854, 13003, 785, 2002, 5400, 18740, 1269, 4009, 631, 3192, 987, 1573, 465, 1121, 7743, 465, 5715, 368, 8755, 987, 1573, 465, 39160, 19512, 9150, 51162], "temperature": 0.0, "avg_logprob": -0.20320701599121094, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.000452569336630404}, {"id": 539, "seek": 318060, "start": 3201.16, "end": 3204.36, "text": " y bueno esto es un resumen de lo que vimos as\u00ed que dejamos por ac\u00e1", "tokens": [51392, 288, 11974, 7433, 785, 517, 725, 16988, 368, 450, 631, 49266, 8582, 631, 21259, 2151, 1515, 23496, 51552], "temperature": 0.0, "avg_logprob": -0.20320701599121094, "compression_ratio": 1.6948356807511737, "no_speech_prob": 0.000452569336630404}], "language": "es"}