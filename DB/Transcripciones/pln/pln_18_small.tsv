start	end	text
0	23200	La clase de hoy y la clase que viene vamos a ver el tema de traducción automática y bueno vamos a
23200	30480	arrancar por esto que se conoce como la nota de weaver o el memorando de weaver warren weaver
30480	37600	era un matemático norteamericano de primera mitad de siglo 20 y el tipo trabajó durante la guerra
37600	42440	especialmente en cosas de criptografía en análisis estadístico de códigos etcétera entonces en un
42440	47720	momento dijo lo siguiente dijo es muy tentador decir que un libro escrito en chino es simplemente un
47720	52560	libro escrito en inglés que ha sido codificado en el código chino si tenemos métodos útiles para
52560	56360	resolver casi cualquier problema criptográfico no será que con la interpretación apropiada
56360	66200	ya tendríamos métodos útiles para traducción el opinaba digamos en este memorándum que los códigos
66200	70440	o los métodos que se utilizan para romper códigos criptográficos que son métodos estadísticos se
70440	75240	podían aplicar al problema de la traducción automática y bueno esto introduce algunas
75240	81760	ideas clave como que puede existir un mapeo automático entre un lenguaje y otro y que codificar
81760	88400	de codificar en un lenguaje es análogo a codificar de codificar en un algoritmo criptográfico y
88400	95480	bueno el tiro esa idea en 1949 tomó como 50 años para que esa idea madurara digamos y después
95480	101440	de 50 años los métodos más utilizados hoy en día son métodos estadísticos que bueno que se
101440	106600	basan un poco en estos principios pero claro en esa época era como muy difícil ver qué era lo que
106600	112640	iba a ocurrir entonces bueno vamos a ver un poco esta esta es la agenda de lo que vamos a mirar
112640	118360	vamos a llegar más o menos hasta la mitad hoy y después la clase siguiente y empecemos con un poco
118360	122800	de historia de lo que es la traducción automática esto empezó como muchas otras tecnologías como
122800	128400	una tecnología militar con fines militares inicialmente era durante la guerra fría era resultado
128400	134320	de interés traducir rápidamente y a bajo costo traducir entre el ruso y el inglés digamos a los
134320	139240	norteamericanos les convenía poder traducir entre el inglés y el ruso y bueno en aquella época se
139240	143120	imaginan lo que era los inicios de la computación las computadoras eran caras en las lentas no tenía
143120	147440	mucho poder de computó pero igual había como mucho optimismo de que en poco tiempo si va a poder
147440	153760	resolver todos los problemas íbamos a tener sistemas que iban a traducir bárbaro y bueno era
153760	157880	más o menos la época del desarrollo de la lingüística computacional inspirado un poco en las teorías
157880	162400	de chonsky estaba la idea que se podía escribir reglas para todo y que a partir de eso se podría
162400	170240	llegar a hacer cosas muy muy buenas en particular para traducción hasta que en 1964 apareció el
170240	175240	reporte al pac al pac que era un comité que estaba estudiando cuál eran los avances en
175240	178760	lingüística computacional porque se estaba poniendo se estaba poniendo mucha plata en muchas
178760	183720	esas cosas y eso se mostraron escépticos acerca de la traducción automática acerca de los logros
183720	188600	que se habían logrado después de todos esos años de meter plata y decía bueno pero se puso
188600	192680	mucho dinero pasó en pasar muchos años pero todavía los humanos lo hacen más barato con
192680	198000	mayor precisión más rápido entonces como que para qué estamos gastando en esto como resultado
198000	201920	de eso hubo un recorte de fondos especialmente en estados unidos para todo lo que es traducción
201920	206680	automática y esto fue parte de lo que se conoció como el invierno de la inteligencia artificial que
206680	210720	un montón de proyectos de inteligencia artificial también no tenía buenos resultados entonces
210720	215960	separó la financiación que había para todo eso durante unos cuantos años entonces se detuvo el
215960	220600	desarrollo de unas cuantas cosas durante unos cuantos años y bueno después empezaron a resurgir de
220600	228440	a poco pero después de esto digamos en los 70 y hasta los 90 más o menos eso logró que la
228440	232280	investigación se frenara un poco en estados unidos pero empezara a aparecer en otros lados del mundo
232280	237320	como por ejemplo en europa o en japón y ahí empezó llano con con files bélicos sino más bien con
237320	243920	fines comerciales entonces había necesidad de tener traducciones o por lo menos dar soporte a los
243920	248680	traductores humanos con algunas traducciones aunque no estuvieran del todo bien pero bueno dar
248680	252080	algunas traducciones de inicio para que los doctores pudieran los doctores humanos pudieran
252080	256080	continuar además las computadoras empezaron a bajar de precio a tener mayor poder de cómputo
256080	262640	y ésta fue como la era de oro de los sistemas de traducción basados en reglas y vamos a caer unos
262640	267280	ejemplos sistemas distrán que todavía se desarrolla aunque ya no está completamente basado en reglas y
267280	274680	bueno y sistemas que se realizaron en japón y en europa y bueno o sea estos sistemas tenían fines
274680	281960	comerciales y no tanto fines militares pero bueno fines de los 90 y después del 2000 en adelante
281960	287600	empezaron a dejarse de usar un poco los sistemas basados en reglas porque porque empezó a ver mayor
287600	291840	poder de cómputo y mayor cantidad de datos disponibles especialmente con la aparición de
291840	298800	internet empezaron a ver muchísimos datos de texto disponibles y eso permitía construir buenos
298800	303760	modelos estadísticos que pudieran explotar las regularidades de los idiomas entonces aparecieron
303760	307440	distintos tipos de modelos estadísticos los primeros los que llamamos traducciones automáticas
307440	311280	estadísticas el otro traducción basado en ejemplos y aparecían las primeras aplicaciones
311280	315560	comerciales que funcionaban bien que utilizaban modelos estadísticos la primera fue lengua
315560	321040	y luego los traductores que más conocemos hoy en día el bing translate de microsoft y bueno el
321120	326520	translate que probablemente lo conozcan lo hayan usado en algún momento y son traductores que la
326520	331760	verdad que hoy en día se puede decir que funcionan bastante bien entonces bueno los métodos
331760	336200	estadísticos empezaron su boom alrededor del año 2000 y siguen siendo el estado del arte
338280	342360	pero bueno primero vamos a ver un poco de lo que son los sistemas basados en reglas que eran estos
342360	349920	primeros sistemas que mencionamos antes en 1968 un investigador de traducción automática se
349960	355480	llamaba pernar bocua hizo un relevamiento de todos los sistemas que se habían construido más o
355480	361720	menos por la época y los clasificó todos dentro de este diagrama el dibujó un triángulo que ahora
361720	366200	se llama el triángulo de bocua y bueno y en este triángulo se ubican los distintos tipos de
366200	372720	sistemas de traducción basados en reglas se ponen como escalones dentro de este triángulo y los
372720	376880	lados del triángulo tienen como distinta interpretación el lado izquierdo si yo voy subiendo por
376920	381600	este lado en realidad lo que aumenta es la cantidad o el esfuerzo de análisis que tengo que
381600	385160	hacer del lenguaje origen yo siempre quiero traducirlo en lenguaje origen o lenguaje destino
385160	389240	bueno entonces de este lado aumenta el esfuerzo de traducción del lenguaje origen y si voy
389240	393800	bajando del lado derecho aumenta bueno si voy subiendo del lado derecho quiero decir aumenta
393800	399000	el esfuerzo de generación en el lenguaje destino entonces qué quiere decir esto yo ubico distintos
399000	406840	sistemas de traducción la traducción directa es simplemente buscar en el diccionario las palabras
406840	412000	y traducir palabra palabra con poca información más entonces eso casi no necesita ningún tipo
412000	417080	de análisis y casi no necesita generación pero para que son de bien yo necesito ponerle muchas
417080	422280	ganas a las reglas o sea las reglas de traducción tienen que ser muy buenas y tienen que tomar en
422280	427080	cuenta muchos casos para que esa traducción llegue a ser buena entonces es como que la flecha de la
427080	431520	transferencia la flecha de la traducción es mucho más larga en cambio si yo hago un poco de análisis
431520	436240	por ejemplo llegó hasta el nivel de análisis intactico tengo un parcer puedo escribir otro
436240	440720	tipo de reglas que pueden ser un poco más expresivas me resulta un poco más fácil y después si tengo
440720	446320	un generador puedo llegar a traducir entonces si sigo subiendo de vuelta voy a necesitar mayor
446320	449880	esfuerzo de análisis de generación pero las reglas pueden ser más expresivas y más fácil de
449880	455480	escribir y probablemente la traducción sea mejor hasta que si llegamos al al vértice del
455480	460880	triángulo llegamos a la interlingua que es una especie de noción en la cual no necesito ningún
460880	467000	tipo de transferencia vamos a ver un poco dentro de un rato de que se trata eso pero bueno empecemos
467000	471720	a ver los distintos niveles de este triángulo de bocua el de más abajo era la traducción directa
471720	476720	es el enfoque más simple lo único que necesito para este para este enfoque es un diccionario
476720	481760	bilingüe yo quiero traducir entre los idiomas y necesito un diccionario que tenga la correspondencia
481760	486160	entre palabras de un idioma y palabras del otro y lo que voy a hacer es traducir palabra-palabra
486160	491680	o sea puedo agregarle alguna cosa extra como por ejemplo algún reordenamiento local yo que
491680	496320	sé para traducir entre español inglés yo diría que en español el nombre se sigue al adjetivo y
496320	499880	en inglés en realidad lo hacen al revés ponen el adjetivo seguido el nombre entonces ese tipo de
499880	507080	reglas simples se las puedo agregar al sistema y bueno el sistema funcionaría un poco así yo tengo
507080	512200	una oración de entrada en el idioma origen mary didn't slap de greenwich le pasa un analizador
512200	517200	morfológico bastante de superficie que no hace mucho en realidad simplemente me dice que esto era
517200	523520	el verbo du en pasado y seguido por un not y bueno el resto de los tokens siguen igual y acá viene
523520	527920	la parte de diccionario digamos lo siguiente que tengo que hacer es buscar en mi diccionario cada
527920	532760	una de las palabras y poner la palabra correspondiente del otro lado entonces mary queda maría du en
532760	538080	pasado como en español no se usa el du usamos simplemente el marcador de pasado no es no slap es
538080	545760	dar una ufetada de es la green es verde witch es bruja con el diccionario voy poniendo todas las
545760	552000	traducciones y después puedo usar mis reglas de reordenamiento local reordenamiento simple como
552000	557080	por ejemplo que el adjetivo seguido en nombre en inglés en realidad en español se corresponde con
557080	561520	nombre seguido adjetivo entonces verdad de bruja lo cambió por bruja verde acá hay otro reordenamiento
561520	567080	digamos donde tengo una marca de pasado y se la pasó para adelante a lo largo y finalmente lo que
567080	573480	hago es una pequeña generación morfológica con estas marcas y digo bueno este dar en pasado se
573480	579720	transforma en dio entonces me queda maría no dio una ufetada a la bruja verde así que partí de
579720	585680	el texto en el idioma origen merited en slap de green witch y llegué a una oración en el idioma
585680	589920	estino maría no dio una ufetada la bruja verde que parece está bastante bien digamos bastante
589920	595480	bien la traducción entonces así es como funcionaría un poco un sistema de traducción directa como les
595480	600400	parece que funcionan estos sistemas en la práctica digamos que también se comportan en la práctica
600400	605480	este tipo de sistemas pues acá vimos un ejemplo que anda bastante bien digamos pero no sé que
605480	615840	claro y hay otro problema más y es
618680	621800	que no tenga todas las palabras pero además que palabras que se pueden traducir de más de
621800	627120	una manera entonces necesitas saber qué palabra tenés que usar entonces bueno
628280	633160	la web está llena de ejemplos de lo que puede salir mal si yo utilizo un sistema de traducción
633160	638800	directa como éste entonces lo que estábamos viendo recién era los sistemas de traducción directa
638800	644680	vamos a subir un poco en la complejidad de los sistemas y llegar a la transferencia sintáctica
644680	650400	entonces para transferencia sintáctica yo lo que voy a necesitar primero es tener un pársar del
650400	656560	lenguaje origen que me lleva a una una análisis sintáctico y además voy a necesitar un generador
656560	660720	del lenguaje destino que agarra un árbol sintáctico del lenguaje destino y genera una oración
661480	667640	entonces yo lo que puedo hacer es escribir reglas que transforma un árbol en el otro y esas reglas
667640	671320	son un poco más fáciles digamos que lo que necesitaría para un sistema de traducción directa
671320	674840	entonces para el inglés por ejemplo para tu siguiente el inglés y el español yo diría que
674840	679560	si tengo un nominal que es un adjetivo nombre un adjetivo sería un nombre en inglés lo transformaría
679560	685880	en un nombre seguir un adjetivo en español y la regla se escribiría algo así diría tengo
685880	689320	un nominal adjetivo nombre entonces lo cambio por nominal nombre adjetivo
691640	698000	entonces ahora que sabemos cómo funciona esto tratemos de hacer el ejemplo en japonés digamos
698000	703240	cómo serían las reglas para transformar el árbol en inglés de giador soliciendo music a el
703240	710000	japonés kareha ongaku uokiku no kadaizuki desu donde está tenemos la correspondencia de cada una de
710000	717200	las palabras pero claro los árboles son un poco distintos el inglés y el español se caracterizan
717200	722320	por ser lenguajes de tipo no sé si esto lo hemos visto ya en el curso pero son lenguajes de tipo
722320	727160	sbo que significa que habitualmente yo suelo escribir un sujeto se dio un verbo seguido de
727160	732320	un objeto el japonés en cambio es un lenguaje de tipo sb porque habitualmente se escribió
732320	736520	el sujeto seguido del objeto seguido del verbo hay muchos lenguajes que pertenecen a esta otra
736520	743560	categoría entonces bueno queremos escribir reglas de transferencia para transformar este árbol en
743560	751200	aquel otro árbol cómo escribiríamos esas reglas que les parece que reglas utilizaría yo para
751200	765400	transformar un árbol en el otro ahí está una de esas en inglés yo escribo
770400	776280	una frase verbal un grupo verbal como un verbo seguido de un grupo precaucional esta es la que
777000	779280	está y la cambio por qué otra cosa
784840	793000	la cambio por un grupo preposicional que sigue un verbo esa es una qué otra regla tendría que
793000	801440	agregar cuál la elaboración que tiene la operación la operación según esto en inglés es un
801440	808920	pronombre seguido de un verbo seguido de un grupo verbal por qué tendría a cambiarlo
810720	818680	ahora en japonés la operación va a ser el pronombre seguido del verse seguido del verbo
818680	820000	bien alguna otra
822440	827840	ahí está el grupo preposicional que está formado por un tú seguido de un nombre
827840	835160	eso es en inglés y en japonés que va a pasar voy a tener un grupo preposicional que es un nombre
835160	841720	seguido de tú bien entonces con eso más o menos creo que tendría las reglas suficientes para
841720	845760	transformar un árbol en el otro los sistemas de traducción vamos a ver si está bien
847760	850840	son los que escribimos esta es la solución del ejercicio
851840	858520	los sistemas de traducción basados en sintaxis en realidad los sistemas de reglas basados en
858520	863720	sintaxis hacen esto a alto nivel digamos tienen un montón de pares de árboles hay gente que
863720	868640	los analiza y escribe reglas de cómo se transforma uno en el otro a veces las reglas son complicadas
868640	875320	porque se pueden super poner entonces hay que definir prioridades y ese tipo de cosas bueno
875320	880520	esos transferencias sintácticas si seguimos subiendo en la en el triángulo de bocua llegamos
880520	885480	a lo que es la transferencia semántica transferencia semántica uno puede pensarla un poco como lo
885480	889720	que habíamos en la clase pasada utilizando roles semánticos yo tengo un etiquetador de
889720	895000	roles semánticos que agarra la oración juan fue a la tienda y me devuelve los roles de los
895000	900680	constituyentes me dice que juan es el agente y a la tienda es el objetivo o gol digamos es el nombre
900680	907000	del rol entonces yo para ciertos idiomas podría escribir reglas más específicas por ejemplo en
907000	912200	chino ocurre que los sintamas preposicionales que son de tipo objetivo se escriben antes del verbo
912200	916360	pero los demás sintamas preposicionales escriben después o sea el chino es un lenguaje de tipo
916360	923800	sbo igual que el inglés o el español pero cuando el objeto es de tipo gol lo que hacen es ponerlo
923800	930280	antes del verbo entonces yo podría escribir una regla un poco más expresiva para este caso del
930280	937480	chino si yo tuviera los roles semánticos yo diría que un grupo verbal es un verbo seguido de esto no
937480	942720	está tachado sino que era la barrita que quedó arriba es un verbo seguido de una de un grupo
942720	948440	preposicional de tipo gol en chino lo cambiaría por un verbo seguido de perdón por un grupo
948440	955200	producción de tipo gol seguido de un verbo es más costoso para generar y para parcer digamos
955200	958960	necesito tener más esfuerzo de par sin más esfuerzo de generación pero puede escribir mejores
958960	965560	reglas que capturan ciertas particularidades de los lenguajes y si yo sigo subiendo en el
965560	969680	triángulo llego a lo que se conoce como interlingua cuál es la gracia del interlingua cuál es la
969680	974000	idea estos sirve si nosotros estamos en un contexto multicultural estamos trabajando por
974000	979960	ejemplo en la ONU o en el parlamento europeo o algo de eso donde se hablan muchos idiomas si
979960	984720	yo quiero mantener un montón de documentos que estén en todos los idiomas a la vez voy a necesitar
984880	988840	para los sistemas que estuve viendo hasta el momento voy a necesitar tener n parsers uno
988840	993440	para acá de idioma n generadores también uno para acá de idioma y después para cada
993440	997720	parte de idiomas voy a necesitar reglas de transferencia entonces voy a necesitar tener
997720	1003920	en total n por n menos un set de transferencia yo tengo 20 idiomas voy a necesitar 380 conjuntos
1003920	1008320	de reglas de transferencia y esos conjuntos de referencia son largos son grandes son complejos
1008320	1013640	hay que mantenerlos pueden tener errores entonces esto claramente no escala es como muy difícil
1014080	1018480	poder mantener un entorno de todos esos idiomas y poder mantener la traducción en base a reglas
1018480	1025800	entonces la idea del interlingua es decir qué tal si pudiéramos parsear lo suficiente o analizarlo
1025800	1030000	lo suficiente como para llevar a una representación común una representación que capture el
1030000	1034520	significado de todos los idiomas a la vez y además tuviéramos un generador para cada uno de los
1034520	1040640	idiomas si eso pasara si nosotros pudiéramos capturar con una representación el significado de
1040640	1045600	los idiomas a la vez no necesitaríamos transferencia simplemente parseamos y llevamos a esa interlingua
1045600	1052840	y después generamos en el otro idioma esto está muy bien digamos del punto de vista ideal pero es
1052840	1058720	muy difícil de obtener en la práctica que se podría usar como representación de interlingua que
1058720	1063520	podría ser un candidato bueno podríamos usar la lógica de primer orden que era lo que veíamos
1063520	1067680	en las primeras clases de semántica como representar veraciones en los primer orden o alguna de sus
1067680	1072320	variantes que dan cuenta mejor de lo que es la lógica del lenguaje natural como la mínima
1072320	1077120	recurso semánticos o la whole semántics o si no algo más parecido lo que veíamos en la clase
1077120	1081880	anterior de frames construirme frames con el estado de las cosas como por ejemplo esta era la
1081880	1086160	misma oración de hoy mary didn't slap de green wish pero escrita como un frame es hay un evento de
1086160	1092480	slapping el agente es mary ocurre en pasado la polaridad negativa el tema de ese evento es la
1092480	1096640	bruja y la bruja de más es verde yo podría construirme este tipo de frames y usarlos como
1096640	1106320	representaciones pero bueno el problema que tiene crear o pensar en crear una interlingua es que
1106320	1111400	esa interlingua seguro que va a ser muy compleja y seguro que va a tener que modelar las características
1111400	1117640	de todos los idiomas al mismo tiempo y hay características que son complicadas en los
1117640	1124000	distintos idiomas y algunas que ni nos ni nos imaginamos o sea por ejemplo en chino existen
1124000	1127280	palabras distintas para decir hermano mayor y hermano menor y no hay una palabra para decir
1127280	1131840	hermano o sea no hay una palabra que quiere decir solamente hermano en español si y en inglés
1131840	1136640	también en inglés puede decir brother pero en chino no en chino tenés que elegir cuando vas a decir
1136640	1141560	hermano si es hermano mayor o hermano menor entonces imagínense que si yo estoy traduciendo del español
1141560	1147720	al inglés y estoy utilizando una interlingua la interlingua en su parcer necesita poder distinguir
1147720	1151600	en algún momento si estoy hablando de un hermano mayor o un hermano menor porque tiene que lograr
1151600	1156360	la representación suficiente como para poder traducir al chino entonces necesita esa información y
1156360	1160000	no sé dónde la va a sacar la puede sacar de contexto lo puede sacar inventar de algún lado
1160000	1164480	pero en algún momento va a tener que averiguar el hermano que se está hablando en español si es un
1164480	1168680	hermano mayor o menor como para poder tener la representación y después de información se va
1168680	1173320	a perder porque cuando baja de vuelta al lado del inglés de vuelta vuelve a ser brother y no importa si
1173320	1178600	es mayor o menor y esto solamente un caso de un fenómeno que ocurre en chino pero digamos imagínense
1178600	1183600	los fenómenos que ocurren en el idioma en en en todo el tiempo digamos y todas las pequeñas
1183600	1190120	variantes que hay y como en realidad no es cierto que podamos traducir exactamente los mismos conceptos
1190120	1193920	como que es muy difícil encontrar conceptos que se correspondan 100 por ciento de un idioma y otro
1193920	1198360	hay una cosa que llama el principio de incertidumbre la traducción y dice eso que en realidad cuando
1198360	1202520	yo tengo un idioma y otro los conceptos no siempre se van a traducir 100 por ciento bien o sea no
1202520	1207400	siempre la traducción es exacta sino que hay cierto solopamiento y a veces va a funcionar y a veces
1207400	1216600	no bien pero a pesar de que es una utopía tener una interlingua que funcione para todo para todos
1216600	1221240	los lenguajes bien este tipo de tecnología si se utilizan para dominios más acotados para dominios
1221240	1226960	pequeños como por ejemplo el de meteorología yo puedo escribir perfectamente puedo construir una
1226960	1230600	representación de todos los estados meteorológicos que hay si hay viento si hay lluvias y nieve hacia
1230600	1236080	y granizo la temperatura la presión etcétera y traducir los distintos las distintas palabras
1236080	1240440	que se usan los distintos idiomas para dar cuenta de estos conceptos entonces ese dominio acotado
1240440	1246600	es bastante bien manejable con una interlingua y otro ejemplo son los manuales técnicos hay empresas
1246600	1252480	que tienen un montón de documentación técnica o describen las apis de sus productos etcétera y
1252480	1257800	uno suele dar cuando cuando mira la página web digamos que aparece como que con su fijo es porque
1257800	1261680	está en español pero si se lo cambia por n automáticamente te genera otra página exactamente
1261680	1265760	igual pero en inglés en realidad lo que hacen es como mantener una representación abstracta de
1265760	1268200	lo que están escribiendo y generarla en los distintos idiomas
1271400	1275240	bien entonces hasta ahí lo que vimos era como un paneo de lo que son los distintos
1275240	1280080	sistemas basados en reglas ahora vamos a pasar a hablar de lo que es la traducción estadística
1280080	1285840	que es el estado del arte hoy en día y vamos a empezar con un ejemplo un ejemplo de una frase
1285840	1292160	en hebreo que es adona y roi que la traducción sería el señor es mi pastor o del or y es
1292160	1299200	my shepherd y esta frase en realidad funciona bien porque nosotros conocemos que son las ovejas
1299200	1304160	digamos la cultura en la que surgió esta frase conocía que eran las ovejas tenían pastores
1304160	1307800	los pastores este cuidaban las ovejas la llevaban a donde estaban los mejores pastos etcétera
1307800	1314600	entonces esta esta metáfora funcionaba bien digamos la gente describía como se sentía
1314600	1320720	en respecto a dios utilizando esta metáfora pero qué tal si quisiéramos expresar esta misma frase
1321200	1326280	a una cultura que no conoce a las ovejas como por ejemplo los primeros misioneros que vendrían
1326280	1333120	de europa y tendrían contacto con los indígenas americanos no conocían ovejas entonces cómo
1333120	1340720	hacemos para expresarles el concepto de adona y roi una forma de expresarlo es decir bueno
1340720	1346200	traduzco la metáfora el significado de la metáfora digo significa el señor me cuidará que en
1346200	1351680	definitiva es un poco la metáfora quiere decir eso aunque pierda un poco del contenido o si no lo
1351680	1357120	que lo otro que puedo hacer es tratar de ser más fiel al significado original y tratar de traducirlo
1357120	1361320	más literalmente y decir bueno el señor será para mí como un hombre que cuida de animales que tiene
1361320	1369560	el pelo como algodón que es bastante más fiel al original pero sin embargo se entiende mucho menos
1369560	1376360	como que te van a mirar y decirte qué me estás hablando y bueno un poco este es el problema que
1376360	1382920	hay que se enfrentan los traductores humanos todos los días o sea es muy difícil tener las dos
1382920	1389360	cosas ser fiel al original y sonar natural que suene bien en el lenguaje destino una traducción
1389360	1394360	queremos que tenga esas dos propiedades pero muy difícil lograrlo a la vez entonces los traductores
1394360	1399320	humanos saben que esto es imposible en la práctica lo que hacen es tratar de traducir de manera
1399320	1403960	de encontrar un punto intermedio en el cual bueno suene bastante bien pero además sea fiel al
1403960	1412120	significado original entonces esto significa que lo que estamos tratando de hacer al traducir es que
1412120	1418360	estamos tratando de maximizar dos cosas a la vez como dos medidas que queremos maximizar una medida
1418360	1422960	es que tan fiel es mi oración traducida a la oración original a esa medida le vamos a llamar
1422960	1430840	adecuación o fidelidad y en inglés es adecuación fidelidad y la otra medida es que tan natural suena
1430840	1435360	la oración que yo traduje en el lenguaje destino y a esa medida le voy a llamar fluidez o en inglés
1435360	1443480	fluency entonces esta idea de que estoy tratando de maximizar dos medidas a la vez después vamos a
1443480	1446720	ver que en realidad lo que vamos a total maximizar es el producto de las dos medidas porque eso
1446720	1453440	significa maximizar ambas al mismo tiempo es una idea que sirve para poder inferir o para poder
1453440	1457880	construir mecanismos para crear los traductores automáticos y también mecanismo para testarlos
1457880	1465120	y vamos a ver un poco cómo es que funciona eso yo voy a intentar traducir a partir de ahora del
1465120	1469000	resto de la clase y la clase que viene vamos a hablar siempre de que voy a traducir de un lenguaje
1469000	1475160	origen f a un lenguaje destino e vamos a ponerlo acá si no nos olvidamos
1481600	1483480	f es el lenguaje origen
1488480	1490520	y es el lenguaje destino
1491000	1500400	esos nombres surgen porque el paper inicial en donde se empezó a hablar de estas cosas
1500400	1503720	de los métodos estadísticos traducía del francés al inglés entonces sacó los nombres
1503720	1510200	ahí dijo en francés f el inglés e entonces traducimos del origen al destino bueno yo quiero
1510200	1516600	traducir una frase del idioma f a otra frase del idioma e lo que quiero tratar de encontrar es el mejor
1516600	1523000	etecho que maximice a la vez la adecuación y la fluidez o sea de todos los e posibles del lenguaje
1523000	1528640	destino quiero encontrar el que maximice la fluidez de o sea que suene natural y además la
1528640	1537440	adecuación entre la oración origen f y ese que estoy buscando esto esta fórmula así escrita
1537440	1541440	de esa manera estás a acordar algo que hayamos visto ya en el curso en algún momento les suena
1541440	1544560	a algún lado entropía si
1547280	1552600	valles si o sea viene por ese lado se parece al modelo de valles porque esto es otra aplicación
1552600	1556120	del modelo de canal ruidoso el modelo de canal ruidoso lo habíamos visto en el curso cuando
1556120	1560960	vimos correcciones de errores hace ya bastante tiempo y también es una aplicación de lo que es la
1560960	1567280	regla de valles entonces el modelo de canal ruidoso aplicado acá funciona de la siguiente manera yo
1567280	1573640	tengo una oración origen en el lenguaje f que es f chica que tiene m palabras y es bueno f sub 1
1573640	1579920	f sub 2 hasta f sub m y quiero encontrar la mejor oración en el lenguaje destino etecho que es
1579920	1586440	es sub 1 hasta vez es su bene hasta es su bene que maximiza y en realidad lo que yo quiero maximizar
1586440	1592400	originalmente como todos esperaríamos es decir bueno yo quiero encontrar la oración e que maximice
1592400	1596880	la probabilidad de e dado f digamos eso es lo que uno se le ocurriría primero diría bueno yo quiero
1596880	1601480	estoy traduciendo la oración f quiero encontrar la e que me de máximo la probabilidad de e dado
1601480	1607520	f bien pero en realidad yo esto lo puedo descomponer por valles digamos y por definición de probabilidad
1607520	1612600	condicional puede decir que la probabilidad de e dado f es igual a la probabilidad de f dado e por la
1612600	1619360	probabilidad de divido la probabilidad de f digamos esa equivalencia es directa por definición de
1619360	1625120	probabilidad condicional y además como estoy maximizando en e esta f se mantiene constante porque
1625120	1631480	lo que voy variando es la e entonces la tacho o sea maximizar sobre una constante no no hace ningún
1631480	1638960	cambio entonces lo que me queda el final es que yo busco un etecho que es el e que hace máximo la
1638960	1646160	probabilidad de f dado e por la probabilidad de y eso que tenemos escrito ahí se parece mucho a la
1646160	1652680	otra ecuación que teníamos antes digamos se parece mucho a esta ecuación de adecuación de f e y
1652680	1663080	fluidez de entonces esto se conoce como la ecuación fundamental de la traducción automática estadística
1663080	1669640	la vamos a ver unas cuantas veces en estas dos clases la vamos a estar refrescando y funciona
1669640	1675200	de la siguiente manera yo quiero encontrar el etecho que es el e que maximiza el producto de estas
1675200	1680480	dos probabilidades la primera probabilidad pdf dado e es la que se encarga de medir qué tal la
1680480	1685960	adecuación digamos de la frase que tan adecuada es la frase f para la frase e la segunda probabilidad
1685960	1692600	la pd es la que se encarga de la fluidez que tan natural suena esa frase en el lenguaje destino
1692600	1698080	y se calculan con modelos distintos la primera se calcula con lo que se conoce como modelo de traducción
1698080	1702440	y la segunda con lo que se conoce como modelo de lenguaje de hecho los modelos del lenguaje ya
1702440	1708960	los hemos visto en el curso vamos a dar un breve repaso de qué se trataba bueno porque esto es
1708960	1714480	una aplicación de canal ruidoso es una aplicación de canal ruidoso por lo siguiente nosotros estamos
1714480	1720320	tratando de traducir del lenguaje f efe la lenguaje origen al lenguaje que es el lenguaje destino y lo
1720320	1725240	estamos pensando al revés estamos pensando como que alguien emitió los sonidos de la oración e
1725240	1730400	la oración del lenguaje destino eso pasó a través de un canal ruidoso y cuando llegó hasta mí yo
1730400	1735480	escuché los sonidos de la oración efe estoy pensando como esa especie de metáfora alguien emitió
1735480	1740120	e pasó por un canal ruidoso y llegaron los ruidos de efe entonces lo que yo trato de hacer como
1740120	1745120	proceso de traducción es encontrar cuál tiene que haber sido esa e original para que yo haya
1745120	1750280	escuchado la efe cuál es la e original que me da probabilidad máxima de que yo haya escuchado esta efe
1753000	1757920	y bueno por eso es una aplicación de canal ruidoso y bueno la realidad es que en realidad
1757920	1762600	damos vuelta esta probabilidad porque nos da toda otra forma de calcular lo que no podríamos
1762600	1767880	hacerlo si calculamos la probabilidad directa es como que hay mejores herramientas para hacer eso
1767880	1772320	bueno de vuelta esto es la ecuación fundamental de la traducción automática estadística e
1772320	1779200	techo es el argumento que hace máximo la probabilidad de efe dado e por la probabilidad e y para poder
1779200	1784160	resolver esta ecuación necesitamos tres cosas necesitamos un modelo de lenguaje pde que es el
1784160	1792000	que se va a encargar de la fluidez esto se calcula mediante la técnica de negramas en general
1793720	1797600	los engramas son bastante fáciles de construir digamos porque yo necesito texto en un solo
1797600	1805880	idioma solo en el idioma destino pdf dado e es la componente que se encarga de la adecuación y
1805880	1809640	se resuelve mediante el modelo de traducción el modelo de traducción no es tan fácil de
1809640	1812880	construir como el modelo de lenguaje porque para el modelo de traducción voy a necesitar
1812880	1817280	texto bilingüe de hecho voy a necesitar un corpus paralelo que sea texto en dos idiomas que además
1817280	1822960	tengan su correspondencia y además necesito una tercera componente esta tercera componente se
1822960	1828320	llama decodificador y se trata de lo siguiente yo cuando estoy buscando cuando se resuelve esta
1828320	1834000	ecuación yo veo la oración efe y quiero buscar la mejor e que maximice esa ecuación pero en
1834000	1838400	realidad lo que tendría que hacer es probar con todas las oraciones e del idioma destino todas las
1838400	1844720	oraciones posibles que cuántas son las oraciones del idioma estino son infinitas oraciones posibles
1844720	1848680	en el idioma estino entonces yo estaría probando con infinitas oraciones hasta que una de ellas me
1848680	1852840	dé el máximo obviamente esto no es un problema tratable yo no puedo probar con infinitas oraciones
1852840	1858080	lo que necesito es un proceso que me limites a cantidad de búsqueda de infinitas oraciones a
1858080	1864240	algo tratable entonces el decodificador va a ser un algoritmo de búsqueda que va a agarrar la oración
1864240	1871040	en origen y va me va a devolver las 100 200 mil oraciones destino candidatas más probable que
1871040	1876560	a veces lo ocurra para que yo pueda resolver y calcular esa ecuación para esas para esas oraciones
1876560	1881440	en vez de para todas las posibles entonces lo que hace es volver este problema tratable vamos a ver
1881440	1890480	también un algoritmo de codificación que se llama beam search bueno entonces un poco más sobre
1890480	1895120	modelos de lenguaje la componente pde de la ecuación era la que medida la fluidez y se
1895120	1899000	calculaba mediante un modelo de lenguaje los modelos de lenguaje son relativamente fáciles de
1899000	1903440	construir porque necesitamos información monolingua información solamente en el lenguaje destino
1903440	1909840	entonces en la web tenemos montón toneladas información de muchos idiomas entonces como
1909840	1915480	solo necesitamos información idiomas sacamos texto web noticias blogs etcétera y compilamos un gran
1915480	1921640	corpus del lenguaje destino los modelos que se utilizan para traducción automática en general
1921640	1926200	son modelos basados en engramas que ya hemos visto en el curso cómo funcionaban se suele usar orden
1926200	1932880	de 4 o 5 en otras tareas de pdn se suelen usar órdenes más chicos pero para acá da buenos
1932880	1938120	resultados en 4 o 5 y bueno lo importante es tener una gran cantidad de material de entrenamiento o
1938120	1944360	sea los mejores modelos que usan google translate y otras empresas usan trillones de palabras y bueno
1944360	1949800	son necesitan hardware especial especialmente diseñado para poder ir rápido y recuperar la
1949800	1955040	información o si no bueno si estoy hablando un dominio acotado puedo usar datos de dominio para
1955040	1962800	entrenar que también va a ser buenos resultados las técnicas de mutin es cuando vos haya alguna
1962800	1967360	engrama que no viste lo que te va a pasar es que la probabilidad cero y ahí te va a dar todo cero
1967360	1971360	en realidad las mejores técnicas de mutin significa darle una buena probabilidad a eso a pesar de que
1971360	1977360	nunca lo has visto se dice que las mejores mejoras digamos las más grandes mejoras en los modelos
1977360	1981120	en la traducción automática de los últimos años se han dado porque hay mejor en modelo
1981120	1987320	el lenguaje que me dan traducciones que son más fluidas y y bueno y usualmente hay como
1987320	1992840	cierta cierta correlación o cierta inclinación hacia las fluidez la gente prefiere cuando las
1992840	1998480	oraciones son sonan más naturales acá un ejemplo esto era sacado un sistema de traducción del
1998480	2004400	chino al inglés el sistema estadístico basado en sintaxis que cuando no utilizaba modelo
2004400	2010120	lenguaje tenía un puntaje de 25 con 2 al incorporar modelo lenguaje subió como un 20
2010120	2016120	por ciento su su performance y llegó a 31 con 2 como 6 puntos esos puntos corresponden a una
2016120	2020040	medida que vamos a ver dentro un rato que se llama medida blu que es una medida muy utilizada en lo
2020040	2027880	que es traducción estadística traducción automática en general pero bueno ahora solamente
2027880	2034720	saber que 6 puntos es una mejora que es muchísimo y como es que mejora esto mejora haciendo que
2034720	2039320	las traducciones que devuelve en general sean más fluidas son más natural en el lenguaje
2039320	2044000	destino y acá hay un ejemplo de traducciones de ese mismo sistema yo tenía una traducción de
2044000	2049760	referencia que era I don't have enough money with me to buy a new airplane ticket el sistema sin el
2049760	2054680	modelo lenguaje devolvía esta traducción decía don't have enough bag on me change please go a
2054680	2060600	new by plane que no nos entiende mucho que lo que dice no es gramatical pero al agregar el modelo
2060600	2065800	de traducción su traducción es la siguiente I have enough money to buy a new one by air que
2065800	2076320	suena mucho mejor que les parece acerca del significado el significado se lo puesto digamos
2076320	2080080	acá está diciendo que tiene suficiente plata para comprar uno por aire y acá dice que no tiene
2080080	2085280	suficiente plata para comprar un pasaje de avión o sea este suena muchísimo mejor porque
2085280	2089040	está ni siquiera gramatical pero está por lo menos mantenía la negación digamos mantenía que
2089040	2095160	era una oración negativa entonces hay cuidado con esto la traducción suena mucho mejor pero
2095160	2098920	a veces podemos estar sacrificando fidelidad sacrificando adecuación de la traducción
2102120	2106640	bien esos son los modelos de lenguaje ahora pasemos a la otra los modelos de traducción
2107280	2114640	la componente pdf dado de la ecuación mide lo que es la ecuación o fidelidad de una traducción
2114640	2120680	y la otra y para esto necesito corpus paralelos o corpus bilingües que para poder entrenar
2120680	2124120	estos modelos los corpus bilingües son bastante más difíciles de construir que los corpus
2124120	2128800	monolingües digamos no alcanza con hacer una pasada por la web y obtener texto de un idioma
2128800	2136040	y bueno los modelos que vamos a ver son los propuestos por brown brown y su equipo en 1993
2136040	2140400	que trabajan en ibm ellos construyeron cinco modelos de cómo construir cinco modelos digamos
2140400	2145760	en creciente complejidad de cómo construir un modelo de traducción para traducción estadística
2147440	2151920	y bueno los modelos la diferencia entre cada modelo se es en la historia de generación de
2151920	2156080	las de las oraciones candidatas y bueno después vamos a ver también otro modelo un poco más
2156080	2162040	moderno pero bueno vamos a empezar viendo más bien los modelos de brown a qué me refiero con
2162120	2167960	historia de generación de las oraciones candidatas una historia de generación esto lo digo ahora
2167960	2171120	pero en realidad lo vamos a profundizar después una historia de generación en realidad es como
2171120	2175360	una especie de proceso mental que seguiría un traductor cuando quiere pasar de una oración a la
2175360	2180880	otra entonces estas historias se basan en decir bueno un traductor agarra una oración en el idioma
2180880	2186560	origen y después elige la cantidad de palabras que voy a tener el idioma destino reordena palabras
2186560	2190880	después va traduciendo una a una según un diccionario después agrega palabras nuevas que
2190880	2195880	no estaban en la oración ese tipo de cosas digamos el tipo de pasos me lo voy a escribir en
2195880	2200560	la historia de generación y para qué sirve eso sirve para que a cada uno de esos pasos yo le
2200560	2205000	puedo dar un valor numérico un valor en cuanto a probabilidades y después lo que voy a hacer
2205000	2209600	cuando entreno mi sistema es tunear esos valores numéricos tunear todas esas probabilidades para
2209600	2214720	darme el cálculo de probabilidad total vamos a profundizar más de en esto después
2216440	2220040	pero antes de pasar a lo que son los modelos de traducción vamos a hablar un poco de cómo
2220040	2225440	se evaluan estos sistemas en general siempre es importante evaluar todo en el pln digamos porque
2225440	2231000	no hay soluciones perfectas entonces voy a tener sistemas que andan mejor o peor que otros y bueno
2231000	2236320	y la traducción automática obviamente no es la excepción entonces me sirve poder evaluar los
2236320	2239600	sistemas para poder saber qué sistema mejor que el otro y además si yo hago cambios en mi
2239600	2244680	sistema poder evaluar de vuelta a ver si mejoré o no entonces qué puedo considerar una buena
2244720	2248280	traducción para empezar eso es una pregunta que es abierto en su
2251440	2256160	digamos es abierto en su respuesta no o sea yo tenía en un sistema de traducción tenía una
2256160	2260600	referencia un candidato de referencia que era de katsat on the mat digamos esa era una traducción
2260600	2267360	de referencia y un sistema me dio seis posibles candidatos para esa traducción o sea originalmente
2267360	2273480	había una frase por ejemplo en chino la traducción de referencia de katsat on the mat y mi sistema
2273480	2279200	a traducir el chino me dio estas opciones tengo de katsat on mat de on the mat de cat de cat on
2279200	2284920	the floor a katsat on the mat de katsat on the mat con minúscula o de katsat on the straw mat
2286920	2291640	cuáles les parecen que son buenas traducciones de estos candidatos que me dio el sistema
2291640	2299440	cuáles les gustan más la e que es de katsat on the mat pero con minúscula en vez de comayúscula
2299440	2307400	que otra la b on the mat de cat que otra la de les gusta también a katsat on the mat
2314120	2318760	capaz que no calienta tanto dependiendo del uso que le vas a dar esa frase en contexto capaz que no
2318760	2323840	calienta tanto y bueno si la verdad no se ve nada cuando están las cosas marcadas en rojo pero bueno
2323840	2330640	en fin créanme acá la cosa macaza en rojo son las que acaban de decir una buena traducción podemos
2330640	2333800	decir que es una traducción que le gusta a la gente que la gente dice si es una buena traducción
2333800	2340200	entonces acá se elige on the mat sat de cat a katsat on the mat y de katsat on the mat en
2340200	2345960	minúscula y bueno como como decimos es le preguntamos a la gente a ver qué traducciones le gustan y
2345960	2351120	bueno y ahí ponemos cuáles son las mejores traducciones o si no le damos a un conjunto de
2351120	2356600	jurados las traducciones y le decimos que hagan un análisis un poco más preciso y nos digan bueno
2356600	2360560	cuánto le dan en uno al diez de adecuación y cuánto le dan en uno al diez de fluidez
2363560	2369600	esa es otra forma de valor digamos y ahí ya nos están dando las dos medidas en general a los
2369600	2373760	humanos nos cuesta realizar esta evaluación en general tenemos una preferencia de la fluidez
2373760	2380200	como pasaba hoy con el caso de traducción del chino al inglés por los pasajes de avión
2381120	2385880	además la gente no se pone de acuerdo además hay un problema que es que hacer este tipo de
2385880	2390040	evaluaciones con usuarios humanos lleva tiempo digamos hay que pagarles a los usuarios por
2390040	2395760	hora para que estén evaluando sistemas y después yo les di un conjunto de traducciones ellos me
2395760	2400440	las evaluaron y si hay un cambio en mi sistema para mejorarlo y de vueltas le tengo que darle
2400440	2404240	conjunto de traducciones a los humanos y de vuelta lo tienen que evaluar y de vuelta tengo que pagar
2405480	2410200	horas de usuarios humanos para que lo evaluen entonces es difícil de reutilizar yo estar
2410200	2414760	haciendo cambios constantemente en mi sistema y bueno y necesito tener una forma más rápida de
2414760	2419560	evaluar a ver si estoy haciendo las cosas mejor entonces como este proceso de evaluación es largo
2419560	2424080	es engorroso es caro lo que se ha vuelto más popular son los métodos automáticos de evaluación
2424080	2430920	y a continuación vamos a ver uno que es muy utilizado en lo que es la traducción automática
2433640	2439560	bueno cómo funciona un método de evaluación en realidad lo que hace alguien alguien que
2439560	2447000	está diseñando un sistema es crearse un conjunto de oraciones con una traducción cada
2447000	2450640	uno con una traducción de referencia que está bien digamos una traducción hecha a mano entonces
2450640	2455000	yo quiero evaluar un sistema que va del español al inglés lo que tengo es un conjunto de oraciones
2455000	2460360	en español y alguien algún traductor humano me tradujo todas esas oraciones en español y
2460360	2465200	me dio un candidato o más candidato tal vez para cada una digamos a eso le voy a llamar referencias
2465200	2469800	traducciones de referencia lo siguiente que tengo que hacer es poder diseñar una métrica de
2469800	2475040	similitud para que cuando mi sistema me da un candidato a traducción yo puedo establecer una
2475040	2479160	similitud entre ese candidato y alguna de las referencias y bueno después lo que voy a hacer
2479160	2485760	es aplicar esa métrica para los pares candidato y referencias y bueno y sacar como un promedio de
2485760	2492040	todos los valores de similitud que tengo entonces se han inventado muchos métodos de este estilo
2492040	2497120	muchos métodos automáticos que vamos a ver en particular se llama blue que es este una
2497120	2503040	métrica muy difundida en lo que es la traducción automática estadística y bueno primero algunas
2503040	2508240	definiciones le vamos a llamar referencia a una traducción que está traducida manualmente
2508240	2512520	o sea consideramos que es una oración correcta eso es una referencia y le vamos a llamar candidato
2512520	2518040	a una traducción que no tiene porque estar correcta porque le tradujo el sistema automático y le
2518040	2523160	vamos a llamar documento al conjunto de todas las oraciones candidatas al conjunto de todas las
2523160	2528000	oraciones traducidas por el sistema que es lo que vamos a estar evaluando así que recuerden tenemos
2528000	2534760	referencia candidato y documento y bueno qué es lo primero que se nos puede ocurrir hacer cuando
2534760	2540880	queremos saber si un candidato es bueno para la referencia o no lo primero que podemos hacer es
2540880	2545560	tratar de contar las palabras que ocurren en ambos entonces yo puedo tratar de contar
2548280	2553120	palabras que ocurren en el candidato y palabras que ocurren en la referencia y ahí diría que la
2553120	2556920	elección de las palabras del candidato si están las palabras del candidato si están también la
2556920	2561160	referencia yo diría que eso se acerca un poco la adecuación se acerca que bueno por lo menos
2561160	2566840	usó palabras que son fieles a la traducción de referencia pero si además esas palabras están
2566840	2571280	usadas en el mismo orden ahí se acerca un poco más a la fluidez o sea si están usadas en ese
2571280	2577600	mismo orden puede sonar tan natural como la referencia y esto se puede hacer automáticamente haciendo
2577600	2586160	conteos de n gramas acá yo tengo un una referencia que es de cazad mi sistema me tenía que haber
2586160	2593240	devuelto de cazad y tenía dos candidatos candidato a era de caz y el candidato b era cazad de entonces
2593400	2596620	en éldogs вел extraordinary lo que puedo hacer es prevailer de n grams cuáles en gramas de los
2596620	2603360	candidatos pertenecen a la referencia entonces para el caso de deidad en la en grama de pertenece
2603360	2608020	la referencia en el en grama cat pertenece a referencia al en grama de cad o sea el bigama de
2608020	2614460	que también pertenece a la referencia para el caso del candidato de el Unic 1999 pertenece el
2614460	2620560	pertenece, el unigrama D pertenece, pero SatCat este bigrama no pertenece la referencia
2620560	2625600	y CatD tampoco pertenece a la referencia. Y además el único trigrama que hay, SatCatD
2625600	2629880	tampoco está en la referencia. Entonces lo que aparece a la derecha son los engramas
2629880	2637000	que sí pertenecen tanto al candidato como a la referencia. Así que bueno, resumiendo,
2637000	2642360	yo puedo contar la cantidad de hits de unigramas, de bigramas, de trigramas y para el candidato
2642360	2645880	hace cumple que todos los unigramas que hay pertenece a la referencia, así que voy a
2645880	2651440	tener dos de dos hits, para los bigramas voy a tener uno de uno, pero para el candidato
2651440	2657240	B los unigramas me dan tres de tres, digamos tres hits, los bigramas no, o sea tengo dos
2657240	2660680	bigramas posibles si ninguno estaba bien y los trigramas tampoco, tengo un trigrama
2660680	2666440	posible y no estaba bien. Entonces por ahora parece que le va ganando de Cat, el candidato
2666440	2673160	A de Cat le va ganando a SatCatD como traducción. Bien, ¿qué puedo hacer con los conteos de
2673160	2679480	engramas? Lo que hago habitualmente, o sea contar engramas, contar unigramas, bigramas
2679480	2684720	y gramas, se acerca un poco a lo que es la noción de una precisión de algo. Entonces
2684720	2688600	lo que voy a hacer es contarlos por separado, voy a decir voy a contar todos los unigramas
2688600	2692360	por un lado, todos los bigramas por otro, todos los trigramas por otro y para cada uno
2692360	2699320	de esos me voy a armar una precisión. Voy a decir que tengo el candidato CSUB, digamos
2699320	2705280	un candidato que voy a considerar, voy a contar los hits de orden N de CSUB, digamos los hits
2705280	2710760	de unigrama de CSUB, le voy a llamar H de CSUB y voy a contar la cantidad de unigramas
2710760	2716320	totales que hay, le voy a llamar T de CSUB. Pero además voy a hacer esto en vez de hacerlo
2716320	2721080	para una sola oración, para un candidato y su referencia, le voy a hacer para todo
2721080	2726480	el documento, voy a contar todos los unigramas que estaban en mis candidatos, voy a ver cuánto
2726480	2731240	de esos estaban bien y voy a hacer esta división, entonces me va a dar que cuál es la precisión
2731240	2738040	en unigramas. Que va a ser, bueno, tanta cantidad de unigramas estaban bien dividido, toda la
2738040	2742880	cantidad de unigramas que genero en los candidatos. Después voy a hacer eso para bigramas, voy
2742880	2746160	a contar toda la cantidad de bigramas que estaban bien, porque estaban en el candidato
2746160	2749680	en la referencia, dividido toda la cantidad de bigramas que hay en el candidato. Voy
2749760	2753520	a hacer lo mismo para trigramas y voy a hacer lo mismo para 4g, en general se suele llegar
2753520	2758400	hasta 4, digamos en traducción automática estadística, la medida blue llega a calcular
2758400	2763880	hasta 4. Entonces bueno, lo que me defino ahí es lo que se llama probabilidad de orden
2763880	2768880	n, la probabilidad, perdón, precisión de orden n, la precisión para unigrama, la precisión
2768880	2776080	para bigramas, la precisión para trigramas, etcétera. Bien, esta métrica que estamos
2776080	2780960	construyendo es bastante fácil de engañar, en realidad yo me definí una probabilidad,
2780960	2784600	por ejemplo la probabilidad de orden 1 y la puedo engañar muy fácil, porque yo me
2784600	2789800	puedo construir un candidato que tiene siempre la misma palabra. Puedo decir, bueno, un candidato
2789800	2797080	para la referencia de katsato nemat es el candidato DDDDD. Como yo justo le envoqué
2797080	2800480	a una palabra que está en la referencia, entonces cuento los unigramas y me da que
2800480	2806400	hay 6 hits de 6, a pesar de que la traducción es horrible. Entonces como hago para evitar
2806400	2810960	esto, lo que se suele hacer es clipping, lo que significa que cuento cuánto es la cantidad
2810960	2814840	máxima de palabras en la referencia y no permito que haya más de eso, entonces yo acá tengo
2814840	2821320	hasta dos palabras D, entonces no puedo contar 6 de 6, tendría que contar máximo 2 de 6.
2821320	2826200	Entonces ahí evitamos ese problema de que bueno alguien se haga el vivo y genere simplemente
2826200	2833400	una sola palabra. Bien, entonces hasta ahora vimos dos cosas, calculamos la precisión
2833400	2838960	de orden n, la precisión de cada uno de los unigramas o bigramas o trigramas, lo segundo
2838960	2842640	que vimos es que vamos a hacer clipping para evitar pasarnos de conteo en las palabras
2842640	2851600	que aparecen más de una vez. Lo tercero que pasa es, veíamos en este ejemplo de acá,
2851600	2857600	acá tenemos dos candidatos de CAT y SAT-CAT-D y lo que pasaba acá era que le estaba yendo
2857600	2864600	mejor a la traducción de de CAT porque tenía todos los unigramas que están en la traducción,
2864600	2867960	están también en la referencia y todos los bigramas también, en cambio el candidato
2867960	2872080	B no, el candidato B tiene unigramas que están pero bigramas y trigramas que no están,
2872080	2877360	entonces en cuanto a precisión el candidato A va bastante mejor. ¿Por qué va bastante
2877360	2882840	mejor el candidato A? Porque es un candidato que es más corto que la referencia, o sea
2882840	2887600	es un candidato que tiene menos palabras. Como venimos definiendo la métrica, si yo
2887600	2892800	tengo una referencia y después tengo un candidato que es justo un prefijo de la referencia,
2892800	2896160	entonces va a cumplir que ese prefijo anda bien en todas las medidas de precisión porque
2896160	2901040	todos los enigramas que tiene van a pertenecer a la referencia. Así que lo que hace la medida
2901040	2910680	blue es penalizar ese tipo de comportamiento, penaliza los candidatos que son muy cortos
2910680	2917200	para que digamos le dé menos puntaje. Entonces, ¿por qué se penalizan los candidatos cortos
2917200	2924280	y no los candidatos largos? ¿Por qué les parece? Candidatos que son demasiado cortos
2924280	2931000	se penalizan pero los demasiado largos no. La respuesta está en la slide, pero bueno.
2931000	2935120	Se penaliza los candidatos cortos porque los candidatos largos, si yo genero un candidato
2935120	2938880	que es mucho más largo que la referencia, lo que va a pasar es que ese candidato tiene
2938880	2943400	enigramas, seguramente tiene enigramas que no pertenecen a la referencia. Entonces, en
2943400	2948040	el conteo de precisión me va a dar un puntaje más bajo. Candidatos largos ya están penalizados
2948040	2953100	por la precisión, candidatos cortos no están penalizados por la precisión. Entonces, necesito
2953100	2959380	otro tipo de penalización para evitar eso. Bien, entonces, lo que vamos a dar es una cosa
2959380	2963980	que se llama penalización por brevedad o brevity penalty, que es un puntaje que se
2963980	2969900	le da en referencia a que tan corto es un candidato respecto a la referencia y bueno,
2969900	2973020	se calcula teniendo en cuenta todo el largo del documento, todo el largo del documento
2973020	2978900	traducido. Entonces acá yo defino que R' es el largo total de todas las referencias,
2978900	2986660	R' es el largo total de todos los candidatos y entonces si el largo de los candidatos es
2986660	2991860	mayor a largo de las referencias, no hay penalización, le pongo un 1, si el largo total de los candidatos
2991860	2998620	es menor a largo de las referencias, entonces lo calculo como e a la 1 menos la división
2998620	3005820	entre los largos. Esto es una definición de probabilidad exponencial, digamos, no es
3005820	3010700	más que eso y en realidad lo que trata de hacer es penalizar traducciones que son
3010700	3016020	muy cortas. Entonces, si yo tenía un candidato que tenía 5 palabras, mientras la referencia
3016020	3021660	tenía 10, lo voy a penalizar fuertemente, le voy a dar un 0.37 de penalización. Si yo
3021660	3026340	tenía un candidato que estaba que era menor pero era más cercano, entonces la penalización
3026340	3031820	no es tanta de 0.78 y después si los largos son iguales o si el candidato es más largo,
3031820	3037820	no penalizo nada, le doy un 1 de puntaje. Bueno, entonces la métrica Blue, que es una
3037820	3043660	métrica muy usada en traducción automática, pone todos estos juntos, digamos, todos estos
3043660	3047980	pedacitos que estuvimos viendo los pone juntos en un solo cálculo. Blue se calcula como
3047980	3056540	la penalización por probabilidad, el brevite penalti, por e a la suma de las precisiones
3056540	3078060	que se ordenen. ¿Qué palabra es ruido? Por ejemplo, Stro. Bueno, esta palabra es un
3078060	3082220	unigrama que le va a dar 0 de precisión, digamos, porque no está. Además, participan
3082220	3085380	un unigrama que también le va a dar mala precisión porque tampoco está el unigrama.
3085380	3090300	Entonces lo que resta en realidad porque no está sumando la precisión. Acá yo tengo
3090300	3097540	1, 2, 3, 4, 5, 6, 7 unigramas de los cuales 6 están bien pero hay uno que no. En cambio,
3097540	3102220	en este tengo 6 unigramas de los cuales los 6 están bien. Entonces acá el hecho de agregar
3102220	3109460	palabras que no están bien, que no están en la referencia ya te penaliza. La diferencia
3109460	3113620	es cuando yo tengo una traducción que es más corta. Si yo diría solo de cut-sat-on,
3113620	3117060	entonces ahí es más corta y no tengo forma de penalizarlo solo con la precisión. Entonces
3117060	3123820	tengo el otro penalizador que es porque la traducción es muy corta. Bien, entonces,
3123820	3133620	les estaba comentando. Acá. La medida Blue se define como una media geométrica, definición
3133620	3138740	de media geométrica, de las precisiones de orden N. También tienes un peso por precisión
3138740	3144220	que se puede variar pero en general se utiliza el mismo peso para todos. Multiplicado por
3144220	3155900	la penalización por brevedad. Bien, eso. O sea, esa es la definición de la métrica
3155900	3161100	Blue que es una métrica que se utiliza muchísimo. Esos puntajes que vemos hoy de 25,2 y 31 con
3161100	3168660	algo eran ejemplos de métrica Blue aplicados a un sistema. Y bueno, una cosa importante,
3168660	3172100	algunos comentarios importantes sobre la métrica Blue es que en general cuando un
3172100	3176020	sistema le da mejor, digamos, un conjunto de traducciones le va mejor en métrica Blue,
3176020	3179940	también le va mejor con un conjunto de humanos que evalúen el sistema. O sea, que tiene una
3179940	3185220	correlación bastante buena con lo que es la evaluación subjetiva humana. Pero como
3185220	3189900	contra, es difícil de interpretar estos puntajes. O sea, si yo tengo un puntaje de, como nos
3189900	3193620	pasaba hoy, que tenía un puntaje de 31, en realidad un 31 es un número que puede ser
3193620	3200540	muy bueno, muy malo, dependiendo del idioma. Pero, o sea, si todo saliera bien y yo tradujer
3200540	3204780	exactamente lo mismo que están las referencias, por construcción la medida me daría uno.
3204780	3208220	Pero en realidad es muy difícil traducir exactamente lo que están las referencias,
3208220	3215020	porque no es cierto que exista una única traducción posible en la traducción, digamos, humana.
3215020	3219020	Oraciones se pueden traducir de manera distinta y estar igualmente bien. Entonces es muy difícil
3219020	3222580	tener un conjunto de referencias que contemple todas las posibilidades. Así que mi traductor,
3222580	3228060	no es el papás que anda bárbaro, pero el puntaje aún no es uno, no es 100, digamos,
3228060	3232580	porque está eligiendo palabras distintas o eligiendo formas de escribir las oraciones
3232580	3240340	distintas. Entonces bueno, por eso es difícil interpretar. Yo tengo un puntaje blue de 30
3240340	3249580	o de 50, o sea, de 0.3 o de 0.5, y puede ser buenísimo para ese sistema. Pero para algo
3249580	3254020	que sí me sirve muchísimo el porcentaje, digamos, el puntaje de blue es para decir,
3254020	3259580	yo tengo mi sistema, lo evaluo, después hago algunos cambios, evaluo de vuelta, y si subió
3259580	3262980	la performance con el puntaje blue, entonces estoy seguro de que mejoró porque hay una
3262980	3265060	correlación con la evaluación subjetiva.
3265060	3281460	Para pasar el español inglés, en realidad lo que pasa es que entrenás otro traductor.
3281460	3287900	No, acá estoy hablando uno solo. Acá estoy hablando solamente en un sentido. Yo tenía
3287900	3297340	un sistema en español, por ejemplo, digo, una oración en español, el gato se sentó,
3297340	3301820	y alguien me dijo, bueno, la traducción de referencia de eso es de cat-sat, y mi sistema
3301820	3306740	me dijo, bueno, pero mis traducciones posibles son de cat y sat-cat-de. Entonces yo tenía
3306740	3310180	un sistema en español, pero que traduce al inglés, digamos, un sistema de traducción
3310180	3316220	de español al inglés, pero no estoy traduciendo en el otro sentido. No, no es como las canciones.
3316220	3320060	Acá, partí del español y llegué al inglés, y estoy tratando de evaluar comparando las
3320060	3324740	frases en inglés esperadas con las frases en inglés generadas. Claro. Probablemente…
3324740	3329460	Acá está el mismo idioma, se entendí. Claro, pero está en el mismo idioma, o sea, lo
3329460	3333780	que nos mostramos acá era cuál era la oración o origen, porque para evaluar no nos importa
3333780	3337860	en realidad, para evaluar nos importa que comparar solamente la oración candidato con
3337860	3343140	la referencia, y la origen nos olvidamos. Sabemos que los dos intentaron traducir de la misma
3343140	3351220	oración, y bueno, y alguno le fue mejor que a otro. Bien, esos son comentarios de
3351220	3356700	Blue, esto era evaluación de los sistemas. Lo siguiente que vamos a ver es el problema
3356700	3360660	de los corpus paralelos. Antes de pasar a lo que son modelos de traducción, vamos a
3360660	3365140	hablar un poco de lo que son los corpus paralelos, que son necesarios para construir un modelo
3365140	3371020	de traducción. Un corpus paralelo consiste en pares de textos en dos idiomas, por ejemplo,
3371100	3375660	tener textos en español y en inglés, pero además yo tengo que tener algún nivel, tengo
3375660	3380060	que tener una correspondencia entre esos textos. De alguna forma, yo tengo que saber cómo
3380060	3386740	se corresponde un texto con el otro. Entonces, bueno, tiene que estar con conjuntos, digamos,
3386740	3391380	ordenados de textos en el lenguaje origen, en el lenguaje destino, y bueno, existen,
3391380	3395660	en el mundo existen corpus paralelos para algunos idiomas, o sea, hay muchos idiomas
3395660	3399780	en el mundo, pero no todos los pares de idiomas tienen corpus paralelo construido, entonces
3399860	3404660	existen paralela de inglés, el chino inglés para la mayoría de los lenguajes europeos,
3404660	3410380	debido a su uso en la Unión Europea, digamos, existen también corpus paralelos para ellos,
3410380	3416300	pero para la gran mayoría de pares de lenguas no hay, digamos, no tengo un par que traduzca
3416300	3421020	entre el chino y el guaraní, por ejemplo, o sea, es poco probable que se construya
3421020	3427540	un par de estilos. Bien, ¿qué es un corpus paralelo? Ya que no se ve nada, de vuelta.
3427540	3434100	Acá hay un ejemplo, que no sé si lo conocen, es un ejemplo famoso de corpus paralelo.
3437620	3444260	Tiene idea de lo que es, lo han visto alguna vez, ¿sí? La piedra de Rosetta. La piedra de Rosetta
3445700	3453900	fue una piedra que la construyeron, o por lo menos la tallaron en el año 196 a.C. y hablaba
3453900	3462540	sobre la coronación de Tolomeo V y su adoración como semi-dios, etcétera, etcétera. Y bueno,
3462540	3469500	estuvo perdida un montón de años hasta que durante las campañas napoleónicas 1799 la
3469500	3475700	encontraron en Egipto, en lugar Rosetta, casualmente, y se la llevaron para Francia y ahí la empezaron
3475700	3480260	a analizar lingüistas, empezaron a tratar de entender qué es lo que decía. Y bueno, descubrieron
3480260	3487140	que tiene tres textos, vieron que tiene como tres regiones, tres textos y después de estudiarla un
3487140	3492420	rato se dieron cuenta que en realidad lo que tiene es el mismo texto en tres idiomas distintos. Y los
3492420	3496860	idiomas eran, el de arriba eran jeroglíficos egipcios, del estilo de lo que uno encuentra dentro de las
3496860	3502740	pirámides, el del medio era egipcio demótico, que era el egipcio vulgar que se usaba digamos en el
3502740	3508780	día a día, y el de abajo el todo era griego antiguo. Entonces, si bien ninguno de los tres idiomas se
3508820	3515460	hablaban, el momento que se encontró la piedra, los tres idiomas antiguos, el griego antiguo por lo
3515460	3520140	menos sí se sabía, digamos, se conocía como idioma, se sabía qué significaba y digamos, había gente
3520140	3524220	que lo estudiaba, los otros dos no, los otros dos eran lenguas completamente perdidas que nadie sabía
3524220	3530220	identificarlas. Pero gracias al hecho de que en realidad se descubrió que los tres textos hablan de
3530220	3535300	lo mismo, son el mismo texto en tres idiomas, entonces ahí se empezó a hacer un trabajo de
3535340	3539860	alineación, digamos, los arqueólogos empezaron a decir, bueno, esta porción de texto acá se
3539860	3543540	corresponde con esta de acá, se corresponde con esta de acá, y etcétera, y a tratar de encontrar
3543540	3548660	correspondencias en los idiomas, y como sabían qué quería decir en griego antiguo, empezaron a poder
3548660	3553300	descubrir qué querían decir en los otros idiomas. Entonces, a raíz de eso, es que empezó, digamos,
3553300	3558740	la egiptología moderna, se pudo empezar a descifrar, que dicen, por ejemplo, los jeroglíficos están en
3558740	3563900	las pirámides y bueno, un montón de cultura egipcia antiguas se conoce gracias a que se pudo descifrar
3563900	3568020	lo que decía esta piedra. Y en definitiva, esto es un ejemplo de corpus paralelos, o sea, tengo el mismo
3568020	3575580	texto en tres idiomas y con un poco de esfuerzo logro alinear cuáles son cada uno de los elementos
3575580	3583780	de mis lenguajes y logro saber la traducción de los tres. Bueno, entonces, eso no llega al concepto
3583780	3589660	de alineación, los corpus paralelos tienen distintos niveles de alineación, lo más fácil de encontrar
3589660	3593180	son corpus que están alineados a nivel de documentos, yo tengo una colección de documentos en
3593180	3597100	español y una colección de documentos en chino y yo sé qué documento se corresponde con qué
3597100	3603020	otro, pero no sé nada más. Sería mejor incluso que estuvieran alineados a nivel de alineación,
3603020	3607580	además de conocer los documentos, yo sé cuál es la relación en español o con cuál es la
3607580	3612420	relación en chino, digamos, tengo una correspondencia entre esas dos, pero sería aún mejor y esto es lo
3612420	3616660	que más nos serviría si estuvieran alineados a nivel de palabra. Cada uno de los caracteres que
3616660	3620260	están en chino se corresponde con qué palabra en español o qué grupo de palabras y cada una de las
3620300	3625540	palabras en español, con qué grupo de caracteres se corresponde en chino. Esto es el ideal, pero claro,
3625540	3630220	o sea, si ya es difícil conseguir cosas que estén alineadas a nivel documento, se imaginan que
3630220	3637660	nadie va a ir a mano alinear a nivel de palabra cada uno de las palabras de los idiomas. Entonces,
3637660	3642820	en la práctica nunca vamos a encontrar un corpus alineado a nivel de palabra, pero vamos a ver que,
3642820	3647900	como resultado de la construcción de los modelos de lenguaje, se produce también como un producto
3647900	3653100	secundario, se produce la alineación de los corpus, entonces obtenés las dos cosas a la vez.
3655660	3661820	Bueno, y otra cosa es que a diferencia del texto monolingua que yo usaba para los modelos
3661820	3667420	de lenguaje, es muy raro que naturalmente se produzcan textos en dos idiomas a la vez, o sea,
3667420	3675260	hay que buscarlos bastante, digamos, bastante cuidadosamente. Existen algunos contextos en
3675260	3679300	donde eso se produce. Por ejemplo, en algunos portales de noticias puede pasar que tengan
3679300	3683660	versiones en distintos idiomas y lo que hagan es traducir las noticias en distintos idiomas.
3683660	3687540	Entonces, si yo puedo encontrar uno de esos, es una buena fuente para construirme un corpus
3687540	3692260	paralelo alineado a nivel de documento. Yo sé, esta noticia se corresponde con esta otra en el otro
3692260	3698660	idioma. Pero un lugar en donde se producen naturalmente este tipo de textos es en los países que
3698660	3704340	son bilingües o multilingües. Por ejemplo, en Canadá, que hablan inglés y francés,
3704340	3709740	las discusiones del Parlamento canadiense siempre por ley tienen que transcribirse en los dos
3709740	3713540	idiomas, tienen que traducirse, si están en inglés se traducen en francés, si están en francés se
3713540	3719380	traducen en inglés, y guardan una correspondencia entre eso, guardan los documentos de todas las
3719380	3724020	discusiones del Parlamento en los dos idiomas. Entonces, ahí, naturalmente se produce un corpus
3724020	3728340	paralelo en el nivel de documentos para el inglés y el francés, ese se conoce como el corpus
3728340	3734500	Hansard. Eso también ocurre en Hong Kong, en Hong Kong se habla inglés y chino, son los idiomas
3734500	3738780	oficiales. Entonces, el corpus más grande que se tiene para inglés y chino está hecho como una
3738780	3742660	compilación de lo que son las discusiones del Parlamento de Hong Kong. Y también pasa en la
3742660	3748780	Unión Europea, en el Parlamento Europeo también tienen la costumbre de traducir todas las discusiones
3748780	3752780	a todos los idiomas o a muchos de los idiomas que se usan en la Unión Europea. Entonces,
3752940	3758420	hay corpus paralelos para casi todos los idiomas de la Unión Europea. Pero claro, todos estos
3758420	3763100	están alineados a nivel de documentos. Yo sé qué documento se corresponde con cuál es otro en el
3763100	3771340	otro idioma, pero no a nivel de oraciones y mucho menos a nivel de palabras. Pero bueno, partiendo
3771340	3777340	de un corpus alineado a nivel de documentos, yo puedo llegar a construirme por lo menos una
3777340	3783900	alineación a nivel de oraciones. Si en un proceso relativamente sencillo, esto se conoce como el
3783900	3794820	algoritmo de Gale y Church, que es un algoritmo relativamente fácil para alinear corpus, o sea,
3794820	3798060	para pasar corpus que están alineados a nivel de documentos, pasarlos a que estén alineados a
3798060	3804020	nivel de oración. Y bueno, esto es un algoritmo que funciona, está un poco basado en lo que era el
3804020	3809540	algoritmo de distancia de edición de Levenstein, que vimos hace bastante tiempo en el curso.
3814620	3819860	Es como muy parecido, también es un algoritmo de programación dinámica, similar a ese, funciona de
3819860	3823780	la siguiente manera. O sea, no vamos a dar lo mucho en detalle, pero vamos a dar una idea de cómo
3823780	3829780	es que funciona. El algoritmo de Gale y Church dice, yo voy a tener un conjunto de oraciones en un idioma
3829780	3841500	y otro conjunto de oraciones en el otro idioma. Entonces considero que un traductor para cada
3841500	3846700	oración pudo haber tenido tres opciones, digamos, para pasarlas al otro idioma. Un traductor,
3846700	3852180	supongan un traductor humano, agarró oraciones que estaban en español y oraciones que estaban en
3852180	3858660	francés. Vamos a no ponerles EIF porque lo que puede confundir con las otras cosas. Así que vamos
3858660	3865860	a decir, el lenguaje origen era F, francés y el lenguaje destino era español. Bien, entonces un
3865860	3870740	traductor humano cada vez que se enfrentaba una oración tenía tres posibilidades. O bien traducía
3870740	3877140	una oración por otra oración, o bien parte esta oración en dos y traduce una oración por dos,
3877140	3882780	o bien borra esta oración. Decide que no es tan importante y agarra y borra la oración. Entonces
3882780	3888140	las tres operaciones que se hacen a nivel de oración son la de transformarla en cero, una o dos
3888140	3898660	oraciones del otro lado. Eso es una cosa. Lo otro es el costo relativo de alinear estas dos oraciones
3898660	3903020	depende del largo relativo de las oraciones. Entonces, si yo tengo dos oraciones que tienen un
3903020	3910780	largo muy parecido, le voy a dar un costo menor para alinearlos, era menor o mayor, si menor. Si
3910780	3915340	tiene un largo muy parecido le voy a dar un valor menor para alinear, si tiene un largo muy
3915380	3920500	distinto, una es muy corta y la otra es muy larga, entonces le doy un valor mayor para alinear. Entonces
3920500	3926740	lo que ellos hacen es pensando en todo este tipo de operaciones que hay, todas las combinaciones
3926740	3935260	de operaciones posibles, o sea, partir esta operación en dos o no partirla o eliminarla o dejarla
3935260	3939700	como está. Entonces, con programación dinámica ven todas las posibilidades, ven todas las posibilidades
3939700	3944980	de operar distinto para llegar al otro lado y calculan las que le da un costo menor. O sea,
3945020	3949780	para cada una de las posibilidades calcula cuál es el costo de cada par de oraciones,
3949780	3957860	suman todos los costos del documento y se quedan con el caso que les dé un costo menor en alineación,
3957860	3962300	eso se puede hacer eficientemente usando programación dinámica, lo mismo que
3962300	3976300	hacíamos con la distancia de edición de Levenstein. Bueno, y este algoritmo que es relativamente
3976300	3982580	sencillo, digamos, es una solución bastante simple, logra una tasa de error muy buena,
3982580	3987940	que es de un 4%, digamos, sobre todo para idiomas relacionados, para idiomas que se
3987940	3992580	parecen como el inglés y el español, etcétera, logra una tasa bastante baja de error de un 4%,
3992580	3996460	hay algunas mejoras que se pueden hacer, pero en realidad un 4% es algo que está bastante bien.
3997460	4002780	Hay un catch que es que para sistemas de traducción distintos o traducciones no literales,
4002780	4006740	esto se rompe un poco, por ejemplo, para traducir entre inglés y chino, que en chino
4006740	4010420	ni siquiera está claro cuáles son los límites de las palabras y eso es más difícil de ver.
4010420	4014780	Entonces, bueno, este tipo de algoritmos no funcionan tan bien. Y bueno,
4014780	4017980	hay variantes que funcionan un poco mejor. Así que bueno.
4021180	4025300	Hoy vamos a dejar por acá y vamos a continuar la próxima con modelos de traducción.
