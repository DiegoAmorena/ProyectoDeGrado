La clase pasa, estuvimos viendo una metodología de clasificación en general, así para cualquier problema de clasificación.
Presialmente, como se para el corpo, que me diga utilizar, una cantidad de aspectos metodológicos que son muy importantes y que lo hicimos independiente del dominio en el que estamos, que es el deprosamiento de la humanidad natural, porque aplica para cualquier problema de clasificación.
Pero en más clasificación, los métodos aplican a utilizar, se pueden definir en general de hecho, en el curso de aprensaje automático, te aprenden con más detalle, lo que vimos en parte del curso.
Es algo automático, aprenden con más detalle, lo que se vimos en la clase sola.
Porque se ven diferentes métodos, estuvimos cual era el método en particular, y hablamos en general un clasificador, un clasificador supervisado,
es, dijimos, aquí el caso donde yo tengo un conjunto de intancias de cosas, un conjunto de clases discreto y tengo que asignarle a cada intancia, a una de tareas, a cada una de esas intancias, uno de los grupos de clases.
Si yo tengo un conjunto de documentos y quiero saber en qué idioma está, lo que te haciendo es un problema de clasificación.
Tengo el conjunto de los documentos, tengo las clases que son los idiomas posible, y yo tengo que sacar una sociedad de una clase.
Podría eventualmente ser más en una clase, podemos tener un problema múltiples, decir ahí variante, ¿no?
Yo podría decir que a cada documento era sin no más de una clase.
Por ejemplo, si lo quiero que la clasificar el tópico de un documento, esto puede ser de espectáculos y de deportes,
estamos hablando de cuantanada con él.
En la clase de hoy, lo que vamos a ver es, vamos a hablar de los mitos que hay de clasificación de un momento.
Y de cómo se aplican algunas tareas del procedimiento de la imaginatura.
O sea, un poco de las características del método y de cómo intanciarlo en algún caso de ejemplo.
Y como yo decía en la clase basada, los mitos de clasificación son muy fundidos en todos los diferentes análisis,
porque generalmente en los elementos de dominio o en los que trabajamos un decreto,
la palabra, la solación, el documento, los tweets, son todos cosas y cretas.
Entonces general vamos a ver metos de clasificación supervisa.
Si yo quisiera, por ejemplo, un proyecto grave que hemos leído pasado, que la clasificaba un tweet,
si era un chiste o no, y ser una tarea de clasificación.
Una tarea que también encaramos, aunque no con demasiado éxito,
era la de calificar el chiste en un rango, en un vacinal, un valor de que también nos estaba.
Si se podía llegar a capturar eso.
Si yo, como lo planteamos, nosotros era que voy a poner una 2, 3, 4, 5 estrellas.
Eso sigue siendo un problema de la estificación supervisa.
Pero si yo lo considero a un contimo, ahí teníamos un problema regresiando,
no son usuarios, pero le más la la la la la la la la la la la la la.
Porque un otro niño, generalmente, es un hijito.
Bueno, probamos a metos de clasificación supervisa.
Y en particular, vamos a hablar de metos probabilistas.
Lo metos probabilistas en general, tenemos la intensidad, o sea, yo no voy a volver
sobre la terminología que vimos en ese pasaje.
Tenemos la intensidad representada por el tributo y queremos asignarlo en la clase.
Pero además, los metos probabilistas, lo que hacen es asignarle
es una probabilidad a cada clase posible.
Entonces, yo no solo te digo esta intensidad,
esta intensidad es un humoristínco,
sino que te digo que este tweet tiene un 85% de chances del humoristico
y no humoristico un 15%.
Y esto, por supuesto, tiene que ser una distribución de probabilidad,
que es un humor uno en tal, es el mayor que cero.
Entonces, y además, los metos probabilistas intentan
obtener una distribución sobre las clasetas de datos.
Sí.
Y, por supuesto, clasificar el general va a ser un nuevo elegido a clasico
en la probabilidad más alta.
Así es que no quiere, simplemente dejar de volver esa distribución
para que otra etapa del proceso lo utilice.
Bueno, yo tengo la posibilidad a hacer eso.
Los metos generativos, que son uno de los tipos de metos que hay,
lo que intentan es, son los que hemos estado viendo hasta ahora
en general, y es lo que tratan de modilar la distribución conjunta.
Es decir, la clases junto con los atributos.
Sí?
Y la setiquetas.
¿Neguardo?
¿Por qué? Porque es lo que necesitan para partir de la redebaixe.
Es decir, yo quiero en la clases.
Dada el conjunto de fiturs, se acuerden que la fitura
era nuestra representación del documento, ¿no?
Características que va a haber un daño caracterizado en el documento.
Entonces, la probabilidad de la clases,
dado los atributos, es igual a la probabilidad conjunta
dividida a la probabilidad de los atributos.
Sí?
Por definición, por la definición de probabilidad condiciona.
Entonces, lo que tratan de modilar es esto.
¿Por qué lo hacen? Porque esta probabilidad es,
realmente son más fácil de estimar que la otra.
Porque la puedo estimar contando más fácilmente.
¿Por qué? Porque fíjense que yo, como condiciono,
dada a la clase.
Y vamos.
Yo, por ejemplo, puedo asumir independencia entre la variabria aleatoria
esta.
O sea, entre los atributos.
Y puedo decir, si estás un independiente,
es pdx12c por pdx2c por no.
Esto no lo puedo hacer en el telado.
Yo no puedo decir pdx1 por porque no nos funciona así,
pero ya ya ya.
La independencia la puedo manajar acá el lado.
¡Y cua izquierda!
¡Quier trap!
¡Cua izquierda!
Piedad de dependencia entre variabria asaliatorias.
Si, si esmina de este lado.
Eso genera toda una teoría que se llama
la de los modelos gráficos,
que ponemos, nosotros no vamos a darle acá,
pero que me dice, bueno, cuál es la estructura
que yo supongo, en términos de dependencia.
Es decir, esta variable dependente de esta no…
y así?
Y podemos bossar que con un gráfico,
¿Cómo va a tomar?
Entonces,
si no es esto, no, la probabilidad de la clase,
¿dó lo atributo en la probabilidad de la clase?
Por la probabilidad de los atributos de la clase,
esto es valles, ¿no?
Ella lo hemos visto varias veces en el curso,
no estamos inventando nada.
Dividido, la probabilidad de los atributos.
Y bueno, y nada, lo que mucho está ahora,
tanto la probabilidad priora y la pese,
como en la probabilidad de veros similitud,
esta, la puedo timar a partir del lado.
Esto ya lo hemos hecho.
Sí.
Pero vamos a tener que simplificar el problema.
El método nadie vaches.
Lo que hace es
asumir que las
lo atributo su independiente entre sí.
Lo cual es una barbaridad conceptual,
si por ejemplo, estamos hablando de un texto y los atributos
son las palabras que tienen.
La mente de la palabra viene en la compañía de esa,
se hace en amiga, entre ellas.
No, es muy palabra positivo, es muy probable que haya otra palabra positivo.
Bueno, es más ese, bueno, sí.
La probabilidad de una palabra solo depende de la clase.
Y, por lo tanto, eso hace que pueda partir la probabilidad,
porque como son independientes,
la probabilidad de que es uno,
dado que es uno por hay que diseñado ese,
pero hay que diseñar ese, pero hay que diseñar ese,
pero hay que diseñar ese, porque la probabilidad de que he dado ese,
es la verdad.
Sí.
Y, bueno, y cómo construyes un clasificador a partir de esto.
Bueno, maximizo lo de arriba,
buscó la clase que maximise lo de arriba,
la abajo es independiente de la clase.
Entonces, buscó la clase que más simise lo de arriba
y ahí tenga un clasificador.
¿De acuerdo?
Es muy sencillo.
Tomo el todo lo va a tener un clasificador,
lo conocí en independiente.
Ahora, vamos a ver en algún ejemplo,
y voy a colar clase que maximiza.
El método nadie va a decir, funciona,
y sea muy bien,
como va a ser para un clasificador.
Y por poco a plata,
uno hace un clasificador,
como la gente que capaqueta le puede llamar
a y en la prensa.
Y ya venido,
yo no sé, cuando es el método de nadie va a decir,
me suena como los años 60,
si bien se va a ser el tema de baixa,
que es 1700,
pero
funciona muy bien,
como primera aproximación rápida,
es algo que uno puede usar,
nadie va a decir,
es sin mucho caro conciencia
y funciona en general muy bien.
En el método de nadie va a decir,
es aplicado a la clasificación de documentos.
Tienen la utilizada,
una de las formas de darloes,
utilizando lo que es una aproximación
vago sports.
Es de ejemplo,
es como el ejemplo cañónico de clasificación,
digamos, no es vago sports,
que es,
yo tengo todo esto,
que es un documento,
que tiene una estructura,
que tiene un orden entre las palabras,
que tiene una sintaxis,
que tiene una relación informada
con una sesmante,
que vamos a un lado caso,
nada,
de eso.
Y lo que hago solamente
es,
considero que esto es una bolsa de palabras,
la bolsa de seguridad,
la bolsa es un set,
pero que puede tener el elemento repetido.
La bolsa de palabras
y tengo el contigo
de cantidad de veces que una palabra aparece
en ese documento.
Mi representación del documento
es esto.
Mi fitur suen esto.
Entonces,
como hago clasificación,
esto fue lo que hubo en el laboratorio
y lo pasaba.
Entonces,
como se instancia en ahí vayes
el problema de clasificación,
el documento bueno,
las posiciones son todas las posiciones
que tengan el documento que quiero evaluar.
Yo quiero evaluar en la clase.
Bueno, quiero evaluar la clasión de documento
entonces tengo las posiciones que son todas
los toques que aparecen en cada palabra.
En el documento.
Sí.
Y la clase,
según ahí vayes,
es
la clase
que maximiza,
como comentar a lo que haga,
y con realidad es un contigo,
pero yo acabo ahí,
lo voy con la CBC.
Por eso es un bago fuerte.
En la posición y consino
todas las posiones posibles
como decía y,
la clase como la clase que maximiza
la probabilidad de cada palabra
que aparecen en el documento
La clase que hace más provable, considerando independencia, que esa palabra está en ese documento,
digamos, ¿no? La probabilidad de doblades su vida o sea. ¿Y cómo vamos para hacer eso? Y bueno,
para que a discular esos valores, para estimar esos valores, yo digo bueno, nuestros mejores estimadores
que este es correcto que decir nuestro estimador, nuestro mejor estimador de la clase, de la
probabilidad priori, de la probabilidad, estamos hablando de la probabilidad de la clase, si no tuviera
mala palabra, es decir, yo puedo tener una distribución, yo tengo dos documentos que son de
deporte o de música, vamos a poner que son exclusivos. La probabilidad de la clase es el número
de documentos, de deporte sobre el total, o sea, mi probabilidad priori, se abuela en el base,
no? Yo tengo una probabilidad priori que es lo que pienso antes de empezar a ver el documento, y
antes de ver el documento, yo puedo decir bueno, en 90% de los documentos son de deporte, entonces,
mi probabilidad a priori es 0, 9. ¿Y bueno? Es mucho más probable a priori que es un documento de
deporte, yo voy a ajustar esa probabilidad con la probabilidad de la palabra de cada uno,
bueno, entonces, yo estimo esa probabilidad priori con el número de clase de documentos, que tiene
la clase de video total de documentos. Y, similarmente, este estimo por contigo, la probabilidad
de cada palabra de la clase contando, del total de veces que aparecen todas las palabras en los
documentos, de esa clase, o sea, de todas las palabras que aparecen en los documentos, de
deportes, cuantas veces aparecen esa palabra de deporte, ¿no? Es una palabra común en un dominio
de deporte, ¿sesta? Y es lo que se pregunta. Y un típica a todas esas probabilidades, que
seguramente operativamente tengamos que usar lo harismo y sumar, porque si no no va a haber
muy chiquita, pero justamente lo mismo. ¿Sentiendes? ¿Por qué, en vez de usar esto, de usar esto?
¿Por qué tengo la selección?
¿Por qué tengo la selección?
¿Por qué no va a hacer esto? ¿Por qué tengo la selección?
La plaza, regregó uno a cada contador para no tener el problema de que si una de esta
probabilidad, lo mismo que no pasó con los enliagramas, si les una conocido, porque
lo mismo, si una de que se aprovella de hacer o se me cansella a toda la clase, la
clase va a ser cero. Entonces, para eso hacemos la plaza, hacemos el muffin, suavezado,
margandole uno a cada contador.
Por ejemplo, bueno, todo esto que yo estoy diciendo está en el capítulo 7, más o menos,
que es real, del capítulo 7 del libro de Martín y Juraki. El libro de Martín y Juraki
está en line, los capítulos nuevos. De hecho, todos los capítulos correspondiendo a clase que
hemos dado, están en line, yo realmente les recomiendo leerlo a su libro que está muy claro,
no va a tener mucha más dificultad que lo que vemos en la clase. Por lo menos, no se
uno pierde perspectiva, ¿no?
Claro, pero, pero...
¿Qué le pasa?
¿Qué le haces en mi gira?
Y ahí pueden, ya hay algunos detalles más, que me parece muy interesante, si a ustedes les
interesa este momento. Bueno, supongo vamos con nosotros, queremos el cuerpo entre el
almiento que tenemos arriba, la población que está en arriba y con una categoría negativa
positivo, algún tipo, en este caso, estamos haciendo sentimientos análisis, es decir, analizar
si la percepción es positivo o negativa sobre un documento, en el cuerpo de los tweets,
hacíamos algo así, algo parecido, es decir, yo necesito saber si la clase del cuerpo
del Twitter de humor o no de no humor. Bueno, y ahí tenemos algunos que es ploridad
y otro positivo, y queremos saber qué pasa con predictador, with no originality.
Entonces, la progilia priori de la clase cual es, y es, en total de documentos, hay
unos dos tres, cuatro, cinco de los cuales tres son negativas y dos son positivas, o sea,
el que está son nuestra progilia priori. Y luego entramos a buscar la progilia de cada
palabra, la progilia de predictador dado que en la clase es negativa, es uno que es la ocurrencia
de predictador, el predictador solo aparece en la segunda oración y en un contexto negativo.
Entonces, acaba uno y ya cada tenemos el más ainte para normalizar el para la plaza,
sea uno más uno y 14, que es el total de palabra, más ainte. 14 y si el total de palabra diferente,
¿dóngono?
No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.
De las palabras diferentes, de las palabras, como fácil es verloca.
Si, en la clase, ¿no?
De la clase.
De la clase.
La gente va a la hora que hay en la clase.
No, no, no son diferentes, son todas.
De el total de palabra que hay, ¿go contar?
Positivo, 1, 2, 3, 4, 5, 6, 7, 8, 9.
No, son todas, porque acá yo estoy considerando toda la ocurrencia.
Es una de las cosas que se le critican, ahí vayas, es general, es eso, que si yo repito
muchas veces algo, le sumo probabilidad, que a veces no es lo que se quede.
Se le atributos que reiteran cosas, es como que están muy relacionados y no están
aportando información.
Bueno, no, entonces, acá toda la probabilidad es la diferente de palabras.
Fíjense, bueno, ahora no fíjame un ejemplo.
¿Cuál es la probabilidad es?
Es nuestra, es como entrenamos, no es troblasificador, esencialmente.
Bueno, es decir, la partida del golpe de entrenamiento es yo, calcula esta probabilidad
de lo que estoy haciendo, es entrenar.
Como ustedes son cuentas muy sencillas, me hacer, y la significador no iba a ir a ventaja
que tiene, es muy rápido, muy muy rápido.
Tanto para entrenar, como para evaluar, entonces cuando uno quiere acercarse un problema
y ver, ¿qué es tan difícil, les va a explicar un cuerpo de humor?
Entonces, se le arrima con un método de esto, que lo entrenando patadas, y más o menos
tienen una idea, ahí se ha mirado que el pude clasificar el 75-80% de los, bueno, es un problema
que tiene para mejorar un poco, tampoco es que es horrible de difícil, ¿no?
Y luego sí empieza a afinar, a ajustar para metro, a cambiar el método, capaz que le meto
en una regal edad, todo el método en una regna uronada que está, una semana entrenando,
pero con esto, es tiene una primera aproximación por lo menos, ahí se alcanza, pasaleen
los médicos, porque depende de la tarea que estamos, bueno, pero qué pasa, entonces
yo como un clasifico, y bueno, si la palabra es prevista o el buen no originality, yo tengo
la probabilidad de la operación de la categoría negativa por la proviación de categoría negativa,
así que es 3-5, por la diferente probabilidad es de la palabra que aparece en la categoría
negativa, si se fíjara acá a todo un nuevo, porque no aparecía, y acá fíjense que,
bueno, que originality es una palabra entinando a positivo, es uno sobre 29, contra uno
sobre 34, o sea que está mejor en la positiva en la negativa, porque aparecía en
un contexto positivo, realmente, el problema que tienen no adelante, que es uno de los problemas
que ahora vamos a ver, pero de todos los modos multiplicando las provididades de cada palabra
si la que es más probable que sea negativa, y porque dice predictable, seguramente, porque
dice no, en realidad esto es nambar crunching, no es porque hay un motivo, uno de la aplicación
es un simple posteriore en esta cosa, bueno, paso de esto, pero realidad esto es lo visos cuentas,
es cierto, es cierto, si nosotros queremos hacer sentimentanálisis, para el caso particular
de la significación de documentos que se llama sentimentanálisis, que es ver la impresión
respecto a algo, en un documento, hay algunas reglas que permiten mejorar la performance, el
lo mismo, es exactamente lo mismo, las clases son las mismas, pero se puede hacer alguna
modificación, por ejemplo, no contar muy tible su ocurrencia en la palabra en lo mismo documento,
esto que yo les decía hoy, cuento una vez sola, y se decía muy, muy linda presión, linda
linda cuando no hay sola, eso se llama vina, vina, vina y valles, el manejo de la negación
es todo un tema, el manejo de la negación, y una aproximación muy sencilla, muy naípero
que mejora la cosa pues, bueno, yo a todo lo que dice después de lo que las vi con
no como el like sino como not like, vendon a palabra la nueva. Podría llegar a ser
alguna cosa un poco más elaborada si tuviera un parcer, porque si yo tengo un parcer, tengo
el árbol y tengo una rama que dice no, todo lo que hay abajo, entonces yo sé la
alcance de no, ahí voy, pero lo más tan como hacer el parcín, pero si yo le agrevo información
de parcín, la cosa puede mejorar, de parcín vamos a hablar la semana que viene, pero yo
diría que al me acordar que había el final de esto, del parcín, esta, pero esta es una
primera aproximación, tendría que crear una palabra nueva ahí, y ahora el like se cuenta
como no es like, es muy naípero, porque dice todo lo que está después de vida, pero podría
ver otras cosas en el medio, no, no es tan sencillo, digamos, la relación es más complicada,
no creo que pienses que hay un que ahí con la relación subordinada, pues es el más complejo
porque esto, pero está, no arrimamos, y otra aproximación por supuesto es usar lo que se
ama en lexico en el sentimiento, que son listas de palabras positivas y listas de
palabras nativas, tengo una lista de recolectas, sí, en el caso de lo que están
hablando como se dio en la palabra nueva ¿no?
A continuación había actos en este tipo, ¿por qué?
es que se consciousness, plampa Foreignería,我想 un decir a este término Ros UN ultimately...
Entonces sí claro claro claro, les tomé sus proponas en todo tipo de Oliver
que supone cómo tiene un gran volumen, si lo conocimos sí que él la será capturar algo
a partir de muchas sucurrencias, pero sí, si no parece, si tenés cero, él es la misma pató.
En el léxico, entonces, vuelvo a hacer esa agregar, en tu clasificador,
simplemente en una flitur que dice la cantidad para leer en un léxico positivo
y la cantidad para leer en un léxico negativo.
Es decir, tiene tres palabras negativas, es un xx.
En demás, uno y aquí se animado, son dos atributos, lo más.
Y la cantidad para leer en un léxico negativo, le averiguo dos atributos que,
si recordamos la clase basada, van a aseguramente estar más correl relacionado con la clase
y no van a poder dar una pista de su comportamiento.
Sin llegar a hacer un método de regla que dice, bueno, el que tiene más palabra positivo haga
porque juega en todas, intervienen muy, muy mucho en la clasificación.
Otro ejemplo, ¿cómo puedo hacer para calcular un tag de partos pitch?
Si tengo la palabra y los postáis de las palabras anterior y siguientes.
Y bueno, es la misma forma, ¿no?
La probabilidad es que es un objetivo, nada que la anterior es un determinante.
El siguiente es un hombre y la palabra es blanco.
Es la probabilidad de que la clase es un objetivo a priori.
Entonces, lo hago por contigo.
La probabilidad de que una palabra hacia blanco, como más objetivo,
es que de todas las veces que hubo blanco cuántas veces.
Y entonces, de todos los objetivos cualquiera blanco,
cuántas veces pasó que antes de una objetivo hubieron determinante
por la probabilidad de que el siguiente sea un hombre si este es un objetivo.
Si simplemente hago contigo toda la vez y que aparecieron cosas antes.
Y las cosieras independientes de ellas, lo cual sabemos que no es cierto.
Pero es lo que es lo que puedo computar.
Y bueno, como yo tengo calculando la probabilidad conjunta de esto,
podría llegar a general ejemplos con la distribución calculada.
Es un me puede ser útil para hacer generación de texto.
Todo esto me permiten.
Por ejemplo, de negra más, me permiten generar también texto.
Es la forma que hacen los generadores que hacen escriben parecido a alguien de agua.
Bueno, a la tacada, los métodos generativos que son listos,
como nadie vaya.
Un método generativo es decir que busca una distribución de todas las clases,
prueba todas las clases y computa la distribución conjunta con los atributos.
Los métodos discriminativos son un poco diferentes,
porque en lugar de...
en lugar de que hay culas la probabilidad de...
de la conjunta y, bueno, no de todo el ejemplo,
cuál de los dos es mejor.
¿Cuál es que la sí es mejor para este ejemplo?
Sin tratar de modelar todas las clases posible.
Sí.
Es decir, no elamos directamente la proye,
butrato intento modelar directamente la probabilidad condicional.
La probabilidad de la clases de agua los atributos.
Sí.
Voy derecho a eso,
¿qué es más probable de todo el atributo?
Nada más.
Y hay varias aproximaciones,
algunas que son propiedades como entropía máxima y otra no.
A ver, hay ese pre-perceptor,
un soporte de tomasino, ahora vamos a ver.
No, vamos a ver la definición de soporte de tomasino.
Pero lo esencialmente lo que te dije en el bueno,
esto está detallado.
Si yo tengo estos puntos así,
entreno,
y te digo, bueno, este punto está del lado de los容易.
No sé qué tan del lado está de nosotros,
de los estos mis probabilistas, por ejemplo.
O puedo hacer lo probabilista,
pero igual lo único, respondo a la calle que le gusta.
Entonces, vamos a ver uno,
que es el modelo entropía máxima,
que es como lo que vamos a ver,
es como la versión discriminativa del método de...
de...
O también conocí como regresión multinomía logística,
que vamos a ver por qué se llama así.
Y son modelos, lo geniales para clasificaciones,
y yo quiero la clase.
Si tengo una serie de tributos,
hago.
Y ahora vamos a ver qué se está, ¿no?
E, ahora vamos a ver qué se está, ¿no?
FSUI.
Son las fiturs.
Son como un indicador de alguna...
Son fiturs de... a partir de los atributos.
Ahora vamos a ver cómo lo abrimos.
Está, pero son derivadas de estos atributos.
Los doble mis son los pesos,
una serie de pesos que yo voy a intentar calcular.
Son los pesos de mi modelo,
lo que yo voy a entrenar a aprender son los pesos de mi modelo.
Y este es el producto,
el dot-product de ambos.
Es decir, esto va a ser doble v1,
por f1, más doble v2, por f2,
más doble v3, por f3, etcétera.
Acuerda, entonces yo,
la fituresta,
que según mi ejemplo, cuando yo voy a llevar,
según mi ejemplo, esto va a leer algo,
lo multiplico por un número fijo,
que va a depender de lo que yo entrené,
es decir, lo que yo quiero aprender es doble,
¿te acuerdo?
Eso elevó a la suma de eso ahora,
¿por qué?
Y esta seta y la simplemente,
un factor de normalización.
Es decir,
de todos los doble v,
surgí que tengo,
esto no necesariamente de generar una distribución de probabilidad.
Entonces,
este seta es como la suma de todos los casos posible.
Para llevarlo a una probabilidad,
a que me des la suma de uno,
a que yo calada,
a una clase de atrás,
generalizado,
y salga a la famosa seta,
que parece una pavada,
pero es lo más difícil de computar,
porque yo tengo que calcular este valor
para todos los attributeos posible
para que me de una distribución.
Entonces,
lo modelo de entropía máxima,
calcula en la probabilidad de la clase
utilizando esta forma,
pero no hay por qué.
Pero antes vamos a leer de otra cosa,
para llegar a eso,
y es de regresión lindial.
Un problema de regresión lindial,
que era lo que yo le decía hoy,
es cuando uno intenta,
lo problema de regresión
y cuando uno intenta que el culiar un valor,
de algo valor real,
valor real.
Si.
Entonces, yo,
si yo quiero saber,
supongo que yo,
tengo estos puntos,
acá.
Si.
Cuando yo goreleció lindial,
lo que hago es trasar,
buscar una...
lindia,
que re que se impare los ejemplos.
Eso esencialmente ese.
Si esto es...
Va a ser una cosa como,
si yo supongo que tengo el que pasa
de origen esta reinta.
Vamos a poner que pasa valor real,
y no vemos como se corriga.
Es.
Vamos a llamar el que hizo un lequido.
Esto es la reinta que rependen a esto.
Es decir, el doble vez,
uno dio levedo,
me va a determinar la legnación de la reinta.
Acá está pasando por el origen,
porque no tiene lindia pendiente.
Yo puedo inventar un doble vez cero,
con un equicero que va a le siempre uno
para agregarlo,
vamos a mover la reinta.
Bueno.
Entonces,
lo que yo cuando digo que hago
regresión lindia,
lo que digo es bueno,
mi punto yo a sumo que son separables
por una reinta.
Ah, perdón.
Yo quiero estimar que he quedado a lo que es uno.
Bueno, entonces yo tengo la reinta
para un nuevo hequis.
Aotitititititititititititit,
vengo acá y que cololí.
Sí.
Entonces iba a ser igual al doble vez
y por Efei, que es esto.
No es la suma historia de doble vez
y por Efei es el dobro de doble vez con Efei.
Bueno,
simplemente estoy haciendo un estimador
lindia al lindio.
Y como hago para
como encuentro esta reinta,
bueno, una de las formas más usuales
es la que minimiza
la suma de los cuadrados
y la violencia entre valores y predicciones.
O sea,
esta reinta
minimiza esta distancia.
Y esta.
Bueno,
la distancia,
la reinta que tenga la distancia
minima de esto al cuadrado
y esto al cuadrado.
Sí?
No hago el cuadrado,
para que la suma sea positivo,
para que no me afecte
si estoy de un lado del otro.
Está vieja,
la vieja regrisible en ya.
Sí?
Entonces,
no voy entre la detalle,
pero yo calculo la fórmula
de los mínimos cuadrados.
¿Qué son?
Sí.
Si lo piensan,
son todos,
multiplicaciones de
cosas al cuadrado,
más cosas al cuadrado.
Bueno,
es un asunción,
positivo y convexa.
Y entonces,
yo lo que trato es buscar
el mínimo de esa función.
La llevo bien en lo que escribiera
eso porque no sé si queda claro.
Este,
ustedes no les importa.
Este,
pero la cuestión es que yo
termino minimizando
una función convexa.
La función convexa,
una función que está haciendo.
Así es una función convexa.
No,
cualquier
par de puntos
que yo una pasantó
en el mínimo,
¿no?
Vamos.
Acá,
esto es una unión convexa.
Sí,
yo puedo unir acá.
Bueno,
como es una función convexa,
qué cosa,
qué característica tiene,
la función convexa,
qué los mínimos,
qué cosa tiene,
la confusión convexa.
No sé,
la función convexa tienen
el tema de que cuando yo encuentro un mínimo,
local es un mínimo global.
Si una función es así,
yo puedo quedarme
buscar el mínimo acá
y encontrarme con este mínimo local
y buscar acá.
Sí,
sí,
sí,
sí,
sí,
claro,
puedo llegar a que era,
a cada cosa.
Si yo tengo una función convexa,
esto es informativo,
si aquí no es el curso de presa
yo puedo buscar un punto cualquiera
y empezar a calcular la derivada y avanzar
en la dirección de la derivada
y al final,
al final del día voy a encontrar
si hago las cosas bien
en el mínimo de la función.
Eso es llamada de censo
por gradiente.
Sí,
y deberían ser diarse en primer año.
Hay otro método de mi limización,
son mitos de mi limización numérica,
son calculos numéricos.
No hay una fórmula cerrada para eso.
Palmito del mínimo cuadrado,
sí, hay una fórmula cerrada,
es decir, una fórmula de algular,
pero es más fácil hacer decisionamente,
pues,
más rápido.
Bueno,
cuestionos que nosotros podemos saber cómo
hacer esto.
Es decir,
que yo puedo aprenderlo,
o sea,
lo doblebe,
que minimizan eso,
lo doblebe,
que queremos,
claro,
es decir,
yo cálculo a partir de los
de el cuerpo entre el ambiente
y todo eso doblebe y lo uso luego.
Eso se trata aprender.
Entonces,
entonces,
esto es un problema de regresión,
donde yo quiero cocinar un número.
Pero acá,
estoy en un problema de clasificación,
no sé lo que yo quiero aprender es una categoría,
una probabilidad.
Entonces,
mi primera aproximación es,
perdón,
es bueno,
yo digo esto,
la probabilidad,
en un número que yo quiero estimar,
es la probabilidad,
de que si acá tenemos un caso positivo
negativo,
la probabilidad de que iba a y gastruo,
o sea,
de la clase,
dado
mi equipo.
Entonces, yo lo que he dicho,
bueno,
hago regresión,
hago regresión,
pero le voy a recalcular un número,
o sea,
si yo calculando un número que el valor real
es el valor de la probabilidad.
Ahora,
¿qué por el matiene esto?
El problema que tiene esto,
es que no es una distribución de probabilidad,
no es un valor de probabilidad,
porque la probabilidad tiene que estar entre 0 y 1.
La cuerda.
Entonces,
esto no me sirve a explicarlo directamente,
como me puede dar cualquier cosa,
yo quiero una probabilidad.
Entonces, lo que he dicho,
bueno,
yo con los ojos,
los ojos que no sé cómo se traduce.
Los ojos como,
las chances,
podrían ser ahí más,
las chances.
Como las apuestas,
no,
tenés dos a uno,
uno a dos,
que es,
esencialmente,
la probabilidad,
que sea verdadero,
comparado con la probabilidad,
que no lo sea.
Esto tampoco mejor,
porque este resultado
está entre 0 y infinito.
Sí.
Pero así es sin estar entre 0 y 1.
A entre,
perdón, entre,
yo quiero llevarlo algo,
que está entre menos infinito y más infinito,
que es esto, ¿no?
Está claro.
Está claro.
Esto está entre menos infinito y más infinitos,
el doble,
por ejemplo,
cualquier cosa.
Acaso,
los reducos a una cosa que está entre 0 y infinito,
mejor.
¿Tá?
Bueno,
pero para,
que esto que entre,
entre 0 y 1,
lo que hago es,
el brico lo harin,
para que que perdón,
dije a lo que es,
para que quede entre 0 y infinito y me he visto el brico lo harin,
el brico lo harin,
muy entonces digo bueno,
esto es lo que buscamos,
yo quiero estimar el lo harin,
de la probabilidad de la voz,
y por eso yo vas a formular tan rara con ni,
porque cuando yo despejo,
y esto se lo dejó de ver,
cuando yo despejo,
p igual true,
es fácil, ¿no?
digamos,
este lo harin,
se transforma en un e a la doble por efe,
sí,
bueno,
se lo dejó de ver,
cuestión,
que queda así,
e a la doble por efe,
dividido uno más e a la doble por efe,
y claro,
la soda se transforma en el que es esta función,
entonces llegue una función
que me dice la probabilidad de que se es verdadero de la clase,
a partir de haciendo unas cosas raras con las fíjurs,
me da menos,
Esa es algo parecido al imial, pero que la corrijo con esta función.
Esto es lo mismo que hacen la red de un normal.
No me.
La función es de una función logística.
Y tiene este aspecto.
¿Cuál es la garantía de la función logística?
Y bueno, que parece un escalón.
Es parecido una cosa que vale.
Si es negativo y uno es positivo, pero que es continua.
Es una linda función.
Es una función smooth.
Pero sí, parecido en el calón.
Si yo logro, si estoy estilado,
más seguramente sea negativo y si estoy estilado, se ha positido.
Pero puedo derivarlas, es la cosa.
La red de un año está mucho eso.
Y hacemos el chiste, no se puede entrar.
No, es para que se sientan más.
Bueno.
Y bueno, ¿cómo clasifican muy fácil?
Si la probabilidad de que sea verdadero,
más yo lo que la probabilidad es que sea falso.
Dada la tributo.
Eso lo mismo que decir que
él a la dole por ese mayor de uno.
Por esto.
Sí.
¿Qué es lo mismo que decir que dole por ese mayor que cero?
Entonces, clasifican muy fácil.
Con el temetón, porque lo único que toca ceres
multiplicar doble por Efe, con los pesos que te acule por la fitur
y si me da mayor que cero quiere decir que positivos y no negativo.
Eso y la red de un lojítica.
Se llama red de nación, aunque se llama red de nación es un método de clasificación.
Ahora.
Y la pregunta es, bueno, pero...
Acá yo todavía no respondí como estimaba los pesos
que no iba allí si se era contando.
Acá tengo que hacer alguna cosa un poco más raras.
Digo que mi doble he estimado ese que maximiza estas este producto
de probabilidades de las diferentes clases.
Y me queda esta función súper rara, súper fea,
súper complicada, pero que hay viven que es convexa.
Y como es convexa, bueno.
Yo quiero buscar el máximo de una función convexa.
Lo mismo que le decía hoy.
Aplico, de eso por alguien, tengo algún otro método de...
Númerico.
Entonces tengo una forma de estimar eso a doble,
en el asunto que tengo esa forma de estimar.
Y...
¿Qué pasa si tengo más de dos clases?
Y bueno, tengo que hacer una cosa así,
calcula la fitura a partir de cada clases con cada triuto.
Vete a lo dentro de la fórmula y volver a normalizar.
Y por eso,
nuestro método se llama multinomía el logístico,
porque es una extensión de la regresión logística
a un caso de un múltiples variable, un múltiples clases.
Y por supuesto no vamos a quedar con la clases que mas inniza la probabilidad de algo.
Bueno.
Y está clases.
Y es un poquito más entramada en detalles matemáticos
que el resto me parece importante entender
por qué sus atributos aparecen y por qué,
por qué aparecen todas esas cosas con él.
La cosa con él, generalmente, es un para cambiar la curva.
Aquí va a ser paiguento y sero yo.
Y por último, como comentario,
¿por qué si se llama un modelo de entropía máxima?
La entropía,
no sé si hablamos algo de entropía en una clases,
la entropía es una medida que trata de ver que están parecidos
son los elementos de las gano.
Entonces, el principio de entropía máxima dice,
bueno, yo si tengo muchas distribuciones posibles,
candidatas, algo.
El hijo,
la que tiene entropía máxima, es decir,
la que es dado unidad a todos,
la que solo assume lo que los datos te dice,
¿qué quiero decir eso?
Si yo no conozco nada sobre un documento,
en el caso del 90 a 20,
90 a 10.
A sumo el 90 a 10,
¿por qué los podamos subir a partir de los datos?
Sí.
Si yo no sé nada,
si yo no sé nada sobre el documento,
no sé nada,
no sé nada,
no tengo ninguna información a priori,
y te habiendo un medio de digo,
de qué clase es,
de deportes de espectáculo,
¿qué haría en ustedes?
Si yo te digo,
50 a 50,
¿qué haría en ustedes?
Si yo te digo,
50 a 50,
eso es aplicar el principio de entropía máxima,
es decir, bueno, yo no tengo información,
esto es equiporóable,
seguramente la etropía máxima,
cuando eso todo es equiporóable.
Si yo agrego un poco de información
y yo te digo tengo un dado,
pero yo te aseguro que el zay no sale nunca,
¿cuál es la probabilidad de agargar uno?
Un punto,
o sea, paso de ser un seto un guinto porque,
porque tengo más información,
pero siempre no tengo un cuarto,
porque no puedo sacarlo de ningún dato,
eso es el principio de entropía máxima.
Si yo,
cuando el hijo esta esta distribución es posible,
aplico solo lo que los atributos me dicen,
sí,
aplicando el principio,
que tenga máxima entropía,
a lo que llegó,
es exactamente el mismo modelo
que presente antes,
por eso también los modelos se llaman
modelos de entropía máxima,
es porque son de forma diferente llegar a lo mismo.
Si usted quiere en el detalle en la literatura está de eso,
no sé si le interesa,
pero aquí no pueden realizar,
está muy...
coinciden con eso,
coinciden con una instrucción de probabilidad
para un modelo logítico luminesal,
cuyo peso más inicial,
pero sí, minuto en los datos realmente.
Por ejemplo,
si yo quiero aplicar un modelo entropía máxima,
al ejemplo del postaí,
la fíjor van a locir así,
tengo una fíjor 1 que dice,
vale 1, vale 1,
si la palabra reis,
y la clase es nombre
y si no vale 0.
Otra fíjor va a ser,
uno,
si el anterior estuvo,
y la clase es verbo,
y si no es 0.
Otra fíjor y,
como se imaginarán,
la fíjor son,
estamos hablando de miles o de millones de fíjores.
No, pues son todas las posibles,
las relevantes,
así, luz en las fíjores,
un modelo entropía máxima,
un montón de fíjores,
y yo lo que voy a hacer,
y además son indicadores,
eso es decir, no,
vale 1, 0.
Y lo que yo voy a hacer
es calculando,
a través de contando,
sí,
voy a calcularlo,
o le ve,
con aquello que hablamos hoy,
y de la fórmula de milimizarla,
¿no?
O sea que cada fíjor va a tener un peso indicando
que tanto afecta la fíjor corresponde para el taguesa.
Por ejemplo,
si yo tengo aquí,
yo se acuerda,
porque queríamos saber que era reis,
y el postaje, no?
Que era la única parar,
no sabía muy lo que era.
Entonces,
estos son los pesos que yo entrené,
lo que me dicen es que
fíjense que los pesos,
que tenemos acá,
lo que me dicen es
que la fíjor más importante es la 6,
en el clazo,
para que sí es un hombre
es tan muy negativa,
o sea que resta,
y esta es muy positiva,
he fe 2 para 1 baro,
no me preguntes,
si son un nombre real,
pero no me acuerdo.
Efe 2 es,
ah,
si le aprende a tu,
si le aprende a tu,
pesa muy positivamente
para que suce un baro,
tu reis,
y la f6,
que es,
y pesa muy negativamente para un nombre,
fíjor diferente por la clase,
porque es un más de ule,
más de 2 quiero decir,
¿no?
Entonces,
yo hago las cuentas,
la probabilidad de que si un nombre dado
la fíjor es,
es,
es exactamente aplicar la fíjor relevante,
acá es,
0,
porque,
voy a explicar la primera,
la segunda y la sexta,
porque eso es la caplica,
pero,
la 2,
dijimos,
y la 6,
eso es la de tu,
0,
no,
no,
es la 1 y la 6,
la 1 y la 6,
correcto,
o sea,
si es,
reis,
la palabra,
y la población pública,
fíjense que como va al encero no pasa nada
con la multiplicación,
porque yo estoy diciendo,
ella la es,
no,
no me molesta el cero en el teca,
y este es el factor de normalización,
es simplemente,
para que esto de,
0,
20 y 0,
sumo esto,
más esto,
sumo todas,
y,
bueno,
entonces yo busco la clase,
que más sin misas,
en el tecazo es vero,
igualo,
bueno,
eso son los modelos en tropia máxima,
hay otro modelo de discriminativos,
que lo voy a mencionar rápidamente,
porque,
en la forma aplicable,
se la misma,
lo único que hay acá,
de diferentes que,
diferente la forma de,
elegir y clasificador,
es decir,
si acá lo hacíamos por regresión logística,
en su porvector machines,
que su método,
que supuso muy de moda,
en principio de este siglo,
en la década pasada,
digamos,
es un método que,
lo que hace es,
buscan separar lindiamente,
pero en vez de eso,
lo por mínimos cuadrados,
lo que dice,
buscan,
la recta,
que separa más,
que queda más en el medio,
digamos,
la inclusión atrás de,
de su porvector machines,
que yo busco,
si yo tengo los ejemplacitos,
tengo muchas rectas que basan,
sí,
la que yo trato a encontrar,
la que maximiza el margen
de los que están más cerca,
y queda en el medio,
señor Né,
por eso ya más,
lo su porvector son estos,
son lo que están más cerca,
lo demás y se fijan un por tan para el clasificador,
cuál es la hipótesis del su porvector machine,
y por qué son tan robustos,
y por qué,
como está el juto en el medio,
quiero decir,
si yo meto un,
lo que está acá,
sí,
si un ejemplo a clasificar,
queda muy cerquita del borde,
me poquivo car,
se tiene,
es más probable que me estoy equivocando,
digamos, yo le pongo en el medio,
bueno, quedan bastante lejos,
digamos,
y de hecho funcionan muy bien,
en clasificando,
fotó,
fotó una revolución,
la su porvector machine,
ahora como ahora están de modo,
las redes neuronales,
la su porvector machine,
hicieron los mismos principios agarraron,
fueron los primeros métodos discriminativos,
la disificación que empezaron a batir todos los recordíamos,
de diferentes tareas,
hasta que pasaron de moda con el tema de la,
si bien se usan mucho,
pasaron de moda con el tema de las redes neuronales,
que volvieron a batirle los recordios,
pero esencialmente el método,
cómo se aplica el mismo,
la diferencia es como,
teoricamente como se calcula que está ahí,
bueno,
no importa,
hay otro método de clasificación,
hace en la aparición automático,
lo aprende,
vecino más cercano,
lo que a venir es nego,
que es,
la cifico,
un documento buscando lo que está más cerca de él,
del punto de vista triunctalo,
una distancia entre documentos
y me quedo lo que está más cerca,
¿no?
es como la idea de,
bueno,
si esto está acá,
¿quién son los vecinos más cercanos?
y bueno,
si volvamos que los tres vecinos más cercanos son estos,
en este caso,
los tres son circulitos,
o sea,
que eso seguramente sea circulito,
o no tenemos problemas,
estamos de manera.
Lo método de,
de vecinos más cercanos,
tienen la ventaja,
obviamente,
de que pueden reconocer,
pueden reconocer clasters,
pueden,
es lo método de vecinos más cercanos de fin,
una cosa así,
no,
pero,
a cosas así,
no pueden reconocer,
cosa que no son lineales,
tienen el problema de que a veces
se sobre ajustan demasiado,
a lo que les decisión,
que no son muy�as,
en el procedimiento de homogenodra,
plan dos fuores,
que eso como una,
hay muchos,
hay muchos métodores,
que laificación,
pero todos,
en todos,
lo que tienen en común,
es que las medidas,
para realizar,
para,
el métodología,
la que se basaba,
y,
eso desde el punto de vista de los métodores,
de clasificación,
puros,
pero también se acuerdan que habíamos visto
los métodores de clasificación,
secuencial,
cuando yo quiero asimiar una secuencia de tas,
de clases,
así,
a sumo que mi atributo tiene una secuencia,
mi,
y estancias,
es una secuencia,
por ejemplo,
una,
una oración,
que es una secuencia de palabras,
y quiero asimitar una secuencia de tas,
bueno,
hay una,
hay inversión en generativas,
en el caso de los,
de los,
la versión generativa en la iglesia,
es esto,
los gírenos,
como del que lo vimos,
bastante detalle,
en alguna clase anterior,
y,
hay una versión también
de clasificación,
es decir,
esto es un poblejo,
los que van las mejores,
que son los,
están en los,
collectiones,
будiciones,
cuando ellos no fikan bien,
fueron todos una,
una novedad,
en, en, en一定as 1958,
pero en хорошía generación,
Así ahora os digo cuando los,
que nos,
así como los chiefs de rock,
la everything va a富 hospiter de la
pero de como herramienta llegamos para las clasificaciones de policías, funcionan muy bien.
Yo diría que si uno va a...
a veces me quedas más.
No, acá tiene un poco de software.
¿Sí?
de estas cosas.
Hueca...
si me para jugar, pero de jugar es genial.
¿Saquí lerne es una herramienta bastante...
una librería bastante polenta de...
me empaito, ni que estaba bastante de moda.
Y acá me faltan...
me faltan toda la nueva bola de rande biblioteca de...
dibilando, ¿no?
de que son de arena una área que son torches...
este teano que eras...
el...
el software.
pero bueno...
y...
sé que lerne es una biblioteca de...
generica de...
Machine Learning en Python.
Ahora está bien.
Eniliteca...
es más...
de proceder en la imaginatura,
pero tiene, por ejemplo,
explícalo de sesión.
Se arrefe más más,
es un tool kit para...
condicionar las normfils.
Pybraing...
creo no...
no...
no corre más, o no sé...
Rene Urona, el John Python.
Se ve a me laitir a la herramienta
de su proyecto Machine.
Cuando estaba en el mundo.
Se me laités en 99,
va a ser una idea y estaba tan testable,
porque...
no hay mucho para...
es muy sencillo el modelo de la su proyecto Machine.
Perdón de como un replito.
Emo...
eh...
yo diría que...
que si...
si...
si vamos a...
a...
a...
a...
a...
de...
a nivel de...
como decir...
de mi...
de mercado, de herramienta...
o de...
no me sale a...
de industria de...
a nivel...
industrial...
no es la palabra correcta, pero...
yo diría que...
el...
el...
el...
en lo que hemos aprendido hasta ahora.
No?
Es decir...
Todas estas cosas que hemos aprendido en la clase de hasta ahora...
ya se encuentra a nivel industria de la nivel industria de la nivel industria...
estoy hablando de las compañías internmentos, ¿no?
Reconocimiento de...
eh...
eh...
eh...
Reconocimiento de caráteres mal escrito...
o...
clasificación...
clasificación de...
sentimentanálisis...
de todo lo que hemos hablado hasta ahora, ¿no?
en el era...
más entonces...
eh...
también es...
a ver...
no es lo único, ¿no?
Machine Translation es...
ejemplo de cosas que han tan muy bien...
este...
pero utiliza metodos más o menos hasta acá.
Lo que quiero decir es que...
y bueno...
ahí hay algún componente semántico también,
que lo van a ver después con Luis,
pero...
eh...
esas cosas más avanzadas,
digamos, recién recién se está empezando a hablar,
pero...
en algunas cosas de...
eh...
eh...
eh...
Reconocimiento de...
tiades,
¿no?
El otro hecho es...
lo que voy a ir en un diario,
digamos, un...
un dos años atrás,
que...
una algo que decía que era una aplicación que reconocía
a partir del medio de ortines,
eh...
el lugar es...
y bueno...
un lugar es y...
cuando arma las citas...
eh...
cuando va a partir de un correo,
te lo voy a tener calendarios.
Ahí lo que está haciendo es reconocimiento de tiades.
Está reconociendo que dice,
el jueve de 23,
se en la contalle,
y lo está viendo,
está haciendo que esificación secuencial,
pero...
eso es una cosa que en...
en la cabeza,
están como hace como 10 años,
digamos,
los conjuntos ahora no fíjlen,
como dicen,
recién están empezando
como entrar ese tipo de...
y...
hay cosas que todavía está
por ver,
se como se van a incorporar,
que son...
las que vamos a ver de haber en elante,
que son en Parcin,
este...
pues se analicin más complejo,
ni que hablar de analicisemántico,
más allá de las semánticas palabras,
son cosas que voy a ver viendo,
pues...
o sea,
hay mucho todavía para mejorar
y en la calle también,
porque...
no,
no,
no,
no,
no,
no,
no,
no,
no,
no,
no,
no,
no,
no,
no,
no,
que han de la vuelta ahí,
bueno,
y que con esto se va a hacer un montón de cosas.
Bueno,
la sé que viene ahora en amo con...
eh, Parcin,
sí.
Gracias.
