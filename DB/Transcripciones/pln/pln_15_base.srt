WEBVTT

00:00.000 --> 00:07.600
Bueno, bienvenidos. En la clase de hoy vamos a ver el tema de redes neuronales que, bueno,

00:07.600 --> 00:12.120
es como digamos, el estado del arte, lo que son las cosas de procesamiento del lenguaje natural

00:12.120 --> 00:18.560
en general hoy en día se rosuelen con redes neuronales. Entonces, es un poco para continuar

00:18.560 --> 00:22.160
con lo que veíamos la vez pasada. Habíamos visto métodos de clasificación, habíamos

00:22.160 --> 00:26.640
visto que había algunos para clasificar cosas en categorías, había algunos secuenciales,

00:26.640 --> 00:31.760
había algunos que llamamos modelos del lenguaje y de los métodos de clasificación, en realidad,

00:31.760 --> 00:36.240
vimos en profundidad, nadie valles, pero vimos que había otro, por ejemplo, algo de la decisión,

00:36.240 --> 00:43.880
regresión logística, Supervector Machines y redes neuronales y para los métodos secuenciales

00:43.880 --> 00:47.400
también aparecian las reneonales, para los modelos del lenguaje también aparecian las reneonales.

00:47.400 --> 00:50.920
Entonces, como que las reneonales son un método muy importante que es muy versátil y se

00:50.920 --> 00:54.480
usa para muchas cosas. Entonces, nos vamos a concentrar un poco, vamos a dar en esta clase una

00:54.480 --> 00:59.400
introducción a lo que son las redes y además ver cómo se usan particularmente para el lenguaje.

00:59.400 --> 01:04.840
O sea, vamos a hablar las técnicas de vectores de palabras y cómo aplicar eso a precisamente

01:04.840 --> 01:10.240
el lenguaje natural. Entonces, ¿cómo empieza esto? Esto empieza inspirado en esto de acá, que es

01:10.240 --> 01:17.520
una neurona biológica, esto lo habrán visto en el hiceo, en biología. Una neurona es un tipo de

01:17.520 --> 01:24.560
célula del sistema nervioso de los animales. ¿Qué tiene distintas partes? ¿Cómo se puede ver ahí?

01:24.560 --> 01:30.440
Sí, puedo apuntar, o puedo apuntar. ¿Aboquela con esto?

01:30.440 --> 01:37.040
Hay, tiene distintas partes, tiene como unos pelitos que entran dentro del cuerpo de neuronas que se

01:37.040 --> 01:42.000
llaman dendritas y después tiene como una especie de cola que sale de la neurona que se llama

01:42.000 --> 01:50.000
acción y, bueno, acá en el centro tenemos lo que sería el cuerpo de la neurona, el soma. Entonces,

01:50.000 --> 01:57.280
en esas por esas dendritas vienen impulsos eléctricos, las dendritas actúan como inhibidores o activadores,

01:57.280 --> 02:03.320
pero vienen impulsos eléctricos, eso se condensan adentro del soma que sea el cuerpo y si se supera

02:03.320 --> 02:07.120
cierto un bral de actividad eléctrica, entonces la neurona dispara un solo punto por el acción,

02:07.520 --> 02:13.960
un solo impulso eléctrico por el acción, lo manda hacia afuera y ese acción está conectado a otras

02:13.960 --> 02:21.000
dendritas que están en otras neuronas. Entonces, esto tiene un montón de entradas, se condensan en el

02:21.000 --> 02:26.600
cuerpo de la célula, de la neurona, dispara un solo pulso eléctrico para afuera y ese pulso eléctrico

02:26.600 --> 02:30.120
viaja a otras neuronas. Entonces, como esas neuronas están conectadas en una especie de red,

02:30.120 --> 02:35.560
cada acción de una neuronas está conectada a las dendritas de otras, entonces la salida de una

02:35.560 --> 02:40.400
es la entrada de otras. Esto conforma una red dentro del cerebro o el sistema nervioso de los animales

02:40.400 --> 02:45.360
y eso es lo que componen una renauronal, en este caso una renauronal natural, una renauronal

02:45.360 --> 02:53.400
biológica. Entonces, en los años 40 se propuso la primera versión matemática de cómo

02:53.400 --> 02:58.240
funciona una neurona, entonces hubo unos científicos que dijieron, bueno, vamos a tratar de simplificar

02:58.240 --> 03:03.040
esto más posible, a estar a verlo y generar una versión en una ecuación que trata de representar

03:03.040 --> 03:08.520
esto. Entonces, ellos diseñaron esta ecuación de acá, en la cual yo dice, bueno, vamos a cambiar

03:08.520 --> 03:13.760
esta neurona biológica que tenía todas estas partes y vamos a crear una especie de neuronar

03:13.760 --> 03:19.520
artificial en la cual yo tengo un conjunto de entradas, un conjunto de pesos de entrada que están

03:19.520 --> 03:24.680
acá, que vendrían a hacer el equivalente a las dendritas. Voy a tener impulso eléctrico de

03:24.680 --> 03:30.400
entrada que son como X1, X2, X3 hasta Xn, que digamos que son los inputs que va a tener esa

03:30.400 --> 03:36.120
neurona. Después, en el centro lo que hago es sumarlos y en realidad lo que estoy sumando es el

03:36.120 --> 03:43.280
producto entre cada impulso de entrada y el peso correspondiente. También le voy a agregar un valor

03:43.280 --> 03:48.560
de sesgo y después la salida le voy a pasar por una función de activación y eso me va a dar

03:48.560 --> 03:53.680
la salida de la neurona. Bien, o sea, esta parte les vamos a estar viendo en detalle. Pero en

03:53.680 --> 03:58.280
definitiva, es como que yo tuviera esta ecuación de abajo, ¿no? Yo tengo la sumatoria de las

03:58.280 --> 04:06.280
entradas multiplicadas por pesos, a eso le subo un sesgo, que se llama B y todo eso se lo aplico

04:06.280 --> 04:13.560
una función sigma que es un poco que son esas funciones sigma. Entonces ven que es una, digamos,

04:13.560 --> 04:19.760
es como una ecuación lineal, ¿no? O sea, la sumatoria ni de XC por WB sub y más B,

04:19.760 --> 04:25.240
todo eso es una, digamos, una fórmula lineal y a eso le agrego un sigma, digamos, se lo aplica un

04:25.240 --> 04:32.280
sigma que esta va a ser una función lineal. Bien, entonces, más adelante para simplificar esta ecuación y

04:32.280 --> 04:36.760
para que después queden más fáciles de calcular las cosas, lo que se hace es decir, bueno, este valor

04:36.760 --> 04:45.320
que veníamos acá, esta B que está sumando, que digamos se usa para que, ahí, esta B que está acá,

04:45.320 --> 04:51.640
que se usa para que, digamos, para poder completar toda la ecuación lineal, lo que se hace

04:51.640 --> 04:56.120
agregarla como un peso más, entonces decimos, bueno, tenemos una entrada más que vale uno y su peso

04:56.120 --> 05:01.320
correspondiente es el sesgo. De eso, en realidad, digamos, después nos olvidamos. Cuando vamos a

05:01.320 --> 05:05.320
trabajar con estas cosas como que no utilizamos mucho el sesgo y nos concentramos en decir, bueno,

05:05.320 --> 05:10.920
vamos a tener un vector que son entradas, que son los X1 hasta XN y un montón de pesos que son

05:10.920 --> 05:15.640
los W1s WBN y adentro la neurona, lo que pasa es que voy a hacer el producto interro entre esos

05:15.640 --> 05:24.280
entre el vector X y el vector W y se lo voy a pasar a la función sigma. Bien, entonces,

05:24.280 --> 05:30.680
esas funciones de activación sigma hay varias, o sea, al principio, digamos, cuando diseñaron

05:30.680 --> 05:36.040
primero esta neurona, lo que se les había ocurrido primero era decir, bueno, yo lo que hago es sumar

05:36.040 --> 05:44.280
todas estas, digamos, todos estos impulsos multiplicados por los pesos, lo sumo y si esa suma supera

05:44.280 --> 05:48.520
cierto umbral, que el umbral lo podrían calcular o mucho que se ha utilizado en uno o en una de esas

05:48.520 --> 05:54.040
cosas, bueno, si supera cierto umbral, entonces mando uno para fuera y si no mando ser. Eso era lo primero

05:54.040 --> 05:59.640
que se le había ocurrido, pero bueno, después empezaron a encontrar otras funciones que eran mejores

05:59.640 --> 06:05.480
para poder entrenar mejores arredes y en definitiva, como que no hay mucho criterio de qué restricciones

06:05.480 --> 06:11.000
que tiene que tener esa función, salvo que tiene que ser derivable, tiene que ser, tiene que ir como

06:11.080 --> 06:16.240
de menos a más, digamos, puede ser de 0 a 1 o de 0 más infinito o de menos infinito más infinito y

06:16.240 --> 06:20.720
tiene que ser no lineal, tiene que tener algún punto de no linealidad. Entonces, estas son algunas muy

06:20.720 --> 06:25.160
usadas, por ejemplo, la función sigma o id o función logística, que es la misma que se usa,

06:25.160 --> 06:31.200
lo que estamos hablando de un rato, digamos, el método de regresión logística utiliza también esta

06:31.200 --> 06:38.480
función, la tangente parólica es otra, la función relu es muy usada y la relu se define como el

06:38.480 --> 06:43.080
máximo entre 0 y 0, relu de 0 es el máximo entre 0 y 0, entonces vale 0 para todos los valores,

06:43.080 --> 06:49.080
excepto para cuando todos los valores menores que 0, pero cuando vale el mayor que 0, vale directamente

06:49.080 --> 06:52.880
el valor. Estas son unas funciones un poco estaña, pues yo les dije que tenían que hacer todas

06:52.880 --> 06:57.640
derivables y esta justo no es derivable en el punto 0, pero después es derivable en todo el

06:57.640 --> 07:03.080
resto de los reales. Bueno, ya hay otras más, pero estas como son como de las más utilizadas.

07:03.080 --> 07:11.240
Bien, lo importante acá es que estas funciones de activación provenan una no linealidad y vamos a

07:11.240 --> 07:18.440
ver, ¿por qué? Ok, bueno, entonces vimos lo que era una neurona, imagínense que en general las

07:18.440 --> 07:28.120
neuronas se ponen como en grupos, digamos, y se distribuyen en capas dentro de una red, ¿no?

07:28.120 --> 07:33.000
Entonces, este es un ejemplo de una de las redes neuronales más simples, más simples que en

07:33.000 --> 07:39.040
realidad son útiles para algo, que se conoce como Perceptron Multi-Capa o Redfield Forward Multi-Capa,

07:39.040 --> 07:44.520
que funcione de la siguiente manera, ¿no? Nosotros tenemos todas las entradas, esas que yo les decía

07:44.520 --> 07:50.920
que la centrada es X1, X2, X3, etcétera, este Xn, sería como una primera capa de entrada y después

07:50.920 --> 07:56.680
yo ubico un montón de neuronas en una segunda capa y las capas que vienen después de la entrada le

07:56.680 --> 08:02.800
voy a llamar capas ocultas, o sea, tengo una primera capa de entrada, esa capa lleva a una capa oculta y

08:02.800 --> 08:06.480
todas las neuronas de la capa oculta están interconectadas con todas las neuronas de la capa de entrada,

08:06.480 --> 08:14.560
o sea, hay pesos que van de todas a todas, después puedo tener otra segunda capa oculta, otra tercera capa

08:14.560 --> 08:19.000
oculta, etcétera, hasta que llevo a una última capa que también está interconectada con la anterior,

08:19.000 --> 08:26.640
que es la capa de salida, bien, pero no hay enlaces que vayan entre la capa inicial y la capa de salida,

08:26.640 --> 08:31.280
digamos, la capa de entrada y la capa de salida, sino que siempre los enlaces van entre una capa y la siguiente,

08:31.280 --> 08:38.600
entonces acá yo digo que tengo una arquitectura en capas donde tengo este, en según esta imagen K,

08:38.600 --> 08:43.640
capas ocultas, tengo la capa oculta 1, capa oculta 2, capa oculta K y después una capa de salida,

08:43.640 --> 08:49.360
bien, entonces esta es como la arquitectura más sencilla, yo tengo un montón de capas, una atrás de

08:49.360 --> 08:54.080
otra y cada capa está completamente interconectada con la anterior, pero nunca saltan entre capas,

08:54.080 --> 09:03.120
bien, entonces analicemos un poco que es lo que pasa dentro de esas capas y para eso vamos a

09:03.120 --> 09:10.680
tratar de mirar la capa, bien, yo tengo entonces en esa imagen es como estamos viendo la frontera

09:10.680 --> 09:18.920
entre una capa y la siguiente, yo tengo la frontera de la capa W1, la capa y y la capa y más 1,

09:19.400 --> 09:26.280
entonces voy a decir que los estados de las neuronas en la capa y que llegan a la capa y son

09:26.280 --> 09:35.120
X1 superí, X2 superí, X3 superí, X4 superí, bien, eso va a ser el estado de la capa y y

09:35.120 --> 09:42.000
quiero calcular cuál va a ser el valor de la capa y más 1 dado que el valor de la capa y era eso,

09:42.000 --> 09:53.440
entonces la capa y yo tenía que valía esto, X1 superí, X2 superí, X3 superí y creo que

09:53.440 --> 10:05.760
llegamos a 4, esta, X4 super, esto es un vector, bien, entonces recuerden como calculábamos

10:06.520 --> 10:11.800
el valor de una neurona, decíamos que por ejemplo para calcular la neurona que está

10:11.800 --> 10:23.120
y arriba que es X1 y más 1 el valor de esta neurona se calculaba como y tenía que hacer las

10:23.120 --> 10:29.640
sumas digamos de los inputs que está donde la de izquierdo por los pesos que llegaban hasta ahí,

10:29.640 --> 10:35.320
entonces en este caso son todas las neuronas que están en la capa y, todos los valores de la

10:35.320 --> 10:44.320
neuronas multiplicados por todos los valores de las flechitas, entonces sería X1 superí por

10:44.320 --> 10:51.920
W y la flechita que está yendo desde la neurona 1 de la capa y hasta la neurona 1 de la capa y

10:51.920 --> 11:02.120
más 1 se llama W1, entonces X1 por W1, más la segunda capa, perdón la segunda neurona de la

11:02.120 --> 11:13.120
capa y por el segundo peso, este era el 2, 1, el peso 2, 1, esto también es de la capa y

11:15.120 --> 11:31.400
más X3 por W3, uno, todo esto es de la capa y, más X4 por W4, uno, bien, entonces la salida

11:32.400 --> 11:43.000
X sub 1 de la capa y más 1 es el producto de todas estas acá, bien, es el producto de la neurona 1 de la

11:43.000 --> 11:47.160
capa anterior por el peso 1, la neurona 2 de la capa anterior por el peso 2, 1, la neurona 3 de la

11:47.160 --> 11:53.400
capa anterior por el peso 3, 1, lo mismo puedo hacer para la otra, puedo decir X2 y sería igual

11:53.400 --> 12:03.320
solo que cambiando acá cambiando los lugares a 2, entonces yo es X1 y por W1, 2 y, más 2

12:03.320 --> 12:17.680
estos, más X4 y por W4, 2 y, bien, sí, decimos, ahí está, cuando estamos en una arquitectura en

12:17.680 --> 12:21.720
capas como ésta es así, cada de todas las neuronas de la capa siguiente están conectadas con

12:21.840 --> 12:27.560
la anterior, pero nunca saltan de capas, nunca cruzan hacia otra, y nunca vuelen hacia atrás,

12:27.560 --> 12:30.720
que es otra cosa que puede pasar en otras arquitecturas de redes, pero esta que es la más

12:30.720 --> 12:37.200
simple es cada capa con la siguiente, bueno, entonces X3 sería lo mismo, X1 y acá el peso 1, 3,

12:37.200 --> 12:50.920
tanda tata, X4, el peso 4, 3, sí, la dimensión es de ahí, los X son vectores de la carrera o de

12:50.920 --> 12:58.200
vez con reales, sí, o sea, no, acá son todos reales, X, todos los X y todos los doles son

12:58.200 --> 13:05.700
todos valores reales, entonces eso quería llegar, yo tengo, X1, X2, X3, X4 son 4 valores reales

13:05.700 --> 13:11.580
que componen un vector, y si yo agarro todos los doles 1, 1, 2, 1, doles 3, 1, doles 4, 1,

13:11.580 --> 13:16.460
doles 1, doles 2, etcétera, esto compone una matriz, en realidad, yo puedo construirme la

13:16.460 --> 13:26.600
matriz de la capa Y, es igual esta matriz que tiene dole 1, 1, hasta dole 4, 3, bien,

13:26.720 --> 13:38.080
W, 1, 3, W, 4, 1, bien, esto es una matriz, entonces al tener eso, en realidad yo puedo expresar la

13:38.080 --> 13:43.320
salida de esta capa, puedo expresar los estados en los cuales digamos los valores en los que quedan

13:43.320 --> 13:47.520
las neuronas de la capa siguiente, los puedo expresar como un producto de matrices, yo digo,

13:47.520 --> 13:55.080
el vector en la capa Y era esto, entonces el vector en la capa Y más 1 va a ser el producto de

13:55.080 --> 14:00.640
XC por WI, digamos esto termina siendo un producto de matrices, si hace el producto de matrices

14:00.640 --> 14:07.800
me daría X1 por W1, X2 por W1, X3 por W3, 1, X4 por W1, que es lo mismo que esté

14:07.800 --> 14:11.340
acá, y si vamos con la segunda columna, me da el mismo de acá, y si vamos con la

14:11.340 --> 14:15.560
tercera columna, me da el mismo de acá, pero es en definitiva la salida de esta capa,

14:16.760 --> 14:22.800
digamos si yo tengo esa neuron ahí, la salida de la capa, a ver dónde les creo,

14:23.520 --> 14:27.240
les pido acá porque esto nos va a tener que quedar para después para poder mirarlo,

14:27.240 --> 14:36.600
pero bueno, tengo X subraí, este es el vector de entrada, y voy a poner acá copiar la matriz

14:36.600 --> 14:52.080
esta, W1 1 hasta W4 1, W4 3, W1 3, la matriz, entonces, digo que el valor de X

14:52.080 --> 15:02.480
y más 1 va a ser el valor en Y por la matriz que representa los pesos de la capa Y,

15:03.360 --> 15:09.640
y a esto lo que me falta agregarle es el sigma, que es la función de activación y el

15:09.640 --> 15:13.400
sigma también puede pertenecer a la capa y digamos yo puedo tener distintas funciones de activación

15:13.400 --> 15:22.120
por capa, bien, entonces, concentremos en esto, decimos que si yo tengo una arquitectura en capa,

15:22.120 --> 15:26.440
donde cada capa está conectada con la anterior, digamos todas las neuronas una capa están conectadas

15:26.440 --> 15:33.840
con todas las neuronas de la anterior, entonces, puedo calcular la activación o los valores que

15:33.840 --> 15:42.720
va a tener la capa Y más 1 en función de la capa Y con esta formula acá, así que supongamos que

15:42.720 --> 15:50.440
tengo, eso creo que es exactamente lo mismo que dice acá, ahí está, tengo esa entrada,

15:51.240 --> 15:56.600
la salida va a ser ese vector, digamos, de tres neuronas y tengo esos pesos,

15:56.600 --> 16:04.640
por lo tanto puedo calcularlo de esta manera, bien, entonces supongamos que tengo una arquitectura

16:04.640 --> 16:14.040
que tiene tres capas, o más, digamos, tiene dos capas ocultas, entonces eso significa que si

16:14.040 --> 16:19.200
tengo dos capas ocultas voy a tener una matriz de pesos, que le voy a llamar W1 y una

16:19.200 --> 16:26.640
matriz de peso que le voy a llamar W2, entonces me va a venir un vector X que va a ser un vector que

16:26.640 --> 16:35.560
tiene un montón de trabajo, X1 hasta Xn, esto es un vector, quiero ver cuál va a ser la salida

16:35.560 --> 16:43.080
de la red suponiendo que tengo una capa de pesos W1 con una función de activación sigma 1 y

16:43.080 --> 16:48.600
una capa de pesos W2 con una función de activación sigma 2, cómo me quedaría la salida de la red,

16:48.600 --> 16:51.840
digamos, de qué, cuál sería la fórmula para la salida de la red,

16:56.400 --> 17:05.240
vamos a llamarle Rn de X a la salida de esta red que es una red que tiene dos capas ocultas y

17:05.240 --> 17:16.120
tiene esa estructura, está estructura en capas, ¿qué les parece? Sí,

17:22.120 --> 17:25.760
ahí está, X por del V1 y esto le aplicamos sigma 1,

17:25.760 --> 17:32.160
y después esto multiplicamos por W2, ahí está, la hacemos W2 y le pasamos sigma 2,

17:32.160 --> 17:40.760
exacto, bien, entonces eso sería, digamos, la ecuación que te queda de una arquitectura con

17:40.760 --> 17:46.840
dos capas ocultas, y bueno, la salida se calcularía de esta manera, tenemos el vector X,

17:46.840 --> 17:50.960
el vector X que le multiplicamos por los pesos de la capa 1, después le pasamos la función

17:50.960 --> 17:55.160
de activación, a esa resultado le multiplicamos por los pesos de la capa 2 y le aplicamos la función

17:55.160 --> 17:59.800
de activación y esa es la salida. Si tuvieramos más capas, si esto fuera un perceptro

17:59.800 --> 18:04.400
multicapa de 30 capas, entonces tendríamos como más añadimiento en esto, pero más o menos

18:04.400 --> 18:13.160
el camino. Bien, entonces ¿qué pasaría si estas funciones de activación fueran la función

18:13.160 --> 18:22.700
identidad o fueran funciones lineales como multiplicar por 4 o algo del estilo, ¿qué

18:22.700 --> 18:32.940
pasaría en ese caso? Ahí está, en ese caso, si esto fuera la identidad o si fuera multiplicado

18:32.940 --> 18:36.940
por una constante, pero supongamos que fuera la función identidad, entonces acá esto

18:36.940 --> 18:44.140
me daría lo mismo que hacer X por W1 por W2, que es lo mismo que hacer X por una cosa que

18:44.140 --> 18:51.020
es un producto entre dos matrices y un producto entre dos matrices vea otra matriz, entonces

18:51.020 --> 18:55.280
si estas funciones fueran una función identidad o fueran una función lineal o fueran una función

18:55.280 --> 19:01.660
de esas, digamos, simples, entonces todo esto sería una ecuación lineal, o sea yo podría

19:01.660 --> 19:05.620
reescribirlo siempre como el producto entre un vector y una matriz, que es un sistema

19:05.620 --> 19:11.520
lineal. Bien, esa es la razón por la cual se necesita que estas cosas acasean no

19:11.520 --> 19:14.740
lineales, que era lo que les decía que bueno, casi que el único requisito que tienen

19:14.740 --> 19:18.580
es tener estas funciones de activación es que sean no lineales, porque si son lineales

19:18.580 --> 19:22.540
cuando yo empiezo a arquitecturar estas cosas en capas, me queda simplemente un producto

19:22.540 --> 19:27.020
de matrices, porque me interesa que sea no lineal y porque, o sea, me molesta que esto sea

19:27.020 --> 19:31.540
un sistema lineal, porque si yo tengo un sistema lineal, digamos, si yo tengo que el

19:31.540 --> 19:35.820
resultado de mi red lo puede expresar como X por una matriz, entonces bueno, hay cierta

19:35.820 --> 19:39.500
clase de problemas, que voy a poder resolver, pero todos los problemas que son no lineales,

19:39.500 --> 19:42.680
todos los problemas que no se pueden capturar por una estructura lineal, entonces no lo

19:42.680 --> 19:53.960
puedo resolver. Bien, sí. Incluso sin la activación, o sea, es una reneural que no tiene

19:53.960 --> 19:59.240
que ir a ninguna, o sea, simplemente es multiplicar un vector por un conjunto de pesos.

19:59.240 --> 20:05.400
Bien, entonces, si yo tengo solamente una función lineal, hay un conjunto de problemas que

20:05.400 --> 20:10.460
puedo modelar, es verdad, pero no son todos, y de hecho no lo vamos a ver, pero hay una

20:10.460 --> 20:16.020
demostración que dice que teniendo funciones de activación no lineales, alcanza incluso

20:16.020 --> 20:20.780
con tener una sola capa oculta y alguna cosita más para modelar cualquier tipo de función

20:20.780 --> 20:24.260
que habiamos interese, digamos, con ciertas propiedades, por lo menos que se ha continuo

20:24.260 --> 20:29.860
en siento intervalo, etcétera, pero asumiendo ciertas propiedades bastante normales, es

20:29.860 --> 20:33.500
posible incluso con una sola capa, con una cantidad ruitada de neuronas, modelar cualquier

20:33.500 --> 20:39.380
función posible. Y eso es un poco el poder que tiene las reneurales, en realidad, son

20:39.380 --> 20:44.820
como suficientemente flexibles como para modelar cualquier cosa, cosa que cuando veíamos,

20:44.820 --> 20:49.100
bueno, no hay valles, era un ejemplo que modelaba ciertos tipos de problemas, si miran

20:49.100 --> 20:52.940
regresión logística, podemos modelarse de dos tipos de problemas, pero algunos no,

20:52.940 --> 20:58.660
las reneurales en calidad son super flexibles y podemos modelar cualquier cosa.

20:58.660 --> 21:04.260
Entonces, sabemos que, para casi cualquier función que aún no le interese modelar, existe

21:04.260 --> 21:08.260
una reneural que podría llegar a cumplirla con suficienta nivel de precisión, que vamos

21:08.260 --> 21:12.700
ahí, teoría y más que les muestran, sin embargo, encontrarla en la práctica no es tan fácil,

21:12.700 --> 21:17.700
o sea, sabemos que existe una, la familia de la reneurales, hay alguna función que me

21:17.700 --> 21:22.500
va a permitir hacer todo lo que quiera, pero bueno, de allá encontrarla no es tan sencillo,

21:22.500 --> 21:26.780
pero bueno, por lo menos sabemos que existe. Igual, con estas cosas que tenemos, o sea,

21:26.780 --> 21:30.820
sabiendo lo más que es arquitecturando en capas y teniendo la función de activación

21:30.820 --> 21:35.620
no lineal en cada una, ya tenés un montón de funciones interesantes que pueden servir

21:35.620 --> 21:51.100
para modelar muchas cosas. Bien, preguntas acá. Bueno, estas otras funciones de activación

21:51.100 --> 21:56.820
interesante que se conoce como la función softmax, se utiliza para los problemas de

21:56.820 --> 22:03.500
clasificación discretos, por ejemplo, hay que tener en el segundo obligatorio que, bueno,

22:03.500 --> 22:08.140
es el problema de clasificar un tweet y lo quiero clasificar en si es positivo, negativo,

22:08.140 --> 22:13.500
neutro o nada, no, tengo esas cuatro clases. Entonces, la función de activación softmax es

22:13.500 --> 22:20.980
como una generalización de la función logística de la sigmoide, que se calcula de esta

22:20.980 --> 22:27.740
manera, dice bueno, yo asumo que los pesos de salida, que son números reales, van a formar

22:27.740 --> 22:31.220
una probabilidad, digamos, lo quiero transformar en una probabilidad, entonces lo calcula de

22:31.220 --> 22:38.780
manera, digo que el valor para y su y su v es e a la y su v sobre la sumatoria de e a la

22:38.780 --> 22:45.180
el resto. Esto solamente para que lo tengan en cuenta, es muy probable que si van a usar

22:45.180 --> 22:49.420
rengebranales en la segunda tarea, tengan que utilizar al final una capa, que se llama

22:49.420 --> 22:53.060
capas softmax, que es una capa que tiene una función de activación especial que sirve

22:53.060 --> 22:56.260
para transformar las salidas en distribución de probabilidad.

22:56.260 --> 23:10.980
Sí, y la mayor, si tiene una distribución de probabilidades, y bueno, la sociedad que

23:10.980 --> 23:22.060
tiene probabilidad mayor, ahí tienes que tener una, sería como una logística independiente

23:22.060 --> 23:28.260
por cada una. Entonces, si es mayor que cero, digo que es valido, si no, o sea, si

23:28.260 --> 23:32.420
voy a decir que puedo tener más de un label a la vez, ahí tendrías que hacer otra

23:32.420 --> 23:36.060
cosa. En softmax, va a intentar que sea una distribución de probabilidades, entonces

23:36.060 --> 23:44.060
probablemente te queda una clase que gane y las demás sea mucho más bajitas.

23:44.060 --> 23:50.860
Bien, bueno, entonces, recuerden que estamos siempre utilizando números, por ahora no hemos

23:50.860 --> 23:54.260
visto nada del lenguaje, eso lo vamos a ver un poco más adelante, ahora son todos

23:54.260 --> 24:00.300
números. En la entrada me vienen números reales, en los pesos tengo números reales,

24:00.300 --> 24:03.700
hago multiplicaciones, le paso funciones de activación, etcétera, y me da otro vector

24:03.700 --> 24:07.860
de números reales, o sea, la salida de esto va a ser un vector de números reales, tener

24:07.860 --> 24:11.340
en cuenta que cada una de estas cosas van a tener sus dimensiones, yo voy a tener, acá

24:11.340 --> 24:17.820
tenía una entrada que tenía cuatro vectores, para cuatro valores, una matriz que tenía

24:17.820 --> 24:21.900
cuatro por tres, entonces al multiplicarlo me devuelve tres, si la siguiente capa es

24:21.900 --> 24:28.060
de tres por ocho, entonces me va a volver ocho y así, o sea, los tamaños de las matrices,

24:28.060 --> 24:33.220
o sea, los tamaños de las capas tienen que coincidir. Pero en definitiva son todos

24:33.220 --> 24:42.060
vectores, ¿no? Por ahora, esto es un cálculo utilizando cálculo numérico vectorial.

24:42.060 --> 24:46.260
Entonces, vamos a hablar un poco de cómo se entrenan estas redes, y vamos a pensar

24:46.260 --> 24:51.620
de la siguiente manera, ¿cómo esto es un método de aprendizaje automático? Yo voy

24:51.620 --> 24:55.740
a tener, como vimos en las clases anteriores, voy a tener un conjunto de entrenamiento,

24:55.740 --> 25:00.420
un conjunto de desarrollo, un conjunto de test, entonces supongamos que yo tengo un

25:00.420 --> 25:04.780
conjunto de entrenamiento que tiene en ejemplos, o sea, en ejemplos significa que voy

25:04.780 --> 25:13.420
a tener en estos vectores y en salidas distintas, que les voy a llamar y, entonces, los vectores

25:13.420 --> 25:18.260
de entrada son estos, los vectores de salidas son estos de acá, y yo tengo que tratar

25:18.260 --> 25:27.540
de ver si la salida se parece al entrada. Entonces, supongamos que la salida es solamente

25:27.540 --> 25:33.220
un valor, ¿no? O sea, para simplificar, vamos a asumir que la entrada de la red es

25:33.220 --> 25:38.940
un vector de cualquier dimensión, y la salida solamente es un valor real, ¿no? Es posible,

25:38.940 --> 25:42.700
¿no? O sea, lo que estoy haciendo es tener una red que tiene muchas capas, lo que sea,

25:42.700 --> 25:47.400
en el final, todo se reduce a una sola salida un valor real, obviamente esto después

25:47.400 --> 25:51.380
se extiende a más valores reales, pero bueno, supongamos que tenemos una sola, ¿no?

25:51.380 --> 26:00.500
Entonces digo que tengo en instancias, o sea, en valores x v, este es mi conjunto de

26:00.500 --> 26:07.260
entrenamiento, supongamos, o el conjunto en el que estoy tratando de medir cosas x v y

26:07.260 --> 26:13.660
me dice que esto es x v, deberían corresponderse con diferentes valores de x v, ¿no?

26:13.660 --> 26:20.660
Este es el conjunto de valores esperados, yo digo que para x v 1 tengo un y v 1, para

26:20.660 --> 26:29.080
x v 2 tengo un y v 2, bien, por ahora son todos números reales, y además tengo que

26:29.080 --> 26:37.280
yo tengo una red neuronal con ciertos pesos, que se la puedo aplicar a x v y con sus

26:37.280 --> 26:43.160
matrices de pesos, entonces mi red neuronal me va a dar cierto valor y le voy a llamar

26:43.160 --> 26:50.800
y subí techo, ¿cómo puedo saber si está bien lo que me da la red neuronal para x v o

26:50.800 --> 26:58.080
no? De qué manera yo puedo llegar a medir si está bien o no, este valor que me dio.

26:59.080 --> 27:08.400
Si, ahí está, o sea, mi salida, en mi conjunto yo decía, bueno, la salida tenía

27:08.400 --> 27:15.120
haber sido y subí, y la salida de medio de la red es y subí techo, ¿cómo puedo saber

27:15.120 --> 27:18.960
si ese está bien o mal? O sea, ¿qué medida me dice si está bien o mal?

27:18.960 --> 27:29.680
Ahí está, lo puedo restar y digo bueno, ¿qué tanto se parecen estos dos? Si esto está

27:29.680 --> 27:33.480
cerca de cero, es un valor muy chiquito, entonces yo puedo decir que estas dos cosas son iguales,

27:33.480 --> 27:40.720
por lo tanto la red me está dando un resultado parecido al que yo esperaba, y si estos dos son

27:40.720 --> 27:45.800
muy diferentes, entonces esto me va a dar un valor bastante alto, entonces yo tengo muchos de

27:45.920 --> 27:51.440
estos, no tengo n ejemplos de este estilo, así que lo que voy a hacer es sumar todos estos,

27:51.440 --> 27:59.440
de igual 1 hasta n, sumo todos los valores, tengo un problema acá que es que a veces yo puedo

27:59.440 --> 28:02.680
le puedo ahorrar por mucho, a veces le puedo ahorrar por poco, pero a veces esto me va a dar

28:02.680 --> 28:06.960
negativo, esto me va a dar positivo, entonces si yo lo sumo todos capaz que me da cero por casualidad,

28:06.960 --> 28:11.880
entonces lo que hago es ponerlos acuadrado, para decir bueno, yo siempre voy a sumar valores

28:11.880 --> 28:16.680
positivos, entonces si mi salida es distinta, el valor esperado siempre esto me va a dar un resultado

28:16.680 --> 28:21.800
positivo, bien, entonces como estoy comparando n ejemplos, a esto lo voy a dividir entre n,

28:23.640 --> 28:30.720
esto de acá me da una métrica condensada que me dice que tanto se equivoco mi red respecto a los

28:30.720 --> 28:34.200
valores, a todos los valores esperados, y de hecho esta es una de las métricas posibles para

28:34.320 --> 28:42.400
medir eso, esta es muy usada, se llama mce, minzcuerderor o error cuadratico medio y es una de las

28:42.400 --> 28:51.280
métricas más conocidas, entonces esto es una métrica que me permite medir la discrepancia que

28:51.280 --> 28:57.520
hay entre los valores esperados de una red acá era isu y, entre los valores esperados de una red y

28:57.520 --> 29:02.720
los valores que la red dio con todos los pesos que tiene hasta el momento, recuerden que este

29:02.720 --> 29:12.520
isu y se calculaba como el resultado de la red para exu y los pesos de red, entonces este tipo

29:12.520 --> 29:17.600
de funciones que miden la diferencia entre los valores esperados y los valores que me dio la

29:17.600 --> 29:23.800
red de verdad se llaman funciones de perdida, bien, o sea el nombre de perdida no se movien de donde

29:23.800 --> 29:28.640
sale, pero se le fue, se le suele llamar funciones de perdida los functions y bueno es uno de los

29:28.640 --> 29:33.280
conceptos que no tiene que aprender cuando aprende de redes neuronales, porque para entrenarlas yo

29:33.280 --> 29:37.960
lo que tengo que hacer es elegir una los function apropiada para problemas, entonces estas de las

29:37.960 --> 29:43.280
más comunes, el error cuadratico medio sirve mucho para problemas donde los valores resultados son

29:43.280 --> 29:49.440
valores reales, no sirve tanto para cuando los valores esperados resultantes son por ejemplo una

29:49.440 --> 29:55.160
distribución de probabilidades o una categoría en muchas como ese problema que tienen en el laboratorio,

29:55.640 --> 30:02.540
para eso utiliza notas, por ejemplo la entropía cruzada o en particular una versión de entropía

30:02.540 --> 30:07.840
cruzada que sirve para decir yo tengo un solo valor correcto de entre muchos que en el laboratorio

30:07.840 --> 30:13.240
les pasa eso digamos que tengo un tweet y es positivo o en negativo o en neutro no puede ser más

30:13.240 --> 30:17.480
de une, entonces para eso se usa la última, es una versión de la entropía cruzada para valores

30:17.600 --> 30:26.400
categoricos, bien y existen unas cuantas más digamos, o sea pero en definitiva siempre tengo que

30:26.400 --> 30:30.200
tener funciones de estilo como pasaba con la función de activación lo que se espera de una

30:30.200 --> 30:36.240
función de perdias es que sea derivable y en el caso de las funciones de perdias lo que se

30:36.240 --> 30:41.700
espera es que cuando la salida de la red se parece muchísimo a los valores esperados,

30:41.700 --> 30:45.840
tiene que estar cercana a cero o tiene que ser un valor mínimo y cuando la salida de la red es

30:45.840 --> 30:57.160
muy diferente tengo que ser un valor más grande, bien, ok entonces ¿por qué es que yo quiero

30:57.160 --> 31:08.000
que todo esto sea derivable? ¿Por qué les parece? Sí, es exacto para minimizar el hecho de

31:08.000 --> 31:17.160
que yo voy a hacer que esto sea derivable digamos que lo que está dentro o sea este es y su

31:17.160 --> 31:24.800
techo y subí techo menos y subí y esto lo calculé con esto que está acá entonces esto

31:24.800 --> 31:35.040
es una sobre n por la sumatoria de 1 esta n de una cosa que tenía la forma sigma de

31:40.160 --> 31:52.520
sigma de sigma de x por w a la 1 por w2, no sé qué menos y subí al cuadrado, bien entonces acá

31:52.520 --> 31:57.680
entonces yo tenía una cosa que era todo derivable y acá afuera tengo otra función que también es

31:57.680 --> 32:02.760
derivable tanto de las funciones de activación como todos los resultados de la red nominal como

32:02.760 --> 32:07.120
la función de perdias como todas estas cosas son todos derivables para que quiero eso porque

32:07.120 --> 32:13.040
efectivamente voy a derivar digamos o sea la técnica se utiliza para entrenar estas cosas se

32:13.040 --> 32:20.400
basa mucho en encontrar derivadas y vamos a dotar de ver por qué bien entonces para entrenar

32:20.400 --> 32:28.240
una de estas redes recordemos que digamos para entrenar estas redes yo recordemos que tengo

32:28.240 --> 32:35.400
conjunto de entrenamiento, un conjunto de desarrollo, un conjunto de test y me interesa tratar

32:35.400 --> 32:43.960
de minimizar esto o sea yo tengo que la red se calcula como dependiendo de el valor de entrada

32:43.960 --> 32:49.480
y el conjunto de pesos que tengo yo voy a multiplicar ese valor de entrada por una matriz y por

32:49.480 --> 32:55.040
otra por la funcional activación etcétera hasta obtener un resultado pero entonces notar que

32:55.040 --> 33:00.920
este valor está en función de la entrada que es que es que es subir y el conjunto de pesos

33:00.920 --> 33:06.080
o leve, no acá yo tengo una función que es que están función de dos cosas estas son las

33:06.080 --> 33:11.080
entradas de conjunto de entrenamiento o del conjunto que estoy mediendo y estos son los pesos que

33:11.080 --> 33:17.640
yo le puedo dar a cada una de las capas entonces una cosa interesante es que yo puedo mirar

33:17.640 --> 33:21.840
este problema desde el punto de vista de que estos valores los dejó fijos digo mi conjunto

33:21.840 --> 33:26.280
de entrenamiento lo conozco entonces los valores están fijos y yo puedo ir cambiando los pesos

33:26.280 --> 33:31.920
hasta encontrar el conjunto de pesos ideales que permita que los valores de entrenamiento multiplicados

33:31.920 --> 33:36.960
por esos pesos me den la salía que yo quiero entonces ahí eso se transforma en un problema como

33:36.960 --> 33:42.360
decían por ahí un problema de demonización, un problema de optimización en el cual lo que

33:42.360 --> 33:50.160
voy a hacer es tomar esto como variable entonces yo lo que quiero encontrar es el argmin para la

33:50.160 --> 33:57.120
familia posible de pesos de las distintas matrices lo leve de esta función acá que es uno sobre

33:57.120 --> 34:07.680
n por sumatoria nn y subitecho menos y subí al cuadrado bien y voy a encontrar el argmin en

34:07.680 --> 34:17.160
w o sea lo que está acá dentro que es rn de xc w le voy variando estos w hasta que hasta

34:17.160 --> 34:26.200
encontrar el ideal bien entonces supongamos que tengo una función así no vamos a ver una función

34:26.200 --> 34:32.840
bastante simple como para ver cómo funciona esto el entrenamiento de una red se da utilizando

34:32.840 --> 34:36.840
una técnica de llama de senso por gradiente hay otras técnicas pero estas por lejos la más

34:36.840 --> 34:43.040
utilizada de todas y la técnica de senso por gradiente funcione la ciente manera si yo tuviera una

34:43.040 --> 34:48.080
función que va solamente en una dimensión y quiero minimizarla y arranco con un punto por acá

34:48.080 --> 34:54.040
digo bueno mi peso inicial me dice que voy a terminar en este lado entonces yo puedo calcular

34:54.760 --> 35:03.000
la derivada en ese lado y decir bueno para aquel lado voy bajando mi función de costo o sea

35:03.000 --> 35:07.600
suponiendo que esta es la función de pérdida o función de costo puedo decir para aquel lado

35:07.600 --> 35:12.280
voy bajando mi función de pérdida y dice bueno lo voy bajando si bajo por esta dimensión si

35:12.280 --> 35:17.640
bajo por esta dirección entonces ahí le digo bueno baja un poquito por ahí y calculame otro

35:17.640 --> 35:21.640
valor que va a estar acá y ahí devuelta voy a calcular la derivada y digo bueno en qué sentido

35:21.640 --> 35:25.660
voy bajando y se voy bajando si voy para allá entonces ahí me encuentro tu valor que este

35:25.660 --> 35:30.640
en esa dirección calculo de vuelta la derivada y así o sea yo puedo ir iterando de esta manera

35:30.640 --> 35:37.160
hasta llegar a un mínimo bien eso me llama de senso por gradiente porque yo tengo quiero encontrar

35:37.160 --> 35:43.440
el mínimo de una función supongamos que esta es mi función de pérdida y empecé teniendo este

35:43.440 --> 35:49.080
valor calculo donde está la dirección en la cual puedo bajar más y voy moviéndome por ahí hasta

35:49.080 --> 35:56.600
llegar al punto bajo esto es un caso ideal en el cual yo tengo una sola variable que estoy tratando

35:56.600 --> 36:03.720
de encontrar en el caso real yo estoy minimizando digamos minimizando esta función respecto a

36:03.720 --> 36:08.640
W que es una cosa que son muchas matrices con muchos pesos con muchas cosas y pueden llegar a

36:08.640 --> 36:16.800
hacer miles de millones de pesos pero bueno en un caso ideal si yo estuviera solamente minimizando

36:16.800 --> 36:21.480
una severía de esta manera cuando yo estoy minimizando millones de variables a la vez lo que pasa

36:21.480 --> 36:25.560
es que esta superficie digamos lo que tengo acá no va a ser una curva tan linda sino que va a ser

36:25.560 --> 36:30.960
una superficie rusa que tiene un montón de de de óptimos locales que no me van a servir pero

36:30.960 --> 36:34.560
cuando yo hago este algoritmo lo que va a hacer es caerse en un óptimo local imagínense que si

36:34.560 --> 36:40.440
esta curva tuviera esta forma entonces este algoritmo llegaría a un óptimo local por acá pero

36:40.440 --> 36:45.560
se perdería el óptimo global que estaba por acá bien eso es algo que puede pasar entonces bueno

36:45.560 --> 36:49.880
no se asusten que cuando uno entra en una rena uronal nunca va a estar seguro de que encontré el

36:49.880 --> 36:54.640
óptimo posible de toda la red de todas las posibles sino que bueno tengo que conformarme con encontrar

36:54.640 --> 37:01.360
una bastante buena probando varias veces bueno entonces decíamos esto sobre entrenamiento

37:05.200 --> 37:10.640
el entrenamiento intenta encontrar los pesos que minimizan esta función de perdiado o sea la

37:10.640 --> 37:18.160
combinación de matricios olv que hace que esta función sea lo menor posible la técnica se utiliza

37:18.160 --> 37:23.480
el senso por gradiente que es lo que está mencionando acá se usa una cosa de llamas de senso por

37:23.480 --> 37:30.240
dancias estocástico que se trata de agarrar cada punto yo agarró cada punto de entrada y

37:30.240 --> 37:34.440
trato de hacer el senso por gradiente consiguiendo solamente ese punto y después agarró otro punto de

37:34.440 --> 37:39.120
entrada y luego varias veces el problema que tiene eso es que es superlento o sea es como que tiene

37:39.120 --> 37:43.280
buena propiedad de convergencia pero es superlento entonces lo que se hace es hacer de senso por gradiente

37:44.280 --> 37:51.040
en lote o en batches que significa bueno en vez de tomar todo el conjunto de entrenamiento que

37:51.040 --> 37:57.360
puede tener 100 mil millones de ejemplos tomo de a 120 o no sé 200 o elijo un batch que digo bueno

37:57.360 --> 38:01.120
tomo este conjunto de ejemplos y hago de senso por ahí después tomo otro conjunto de

38:01.840 --> 38:12.120
senso por la gente por ahí y hasta llegar a un óptimo bien los siguientes backpropagation entonces

38:12.120 --> 38:19.320
yo les dije hasta ahora que todas las cosas tenían que ser derivables y el hecho de que sean

38:19.320 --> 38:23.320
derivables implica que lo vamos a derivar en algún momento no vamos a hacer acá ninguna derivada

38:23.320 --> 38:28.840
digamos porque en realidad los paquetes que se utilizan para trabajar con estas con estas cosas

38:28.840 --> 38:33.160
en realidad son paquetes que permiten hacer derivación automática o sea toda la gracia de construir

38:33.160 --> 38:37.800
redes neuronales utilizando ciertas librerías es que las librerías permiten definir todas estas

38:37.800 --> 38:41.640
cosas como vectores y después ellos hacen las derivadas automáticamente calculan todo automáticamente

38:41.640 --> 38:48.000
pero en definitiva la técnica que se usa para calcular se llama propiedad que implica que cuando

38:48.000 --> 38:54.000
yo voy calculando los pesos de una red para los valores de una red yo digo el lo que tornen

38:54.720 --> 38:58.960
lo multiplico por del V después le paso la función de activación lo multiplico por otra V

38:58.960 --> 39:03.120
le paso la función de activación a medida que voy calculando eso voy dejando como todos los

39:03.120 --> 39:08.760
valores intermedios esos valores se usan de atrás para adelante por el llamado propiedad

39:08.760 --> 39:13.400
y son para calcular las derivadas porque en realidad todos los valores de sumas multiplicaciones

39:13.400 --> 39:17.160
etcétera que yo fui dejando el medio se utilizan como que siempre calculan para después

39:17.160 --> 39:21.560
calcular la derivada y el vaculopadillo es una técnica que me ayuda a hacer eso rápidamente

39:22.560 --> 39:27.880
bien entonces esta es la pregunta que le decía hoy yo puedo encontrar la mejor función posible

39:27.880 --> 39:33.960
puedo contar la mejor reneuronal que explique mi problema 100% bien la verdad que no porque en general

39:33.960 --> 39:40.160
este proceso se cae en óptimos locales y este tipo de funciones que tienen miles de millones

39:40.160 --> 39:45.880
de parámetros lo que pasa que tiene muchísimos últimos locales y bueno el entrenamiento se va a

39:45.880 --> 39:51.240
caer siempre en un óptimo local lo que no hace para evitar eso de alguna manera es por ejemplo

39:51.240 --> 39:55.360
entrenar varias veces una misma red diciendo bueno tengo una misma récola, los mismos parámetros

39:55.360 --> 39:59.440
le entrenó muchas veces y veo cual cual le fue mejor de todos los entrenamientos esa es una de

39:59.440 --> 40:05.640
las formas y el otro problema detiene es el sobre ajuste creo que no lo mencionamos en la

40:05.640 --> 40:11.120
clase anterior sobre ajuste significa que las reneuronales tienen un problema que lo tienen

40:11.120 --> 40:15.920
otro método de clasificación pero las reneuronales es en particular porque como que son muy

40:15.920 --> 40:20.480
versátiles y es que se pueden aprender muy fácil todo el conjunto de entrenamiento yo puedo

40:20.480 --> 40:24.880
entrenar una red que se aprenda muy bien en conjunto de entrenamiento y me diga sí para este X le

40:24.880 --> 40:30.520
corresponde este I y anda bárbaro y el la función de los vea casi cero y sin embargo lo pruebo en

40:30.520 --> 40:36.800
conjunto de test y le va horrible y eso es muy fácil porque como les decía como las reneuronales

40:36.800 --> 40:40.200
pueden modelar cualquier tipo de función entonces es muy fácil que se aprendan todo el conjunto

40:40.200 --> 40:45.480
de entrenamiento y después para el conjunto de tele vaya espantoso ese ese fenómeno se llama

40:45.480 --> 40:50.080
sobre ajuste entonces bueno hay como distintas técnicas para tratar de evitarlo y que la red no

40:50.080 --> 40:58.640
digamos no sea ajustes a los datos sino que se va a generalizar más etcétera bien entonces sí

41:06.800 --> 41:15.520
es una pregunta interesante en realidad hay un conjunto de técnicas que sirven para decir yo

41:15.520 --> 41:21.080
puedo entrenar una red con un conjunto de datos más amplio que capaz que no está el todo correcto y

41:21.080 --> 41:25.360
después una vez que tengo una red de entrenada la entreno de vuelta con un conjunto más chico pero

41:25.360 --> 41:30.080
que tiene mejor calidad y eso da mejor resultado que entrenarla directamente con el conjunto más chico

41:30.080 --> 41:34.800
o con otro tipo de datos entonces de ahí hay variante es cierto que una red una vez que ya

41:34.800 --> 41:38.700
conseguí los pesos de la red lo puedo seguir entrenando usando otros conjuntos y eso es valido

41:40.700 --> 41:46.600
o sea se usa es una técnica que se usa y está buena porque da buenos resultados bien igual en

41:46.600 --> 41:52.200
atare de ustedes no sé no sé si va a la pena hacerlo o sea probablemente si van a entrenar

41:52.200 --> 41:56.440
reneurales lo hacen lo van con los datos que tienen no no creo que sean necesarios a mucha

41:56.440 --> 42:03.480
cosa más pero sí tratar de ver un poco lo que vamos a ver ahora que hasta ahora vieron que

42:03.480 --> 42:07.480
estamos viendo números reales o sea entra a un vector de números reales salían números reales

42:07.480 --> 42:26.320
vectores de números reales sí sí sí sí sí se usan a veces en la práctica da mejor resultado

42:26.320 --> 42:32.360
probar varias veces y o hacer una prueba digamos tipo grid search en el cual digo tengo tantos

42:32.360 --> 42:38.840
parámetros y probar con todos o aleatoriamente probar asampliando distintos parámetros y entrenar es

42:38.840 --> 42:43.600
cierto que también se usan meteorísticas evolutivos y algunas otras para tratar de optimizar la

42:43.600 --> 42:49.280
red pero no sé en la práctica si es que dan tan buenos resultados o simplemente ir probando

42:49.280 --> 42:53.920
con distintas combinaciones andando mejor bueno o general encontrar buenos resultados igual sí

42:53.920 --> 43:14.520
sí sí sí bueno claro pero el problema es que la función de pérdida no va a tener un

43:14.520 --> 43:20.080
óptimo global normalmente no va a tener porque la función de pérdida tiene esta cosa en el medio

43:20.640 --> 43:26.960
no estoy minimizando una cosa que es algo no lineal y que tiene millones de parámetros y yo

43:26.960 --> 43:30.720
puedo ir en la dirección de cualquiera de los millones de parámetros entonces por eso normalmente

43:30.720 --> 43:36.160
digamos eso te genera una superficie super rubosa que tiene un montón de de su día así bajada por

43:36.160 --> 43:41.800
todos lados y justo en boca la el óptimo global es muy difícil o sea nada te garantiza que puedas tener

43:41.800 --> 43:52.000
el óptimo global claro sí sí pero acá queremos esplicitamente que la función de activación sea algo

43:52.000 --> 43:57.880
que me deje la función complicada digamos si vos claro si claro si vos hace que la función

43:57.880 --> 44:03.280
de activación sea tan simple que esto queda como una función convexa entonces pierde

44:03.280 --> 44:08.760
capacidad de generalización la red por eso se dice también que esto es un problema de

44:08.760 --> 44:13.520
optimización no convexa entonces en optimización convexa uno puede asegurar que siempre tenemos un

44:13.520 --> 44:17.120
óptimo global y lo podríamos llegar a encontrar con alguna técnica pero esto es la minimización no

44:17.120 --> 44:21.600
convexa la forma de la gráfica siempre va a tener subidas y bajadas en algún lado

44:24.160 --> 44:30.920
bien más preguntas acá entonces pasemos a la parte del lenguaje bien decíamos hasta el momento

44:31.800 --> 44:38.360
teníamos una reneuronal que a la cual le entraban valores reales y salían valores reales

44:38.840 --> 44:43.640
pero nosotros en realidad nos interesa trabajar con texto nos interesa trabajar con palabras oraciones

44:43.640 --> 44:50.720
documentos tweets en el caso del oriatorio y el problema de este que tenemos una red que le entra

44:50.720 --> 44:54.800
valores reales no es un problema raro digamos un problema que le pasa a la mayoría de los métodos

44:54.800 --> 44:58.280
de prenses automáticos y estuvieron mirando algo de regresión logística etcétera siempre yo

44:58.280 --> 45:04.080
tengo que mandarle valores reales a las cosas salvo en la iglesia es que más o menos uno puede decir bueno

45:04.080 --> 45:08.520
trabajo con palabras o sea como en la extracción esto trabaja en nivel de palabras en el resto

45:08.520 --> 45:14.000
siempre está esperando que yo le mande valores numéricos entonces yo necesito poder tener una

45:14.000 --> 45:21.360
buena representación numérica de los textos y de paso voy a pedir una propiedad más que es

45:21.360 --> 45:26.380
que esa representación numérica tenga algunas propiedades interesantes como por ejemplo una

45:26.380 --> 45:31.280
métrica de distancia que haga que las palabras más cercan las palabras más similares y

45:31.800 --> 45:36.840
este más cerca y la más diferente de este más lejos por ejemplo puedo pedir eso en una

45:36.840 --> 45:44.240
en una representación entonces vamos a ver una técnica de llamar warden bedings o

45:44.240 --> 45:48.680
vectores de palabras que se utiliza para representar las palabras y después eso lo pudo

45:48.680 --> 45:54.120
utilizar como entrada una red y la técnica se basa en la hipótesis distribucional que son hipótesis

45:54.120 --> 46:02.020
que surgió en los 50 con este Firth que era un linguista lógico etcétera y decían lo

46:02.020 --> 46:05.880
siguiente bueno las palabras que aparecen en contextos similares tienden a tener significados

46:05.880 --> 46:12.120
similares y acá tenemos un ejemplito que dice este ejemplito tiene como algunas palabras y

46:12.120 --> 46:16.680
algunas ideas de contexto no la milanesa con queso más rica la uruguayas si es rica la

46:16.680 --> 46:21.800
hamburguesa con queso la milanesa con queso musarelas le decimos una politana no sé qué está

46:21.800 --> 46:25.560
eso como que está hablando de milanesas hamburguesas comida y después el otro dice el

46:25.560 --> 46:28.840
autoño es una de las citaciones del año el verano es mi situación de favoritas el

46:28.840 --> 46:33.080
invierno en invierno hace pila de frío en verano nunca hace frío y está hablando como de otra

46:33.080 --> 46:37.880
cosa no claramente las palabras rojas se parecen más entre sí las palabras azules se

46:37.880 --> 46:42.160
parecen más entre sí entonces idealmente yo quería tener una representación que a las rojas las

46:42.160 --> 46:47.680
deje más o menos cerca y a las azules violetas las deje más o menos en otro lado

46:51.480 --> 46:57.400
bueno una primera idea que surgía es lo que se conoce como matriz término término que

46:59.400 --> 47:05.040
se se realiza contando palabras contando cuando una palabra parece cuánta vez aparece una

47:05.040 --> 47:11.400
palabra en el contexto de otra entonces por ejemplo en este caso yo digo yo tomo alrededor de

47:11.400 --> 47:16.520
una palabra en palabras de contexto alrededor y cuento cuánta vez se aparece otra en ese contexto

47:16.520 --> 47:23.480
entonces como ejemplo tenemos bueno estos son los ejemplos anteriores no la milanesa con queso

47:23.480 --> 47:28.400
más rica la hamburguesa no sé qué el autoño tal cosa y pregunta cómo quedaría la matriz

47:28.400 --> 47:34.720
utilizando un contexto de cuatro palabras y acá no sé si lo llevan a ver todos pero me

47:34.720 --> 47:42.320
aparece que por ejemplo la palabra milanesa tiene las palabras rica y queso en su contexto

47:42.320 --> 47:47.120
de la palabra hamburguesa también pero la palabra toño no la palabra toño tiene en su contexto

47:47.920 --> 47:52.360
bueno acá justo como estoy tomando en igual cuatro no pasa pero las palabras verano y invierno

47:52.360 --> 48:01.240
tienen en su contexto la palabra frío y no tienen ni rica ni queso entonces eso es con

48:01.240 --> 48:06.120
en igual cuatro no contando cuatro palabras alrededor si yo considerara en igual cinco entonces ahí

48:06.120 --> 48:14.200
si aparecería o toño tiene la palabra estaciones en su en su contexto y verano también tiene

48:14.200 --> 48:18.880
estaciones en su contexto entonces es como que me van quedando zonas de la matriz que están

48:18.880 --> 48:24.520
como más acopladas entre sí no como que tienen mayor nivel de proximidad y otras zonas que no

48:24.520 --> 48:31.680
entonces ahí ya tendría como una especie de primera aproximación a lo que sería en

48:31.680 --> 48:35.440
vectores de palabras que es decir bueno yo puedo representar cada palabra con una fila de esta

48:35.440 --> 48:39.400
matriz y esa fila de la matriz va a tener ciertas propiedades cosas de que palabras que están

48:39.400 --> 48:45.360
cerca van a semánticamente similares van a estar cerca en esas en esas filas un problema que tiene

48:45.360 --> 48:49.120
esta representación que dice ahí abajo es que son vectores muy grandes yo tengo vectores de

48:49.240 --> 48:53.920
tamaño básicamente el tamaño del vocabulario si yo tengo considero 10 mil para

48:53.920 --> 48:58.640
ver vocabulario o tener vectores de tamaño de 10 mil donde la mayoría de los de los números

48:58.640 --> 49:03.140
van a ser cero y algunos van a ser valores distintos de cero entonces me va a pasar que los

49:03.140 --> 49:11.720
vectores son dispersos o spars bien entonces ahí como refinaciones esta técnica que se utiliza

49:11.720 --> 49:17.440
bastante o sea esta esta técnica de de construir matriz y determinos términos se puede usar como

49:17.440 --> 49:22.000
base para calcular ciertos tipos de vectores de palabras el algoritmo glob se basa en

49:22.000 --> 49:28.800
comentar comenzar en esta matriz los algoritmos de pca de principal componente análisis se pueden

49:28.800 --> 49:33.640
usar para reducir la dimensionelidad de esta matriz en realidad este tipo de matrices tiene sus

49:33.640 --> 49:39.600
usos pero la que vamos a ver es una técnica un poco posterior a las matrices también

49:39.600 --> 49:46.160
o término que digamos que está como en el inicio de lo que fue las revoluciones que se han dado

49:46.160 --> 49:52.360
en pelea en los últimos años no este este es un trabajo de 2013 un trabajo de bueno un

49:52.360 --> 49:57.280
investigador que ya no es mi collode que propuso en 2013 una técnica que en realidad son dos

49:57.280 --> 50:03.640
acorimos distintos que se llama Word to back o sea ir acorribo para ir de palabras a vectores y

50:03.640 --> 50:09.480
que su idea era construir vectores de ensos o sea vectores que tuviera una dimensión mucho más

50:09.480 --> 50:13.600
chica de vocabulario en vez de tener un vectore tamaño de 10 mil yo voy a tener un vectore tamaño 100 o

50:13.600 --> 50:20.360
250 o 300 y por el hecho de comprimir todo el vocabulario en esos vectores más densos entonces

50:20.360 --> 50:27.440
ganó esas propiedades de que palabras más cercanas son simánticamente similares entonces bueno

50:27.440 --> 50:32.840
obviamente no lo van sólo por comprimir sino por cómo se entrena esto entonces la idea de los

50:32.840 --> 50:40.000
algoritmos de Word to back es decir bueno en vez de contar como la matriz determinó término

50:40.000 --> 50:43.680
de las palabras dentro de un contexto yo lo voy a ver con un problema de clasificación un

50:43.680 --> 50:49.960
problema de probabilístico en el cual voy a predecir qué tan probable es que la palabra C aparezca

50:49.960 --> 50:57.880
en el contexto de la palabra W bien entonces voy a tener una predicción la producción de que es

50:57.880 --> 51:02.360
cierto que aparece la palabra W en el contexto de la palabra C en el contexto de la palabra W

51:02.360 --> 51:07.560
eso sería P de más WS pero a su vez tengo que tener una predicción negativa o sea yo tengo que

51:07.560 --> 51:12.320
saber cuáles son los ejemplos positivos y cuáles son los ejemplos negativos entonces

51:13.960 --> 51:18.920
lo que se hace para esto es decir bueno yo tengo un gran corpus una gran colección de palabras y

51:18.920 --> 51:25.440
yo puedo medir puedo llegar a medir cuáles son los contextos donde aparece la palabra C en el

51:25.440 --> 51:31.000
contexto de la palabra W pero además puedo llegar a medir los casos en los cuales no pasa o sea

51:31.000 --> 51:35.800
yo puedo soltear palabra solatoria si decir bueno una palabra aleatoria no siempre están en

51:35.800 --> 51:40.960
contexto de una palabra W entonces con eso me invento ejemplos negativos tengo ejemplos

51:40.960 --> 51:46.760
positivos que son bueno la palabra queso aparece en el contexto de la palabra burbesa ejemplos

51:46.760 --> 51:51.880
negativos son sortes de una palabra cualquiera y salió yo que se árbol bueno la palabra árbol

51:51.880 --> 52:01.440
no aparece en el contexto de la palabra burbesa bien entonces el algoritmo skip gran que es uno

52:01.440 --> 52:08.880
de los algoritmos de portubec más más utilizados utiliza este ese principio y lo ve como una red

52:08.880 --> 52:14.320
neuronal intenta modelar esto como una renebronal en la cual yo tengo una capa de entrada y la capa

52:14.320 --> 52:18.760
de entrada va a ser una representación one hot esto lo mencionamos la de pasada la representación

52:18.760 --> 52:25.840
one hot es así no en la representación one hot yo voy a tener un vector para la palabra queso y un

52:25.840 --> 52:36.240
vector para la palabra hamburguesa donde voy a tener una columna para cada una de las palabras

52:36.240 --> 52:42.280
posibles entonces voy a tener acá perro acá voy a tener comer acá voy a tener árbol

52:42.280 --> 52:48.000
tan tata y acá va a estar queso en angulado y acá va a estar hamburguesa en otro lado y acá

52:48.000 --> 52:53.760
va a dar más cosas y entonces la representación de la palabra queso es cero en todos lados

52:54.720 --> 53:01.640
y un uno acá y cero en todo el resto la palabra burbesa es cero en todos lados cero acá y

53:01.640 --> 53:07.080
un uno en la burbesa y cero en todo el resto esto es la representación one hot entonces esta red

53:07.080 --> 53:12.320
neuronal en realidad digamos es una renebronal que intenta predecir este problema probabilístico

53:12.320 --> 53:19.280
toma como entrada ese vector de cero y uno es el vector one hot donde la entrada es todo el vocabulario

53:19.280 --> 53:26.000
posible tiene una capa oculta en el medio es una red que tiene una sola capa oculta y como salida

53:26.000 --> 53:32.000
tiene una distribución de probabilidades de todas las palabras en contexto entonces la entrada

53:32.000 --> 53:40.040
es supongamos que esto tiene tamaño 10 mil no tengo 10 mil palabras posibles 10 mil palabras en

53:40.040 --> 53:51.440
el vocabulario entonces la entrada de la red va a ser una cosa de tamaño 10 mil entrada tiene

53:51.440 --> 54:02.320
tamaño 10 mil y la salida va a tener c por 10 mil c es cuántas palabras de contexto estoy contando

54:02.320 --> 54:08.120
o sea si yo estoy contando no sé 10 palabras alrededor de la que estoy mirando entonces va a ser

54:08.120 --> 54:14.720
una salida c por 10 mil esto c por 10 mil representan cuál es la probabilidad de que una palabra

54:14.720 --> 54:20.120
cualquiera por ejemplo hamburguesa esté en un contexto de tres palabras para atrás de la palabra

54:20.120 --> 54:24.680
queso cuál es la probabilidad que la palabra perro esté en un contexto de dos palabras para

54:24.680 --> 54:30.600
adelante la palabra queso y así eso es las c por 10 mil salidas y en el medio tiene una capa

54:30.600 --> 54:47.800
que ahí dice en edim la capa oculta que tiene tamaño 10 mil por dim y dim es la dimensión

54:47.800 --> 54:52.600
de los vectores que eso que les decía que podía hacer dimensión 100 o dimensión 300 o dimensión

54:52.600 --> 55:00.480
150 es un número mucho más chico que vocabulario entonces pensemólo como esto la tano mientras

55:00.480 --> 55:06.520
es un vector o un hot que tiene un uno y un montón de ceros y después lo paso por una

55:06.520 --> 55:13.040
matriz de pesos que tiene este tamaño 10 mil por por ejemplo 300 10 mil por 300 entonces al

55:13.040 --> 55:19.840
multiplicar eso por mi vector acá esto me devuelve una sola fila de esa matriz que tiene dimensión

55:19.840 --> 55:28.320
300 y eso se lo voy a pasar a la función de activación a su vez eso tiene como una especie

55:28.320 --> 55:33.480
de segunda capa en la cual aparece más peso para poder calcular estas salidas pero en realidad al

55:33.480 --> 55:38.000
método después de que se entrena con un montón de valores positivos un montón de valores negativos

55:38.000 --> 55:42.040
dice bueno que eso parece en el contexto de muruesas pero perro no aparece en el contexto de

55:42.040 --> 55:47.800
la muruesa etcétera tengo un montón de valores de este estilo cuando termina de entrenar y

55:47.800 --> 55:52.640
se bueno llegué al mejor cálculo de probabilidades en realidad yo tiro todo el resto de las capas y me

55:52.640 --> 55:59.160
quedo solamente con esta de acá con la capa oculta la capa oculta es una tabla que me dice para

55:59.160 --> 56:04.880
cada una de las palabras hay 300 valores reales que le representan bien entonces me dice bueno para

56:04.880 --> 56:09.920
la palabra que eso esto 300 valores van a ser menos uno tres con cuatro ocho con seis no sé qué

56:09.920 --> 56:14.800
está todo así 300 valores y para las palabras muruesa menos dos tres con uno etcétera etcétera

56:14.800 --> 56:19.160
o sea voy a tener un montón de valores reales que le representan que representan esos números

56:19.160 --> 56:25.600
no lo sé y nadie lo sabe pero sabemos que ahí está codificada la información importante para

56:25.600 --> 56:33.440
poder después trabajar con esos números con esos con esas palabras bien hay eso se le llama

56:33.440 --> 56:40.200
Word in badings esta capa oculta que está acá en esta en esta técnica de Wordback se le llama

56:40.200 --> 56:44.800
capa de badings a la capa oculta que queda entrenada después de esto bien preguntas

56:49.160 --> 56:57.200
ustedes que palabra que esto sí es para ir a la capa oculta y por el producto porque la

56:57.200 --> 57:02.760
matriz doble vez una matriz de 10 mil por dimensión y mi vector one hot es un vector que tiene

57:02.760 --> 57:07.040
tamaño de 10 mil pero hay un solo uno no son todos heros y uno entonces al hacer el producto me

57:07.040 --> 57:12.640
queda exclusivamente la fila que representa la matriz la palabra que eso

57:13.640 --> 57:22.440
bien entonces con esto se logra con esta técnica Wordback y otras técnicas de construcción

57:22.440 --> 57:33.800
de Word in badings sí no el resultado de la capa oculta se lo pasas en esta técnica por lo

57:33.800 --> 57:38.840
menos le pasas el resultado de la capa oculta a otros pesos que van a ir a la salida y esos pesos

57:38.840 --> 57:43.720
son los que calculan la probabilidad de salida pero en realidad después todos esos pesos que aparecen

57:43.720 --> 57:48.240
después no me importa o sea después de que yo termino de entrenar todo la única capa con la

57:48.240 --> 57:51.600
que me voy a quedar es con la del medio que es la que me he interesado entrenar el resto es

57:51.600 --> 57:56.680
como una especie de excusa que se usa para esta tarea para poder encontrar la capa del medio

57:59.760 --> 58:05.920
la salida tiene c por 10 mil que significa yo estoy prediciendo cuál es la probabilidad en todas las

58:05.920 --> 58:08.120
seis palabras de contexto de caparesca alguna palabra

58:12.960 --> 58:18.560
bien entonces les hicimos logramos nuestro objetivo que era decir que hago que puedo

58:18.560 --> 58:25.200
asociar a una palabra a un string un vector de valores reales entonces tengo la palabra perro y

58:25.200 --> 58:31.240
me va a dar un vector de valores reales la palabra comer y me va a dar otro vector de valores reales

58:32.200 --> 58:38.920
además se cumple que los vectores cuanto más cercanos están en ese espacio de dimensión

58:38.920 --> 58:44.000
300 entonces significa las palabras son más similares en algún sentido o si están más

58:44.000 --> 58:51.240
lejanos entonces son más disímiles puedo utilizar por ejemplo la similiaridad coseno para

58:51.240 --> 58:54.520
eso si yo calculo el coseno del ángulo del otro lado vectores eso es una buena medida para

58:54.520 --> 58:58.000
saber qué están parecidos son o incluso para usar la distancia o clídea también para

58:58.000 --> 59:04.520
calcular eso pero la similiaridad coseno es la que más se usa y además de que tiene esa

59:04.520 --> 59:10.960
propiedad de que las palabras más cercanas son más parecidas de alguna manera estas técnicas

59:10.960 --> 59:16.360
descubren cosas interesantes que uno no las entrenó para que las descubran digamos sino que aparecen

59:16.360 --> 59:21.280
como de yapa y aparecen cosas como que por ejemplo yo puedo hacer operaciones entre los vectores

59:21.280 --> 59:25.440
entonces si yo tengo el vector de rey y le resto el vector de hombre y le sumo el vector de

59:25.440 --> 59:29.920
mujer me queda el vector de rey y eso es una propiedad que aparece después de que yo

59:29.920 --> 59:36.080
entren estos vectores suele suceder en alendenar estas colecciones de vectores que agarro el vector de

59:36.080 --> 59:42.960
mujer le resto de hombre y le sumo rey y me queda rey o agarro el vector de Uruguay le resto

59:42.960 --> 59:48.280
donde veo le sumo Francia me da París no entonces ahí en un caso estoy haciendo una transformación

59:48.280 --> 59:53.520
un poco morfológica decir bueno este hombre es la mujer como rey esa rey y el otro estoy

59:53.520 --> 59:57.480
haciendo una transformación más semántica como diciendo la capital de Uruguay y la capital

59:57.480 --> 01:00:02.280
de Francia es París y de alguna forma yo nunca le dije al sistema que tiene que aprender eso pero

01:00:02.280 --> 01:00:09.200
por la forma que hayan creado los vectores suelen tener propiedad de este estilo bien eso fue como

01:00:09.200 --> 01:00:14.880
lo lo primero sorprendente que encontraron acerca de estos métodos que es que se pueden como que

01:00:14.880 --> 01:00:19.000
derrebo de aprender esas cosas pero no están excesos del problema como por ejemplo si yo tengo

01:00:19.000 --> 01:00:24.080
una palabra la palabra vela voy a tener un solo vector que representa la palabra vela y vela

01:00:24.080 --> 01:00:29.360
es una palabra que es amigo o sea es polisémica yo puedo tener una vela para aprender una vela

01:00:31.360 --> 01:00:36.080
vamos para poner una vela de cumpleaños o sea una pagón o puedo tener un barco a vela y bueno

01:00:36.080 --> 01:00:41.120
en los dos casos tengo la misma representación o el gato hidráulico y el gato animal también tengo

01:00:41.120 --> 01:00:45.640
la misma representación el banco de sentarse y el banco de financiero también tengo la misma

01:00:45.640 --> 01:00:50.120
representación etcétera entonces eso es un problema que tienen estos estas técnicas y es que yo

01:00:50.120 --> 01:00:54.240
no tengo digamos no estoy usando por ejemplo wordnet que vieron guarnas en una acción

01:00:54.240 --> 01:01:00.520
es clase no no tengo un repositorio significado de wordnet que me ayude a decir cuál es cuál sino

01:01:00.520 --> 01:01:08.720
que acá solamente tengo un representante para cada palabra bien y bueno esta esta técnica tiene

01:01:08.720 --> 01:01:12.680
ese problema después hay otras tecnicas me permiten crear vectores contextuales que día bueno

01:01:12.680 --> 01:01:19.120
es la palabra gato en esta oración donde probablemente sea un gato animal y no un gato hidráulico

01:01:22.120 --> 01:01:29.080
bien entonces una vez que construimos esta colección de vectores como los evaluamos cómo sabemos

01:01:29.080 --> 01:01:35.280
están bien bueno hay como dos formas de evaluarlos bastante comunes se habla de test intrínsecos y

01:01:35.280 --> 01:01:40.600
test en extrínsecos que significan cosas distintas intrínsecos significa yo mido propiedades del

01:01:40.600 --> 01:01:47.120
conjunto de vectores que construí entonces una de las que se miden es exactamente lo que decía

01:01:47.120 --> 01:01:53.600
no recién me diamos que aparece una propiedad que es que yo puedo hacer dibujar como en

01:01:53.600 --> 01:01:58.720
especie para el logramos en el cual digo que hombre es a mujer como rey esa y espero que

01:01:58.720 --> 01:02:05.800
es mi colección de vectores haya quedado reina digamos como resultado de su operación o Uruguay

01:02:05.800 --> 01:02:11.560
esa montevideo como francia esa y espero que haya quedado parís en ese lugar entonces bueno una

01:02:11.560 --> 01:02:17.160
forma de valor estos estos sistemas es construirme una colección grande de estos test se llaman test de

01:02:17.160 --> 01:02:24.160
analogías entonces me puedo hacer una colección grande estos test y ver a cuántos le moca mi colección

01:02:24.160 --> 01:02:27.960
entonces con yo tengo varias colecciones en vez de distintas veo que este le invoco más veces y

01:02:27.960 --> 01:02:35.640
el invoco menos veces otro son los test de similitud o similaridad que esto se hacen con

01:02:35.640 --> 01:02:40.120
intervención humana un poco más fuerte que es preguntar un montón de personas por ejemplo

01:02:40.120 --> 01:02:46.920
que es más parecido a un durasno una silla una mesa o una manzana a un avestrus o cosas de

01:02:46.920 --> 01:02:52.360
estilo entonces tal le dicen a la gente trata de arranquear estas cuatro cinco palabras de cuál es

01:02:52.360 --> 01:02:56.200
más parecida menos parecida entonces le preguntar un montón de personas las personas hacen sus

01:02:56.200 --> 01:03:01.120
listas y después mirás dentro de tu colección de vectores si las distancias relativas entre esas

01:03:01.120 --> 01:03:05.880
palabras son similares o no a la que esperaban los humanos entonces cuanto más similar sea haciendo

01:03:05.880 --> 01:03:11.280
el el test de Spirman para eso el descorrelación de Spirman se puede sacar una medida de qué tanto

01:03:11.280 --> 01:03:17.080
se parece a la intuición humana lo que el sistema dice eso es llamante es intrínseco porque yo

01:03:17.080 --> 01:03:22.520
estoy abarrando la colección de vectores que construí y las estoy testeando sola los test

01:03:22.520 --> 01:03:28.920
extrínsecos se refieren a agarro mi colección de vectores y la meto en una tarea de p l n un

01:03:28.920 --> 01:03:34.640
poco más grande y veo qué tal le va entonces acá significa bueno yo supongamos que tengo un

01:03:34.640 --> 01:03:40.680
sistema de p l n que hace traducción automática o análisis de sentimiento o recuperación de información

01:03:40.680 --> 01:03:47.640
o un chatbot o lo que sea si yo tengo un sistema que ya funciona y le cambio su capa de

01:03:47.800 --> 01:03:52.360
su colección de vectores por la mía que yo entrené y el sistema mejora en su performance entonces

01:03:52.360 --> 01:03:57.400
digo que puedo decir que mi colección de vectores mejora la performance entonces puedo decir que la

01:03:57.400 --> 01:04:02.040
colección de vectores es buena eso de llamas test extrínseco o sea no estoy probando directamente

01:04:02.040 --> 01:04:06.280
las propiedades de los en vectores sino que estoy probando cómo se comportan en un sistema más grande

01:04:10.760 --> 01:04:16.080
bien entonces otra forma de evaluar esto más bien no creo que lleguen a ver nada porque está

01:04:16.080 --> 01:04:20.960
muy chiquito pero bueno vamos a mencionarlo es visualizarlos en bedings recuerden que esto tenía

01:04:20.960 --> 01:04:28.840
dimensión 100 350 que era una dimensión mucho más chica que el vocabulario pero igual es una

01:04:28.840 --> 01:04:32.760
dimensión muy grande o sea lo sumano podemos visualizar dos tres dimensiones de lo sumo y más de

01:04:32.760 --> 01:04:38.440
eso ya nos mareamos y estos son vectores de 300 dimensiones pero una forma de visualizarlos es

01:04:38.440 --> 01:04:44.000
usar las técnicas de reducción de dimensionalidad por ejemplo p c a y t s n s son de las más comunes

01:04:45.000 --> 01:04:49.160
son técnicas que me permiten agarrar 300 dimensiones y bajarlas a dos para poder dibujarlo en un

01:04:49.160 --> 01:04:53.440
plano entonces acá no llegan a ver estos son dos trabajos que hicimos en el grupo para distintos

01:04:53.440 --> 01:05:00.840
colecciones en bedings en distintos idiomas voy a agarrar esto ahora sí sí queda bien entonces en

01:05:00.840 --> 01:05:06.200
este tenemos un trabajo hecho para el español son vectores de palabras en español y

01:05:06.200 --> 01:05:10.200
están no van a llegar a verlo lo que están acá porque se ve muy chiquito pero por ejemplo acá

01:05:10.200 --> 01:05:16.080
aparece un cláster de años que están todos juntos acá aparecen nombres de personas que

01:05:16.080 --> 01:05:22.720
están todos juntos abajo aparecen lugares perú, Uruguay, Bolivia que aparecen como clásterizados

01:05:22.720 --> 01:05:27.040
todos juntos entonces uno espera que una colección de vectores que haya quedado bien entrenada aparezcan

01:05:27.040 --> 01:05:32.200
como clásters con cosas que son semánticamente similares y el trabajo de la derecha es un trabajo

01:05:32.200 --> 01:05:36.880
similar pero que está hecho para el Guarani y bueno acá se ve también más claro que aparecen

01:05:36.880 --> 01:05:42.760
cosas como relacionadas con fechas están en héroes las relacionadas con colores están en

01:05:42.760 --> 01:05:52.960
en cian las relacionadas con no se bien que dice ahí animales están en verde etcétera países

01:05:52.960 --> 01:05:57.920
están en azul etcétera como que no puede estar en esas regiones obviamente esto no es perfecto

01:05:57.920 --> 01:06:03.280
van a quedar alguna cosa por fuera etcétera pero si uno logra ver que más o menos clásteriza

01:06:03.280 --> 01:06:10.920
entonces tiene como cierta cierta entuición de que andan mejor esos vectores bien preguntas

01:06:13.480 --> 01:06:19.760
entonces los guerden beings fueron en definitiva una de las primeras revoluciones que ocurrieron los

01:06:19.760 --> 01:06:26.320
últimos años en lo cual es pln y posible que después se empezaron a utilizar arquitecturas de

01:06:26.320 --> 01:06:30.760
redes más complejas o sea gracias a que tenemos en bedings y decimos puedo representar una

01:06:30.760 --> 01:06:35.000
palabra como un vector de 30 dimensiones ese vector de 30 dimensiones que son números reales

01:06:35.000 --> 01:06:40.160
se lo pueden chufar como entrada a una red neuronal y puedo obtener cosas más complicadas a mí me

01:06:40.160 --> 01:06:46.680
interesaba de hace un rato dijimos tener representaciones de palabras pero además de oraciones o de

01:06:46.680 --> 01:06:52.480
tweets o de documentos enteros y bueno por lo menos yo tengo representación de palabras usando

01:06:52.480 --> 01:06:58.000
guerden bedings como que eso está bastante bien resuelto y gracias a que ahora tengo guerden

01:06:58.000 --> 01:07:02.480
bedings para usar arquitecturas más complejas como las redes convolucionales las redes

01:07:02.480 --> 01:07:08.680
lstm las redes tipo transformers los transformers son lo que más se utiliza bien día pero además

01:07:08.680 --> 01:07:12.760
puedo hacer otra cosa con los en bedings algo un poco más simple pero que a su vez me sirve para

01:07:14.360 --> 01:07:17.360
resolver otras problemas y es usar la técnica de centróide

01:07:18.840 --> 01:07:23.160
que es así esta les va a servir en la tarea salvo que quieran entrenar una red más compleja que

01:07:23.160 --> 01:07:28.800
también son bienvenidos si quieren entrenar una lstm un transformer pero el centróide es una

01:07:28.800 --> 01:07:34.200
técnica es muy sencilla supongamos que yo tengo mi mi capa en bedings que tiene bueno dice quesos

01:07:34.200 --> 01:07:40.040
representas y hamburguesas representas y perro es así gato es así etcétera tengo

01:07:40.040 --> 01:07:46.200
vectores para cada palabra y tengo ahora un tweet que le quiero representar utilizando la

01:07:46.200 --> 01:07:50.560
colección en bedings yo simplemente puedo agarrar todas las palabras del tweet buscar todos los

01:07:50.560 --> 01:07:55.640
vectores correspondientes y hacer el promedio a eso se llama hacer un centróide de todos los

01:07:55.640 --> 01:08:01.720
en bedings del tweet y uno dice está apreciado el promedio de perro gato no o sea el tweet dice

01:08:01.720 --> 01:08:07.880
este no me gustó la película se hago el promedio no me gustó la película y aún promedio todos

01:08:07.880 --> 01:08:14.160
en bedin mediar papa frita pero sin embargo funciona bastante bien es como un poco antintuitivo pero

01:08:14.160 --> 01:08:19.400
hacer el promedio de todas esas 300 dimensiones de las distintas palabras después yo utilizo eso

01:08:19.400 --> 01:08:23.880
como entrada para otro otro sistema de clasificación no sólo una rena urnal sino que ahí ya

01:08:23.880 --> 01:08:28.520
puede utilizar otro tipo de cosas como su proyecto en machines o regresión logística y

01:08:28.520 --> 01:08:32.840
anda bastante bien o sea es como extraño pero sobre todo para el problema de análisis de

01:08:32.840 --> 01:08:38.160
sentimiento anda bastante bien bueno esa es la técnica del centróide es una técnica fácil de decir

01:08:38.160 --> 01:08:43.920
si yo tengo una colección en bedings puedo hacerme en bedings de oraciones o en bedings de textos

01:08:43.920 --> 01:08:51.480
un poco más grandes simplemente promediendo los en bedings que tengo bien entonces ahora lo que

01:08:51.480 --> 01:08:57.880
vamos a ver en el resto de la clase en unos minutos son ejemplos de cómo funcionan estas

01:08:57.880 --> 01:09:02.280
arquitecturas más complejas que puede utilizar gracias a que tengo en bedings no las vamos a

01:09:02.280 --> 01:09:06.320
ver en profundidad sino que simplemente vamos a pasar por arriba pero es una idea para ver qué clase

01:09:06.320 --> 01:09:14.640
de cosas se pueden hacer y empecemos por las convolutivas las redes tipo CNN se llaman

01:09:14.640 --> 01:09:20.800
redes convolutivas o convolucionales y originalmente se utilizaban como para procesar imágenes o sea

01:09:20.800 --> 01:09:26.520
también se utilizan todavía en día para procesar imágenes y lo que hacen es ir recorriendo como que

01:09:26.520 --> 01:09:31.120
se aumenta una imagen en cuadraditos y lo van recorriendo digamos y después obtienen como

01:09:31.120 --> 01:09:37.960
información de cada uno de los cuadraditos bueno pero también se han aplicado al lenguaje y la

01:09:37.960 --> 01:09:42.000
forma que se aplica el lenguaje es como decir va tomando de enegramos y va viendo yo que se

01:09:42.000 --> 01:09:47.080
por ejemplo tres palabras a la vez y va obteniendo datos de cada una de las tres palabras a la vez y

01:09:47.080 --> 01:09:55.240
después con eso después saca un total entonces lo interesante es que digamos puedo pasar a tener

01:09:55.240 --> 01:09:59.000
cosas de orden más grande que una palabra o sea ahora en bedes para usar una sola palabra estoy

01:09:59.000 --> 01:10:04.520
produciendo toda una variación entonces tenias una pregunta bien entonces un ejemplo

01:10:04.520 --> 01:10:09.360
como funciona esto supongamos que estoy tratando de clasificar tweets y digo la película fue

01:10:09.360 --> 01:10:15.360
muy aburrida yo me puedo construir una red neuronal de tipo convolutiva que lo que va a ser es decir

01:10:15.360 --> 01:10:21.440
bueno a los embeddings de la de a tres palabras los voy tomando de a tres palabras considero

01:10:21.440 --> 01:10:26.880
los embeddings de la película fue y a esos tres embeddings se los paso a una red a esa unidad

01:10:26.880 --> 01:10:32.240
convolutiva que lo que va a ser es mirar estas tres palabras y tratar de sacar información de

01:10:32.240 --> 01:10:37.640
las tres y devolverme una cosa que tenga cierto tamaño fijo y después se va a mover la ventana y en

01:10:37.640 --> 01:10:42.240
vez de la película fue va a considerar las palabras películas fue muy y de vuelta lo va a pasar por

01:10:42.240 --> 01:10:46.560
esa subred y va tratar de sacar salidas y después fue muy aburrida lo va a pasar por la misma subred

01:10:46.560 --> 01:10:52.160
tratar de sacar salidas después voy a tener una capa que dice bueno de todas estas salidas

01:10:52.160 --> 01:10:58.440
intermedias que tuve obtengo los máximos y esos máximos los uso para calcular mi salida que

01:10:58.440 --> 01:11:04.760
mi salida final sería positivo o negativo en el otro o no no estas redes esta capa convolutiva

01:11:04.760 --> 01:11:09.440
que ahí en el medio parece como capa convolutiva es entonces a sus redes que estoy viendo ahí en

01:11:09.440 --> 01:11:13.840
realidad son los mismos pesos no es como la misma que se va moviendo y me va dando resultados distintos

01:11:14.720 --> 01:11:19.920
bien entonces lo bueno que tiene es que yo agarró todo una entrada que son muchas palabras y me

01:11:19.920 --> 01:11:25.520
va a dar una salida única digamos condensa todas las palabras se queda como con las digamos las

01:11:25.520 --> 01:11:30.840
dimensiones máximas de cada una que le quede más le interesen y con eso calcula una salida bien esa es

01:11:30.840 --> 01:11:39.320
la red tipo convolutiva las redes el STM pertenecen a un grupo más grande de redes que se llama

01:11:39.320 --> 01:11:44.760
las redes recurrentes que significa son redes con memoria que van mirando una cada palabra a la

01:11:44.760 --> 01:11:49.200
vez y van recordando lo que viene hasta el momento entonces esto me sirve para obtener una salida final

01:11:49.200 --> 01:11:53.720
o también para obtener salidas por palabras entonces vamos a ver cómo funciona de estas

01:11:55.320 --> 01:12:01.080
esto como una especie de diagrama de cómo sería una una red recurrente similar a la que veíamos

01:12:01.080 --> 01:12:06.200
hace un rato digamos en capas pero ahora yo voy a tener una capa que tiene un enlace ese asimismo

01:12:06.200 --> 01:12:10.800
digamos todas las neuronas de esa capa van a tener un enlace de vuelto de vuelta a ese asimismo se

01:12:10.800 --> 01:12:15.840
llama capa recurrente y bueno después voy a tener una capa de salida entonces cuando yo voy a

01:12:15.840 --> 01:12:20.920
ver cómo funciona eso con un tweet que quiero clasificar como la película fue muy aburrida funcionaría

01:12:20.920 --> 01:12:25.260
esta manera yo digo bueno primero agarró la palabra a la el embedding de la palabra a la se lo

01:12:25.260 --> 01:12:31.560
paso a la red y después voy a agarrar el embedding de la palabra película y se lo paso de vuelta a la

01:12:31.560 --> 01:12:36.640
red pero esta vez además de poner el embedding de la palabra película voy a poner también la salida

01:12:36.640 --> 01:12:43.640
del paso anterior entonces esto va consumiendo una palabra a la vez y siempre consumiendo la salida de

01:12:43.640 --> 01:12:50.680
la etapa anterior entonces va consumiendo la película fue muy aburrida cuando llegó aburrida ya

01:12:50.680 --> 01:12:56.800
consumió la salida de todas las capas anteriores y la palabra nueva y ahí es como que la salida de

01:12:56.800 --> 01:13:00.360
ese último paso ya medio tiene como una especie de versión condensada de todo lo que era la

01:13:00.360 --> 01:13:06.080
elaboración y ahí con esos últimos pesos calculo la salida positivo, negativo, neutro o no

01:13:07.080 --> 01:13:14.720
además si yo quisiera podría ir sacando para ir sacando los pesos de cada una de las salidas

01:13:14.720 --> 01:13:18.760
entonces ahí tendría como una salida por palabra entonces esto podría servir por ejemplo para

01:13:18.760 --> 01:13:23.120
los problemas de clasificación de secuencia que veíamos la despasada bueno con una red de estetilo se

01:13:23.120 --> 01:13:26.720
puede hacer clasificación de secuencia sacando una salida por palabra sí tenía una pregunta

01:13:29.720 --> 01:13:33.720
el embedding exacto sí sí la entrada en esto caso yo digo bueno asumo que tengo

01:13:33.720 --> 01:13:43.400
bordemente yo ya puedo utilizar estas redes más complejas bien y la que es la arquitectura del

01:13:43.400 --> 01:13:48.640
estado del arte hoy en día es la arquitectura de tipo transformer que también es una arquitectura que

01:13:48.640 --> 01:13:52.280
utiliza secuencias de entrada pero es una arquitectura bastante más compleja acá vamos a

01:13:52.280 --> 01:13:56.480
ver solamente una idea muy muy básica como funciona pero es una arquitectura que tiene con muchos

01:13:56.800 --> 01:14:03.440
muchos pedazos y hace muchas cosas distintas y bueno se basa en una cosa que se llama capas

01:14:03.440 --> 01:14:07.760
autotensionales ahora no vamos a ver qué es el modelo attentional pero lo vamos a ver la clase que

01:14:07.760 --> 01:14:14.240
viene normalmente como bueno un ejemplo de cómo funciona el sistema de traducción automática

01:14:14.240 --> 01:14:20.000
que utiliza modelos autotensionales bueno una variante de eso es el modelo autotensional que

01:14:20.000 --> 01:14:25.520
lo que hace es construir una matriz entre las palabras de una oración y sí misma yo tengo una

01:14:25.520 --> 01:14:30.800
oración que tiene en palabras y va a tratar de cruzar las n palabras con las propias n palabras

01:14:30.800 --> 01:14:35.120
y tratar de establecer conexiones entre cada uno de los pares entonces termina calculando una

01:14:35.120 --> 01:14:40.880
matriz y lo bueno que tiene es que me permite construir en beding contextuales por palabra o

01:14:40.880 --> 01:14:45.440
sea en bedings de una palabra vista en contexto y además un embeding total de la oración entonces

01:14:45.440 --> 01:14:49.480
funciona más o menos así esto como una especie de representación muy vaga de lo que es un

01:14:49.480 --> 01:14:54.840
transformer no o sea de forma de realidad tiene como muchas partes más complejas pero imagínense

01:14:54.840 --> 01:14:59.640
que funciona esta manera no yo digo tengo una oración en la película fue muy aburrida entonces

01:14:59.640 --> 01:15:05.520
la voy a pasar por una capa autotensional que significa yo cruzo todas las palabras con todas y

01:15:05.520 --> 01:15:12.080
calculo la relevancia de cada palabra contra las demás eso me va a dar una serie de salidas y eso

01:15:12.080 --> 01:15:16.680
lo que hace es construirme como una colección de en bedings de nivel 1 o sea yo empecé con los

01:15:16.680 --> 01:15:22.080
borde en bedings de la película fue una fue muy aburrida y ahora voy a tener una colección de

01:15:22.080 --> 01:15:27.480
en beding de nivel 1 que ya mirando algo de contexto eso eso es en beding de nivel 1 a su vez

01:15:27.480 --> 01:15:32.200
de los pasos de vuelta a otra capa autotensional que devuelta los cruzos a todos con todos y me

01:15:32.200 --> 01:15:37.320
devuelta a dar una salida que son los en beding de nivel 2 y eso lo sigo pasando por varias capas

01:15:37.320 --> 01:15:42.360
autotensionales que los cruzan todos con todos hasta que al final me terminan dando o sea lo voy

01:15:42.360 --> 01:15:49.240
a pasar en el capas y me terminan dando una salida de nivel 5 digamos entonces al inicio tenía

01:15:49.240 --> 01:15:54.480
borde en bedings que miraban solamente una palabra a la vez y lo que tengo al final ya son como

01:15:54.480 --> 01:15:58.760
en bedings contextuales en los cuales ya considero varias veces cruzar todas las palabras con todas

01:15:58.760 --> 01:16:06.000
entonces como que eso va ganando información en cada paso a su vez a bien después de que yo

01:16:06.000 --> 01:16:10.880
tengo estos en beding contextuales en general se utiliza otra red más de tipo de coder puede ser

01:16:10.880 --> 01:16:15.360
un transforme puede ser una lstm algo más pero necesito otra cosa que es la que me diga por

01:16:15.360 --> 01:16:20.120
ejemplo si es positivo negativa o neutro etcétera pero es otro tipo de red que después decodifica

01:16:20.120 --> 01:16:25.440
esa información pero bueno por lo menos atacayo ya construí en bedings de cosas pero bien lo que

01:16:25.440 --> 01:16:31.520
tengo acá son tenía la película fue muy aburrida y eso lo transformé en tenía cinco palabras y lo

01:16:31.520 --> 01:16:36.120
transformé en cinco en bedings digamos que de distintos niveles pero siempre son cinco en bedings

01:16:37.120 --> 01:16:41.600
entonces yo diría que el primero se corresponde con la el segundo con película tercero con fue

01:16:42.160 --> 01:16:47.120
es una una versión contextual del en beding porque significa la palabra película en el contexto de

01:16:47.120 --> 01:16:51.600
la película fue muy aburrida no es la palabra película en general entonces si yo tuviera una

01:16:51.600 --> 01:16:56.480
bración que tiene gato sería gato en el contexto de el gato como pescado que no sería lo mismo que

01:16:56.480 --> 01:16:59.600
cuando estoy hablando un gato y un gato y un gato y un gato probablemente o sea los en bedings

01:16:59.600 --> 01:17:06.680
que de distintos bien pero además me interesa tener una representación de la oración entera y

01:17:06.680 --> 01:17:11.480
para eso lo que se hace es agregar un toque en extra un toque en llamado celse se pone al

01:17:11.480 --> 01:17:17.440
principio de la oración y se lo hace jugar con todos los las capas atenciónales del medio entonces

01:17:17.440 --> 01:17:22.640
yo tengo una palabra extra que como no es una palabra de la oración no tiene un en beding contextual

01:17:22.640 --> 01:17:26.920
sino lo que hace es capturar la información de toda la oración a la vez entonces es en beding que

01:17:26.920 --> 01:17:32.680
me queda afuera el en beding que corresponde al toque en celse ese que después yo puedo utilizar

01:17:32.680 --> 01:17:38.160
para predecir cosas yo lo utilizo como un en beding que tiene cierto tamaño y se lo paso una

01:17:38.160 --> 01:17:46.960
capa de softmax para que me prediga así esa es positiva negativo a neutra o no bien bueno y para

01:17:46.960 --> 01:17:52.320
terminar terminar comentarles lo el tipo de herramientas que pueden utilizar para trabajar con

01:17:52.320 --> 01:17:57.200
redes neuronales obviamente para el segundo laboratorio van a poder utilizar redes neuronales si

01:17:57.520 --> 01:18:03.360
quieren de todo tipo si quieren colecciones en bedings nosotros también les podemos dar o pueden

01:18:03.360 --> 01:18:07.640
bajar algunas que estén disponibles en la web pero bueno herramientas habituales para trabajar con

01:18:07.640 --> 01:18:12.560
esto son por ejemplo tensorflow y pator que son dos ilotecas el tensorflow de google y pator es

01:18:12.560 --> 01:18:18.200
de meta o de facebook y bueno queras en general trabaja contar sonflog y hanginfaces es un

01:18:18.200 --> 01:18:21.960
repositorio que tiene un montón de modelos ya aprendrenados para muchos idiomas y para muchas

01:18:21.960 --> 01:18:28.160
cosas que ya se pueden utilizar autos de box y funciona muy bien y bueno estas son estas

01:18:28.160 --> 01:18:33.840
herramientas y otras más las van a poder utilizar en laboratorio bueno por hoy eso la

01:18:33.840 --> 01:18:35.640
próxima vez vamos a ver producción automática

