{"text": " En la clase de hoy, vamos a ver un tema nuevo que es el de los modelos de lenguaje. Si se acuerdan a la clase pasada, vimos dos temas que eran bastante diferentes. El de los traductores para resolver el tema de la morfolog\u00eda de estado finito, unos artefactos de estado finito que permiten resolver temas a trav\u00e9s de un m\u00e9todo de reglas. Y de esa forma resuelvo el tema de convertir de la palabra a su an\u00e1lisis y viceversa. En la segunda parte vimos un m\u00e9todo que era bastante diferente de su concepci\u00f3n, que es su m\u00e9todo estad\u00edstico, que lo que hace era aplicando el modelo del canal ruidoso, aproximarse al problema de corregir errores ortogr\u00e1ficos. Cuando yo hablo de un modelo probabilista, lo que estoy diciendo es que adem\u00e1s de, por ejemplo, clasificar o sugerir una soluci\u00f3n, lo que hace es asignarle probabilidades a las posibles respuestas. Un m\u00e9todo probabilista, t\u00edpicamente no da una respuesta, sino que devuelve una distribuci\u00f3n de probabilidad. Es decir, si yo tengo varios eventos posibles, una distribuci\u00f3n de probabilidad es un n\u00famero entre 0 y 1 que yo asigno a cada evento posible, de forma que la suma de todos los eventos dan 1, eso es lo que llamamos una distribuci\u00f3n de probabilidad. Entre 0 y 1 son todos, son todos mayores o iguales que 0, menores y iguales que 1, y adem\u00e1s su suma da 1, eso es una distribuci\u00f3n de probabilidad. 0, 5, 0, 25, 0, 25 es una distribuci\u00f3n de probabilidad. Si el evento 1 tiene probabilidad 0, 5, el otro 0, 25, y el otro 0, 25, eso es una distribuci\u00f3n de probabilidad. Si no suma a un 1, no son una distribuci\u00f3n de probabilidad. Y si yo, por ejemplo, tengo un evento que ocurre 10 veces, si por ejemplo hago conteo de frecuencia, por ejemplo, no digo hay un evento 1 que ocurre 10 veces, hay un evento 2 que ocurre 5 y hay un evento 3 que ocurre 5. Eso no es una distribuci\u00f3n de probabilidad, porque esto no est\u00e1 entre 0 y 1, porque no suman 1. \u00bfC\u00f3mo hago yo para convertir esto en una distribuci\u00f3n de probabilidad? Lo que hago es dividir por el total de ocurrencias, \u00bfverdad? Que en este caso es 20 y eso me da la proporci\u00f3n respecto a 1, y eso es siempre una distribuci\u00f3n de probabilidad. Esto se llama normalizar para obtener una probabilidad. Esto ustedes lo van a ver que lo vamos a ver en varias veces. El m\u00e9todo de este correcci\u00f3n utilizaba fuertemente la regla de Valles para modelar la situaci\u00f3n. Hasta ahora hemos hablado en todas las cosas que hemos tratado de palabras aisladas. La morfolog\u00eda estudia, primero hablamos de c\u00f3mo separar las palabras y despu\u00e9s vimos c\u00f3mo analizarla internamente, pero siempre habl\u00e1bamos de palabras aisladas. Ac\u00e1 lo que vamos a empezar a mirar es qu\u00e9 pasa cuando las palabras aparecen juntas. Es decir, nosotros lo que vamos a hablar es de la probabilidad de una secuencia de palabras. \u00bfPor qu\u00e9 esto importa? Porque, como ustedes bien sabr\u00e1n, las palabras en el idioma pa\u00f1\u00f3n las aparecen solas. Y no cualquier palabra sigue a otra palabra. Nosotros tenemos una cantidad de reglas para expresar en el idioma que hace que el orden importe. Es decir, lo que se trata es de ver c\u00f3mo tener en cuenta ese orden nos puede ayudar a otra estaria. Creo que con alg\u00fan ejemplo lo vamos a ver m\u00e1s claro. Primero que nada, vamos a recordar a Chonky, que esto yo lo comentaba en la primera clase, aquello de que Chonky dijo la noci\u00f3n de probabilidad de una oraci\u00f3n es completamente in\u00fatil bajo cualquier interpretaci\u00f3n de este t\u00e9rmino. Y tranc\u00f3 por 20 a\u00f1os, la investigaci\u00f3n hasta ac\u00e1 apareci\u00f3. Chelline, que volvi\u00f3 a revivir el tema de los m\u00e9todos probabilistas o basados en conteos para aproximarse a los problemas de procesamiento en el lenguaje natural. Chonky lo que dec\u00eda esencialmente es cuando nosotros lo hacemos con teo y sacamos conclusi\u00f3n en base a cuenta, en base a n\u00fameros, en base a experiencia, que es t\u00edpicamente lo que vamos a ver en este caso de los enegramas, estamos obteniendo soluciones a problemas, pero no estamos entendiendo qu\u00e9 es lo que est\u00e1 pasando. Y eso es una discusi\u00f3n catalida de hoy s\u00ed. Es decir, hay una famosa discusi\u00f3n por ah\u00ed en internet entre Chonky, esto te hablando hace dos o tres a\u00f1os, o cinco a\u00f1os, entre Chonky y Peter Norby, que discute un poco esto. Es decir, si esto que estamos haciendo ahora y que ha tenido tan buenos resultados desde el punto de vista de reconocimiento del habla y el procesamiento del lenguaje natural es en realidad inteligencia artificial o es solamente number crunching que no nos aporta mucho. Norby lo que le dice es bueno, de hecho la ciencia es siempre m\u00e1s o menos funcionada as\u00ed. Bueno, entonces \u00bfcu\u00e1l es el objetivo de lo que vamos a hablar ac\u00e1? Son de modelos del lenguaje. El objetivo del modelo del lenguaje es calcular la probabilidad de una secuencia de palabra. Es decir, qu\u00e9 tan probable es en mil lenguajes que una secuencia se d\u00e9. \u00bfDe acuerdo? \u00bfPara qu\u00e9 nos puede servir eso? Bueno, imag\u00ednense ustedes que, y ac\u00e1 vamos a recordar otra vez el modelo del canal ruidoso, de la otra vez, imag\u00ednense ustedes que tengo este texto escrito y por medio de un m\u00e9todo que no s\u00e9 cu\u00e1l es. Tengo dos oraciones candidatas, \u00bfde acuerdo? Dos textos candidatos. Uno que es PRNEVA para el curso de PLN y PREVA para el curso de PLN. \u00bfDe acuerdo? Y adem\u00e1s supongamos que el m\u00e9todo que utilic\u00e9 para reconocer la escritura me dice que este es m\u00e1s probable que este. \u00bfCu\u00e1l vamos a elegir? \u00bfCu\u00e1l vamos a elegir? Vamos a elegir el de abajo. \u00bfPor qu\u00e9? \u00bfPor qu\u00e9 esto no es una palabra v\u00e1lida? Pero a\u00fan siendo una palabra v\u00e1lida, o a\u00fan suponiendo que fuera una palabra v\u00e1lida, podr\u00eda darse un caso donde yo identifico una palabra v\u00e1lida, \u00bfse acuerdan lo correcci\u00f3n? A\u00fan as\u00ed, yo podr\u00eda decir, bueno, pero en este lugar, esa palabra no calza, digan. Si de alguna forma yo s\u00e9. Es decir, si yo logro detectar que esta oraci\u00f3n es m\u00e1s probable que esta, de alguna forma, eso me va a ayudar en la tarea de reconocimiento. Lo mismo pasa con el reconocimiento del habla, de lo que hablamos el otro d\u00eda con el speed recognition, y cuando yo hablo y digo una palabra, ustedes me escuchan. Entonces, los modelos de lenguaje sirven para ayudar en este tipo de tarea. T\u00edpicamente los modelos de lenguaje ayudan en otra tarea. Nos agregan mucha informaci\u00f3n. Entonces, cuando nosotros hacemos reconocimiento de escritura, un poco lo que decimos es, \u00bfCu\u00e1l es la probabilidad de la oraci\u00f3n origen dada la observaci\u00f3n que tengo? Yo tengo una observaci\u00f3n, \u00bfs\u00ed? \u00bfCu\u00e1l es la probabilidad de una oraci\u00f3n origen? Es proporcionar a la probabilidad de la observaci\u00f3n dada la oraci\u00f3n por la probabilidad de la oraci\u00f3n. \u00bfY esto qu\u00e9 es? Eso es valles, es la regla de valles. Entonces, nosotros por valles sabemos eso, y como ven, ac\u00e1 aparece la noci\u00f3n de probabilidad de la oraci\u00f3n. Por eso es que nos interesa conocer la probabilidad de la oraci\u00f3n. Ahora, \u00bfC\u00f3mo calculamos la probabilidad de la oraci\u00f3n? Bueno, hay alg\u00fan ejemplo m\u00e1s, \u00bfno? Por ejemplo, en la traducci\u00f3n auton\u00e1tica, en la traducci\u00f3n auton\u00e1tica, si tenemos estos tres candidatos, nuevamente a m\u00ed me va a ayudar conocer el orden o saber cu\u00e1l es la m\u00e1s probable en mi lenguaje. En la correcci\u00f3n de errores, como vimos en la vez pasada, hordas de botero es una secuencia muy de poca probabilidad, y pensemos un poquito, \u00bfPreguntemos no? \u00bfPor qu\u00e9 esta oraci\u00f3n no les parece que sea muy probable? \u00bfQu\u00e9 nos podr\u00eda determinar que esta oraci\u00f3n no es muy probable? \u00bfO esta? \u00bfImplementaci\u00f3n a la educaci\u00f3n ley? \u00bfPor qu\u00e9 podemos suponer que esa no es probable? Bueno, a m\u00ed se me ocurren dos razones principales, dos aproximaciones, una es por la sintaxis, \u00bfno? La sintaxis del idioma pa\u00f1\u00f3n no es as\u00ed. \u00bfNos decimos educaci\u00f3n ley, educaci\u00f3n que...? En la segunda, porque no publicamos la procesi\u00f3n. \u00bfPor qu\u00e9 no qu\u00e9? En la procesi\u00f3n, porque si tenemos sus y de botero, estamos publicando, \u00bfverdad? Ah, bueno, pero ese podr\u00eda ser un sus de un tercero, \u00bfno? Ac\u00e1 seguramente lo que hay es un error tor\u00e1fico de sus hordas de botero. O sea, ac\u00e1 tenemos un tema de sintaxis. Ac\u00e1 no tenemos un tema de sintaxis. Deber\u00edamos conocer un poco de sem\u00e1ntica para asociar botero, que pintaba mujeres gordas y entonces... O una aproximaci\u00f3n un poco m\u00e1s humilde, que es la segunda, es la una aproximaci\u00f3n m\u00e1s detal\u00edstica, porque si nosotros... y que juega con el hecho de que tenemos grandes vol\u00famenes de texto y de ah\u00ed el cambio de los modelos probabil\u00edsticos, es que sus gordas de botero seguramente apareci\u00f3 antes en mis corpus de texto y hordas de botero, no. Es una aproximaci\u00f3n mucho m\u00e1s detal\u00edstica. Eso es lo que vamos a hacer en los modelos de diagrama, justamente. A partir de grandes vol\u00famenes de texto, detectar e calcular las probabilidades. Es una aproximaci\u00f3n puramente estad\u00edstica, es bien salvaje, es. Yo no s\u00e9 qu\u00e9 estructura tiene esto, pero s\u00e9 que esto no se dio nunca de botero, s\u00ed, muchas veces. Entonces, es m\u00e1s probable que me haya equivocado. A ver, relacionado con esto, ahora vamos a ver por qu\u00e9 est\u00e1 relacionado. Est\u00e1 el tema de la predicci\u00f3n de la siguiente palabra. \u00bfCu\u00e1les se imaginan que es la siguiente palabra a la primera oraci\u00f3n? \u00bfCu\u00e1l puede ser la siguiente palabra? \u00bfPara? \u00bfPara? Para. \u00bfPara? \u00bfPara? \u00bfPara? \u00bfPara? \u00bfPara es una preposici\u00f3n, no? \u00bfQu\u00e9 m\u00e1s? \u00bfQu\u00e9 otra cosa puede decir ah\u00ed? \u00bfCu\u00e1l, por ejemplo? \u00bfUn pron\u00f3stico alentador? \u00bfUn pron\u00f3stico alentador? \u00bfO puede decir un pron\u00f3stico terrible? \u00bfUn pron\u00f3stico... \u00bfO qu\u00e9 otra cosa m\u00e1s? Hay uno m\u00e1s com\u00fan para m\u00ed. Elmiti\u00f3 un pron\u00f3stico meteorol\u00f3gico, no? \u00bfA ra\u00edz de este fen\u00f3meno se suceder\u00e1n tormentas? Fuertes, importantes, muy. No creo que hay diga tormentas gatito, \u00bfno? Esto no es muy probable que sea la palabra siguiente. Nuevamente, \u00bfpor qu\u00e9 sabemos esto? Porque es muy raro que hay un d\u00eda tormentas gatito, digamos, \u00bfno? Entonces, esto que tenemos ac\u00e1 es las posibilidades que hay de siguiente palabra. Dadas todas las anteriores. Es decir, yo tengo todo el contexto, lo que se llama contexto, dado el contexto de la palabra que sigue ac\u00e1. \u00bfS\u00ed? Una de las, lo que nosotros vamos a querer hacer en un modelo de lenguaje, como camino para calcular la probabilidad de una oraci\u00f3n, es dado el contexto calcular la palabra. Siguiente. \u00bfS\u00ed? Rachas de viento fuerte de componente. Veremos que. Bueno, resulta ser que de los ejemplos que yo tom\u00e9, ah bueno, puse viento fuerte de componente, el lin\u00f3mede emiti\u00f3 pron\u00f3tico especial, o sea que le ramos, se sonan tormentas fuertes, viento fuerte de componente sudo este, ejemplo, predicci\u00f3n. Vamos a poner un poquito de notaci\u00f3n antes de que, antes de seguir, porque vamos a ver c\u00f3mo enfrentamos este problema, es decir, \u00bfc\u00f3mo calculamos esa probabilidad? Un poco de notaci\u00f3n para seguir, yo lo que estoy diciendo es la probabilidad de que una variable aleatoria ah\u00ed valga, tome el valor conocimiento, en este caso tendr\u00eda una variable aleatoria por cada posici\u00f3n en el texto, \u00bfverdad? Tengo una X1, que es la primera palabra, aqu\u00ed dos, que es la segunda, aqu\u00ed tres. Son variables aleatorias, que la variable aleatoria es un mapeo, es una funci\u00f3n que mapea de un evento, un n\u00famero entre 0 y 1. \u00bfLa probabilidad de una? La probabilidad de una. Perd\u00f3n. Perd\u00f3n. Bueno, no vamos a entrar en definiciones, mapea con un real y la probabilidad me devuelve un n\u00famero entre 0 y 1, es decir, yo defino la probabilidad de una variable aleatoria, como la distribuci\u00f3n de probabilidad de una variable aleatoria, dado los diferentes valores que puede tomar \u00bfCu\u00e1l es el valor de cada uno de ellos? \u00bfS\u00ed? Y esto, \u00bfcu\u00e1l es el rango? \u00bfQu\u00e9 valor es probable que tiene ac\u00e1 una variable aleatoria que refiera palabras? Todo el vocabulario, \u00bfno? Todas las palabras diferentes que yo puedo tener. \u00bfDe acuerdo? Entonces nosotros vamos a poner en notaci\u00f3n probabilidad de conocimiento, de que la palabra sea conocimiento. Vamos a denotar W1n 1n a la secuencia de palabras W1 W2 Wn, por ejemplo, en una eraci\u00f3n y vamos a decir vamos a decir que vamos a hablar de la probabilidad de la secuencia de palabras queriendo decir, bueno, la probabilidad de la que la primera sea W1 que la segunda sea W2, etc. \u00bfDe acuerdo? O sea que esta distribuci\u00f3n de probabilidad tiene como rango todas las secuencias posibles de palabras. \u00bfS\u00ed? O sea que si mi vocabulario es B tengo N a la B V a la N V a la N Posita V a la N O sea que es enorme esencialmente, \u00bfno? Todas las posibles secuencias. Y vamos a recordar la chain rule la regla de multiplicaci\u00f3n de las probabilidades que es, si yo tengo la probabilidad de una secuencia de palabras W1, Wn esto es la probabilidad de la primera palabra, de alguna forma la calculo por la probabilidad de la segunda dada la primera, dado que la primera fue W1 observen ac\u00e1 que no son independientes es decir, las palabras por definici\u00f3n ac\u00e1, no son eventos independientes es decir tengo una cierta probabilidad de que empiece con W1 la multiplico por la probabilidad de que la segunda sea W2, dado que la primera fue W1 por la probabilidad que la tercera sea W3, dado que las dos primeras fueron uno de hoy as\u00ed de acuerdo de esa forma con esta regla yo y al final Wn la \u00faltima dada todas las anteriores esto se llama regla de la cadena yo con la regla de la cadena puedo calcular la probabilidad de una secuencia o de una oraci\u00f3n dada la secuencia si logro calcular estas probabilidades, o sea si logro calcular predecir las palabras correctamente voy a poder predecir la secuencia de esa forma paso de la predicci\u00f3n al c\u00e1lculo de toda la probabilidad de la oraci\u00f3n \u00bfse entienden? bien entonces vamos a quedarnos con esa notaci\u00f3n entonces yo digo bueno un ejemplo no, si yo quiero saber la probabilidad de viento fuerte de componente sudoeste como el que est\u00e1 soplando no s\u00e9 si de componente sudoeste pero es fuerte es la probabilidad de viento por la probabilidad de fuerte dado viento por la probabilidad de dado viento fuerte etc. nada menos que la regla de la cadena entonces yo quiero saber la \u00faltima p de sudoeste dado viento fuerte de componente y vos con google por ejemplo y digo bueno viento fuerte de componente aparece 9.230 veces viento fuerte de componente sudoeste aparece 347 veces y yo entonces voy a estimar la probabilidad de esa por medio de conteos la cantidad de veces que apareci\u00f3 viento fuerte de componente sudoeste dividido la cantidad de veces que aparece fuerte de componente dividido 9.230 \u00bfAguardo? y esta es la probabilidad de que la siguiente palabra sea sudoeste en mi estimaci\u00f3n si ustedes se fijan esto es una probabilidad porque contando todas las palabras posibles que pueden seguir ac\u00e1 si yo logro determinar cu\u00e1les son yo s\u00e9 que van a ver 9.230 van a sumar 9.230 \u00bfno? es decir todos los casos posibles miro todos los casos junto a lo que es la siguiente palabra eso hace que como esto me va a dar 9.230 a la suma de todas las cantidades esto va a dar uno el total entonces esto s\u00ed es una distribuci\u00f3n de probabilidad entonces que estamos bien efectivamente aquello es una probabilidad \u00bfde acuerdo? esto lo que me dice es bueno el 3,76% de las veces es sudoeste la siguiente palabra eso que acabamos de hacer es estimar la probabilidad a partir de la frecuencia de ocurrencia en un cuerpo grande eso Google es un cuerpo grande muy grande y eso se llama principio m\u00e1ximo pero similitud que lo vimos en la de pasada es trato de hacer calcular la probabilidad en base a lo mejor posible a los datos que tengo es decir considero yo estoy considerando que los datos que tengo es decir el corpo de Google es una buena aproximaci\u00f3n del mundo de lenguaje en realidad yo no s\u00e9 si en realidad efectivamente cuando los seres humanos hablamos hay un 3,76% de probabilidad de que despu\u00e9s de decir viento fuerte componente viene sudoeste pero el corpo de Google que es lo mejor que tengo como aproximaci\u00f3n me dice eso y eso es lo que yo utilizo como un estimador de m\u00e1xima verosimilitud es lo mejor que puedo acercarme con el cuerpo que tengo eso es lo que vamos a hacer todo el tiempo ac\u00e1 calcular componentes de m\u00e1xima verosimilitud pero tenemos alg\u00fan problema y es en el otro casos dice, a ra\u00edz de estos fen\u00f3menos se producir\u00e1n tormentas fuertes la probabilidad de fuertes y a ra\u00edz de estos fen\u00f3menos se producir\u00e1n tormentas tienen un problema y es que nunca apareci\u00f3 en mi corpus a ra\u00edz de estos fen\u00f3menos se producir\u00e1n tormentas y nunca apareci\u00f3 en mi corpus a ra\u00edz de estos fen\u00f3menos se producir\u00e1n tormentas fuertes y Y eso nos da una horrible visi\u00f3n por cero, que queremos evitar, o sea que nuestra probabilidad da infinito, no s\u00e9, no est\u00e1 definida. Esto, una pregunta, \u00bfesto les parece que es un fen\u00f3meno com\u00fan o no, que nos puede pasar cuando estemos estimando todo el tiempo, porque por m\u00e1s grande que sea el corpus, el lenguaje es muy creativo? Entonces tenemos que buscar forma y adem\u00e1s porque estamos haciendo un conteo de palabras de oraciones muy largas. O sea que la regla de la cadena no resuelve mi problema, porque yo, una aproximaci\u00f3n bien naif para calcular la probabilidad de calcular toda la secuencia posible, \u00bfs\u00ed? \u00bfcu\u00e1ntas veces aparece la secuencia que quiero calcular, la oraci\u00f3n del total de oraciones? Bueno, tengo un corpus evidentemente grande, pero esta aproximaci\u00f3n tampoco nos ayuda mucho porque sigo teniendo contestos muy largos, porque si ustedes se fijan en la regla de la cadena, bueno, en lo que acabamos de hacer, la \u00faltima probabilidad es casi la misma que la primera, menos una palabra, tengo que buscar una forma de achicar eso. Entonces, una de las ideas fuerzas para computar esta probabilidad es en lugar de tomar todas las palabras, tomar sobre las \u00faltimas, es decir, yo me quedo con las \u00faltimas n menos un palabras, \u00bfs\u00ed? n menos n, bueno. \u00bfs\u00ed? En esto es enigrante, \u00bfno? Y las otras no las considero, digo, bueno, mi humilde aproximaci\u00f3n para que esto se pueda volver manejable es decir, bueno, yo en realidad solamente me importan las \u00faltimas palabras afectan a la que voy a predecir, son las \u00faltimas. Y de eso se tratan los modelos enigramas que utilizan lo que se llama, eso que acabo de decir, yo llamo hip\u00f3tesis de marcovo, hip\u00f3tesis marcoviana. Solamente las \u00faltimas palabras afectan a siguiente, hay un l\u00edmite. Y f\u00edjense que en la hip\u00f3tesis de bigrama, yo digo, cada palabra es la pr\u00f3xima por la anterior, simplemente, estoy diciendo una cosa tan sencilla como la \u00faltima, la \u00faltima palabra es la \u00fanica, cada palabra es la siguiente, pero las anteriores no. Es muy fuerte, \u00bfno? Y de trigramas son dos y con n, con n son n. \u00bfs\u00ed? Con la hip\u00f3tesis de bigrama, mi probabilidad es mucho m\u00e1s sencilla que antes, porque es como cada palabra solo depende, vamos a mirar, uno bueno uno no est\u00e1 m\u00e1s, pero cada palabra depende de la anterior, simplemente me queda que la probabilidad de una secuencia es la probabilidad de la primera, por la probabilidad de la segunda a la primera, por la probabilidad de la tercera a la segunda, etc\u00e9tera. \u00bfLe guardo? Ac\u00e1 nos falta este PW1 en esa f\u00f3rmula, pero no nos preocupa demasiado porque eso lo resolvemos poniendo una marca al comienzo de la secuencia que siempre vale uno, su probabilidad, es decir, que toda la graci\u00f3n empieza con una marca. Y si no, multiplico ac\u00e1, \u00bfno? Si no, si lo quiero hacer de otra forma, agregue un PW0 ac\u00e1 y lo mismo. Pero esencialmente lo importante ac\u00e1 es que esto se transforma en una simple multiplicaci\u00f3n de probabilidades de una palabra dada en anterior. \u00bfY c\u00f3mo hago para calcular esto? \u00bfC\u00f3mo puedo calcular esto ac\u00e1? \u00bfC\u00f3mo calcula la probabilidad de una palabra dada en anterior? Contando, pero solamente tienen cuenta dos, lo cual lo vuelve un problema mucho m\u00e1s manejable. Y eso es justo lo que vamos a hacer. Un modelo de lenguaje intenta predecir la pr\u00f3xima palabra de una oraci\u00f3n a partir de las n menos una anteriores y, por supuesto, que importa el orden en ese c\u00e1lculo, \u00bfno? Tambi\u00e9n tenemos que plantearnos cuando hagamos los engramas, cuando calculemos la probabilidad de una palabra, bueno, cosas que ya hemos conversado. \u00bfQu\u00e9 elemento vamos a contar? Por ejemplo, tengo un tema de toquenizaci\u00f3n, esta coma, \u00bfla tengo que considerar un engrama o no la tengo que considerar un engrama? \u00bfLa tengo que considerar un token o no la tengo que considerar un token? \u00bfMe interesa? Bueno, eso seguramente va a depender un poco de la aplicaci\u00f3n en la que lo estoy aplicando, a lo que lo estoy utilizando. O tengo un cuerpo oral donde tengo disfluencias, disfluencias, creo que se llama esto. \u00bfQu\u00e9 tengo que hacer con las may\u00fasculas? \u00bfQu\u00e9 hago con la forma flexionada? Todo los problemas de la toquenizaci\u00f3n me parecen en los engramas, es decir, estos son cascadas, digamos, \u00bfno? Yo acabo de tener la toquenizaci\u00f3n realizada. En realidad no hay respuesta universal, depende de la tarea que estamos haciendo. Por ejemplo, t\u00edpicamente los corporeales est\u00e1n todos pasados a may\u00fasculas, porque como son m\u00e1s continuos, la identificaci\u00f3n de oraciones no es tan importante. Si yo voy a hacer an\u00e1lisis, si estoy haciendo un an\u00e1lisis de c\u00f3mo se usan los signos de puntuaci\u00f3n en mi lenguaje, obviamente la coma la tengo que identificar, sino capaz que no me interese. O me puede interesar todo estos, mapearlos a una cosa sola que se llama signo de puntuaci\u00f3n y juntar los puntos con las comas. Van a tener que hacer eso en el laboratorio. Ya se van a colar. Bueno, nada, se necesita un pretratamiento disponible al menos para las oraciones y el modelo no hay modelos generales. Tambi\u00e9n va a depender un poco, nuestros n\u00fameros van a depender de la cantidad de palabras. El dictionary, el Oxford English dictionary tiene 290.000 entradas, el Tresor de la langue franc\u00e9s tiene 54.000 y el dicionario de la radio 88.000. \u00bfPor qu\u00e9 les parece que hay tantas m\u00e1s ac\u00e1 que ac\u00e1? Porque el dicionario no parece en la forma flexionada y el espa\u00f1ol est\u00e1 mucho m\u00e1s flexionado. O sea que el ingl\u00e9s la tiene que arreglar m\u00e1s solito. Bueno, y despu\u00e9s tenemos corpus. Esto ya hablamos un poco y aquello distinguir entre el n\u00famero de token que son la cantidad de ocurrencias que hay en el texto y el n\u00famero de palabras distintas, el vocabular. Ac\u00e1 est\u00e1 la respuesta a la pregunta que hac\u00edamos antes. \u00bfC\u00f3mo estimamos los bigramas utilizando otra vez lo que se llama un estimador de m\u00e1xima verosimilituos, lo que se llama m\u00e9todos de frecuencias relativas? Que es, cuento las cantidades de la cantidad de veces que apareci\u00f3 una palabra con, por ejemplo, la probabilidad de fuerte, dado viento, se aproxima como la cantidad de veces que aparece viento fuerte. Por la dividida de la cantidad de veces que apareci\u00f3 dividido todas las posibles continuaciones. \u00bfDe acuerdo? Viento fuerte, viento calmo, viento, viento dile, viento, no s\u00e9, lo que quieras. Y sumo todas las posibles, lo que estoy haciendo es normalizando, como hablamos al principio de, como hablamos ac\u00e1, \u00bfno? Estoy normalizando. Ahora, esto aqu\u00ed es equivalente. \u00bfC\u00f3mo puedo simplificar esto? Si yo tengo todas las veces que aparecen viento fuerte, viento calmo, no s\u00e9, \u00bfcu\u00e1l es la suma de todo eso? Es la cantidad de veces que apareci\u00f3 viento. Esto es igual a la cantidad de secas que aparecen vientos en el cuerpo. \u00bfC\u00f3mo puedo recordar? Como son todas las posibles ocurrencias. Ah\u00ed tenemos la simplificaci\u00f3n y, adem\u00e1s, para tener en cuenta la primera y \u00faltima palabra en oraci\u00f3n, le vamos a agregar siempre los s\u00edmbolos de comienzo y de fin. Eso para asegurarnos de que, para no tener que calcular separada la probabilidad de la primera palabra. Yo s\u00e9 que la primera palabra siempre es ese y calculo la probabilidad de la primera en el texto, digamos, ponerle el dado que la anterior era ese. De acuerdo? Y as\u00ed lo dejo en una sola forma. Por ejemplo, si supongamos que yo tengo ese cuerpo, \u00bfno? Juan abri\u00f3 la puerta, el viento abri\u00f3 la puerta, enero abri\u00f3 limones en tus mejillas nuevas, Juan recoge limones. Y quiero saber la probabilidad de estas oraciones. Evidentemente no las tengo en el cuerpo, o sea, que no puedo contar directamente. Pero quiero utilizar un modelo de diagrama para calcular. Y con lo que sabemos es bastante sencillo. Primero que nada decimos, bueno, la probabilidad de Juan abri\u00f3 limones es probabilidad de Juan dado el comienzo. Probabilidad de comienzo siempre es uno. Probabilidad de abri\u00f3 dado Juan, probabilidad de limones dado abri\u00f3, etc\u00e9tera, \u00bfno? F\u00edjense que la probabilidad Juan dado el comienzo de la cantidad de veces que apareci\u00f3 Juan en la marca de comienzo dividido de la cantidad de marcas de comienzo que es uno. Entonces esto me da 2 de 4. Ah, porque hay cuatro oraciones. Perd\u00f3n. Claro, porque yo estoy haciendo contegos directamente. No, no estoy haciendo 2 de 4 veces arranc\u00f3 con Juan, \u00bfs\u00ed? Juan abri\u00f3 es una de dos. Ya hab\u00eda aparecido Juan abri\u00f3 en el corpus y Juan aparece dos veces. O sea, de dos veces le apareci\u00f3 Juan y la siguiente apareci\u00f3 una vez abri\u00f3. Y as\u00ed sigo multiplicando y como me multiplico la fracci\u00f3n y me da, bueno, 0-0-42. Esa es la probabilidad de Juan abri\u00f3 el lim\u00f3n. Enero abri\u00f3 la puerta 0-17. Esto no tiene mucho sentido, \u00bfno? A ver, justamente el hecho de que sea un ejemplo de juguete le hace perder la gracia todo esto porque esto funciona porque tengo grandes vol\u00famenes, sino es una pasada. \u00bfY ac\u00e1 que nos pas\u00f3? \u00bfQu\u00e9 puede haber pasado ac\u00e1? La palabra come nunca est\u00e1. \u00bfY en la puerta? En la puerta est\u00e1. La primera se explica porque come nunca est\u00e1, \u00bfno? Creo que est\u00e1 as\u00ed. Perd\u00f3n. La as\u00ed, la puerta. \u00bfPor qu\u00e9 da 0? Porque lo que no est\u00e1 es en la. En la no aparece nunca. Si ustedes miran ac\u00e1 la probabilidad de, perd\u00f3n, la cantidad de, la probabilidad de esto es la probabilidad de que empiece con \u00e9l, ya tenemos un problema con el comienzo con \u00e9l porque creo que no hay ninguna. Ninguna empieza con \u00e9l y eso ya tiene un problema. Y adem\u00e1s en la tampoco est\u00e1. O sea que el conteo me va a dar 0. Si el bigrama no aparece en el cuerpo de entrenamiento, siempre mi problema da 0. Y m\u00e1s interesante a\u00fan, si cualquier bigrama de todos los que aparecen en la oraci\u00f3n da 0, la probabilidad de la oraci\u00f3n es 0. Eso es un gran problema. Resolver el problema de eso, de lo que se llama el suavizado de engramas que vamos a ver c\u00f3mo. Tenemos que buscar alguna forma de resolver eso que nos va a pasar siempre. Es decir, como nuestro cuerpo nunca puede ser tan, aunque solo sean dos palabras, igual puede parecerme parezca de palabras que no aparecieron y yo no me puedo trascar con eso. \u00bfDe acuerdo? Bueno, nos queda ese pendiente de 0 que lo vamos a ver despu\u00e9s porque ya te quiero comentar alguna cosa. Pero vamos a acordarnos de eso, que tuvimos este problema pendiente. Bien, en general, ustedes dir\u00e1n, bueno, pero \u00bfcu\u00e1l es el mejor N? \u00bfPor qu\u00e9? \u00bfCu\u00e1l es el tema? \u00bfEs? \u00bfCuanto? \u00bfCuanto m\u00e1s largo sea el tigrama que yo utilizo? M\u00e1s informaci\u00f3n tengo de contexto. Es decir, intuitivamente es mejor estimar con cinco palabras que con una. Estamos guardados con eso. \u00bfCu\u00e1l es el problema de los tigramas largos? \u00bfPor qu\u00e9 no puedo usar 15? Porque tenemos el mismo problema por el que llegamos ac\u00e1, que con 15 no tengo cuerpo suficientemente grande como para que aparezcan esa ocurrencia. Entonces, ese balance entre cantidad de ocurrencia, porque si yo no tengo una buena estimaci\u00f3n de la cantidad de ocurrencia, no voy a poder estimar bien la probabilidad. Con lo que yo estoy estimando la probabilidad partido en conteos. Si yo tengo una, dos, tres ocurrencias, seguramente esa probabilidad sea artificial. Porque si hubo una ocurrencia en un cuerpo de miles de millones de palabras, no me est\u00e1 diciendo mucho. Generalmente en igualtr\u00e9 se obtienen buenos resultados. Por lo menos para aproximarse da muy bien. Google hace unos a\u00f1os atr\u00e1s sac\u00f3 un cuerpo de negra, un s\u00ed, una lista de negramas de hasta cinco. Me acuerdo en esa \u00e9poca bien en ese. O sea que determinar n va a depender un poco de la tarea y ese se me dio a ojo, digamos, pues una tarea un poco complica. Ahora vamos a ver un poco de evaluaci\u00f3n. Ita y lo que dec\u00edamos, \u00bfno? \u00bfSe agregan? Como cuando son trigramas, tengo que agregar dos s\u00edmbolos al comienzo de la oraci\u00f3n. Tengo a poner. Enero, abri\u00f3. Porque yo necesito dos de contexto para calcular el trigramo en detalle. Y bueno, y la pregunta es c\u00f3mo calculamos desde el punto de vista metodol\u00f3gico, c\u00f3mo hacemos para calcular buenas probabilidades. Ya vimos c\u00f3mo se hace el conteo. Ya ahora quiero ver c\u00f3mo organiza el corpus y me parece que es interesante ver esto porque nos va a pasar en muchas cosas, en este tema de preservamiento del lenguaje natural y que muchas veces induce el mal uso metodol\u00f3gico de estas cosas lleva error. Entonces me parece que vale la pena comentarlo esto. Yo. Yo dije que iba a hacer conteo para calcular las probabilidades, \u00bfno? Entonces yo por ac\u00e1 tengo un corpus, un corpus de texto. S\u00ed. Entonces esencialmente lo que tengo son muchos textos, \u00bfno? Obviamente, esencialmente no, tengo muchos textos. Esa es la definici\u00f3n de corpus. Y yo voy a establecer, voy a crear un modelo de una, de un, un modelo de un lenguaje. Es decir, yo lo que quiero construir con esto de las probabilidades de las olaciones es un modelo del idioma espa\u00f1ol. Yo tengo un corpus de texto en espa\u00f1ol y quiero hacer un modelo del idioma espa\u00f1ol. Supongo que yo entren un modelo, entrenar el modelo en este caso quiere decir calcular todas esas probabilidades. \u00bfC\u00f3mo hago para saber qu\u00e9 tan bueno es? \u00bfS\u00ed? \u00bfC\u00f3mo lo eval\u00fao? Supongo que yo, ahora vamos a hablar de cu\u00e1l es la medida, pero supongo que yo tengo una medida de performa que me dice, bueno, aplicarle tu modelo a este texto. S\u00ed. Supongamos que la medida es el que le asigne, ahora vamos a ver por qu\u00e9, pero el que le asigne mayor probabilidad a todo el texto, a las oraciones del texto, es el mejor. El mejor modelo es el que la asigna probabilidad mayor a las oraciones que tengo en el texto. Si yo aplico mi m\u00e9todo, mi modelo, o sea, eval\u00fao mi modelo. Sobre este mismo corpus, \u00bfqu\u00e9 problema tengo? Que me va a dar b\u00e1rbaro porque lo calcul\u00e9 ah\u00ed. Es decir, yo nunca puedo, nunca, pero nunca, nunca, evaluar un modelo en el mismo corpus en el que entren\u00e9. Esto aplica siempre. Cabe que yo utilizo un m\u00e9todo estad\u00edstico, aprendizaje autom\u00e1tico. Lo m\u00e1s importante a saber en el aprendizaje autom\u00e1tico es nunca eval\u00faes tu modelo en un corpus, en el mismo corpus que entrenaste, porque por definiciones est\u00e1s haciendo trampa, eso lo que se llama sobreajuste. Vos sobreajustas a tu corpus de entrenamiento. Entonces yo lo que voy a hacer es dividir mi corpus en dos y voy a decir, este es el corpus de entrenamiento, voy a poner en ingl\u00e9s, y el corpus de evaluaci\u00f3n. Entonces lo que yo voy a hacer es entrenar y \u00bfcu\u00e1nto se para ac\u00e1? Bueno, la regla m\u00e1s o menos es 80-20. Pregunto, \u00bfpor qu\u00e9 me interesar\u00eda que esto fuera lo m\u00e1s grande posible? Para que tener m\u00e1s informaci\u00f3n. \u00bfY por qu\u00e9 no abuso 90-10 o 95-5 o 97-3? \u00bfC\u00f3mo? \u00bfQuieres evaluarlo con una cantidad de datos? Tengo que solucionar ese balance, entretener una cantidad razonable de datos para hablar, porque si yo le evaluo sobre una oraci\u00f3n, la varianza es muy grande, es decir, la posibilidad de equivocarme es muy grande. Entonces una regla es m\u00e1s o menos 80-20. \u00bfS\u00ed? Y bueno, ah\u00ed habla de 90-10, yo tengo la regla de 80-20. Va a surgir un problema adicional ac\u00e1, y es que ahora lo que voy a mover es, por ejemplo, si yo quiero saber cu\u00e1ntos eleg\u00ed el n, \u00bfno? Yo quiero elegir el n, yo necesito, lo que puedo hacer es pruebo con un n ac\u00e1, modelo 1, n igual 2, y hago modelo 2, n igual 3. Esto es un poco m\u00e1s \u00fatil de ver. Y lo evaluo ac\u00e1 y digo m1 y m2, y me quedo con el que me da mejor. Eso metodologicamente no est\u00e1 bien. \u00bfPor qu\u00e9? Y esto es una de las cosas que es m\u00e1s dif\u00edcil de entender a veces. Si yo pruebo los dos modelos ac\u00e1, de alguna forma tambi\u00e9n estoy haciendo trampa, porque sup\u00f3nganse que yo tengo no dos par\u00e1metros, porque ac\u00e1 tengo un par\u00e1metro que tiene dos valores. Supongamos que yo quiero ajustar otro par\u00e1metro de mi m\u00e9todo que puede tomar 500 valores posibles. Si yo hago 500 entrenamientos y 500 pruebas, muy probablemente tambi\u00e9n est\u00e9 ajustando ac\u00e1, est\u00e9 sobreajustando ac\u00e1, porque estoy eligiendo de los 500, y a veces pueden ser miles o cientos de miles, el que mejor anda en este cuerpo de evaluaci\u00f3n, o sea que estoy sobreajustando el cuerpo de evaluaci\u00f3n. Entonces, para el ajuste de par\u00e1metros, yo usualmente lo que tengo que hacer es definir dividir este corpus, sacar un pedacito del cuerpo de entrenamiento, que lo llamo corpus held auto, corpus de desarrollo. Y lo que hago es entreno sobre esta parte y evaluo sobre el held auto, y me reservo este de evaluaci\u00f3n, solamente para cuando tengo mi modelo definitivo y quiero saber su performance, con su medida de evaluaci\u00f3n, \u00bfde acuerdo? Esto lo van a, algo como esto van a tener que presentar en el laboratorio, decir, c\u00f3mo evaluar\u00edan el m\u00e9todo, un m\u00e9todo. Hay otras posibilidades que no implican un corpus held auto, por ejemplo, hacer lo que se llama cross validation, que es separo este pedacito, entreno sobre esto y evaluo sobre este, si, despu\u00e9s separo otra franjita, entreno sobre el resto y evaluo sobre la franjita y as\u00ed con en cada franca, franjas y saco el promedio, eso me sirve para no desperdiciar, digamos, esta parte del corpus, para poder utilizar todo el cuerpo de entrenamiento, se llama cross validation. Vamos a volver a hablar un poquito de cross validation, cuando hablemos de clasificaci\u00f3n, pero lo que me interesa es que le quede claro la diferencia entre estos corpus y cuando, como dec\u00eda, cuando tengo el modelo final, uso esto solamente para evaluar las performas, en una medida que determinar\u00eda ese unitaria. \u00bfC\u00f3mo evaluamos un modelo bueno? La manera correcta de evaluar un modelo deber\u00eda, ser\u00eda emp\u00edricamente, es decir, si yo quiero valorar un modelo de lenguaje y lo estoy usando para el reconocimiento del habla, deber\u00eda ser una evaluaci\u00f3n de qu\u00e9 tambi\u00e9n reconozco el habla o qu\u00e9 tambi\u00e9n reconozco la escritura, pero eso puede ser muy costoso a veces, o yo puedo estar haciendo un modelo en lenguaje y no s\u00e9 para qu\u00e9 se va a usar, entonces me interesa mucho o me puede interesar tener una media intr\u00ednseca de la performa de mi modelo. Entonces, vamos a ver una forma de evaluar. A m\u00ed esta parte de este parte en el libro est\u00e1 puesta como un tema avanzado, pero a m\u00ed me parece interesante mostrarlo porque porque la entrop\u00eda es un concepto que aparece muchas veces en el profesoramiento del lenguaje natural y en otras cosas me parece que le vale la pena por lo menos aproximarse. Supongan que yo tengo una variada de la aleatoria y todo esto voy a llegar a una forma de evaluar un modelo, \u00bfno? No hay que empezar a hablar de esto porque s\u00ed. Supongan que yo tengo una variada de la aleatoria que tiene varios eventos posibles, en nuestro caso dijimos que eran las palabras posibles. La entrop\u00eda, la entrop\u00eda es una variada de la aleatoria que es un concepto que viene de la teor\u00eda de la informaci\u00f3n, de Claude Shannon. La teor\u00eda de informaci\u00f3n lo que hablaba era, bueno, alguno capaz que hicieron, lo vieron en un curso, pero la teor\u00eda de informaci\u00f3n lo que trataba era de medir cu\u00e1nto me cuesta a m\u00ed transmitir un mensaje. \u00bfC\u00f3mo puedo transmitir un mensaje de forma \u00f3ptima? Digamos, es un poco la idea, o qu\u00e9 hay atr\u00e1s de una comunicaci\u00f3n. La noci\u00f3n de entrop\u00eda, esta funci\u00f3n es, tengo el evento, quiero decir, la probabilidad del evento por el valorismo de esa probabilidad, \u00bfs\u00ed? La entrop\u00eda tiene como caracter\u00edstica fundamental que es una medida que, si hay un evento que tiene toda la masa de probabilidad, la entrop\u00eda es m\u00ednima. Es decir, si yo tengo un dado que est\u00e1 tan cargado y una forma, algo que, equivalentemente se puede decir que la entrop\u00eda mide migrado disertidumbre sobre un evento. Si yo tengo un dado que est\u00e1 tan cargado, que cabe que lo tiro, s\u00e9 que siempre va a salir seis, no tengo disertidumbre. Mi entrop\u00eda es cero. En cambio, si el dado est\u00e1 perfectamente calibrado, equilibrado, \u00bfs\u00ed? Mi entrop\u00eda es m\u00e1xima. Es decir, \u00bfpor c\u00f3mo est\u00e1 definida la entrop\u00eda? No puedo tener entrop\u00eda m\u00e1s alta que cuando los eventos est\u00e1n equipobables. Entonces, justamente la entrop\u00eda, generalmente lo que uno mide con la entrop\u00eda es eso. \u00bfQu\u00e9 tan parecido son los resultados? \u00bfQu\u00e9 tan balanceados est\u00e1n de alguna forma? Cuanto m\u00e1s incertidumbre tengo, \u00bfpor qu\u00e9 tan m\u00e1s balanceados? Si yo no tengo ni la menor idea de la palabra que sigue, mi entrop\u00eda es m\u00e1xima. Y adem\u00e1s tiene otra caracter\u00edstica que es que si el logaritmo es en base 2, este n\u00famero, la entrop\u00eda me mide la cantidad de bits que yo necesito, m\u00ednimo para transmitir los eventos. Esto es lo mejor forma de verlo con un ejemplo. Supongamos, y es el ejemplo que aparece en el libro, supongamos que yo tengo ocho caballos. S\u00ed, tengo ocho caballos y quiero transmitir las apuestas que se est\u00e1n haciendo por un cable. Entonces digo, bueno, una forma cantada de transmitirlo o directa de transmitir llamar al primer caballo 001, 010, 011, 100, 101, 110, 111. \u00bfDe acuerdo? Ac\u00e1 yo uso ocho bits. Cada vez que se apuesta por el caballo 01, yo pongo 001, blablabla. Entonces en total yo utilizo tres bits para transmitirlo por un cable. Tres bits por cada apuesta, \u00bfno? Ahora, cuando nosotros vemos las apuestas descubrimos que la mitad de las veces se apuesta por el caballo 1. Un cuarto del caballo 02, un tercio, blablabla. Un octavo del caballo 03, un 16ado del caballo 04, y todos estos se apuestan mucho menos. Teniendo en cuenta eso, yo lo que trato de hacer ahora es decir, bueno, quiero proponer una codificaci\u00f3n mejor que hace que yo, los caballos que se apuestan m\u00e1s, o sea que tengo que transmitir m\u00e1s seguido, los codifico con menos bits. \u00bfDe acuerdo? La mitad de los bits, el primer bit, lo utilizo solo para el caballo 01. Es decir, que si es un 0, es que transmitir el caballo 01 necesita un solo bit. Si es un 01, si es un 01 y un 0 despu\u00e9s, es el caballo 02. Si son 01 y un 0, despu\u00e9s es el caballo 03. Si son 01 y un 0, f\u00edjense que yo para transmitir estos caballos utilizo 1, 2, 3, 4, 5, 6 bits. Utilizo m\u00e1s bits. Pero como son mucho menos probables, mi entrop\u00eda me da 2 bits. O sea, el promedio de bits que yo utilizo seg\u00fan la distribuci\u00f3n es 2 bits, que es m\u00e1s baja que los 3 bit originales. \u00bfSe entiende? Incorporando la informaci\u00f3n de la distribuci\u00f3n bajo. Podemos mejorar eso. No, no podemos mejorar eso. Nunca vamos a, la entrop\u00eda lo que nos dice es eso. Nunca vas a encontrar una, porque justamente la entrop\u00eda es 2. Como la entrop\u00eda es 2, la entrop\u00eda me da una cota inferior sobre cu\u00e1nto puedo llegar. Con menos de 2 bits no puedo. \u00bfDe acuerdo? Entonces se preguntar\u00e1n para qu\u00e9 sirve esto. De hecho no, la entrop\u00eda es una cota de lo que dec\u00eda, una cota m\u00ednima para el n\u00famero de bits necesarios. A partir de la entrop\u00eda yo puedo calcular la entrop\u00eda de una secuencia. La entrop\u00eda de una secuencia es de todas las combinaciones posibles, de una secuencia la probabilidad de esa combinaci\u00f3n es lo mismo para aplicado a secuencia. Este si lo ven es un n\u00famero muy complicado porque es la sumatoria de una cantidad impresionante del n\u00famero, porque son todas las combinaciones posibles de secuencia. Eso es lo que me dice es la entrop\u00eda de la secuencia. \u00bfQu\u00e9 tanta incertidumbre hay en una secuencia? Y la tasa entrop\u00eda ser\u00eda eso dividido de n, es decir el promedio, porque si no la secuencia m\u00e1s larga o no la entrop\u00edamos antes. El promedio por palabra de la entrop\u00eda. Entonces, la entrop\u00eda de un lenguaje que ser\u00eda como la medida de qu\u00e9 tanta incertidumbre hay en un lenguaje. \u00bfQu\u00e9 tanto puedo yo llegar a predecir lo que va a seguir diciendo el lenguaje? Ese al l\u00edmite, pero como valor\u00f3, no en un contexto en general en el lenguaje, es una medida para el lenguaje. Ese al l\u00edmite cuando la secuencia tiene infinito de la tasa entrop\u00eda. Y que s\u00e9 que ac\u00e1 es la suma, como dec\u00edamos, es la suma de todas las secuencias posibles. Es decir, que es una cosa imposible, calcular. Pero hay un teorema que es el de Llano Muam\u00ed Lambrayman que dice que el lenguaje es estacionario y erg\u00f3dico. Estacionario y erg\u00f3dico quiere decir que no importa d\u00f3nde yo est\u00e9 parado en una secuencia, todas las posiciones, las probabilidades son las mismas de continuidad. Lo cual no es as\u00ed en el lenguaje, porque lo que yo digo ahora incide dentro de lo que estoy diciendo entre un minuto m\u00e1s. No, no es aleatorio, digamos. Pero suponiendo eso es una simplificaci\u00f3n, lo que me permite es simplemente para calcular la entrop\u00eda, la tasa de entrop\u00eda del lenguaje es simplemente uno sobre n dividido en logaritmo. F\u00edjense que perd\u00ed las probabilidades de cada una de las de la secuencia. Es como que si yo tomo una secuencia suficientemente larga del lenguaje, voy a incluir a todas las subsecuencias. O sea que si yo una secuencia suficientemente larga, puede ser el cuerpo de evaluaci\u00f3n. Yo puedo calcular la entrop\u00eda sobre el cuerpo de evaluaci\u00f3n. Entonces esto es un n\u00famero, hasta ahora lo que dije ac\u00e1 es un n\u00famero, no sabemos por qu\u00e9 tengo esto. Pero f\u00edjense que si yo puedo calcular lo que se llama la entrop\u00eda cruzada, porque yo que tengo, yo tengo un lenguaje que genera las palabras con una cierta distribuci\u00f3n de probabilidad, que es lo que queremos averiguar, que es tan lo que es lo que es nuestro problema original, c\u00f3mo da las palabras anteriores y genera la siguiente. Eso es algo que he desconocido, no sabemos c\u00f3mo es, porque es el del lenguaje espa\u00f1ol el que yo quiero pero yo tengo un modelo M, que es el modelo de negramas. La entrop\u00eda cruzada lo que dice es bueno calculamos esta H utilizando la probabilidad original por el logaritmo de la probabilidad asignada por el modelo. La probabilidad de la secuencia es la que ten\u00eda el lenguaje general, que no la conozco, y el logaritmo s\u00ed, o sea esa distancia, esa largo env\u00edcese del modelo. Seg\u00fan el teorema otra vez, ya no manmilan, yo puedo sacar esta probabilidad simplific\u00e1ndola, suponiendo que es ergodico, y digo bueno, la entrop\u00eda cruzada depende solo del logaritmo de la probabilidad asignada por el modelo. Y esto es lo interesante, cualquier entrop\u00eda cruzada que yo obtenga, que yo calcule con un modelo, va a ser mayor necesariamente que la entrop\u00eda d\u00e9 lenguaje. Cualquier modelo va a asignarme una entrop\u00eda mayor a la de lenguaje, esto es la cota inferior. Entonces f\u00edjense que como son todas mayores, cuanto m\u00e1s parecido sea mi modelo, al modelo del lenguaje, cuanto m\u00e1s aparecido, asigne probabilidad m\u00e1s parecida de las de ac\u00e1, por como est\u00e1 definido, va a ser mejor. Entonces, cuanto menor sea la entrop\u00eda cruzada de mi modelo, evaluado sobre una secuencia suficientemente larga, es decir, sobre el corpo de evaluaci\u00f3n, mejor va a ser mi aproximaci\u00f3n. Y justamente, la medida de esa intr\u00ednseca que est\u00e1bamos buscando era esto, que es dos, \u00bfpor qu\u00e9 es dos? No lo s\u00e9, porque es lo mismo, es para sacarlos logaritmos nada m\u00e1s, es dos a la entrop\u00eda cruzada, a este valor, y esto se llama perplejidad. La perplejidad es lo que mide qu\u00e9 tan bueno es intr\u00ednseamente mi modelo sobre mi cuerpo de entrenamiento, sobre mi cuerpo de evaluaci\u00f3n. Es decir, si yo tengo dos modelos, el que asigne mayor probabilidad, menor perplejidad, mayor probabilidad al cuerpo de evaluaci\u00f3n, es mejor desde ese punto de vista, lo consideramos mejor. \u00bfPor qu\u00e9? Porque tiene menos dudas de c\u00f3mo se comporta, porque la perplejidad es como la incertidumbre que yo tengo ante... Dada una palabra, cuando yo me paro una palabra, \u00bfcu\u00e1l es mi incertidumbre? Mi branching factor, en cuanto se puede abrir la siguiente palabra en promedio? Un poco eso es lo que captura la perplejidad. Mi lenguaje va a tener un branching factor, es decir, no es que es cero, pero mi modelo siempre va a calcular algo mayor o igual a ese branching factor. Cuanto m\u00e1s bajo sea, es que quiere decir que yo no estoy acercando m\u00e1s a la perplejidad posta, por eso la perplejidad es la medida de que tambi\u00e9n hace las cosas. \u00bfDe acuerdo? Bueno, no, eso es cuentas. Por ejemplo, si nosotros entrenamos unigramas, bigramas y triramas en un corpo de art\u00edculo de Wall Street Journal de 38 millones de palabras, probaron el cuerpo sobre un modelo de un cuerpo de prueba de 1,5 millones de palabras y calcularon la perplejidad. Y f\u00edjense que la perplejidad con los unigramas es de 962. \u00bfNo sabemos cu\u00e1l es el m\u00ednimo de esto? No sabemos cu\u00e1nto puede bajar, pero sabemos que con bigrama llega a 170 y con triramas a 109. Es decir, si yo tengo dos palabras antes, puedo predecir con mejor, porque ac\u00e1 es con unigrama, es la probabilidad que a palabra no dice mucho. Si yo tengo el anterior, lo r\u00e1pidamente baja. Y si se fija, cuando agrega un tercero baja, pero no tanto, ni cerca tanto. Bueno, lo \u00faltimo que nos queda hablar es muy bien. \u00bfQu\u00e9 pas\u00f3 con las probabilidades nuladas? \u00bfSe acuerdan que nos quedaban las probabilidades nuladas cuando no hab\u00eda conteo? Bueno, uno de los problemas es la palabra que no existen. La palabra que no existen lo \u00fanico que podemos hacer o lo que t\u00edpicamente se hace es crear un vocabulario fijo y sustituyo las palabras de conocida por una especial. Esto es t\u00edpicamente lo que se hace. Es decir, todas las palabras de conocida las considero una sola palabra que nos equivalece. Y cuando aparecen enigramas que no ocurren, este es el caso de come que no aparec\u00eda, pero puede ser que el enigrama no ocurra, lo que voy a hacer son t\u00e9cnicas de suavizado. Yo tengo, \u00bfse acuerdan? Tengo el contador de, por ejemplo, ac\u00e1 es un enigrama, \u00bfno? Contador de la palabra, el cantidad de veces de la palabra, dividido el total de token que hay. Y as\u00ed calculo las probabilidades. La t\u00e9cnica de la plaz, lo que dice es, bueno, le agrego uno a cada contador, o sea que nunca me va a dar cero, lo hago a lo bestia, digamos, \u00bfno? Compare que no me d\u00e9 cero, le sumo uno. Y le sumo B, \u00bfse acuerdan? Que lo he vivido en la clase pasada. Le sumo B para que esto me siga dando una distribuci\u00f3n de probabilidad. Esto simplemente lo que hace es calcular un contador ajustado, multiplica por T y divide por T m\u00e1s B, es decir, multiplica por el junial y divide por esto, \u00bfno? Por el PWBI. Por ejemplo, si yo digo, si este es mi corpo entrenamiento, esta es la historia de un hombre de la ciudad que creo, f\u00edjense que me cont\u00e9 o da uno, y quiso me da cero. Perd\u00f3n, este es el conteo. Ah\u00ed va, el conteo de esta es uno, de la es dos y de quiso es cero. La probabilidad de esta es uno dividido trece. En total de palabras, una es esta y es cero, cero, ocho. La es dos dividido trece y quiso me da cero en la probabilidad de que no queremos que no de cero. Si nosotros aplicamos la plaza, lo que me da es sumo veinticinco, \u00bfno? Son doce palabras en el vocabulario porque la \u00fanica que est\u00e1 repetida es la. O sea que tengo doce en el vocabulario, no trece, trece es T y doce es B. Entonces, se hago dos dividido veinticinco y as\u00ed me da las nuevas probabilidades. Y ac\u00e1 quiso deja de ser cero. El contador ajustado de lo que nos permite es comparar lo que ten\u00edamos antes con lo que ten\u00edamos ahora. Por ejemplo, esta val\u00eda uno y baja a cero noventa y seis. \u00bfDe acuerdo? La val\u00eda dos y baja a uno cuarenta y cuatro. Y quiso va de cero a cero cuarenta y ocho. Si se fijan ac\u00e1, lo que se llama descuento, que es el cociente entre los dos valores, me permite ver que le estoy sacando m\u00e1s masa de probabilidad a la que a esta, que queda casi igual. Es decir, le ten\u00eda a la plaza el problema, porque \u00bfqu\u00e9 es lo que est\u00e1 pasando ac\u00e1? Esto es lo que me muestra, es que yo le tengo que sacar masa de probabilidad a los que aparecen, porque todo me tiene que sumar uno, todas las probabilidades me tienen que sumar uno. Si yo iba a agregar diagramas que antes estaban en cero, tengo que sacarle probabilidad a los que est\u00e1n, pues no me suma m\u00e1s que uno. Entonces, esto es lo que tiene que castiga mucho a los m\u00e1s frecuentes. Les sacan mucho probabilidad a los m\u00e1s frecuentes y como que premia demasiado a los que no aparecen. Hay otras t\u00e9cnicas, no vamos a entrar en eso que tratan de ajustarlo un poco mejor, pues ahora vamos a mover algunas, perd\u00f3n. Mueve demasiada probabilidad. Otra posibilidad es usar un delta en lugar de uno. Ese delta tengo que calcularlo, se acuerdan lo calamos del cuerpo de, siempre que yo tengo esos par\u00e1metros para calcular, los calculo sobre el cuerpo de desarrollo. Finalmente, hay otra, esa es una aproximaci\u00f3n, es decir, con t\u00e9cnicas sobre el contenci. Hay otra posibilidad que son un poco m\u00e1s avanzadas, digamos que es cuando yo quiero estimar, por ejemplo, en t\u00e9cnicas de trigrama, una palabra a partir de las dos anteriores y no existen casos de las dos anteriores en el texto, de las dos anteriores seguidas a W, ac\u00e1 es WN, perd\u00f3n. S\u00ed, lo que hago es hacer lo que se llama BACOV, calcularlo a trav\u00e9s de la probabilidad de la anterior. Bueno, si no tengo la anterior, pruebo con la anterior, se entiende, eso se llama hacer BACOV. El BACOV, ten\u00e9s que resolver tambi\u00e9n que ahora otra vez se est\u00e1 introduciendo nuevas, luego caso que no ten\u00edas antes. Estas probabilidades tengo que calcularlo y darle m\u00e1s a la probabilidad. O sea, otra vez tengo que mover probabilidad. Cuando los corpus son muy, muy, muy grandes, una forma alternativa y es un m\u00e9todo muy nuevo, se llama STUPID BACOV, que es, como mi cuerpo es muy grande, t\u00edpicamente el cuerpo de Google, no normalizo nada las probabilidades, conteo nom\u00e1s como me fu\u00e9 y ya est\u00e1. Si una no me da pruebo con la anterior, si es igual tengo un mont\u00f3n de edad. O tambi\u00e9n se puede hacer interpolaci\u00f3n, es decir, la probabilidad de una palabra dada a las dos anteriores es la probabilidad de la palabra, la probabilidad nueva, es la probabilidad original de la palabra dada a las dos anteriores por un cierto lambda, m\u00e1s un cierto lambda 2 por la probabilidad de la palabra dada solo en el bigrama, m\u00e1s la probabilidad del unigrama. Y combino las tres a la vez, es como combino las tres t\u00e9cnicas a la vez, \u00bfde acuerdo? Es decir, le doy un cierto peso a las probabilidades que yo quiero. De esta forma, porque ac\u00e1 podr\u00eda ser que existiera el bigrama anterior, pero existiera una vez sola, entonces yo no le tengo mucha confianza a esa. Puedes usarme y no le tenga mucha confianza, entonces le doy un cierto peso a este tambi\u00e9n, capaz que le doy un poquito m\u00e1s alto a este. O sea, el 7 existe, est\u00e1 todo bien, pero este siempre me ayuda. Y de esa forma va balanceo. \u00bfC\u00f3mo calculo estos lambda y con el cuerpo de... tengo que de alguna forma calcularlo sobre el cuerpo de desarrollo o el cuerpo gel dado? Tambi\u00e9n hay interpolaci\u00f3n condicionada por el contexto, o sea, hay un lambda, ac\u00e1 ya lo que pasa es un poco m\u00e1s raro, y un poco m\u00e1s moderno. Digamos que es que m\u00e1s de estas \u00e9pocas, digamos, donde a m\u00ed ya no me preocupa tanto tener muchos par\u00e1metros. Ac\u00e1 estoy definiendo un par\u00e1metro para cada combinaci\u00f3n de palabras. Y hasta aqu\u00ed llegamos hoy. Esto es el... es este cap\u00edtulo que tengo ac\u00e1, cap\u00edtulo 4 del libro de Juraski. Tiene algunas cositas m\u00e1s, pero esencialmente es eso. Y es lo que vamos a hablar de en este curso de Enigrama. La clase que viene presentamos laboratorio.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 26.68, "text": " En la clase de hoy, vamos a ver un tema nuevo que es el de los modelos de lenguaje.", "tokens": [50364, 2193, 635, 44578, 368, 13775, 11, 5295, 257, 1306, 517, 15854, 18591, 631, 785, 806, 368, 1750, 2316, 329, 368, 35044, 84, 11153, 13, 51698], "temperature": 0.0, "avg_logprob": -0.27992492062704905, "compression_ratio": 1.1066666666666667, "no_speech_prob": 0.08749765902757645}, {"id": 1, "seek": 2668, "start": 27.68, "end": 35.68, "text": " Si se acuerdan a la clase pasada, vimos dos temas que eran bastante diferentes.", "tokens": [50414, 4909, 369, 696, 5486, 10312, 257, 635, 44578, 1736, 1538, 11, 49266, 4491, 40284, 631, 32762, 14651, 17686, 13, 50814], "temperature": 0.0, "avg_logprob": -0.29610208783830916, "compression_ratio": 1.5088757396449703, "no_speech_prob": 0.43126651644706726}, {"id": 2, "seek": 2668, "start": 35.68, "end": 44.68, "text": " El de los traductores para resolver el tema de la morfolog\u00eda de estado finito, unos artefactos", "tokens": [50814, 2699, 368, 1750, 2479, 11130, 2706, 1690, 34480, 806, 15854, 368, 635, 1896, 69, 29987, 368, 18372, 962, 3528, 11, 17780, 29159, 44919, 329, 51264], "temperature": 0.0, "avg_logprob": -0.29610208783830916, "compression_ratio": 1.5088757396449703, "no_speech_prob": 0.43126651644706726}, {"id": 3, "seek": 2668, "start": 44.68, "end": 54.68, "text": " de estado finito que permiten resolver temas a trav\u00e9s de un m\u00e9todo de reglas.", "tokens": [51264, 368, 18372, 962, 3528, 631, 13423, 268, 34480, 40284, 257, 24463, 368, 517, 20275, 17423, 368, 1121, 7743, 13, 51764], "temperature": 0.0, "avg_logprob": -0.29610208783830916, "compression_ratio": 1.5088757396449703, "no_speech_prob": 0.43126651644706726}, {"id": 4, "seek": 5468, "start": 54.68, "end": 70.68, "text": " Y de esa forma resuelvo el tema de convertir de la palabra a su an\u00e1lisis y viceversa.", "tokens": [50364, 398, 368, 11342, 8366, 725, 3483, 3080, 806, 15854, 368, 7620, 347, 368, 635, 31702, 257, 459, 44113, 28436, 288, 11964, 840, 64, 13, 51164], "temperature": 0.0, "avg_logprob": -0.24408481545644264, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.449770987033844}, {"id": 5, "seek": 5468, "start": 70.68, "end": 74.68, "text": " En la segunda parte vimos un m\u00e9todo que era bastante diferente de su concepci\u00f3n,", "tokens": [51164, 2193, 635, 21978, 6975, 49266, 517, 20275, 17423, 631, 4249, 14651, 20973, 368, 459, 10413, 39859, 11, 51364], "temperature": 0.0, "avg_logprob": -0.24408481545644264, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.449770987033844}, {"id": 6, "seek": 5468, "start": 74.68, "end": 81.68, "text": " que es su m\u00e9todo estad\u00edstico, que lo que hace era aplicando el modelo del canal ruidoso,", "tokens": [51364, 631, 785, 459, 20275, 17423, 39160, 19512, 2789, 11, 631, 450, 631, 10032, 4249, 18221, 1806, 806, 27825, 1103, 9911, 5420, 7895, 78, 11, 51714], "temperature": 0.0, "avg_logprob": -0.24408481545644264, "compression_ratio": 1.4606741573033708, "no_speech_prob": 0.449770987033844}, {"id": 7, "seek": 8168, "start": 81.68, "end": 87.68, "text": " aproximarse al problema de corregir errores ortogr\u00e1ficos.", "tokens": [50364, 31270, 11668, 419, 12395, 368, 1181, 3375, 347, 45935, 495, 23564, 47810, 23858, 329, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09424101829528808, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.15466344356536865}, {"id": 8, "seek": 8168, "start": 87.68, "end": 95.68, "text": " Cuando yo hablo de un modelo probabilista, lo que estoy diciendo es que adem\u00e1s de, por ejemplo,", "tokens": [50664, 21907, 5290, 3025, 752, 368, 517, 27825, 31959, 5236, 11, 450, 631, 15796, 42797, 785, 631, 21251, 368, 11, 1515, 13358, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09424101829528808, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.15466344356536865}, {"id": 9, "seek": 8168, "start": 95.68, "end": 103.68, "text": " clasificar o sugerir una soluci\u00f3n, lo que hace es asignarle probabilidades a las posibles respuestas.", "tokens": [51064, 596, 296, 25625, 277, 459, 1321, 347, 2002, 24807, 5687, 11, 450, 631, 10032, 785, 382, 788, 36153, 31959, 10284, 257, 2439, 1366, 14428, 1597, 47794, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09424101829528808, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.15466344356536865}, {"id": 10, "seek": 8168, "start": 103.68, "end": 110.68, "text": " Un m\u00e9todo probabilista, t\u00edpicamente no da una respuesta, sino que devuelve una distribuci\u00f3n de probabilidad.", "tokens": [51464, 1156, 20275, 17423, 31959, 5236, 11, 256, 28236, 23653, 572, 1120, 2002, 40585, 11, 18108, 631, 1905, 3483, 303, 2002, 4400, 30813, 368, 31959, 4580, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09424101829528808, "compression_ratio": 1.608695652173913, "no_speech_prob": 0.15466344356536865}, {"id": 11, "seek": 11068, "start": 110.68, "end": 120.68, "text": " Es decir, si yo tengo varios eventos posibles, una distribuci\u00f3n de probabilidad es un n\u00famero", "tokens": [50364, 2313, 10235, 11, 1511, 5290, 13989, 33665, 2280, 329, 1366, 14428, 11, 2002, 4400, 30813, 368, 31959, 4580, 785, 517, 14959, 50864], "temperature": 0.0, "avg_logprob": -0.10690498352050781, "compression_ratio": 1.9558823529411764, "no_speech_prob": 0.028123706579208374}, {"id": 12, "seek": 11068, "start": 120.68, "end": 128.68, "text": " entre 0 y 1 que yo asigno a cada evento posible, de forma que la suma de todos los eventos dan 1,", "tokens": [50864, 3962, 1958, 288, 502, 631, 5290, 382, 788, 78, 257, 8411, 40655, 26644, 11, 368, 8366, 631, 635, 2408, 64, 368, 6321, 1750, 2280, 329, 3277, 502, 11, 51264], "temperature": 0.0, "avg_logprob": -0.10690498352050781, "compression_ratio": 1.9558823529411764, "no_speech_prob": 0.028123706579208374}, {"id": 13, "seek": 11068, "start": 128.68, "end": 130.68, "text": " eso es lo que llamamos una distribuci\u00f3n de probabilidad.", "tokens": [51264, 7287, 785, 450, 631, 16848, 2151, 2002, 4400, 30813, 368, 31959, 4580, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10690498352050781, "compression_ratio": 1.9558823529411764, "no_speech_prob": 0.028123706579208374}, {"id": 14, "seek": 11068, "start": 130.68, "end": 134.68, "text": " Entre 0 y 1 son todos, son todos mayores o iguales que 0, menores y iguales que 1,", "tokens": [51364, 27979, 1958, 288, 502, 1872, 6321, 11, 1872, 6321, 815, 2706, 277, 10953, 279, 631, 1958, 11, 1706, 2706, 288, 10953, 279, 631, 502, 11, 51564], "temperature": 0.0, "avg_logprob": -0.10690498352050781, "compression_ratio": 1.9558823529411764, "no_speech_prob": 0.028123706579208374}, {"id": 15, "seek": 11068, "start": 134.68, "end": 137.68, "text": " y adem\u00e1s su suma da 1, eso es una distribuci\u00f3n de probabilidad.", "tokens": [51564, 288, 21251, 459, 2408, 64, 1120, 502, 11, 7287, 785, 2002, 4400, 30813, 368, 31959, 4580, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10690498352050781, "compression_ratio": 1.9558823529411764, "no_speech_prob": 0.028123706579208374}, {"id": 16, "seek": 13768, "start": 137.68, "end": 140.68, "text": " 0, 5, 0, 25, 0, 25 es una distribuci\u00f3n de probabilidad.", "tokens": [50364, 1958, 11, 1025, 11, 1958, 11, 3552, 11, 1958, 11, 3552, 785, 2002, 4400, 30813, 368, 31959, 4580, 13, 50514], "temperature": 0.2, "avg_logprob": -0.12450327524324743, "compression_ratio": 2.119565217391304, "no_speech_prob": 0.25018227100372314}, {"id": 17, "seek": 13768, "start": 140.68, "end": 146.68, "text": " Si el evento 1 tiene probabilidad 0, 5, el otro 0, 25, y el otro 0, 25, eso es una distribuci\u00f3n de probabilidad.", "tokens": [50514, 4909, 806, 40655, 502, 7066, 31959, 4580, 1958, 11, 1025, 11, 806, 11921, 1958, 11, 3552, 11, 288, 806, 11921, 1958, 11, 3552, 11, 7287, 785, 2002, 4400, 30813, 368, 31959, 4580, 13, 50814], "temperature": 0.2, "avg_logprob": -0.12450327524324743, "compression_ratio": 2.119565217391304, "no_speech_prob": 0.25018227100372314}, {"id": 18, "seek": 13768, "start": 146.68, "end": 150.68, "text": " Si no suma a un 1, no son una distribuci\u00f3n de probabilidad.", "tokens": [50814, 4909, 572, 2408, 64, 257, 517, 502, 11, 572, 1872, 2002, 4400, 30813, 368, 31959, 4580, 13, 51014], "temperature": 0.2, "avg_logprob": -0.12450327524324743, "compression_ratio": 2.119565217391304, "no_speech_prob": 0.25018227100372314}, {"id": 19, "seek": 13768, "start": 150.68, "end": 157.68, "text": " Y si yo, por ejemplo, tengo un evento que ocurre 10 veces, si por ejemplo hago conteo de frecuencia,", "tokens": [51014, 398, 1511, 5290, 11, 1515, 13358, 11, 13989, 517, 40655, 631, 26430, 265, 1266, 17054, 11, 1511, 1515, 13358, 38721, 34444, 78, 368, 2130, 66, 47377, 11, 51364], "temperature": 0.2, "avg_logprob": -0.12450327524324743, "compression_ratio": 2.119565217391304, "no_speech_prob": 0.25018227100372314}, {"id": 20, "seek": 13768, "start": 157.68, "end": 161.68, "text": " por ejemplo, no digo hay un evento 1 que ocurre 10 veces,", "tokens": [51364, 1515, 13358, 11, 572, 22990, 4842, 517, 40655, 502, 631, 26430, 265, 1266, 17054, 11, 51564], "temperature": 0.2, "avg_logprob": -0.12450327524324743, "compression_ratio": 2.119565217391304, "no_speech_prob": 0.25018227100372314}, {"id": 21, "seek": 16168, "start": 161.68, "end": 168.68, "text": " hay un evento 2 que ocurre 5 y hay un evento 3 que ocurre 5.", "tokens": [50364, 4842, 517, 40655, 568, 631, 26430, 265, 1025, 288, 4842, 517, 40655, 805, 631, 26430, 265, 1025, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12506398158286935, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00785677507519722}, {"id": 22, "seek": 16168, "start": 168.68, "end": 178.68, "text": " Eso no es una distribuci\u00f3n de probabilidad, porque esto no est\u00e1 entre 0 y 1, porque no suman 1.", "tokens": [50714, 27795, 572, 785, 2002, 4400, 30813, 368, 31959, 4580, 11, 4021, 7433, 572, 3192, 3962, 1958, 288, 502, 11, 4021, 572, 2408, 282, 502, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12506398158286935, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00785677507519722}, {"id": 23, "seek": 16168, "start": 178.68, "end": 185.68, "text": " \u00bfC\u00f3mo hago yo para convertir esto en una distribuci\u00f3n de probabilidad?", "tokens": [51214, 3841, 28342, 38721, 5290, 1690, 7620, 347, 7433, 465, 2002, 4400, 30813, 368, 31959, 4580, 30, 51564], "temperature": 0.0, "avg_logprob": -0.12506398158286935, "compression_ratio": 1.5675675675675675, "no_speech_prob": 0.00785677507519722}, {"id": 24, "seek": 18568, "start": 186.68, "end": 191.68, "text": " Lo que hago es dividir por el total de ocurrencias, \u00bfverdad?", "tokens": [50414, 6130, 631, 38721, 785, 4996, 347, 1515, 806, 3217, 368, 26430, 1095, 12046, 11, 3841, 331, 20034, 30, 50664], "temperature": 0.0, "avg_logprob": -0.11863091412712545, "compression_ratio": 1.4036144578313252, "no_speech_prob": 0.009600064717233181}, {"id": 25, "seek": 18568, "start": 191.68, "end": 202.68, "text": " Que en este caso es 20 y eso me da la proporci\u00f3n respecto a 1,", "tokens": [50664, 4493, 465, 4065, 9666, 785, 945, 288, 7287, 385, 1120, 635, 41516, 5687, 35694, 257, 502, 11, 51214], "temperature": 0.0, "avg_logprob": -0.11863091412712545, "compression_ratio": 1.4036144578313252, "no_speech_prob": 0.009600064717233181}, {"id": 26, "seek": 18568, "start": 202.68, "end": 204.68, "text": " y eso es siempre una distribuci\u00f3n de probabilidad.", "tokens": [51214, 288, 7287, 785, 12758, 2002, 4400, 30813, 368, 31959, 4580, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11863091412712545, "compression_ratio": 1.4036144578313252, "no_speech_prob": 0.009600064717233181}, {"id": 27, "seek": 18568, "start": 204.68, "end": 209.68, "text": " Esto se llama normalizar para obtener una probabilidad.", "tokens": [51314, 20880, 369, 23272, 2710, 9736, 1690, 28326, 260, 2002, 31959, 4580, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11863091412712545, "compression_ratio": 1.4036144578313252, "no_speech_prob": 0.009600064717233181}, {"id": 28, "seek": 20968, "start": 209.68, "end": 215.68, "text": " Esto ustedes lo van a ver que lo vamos a ver en varias veces.", "tokens": [50364, 20880, 17110, 450, 3161, 257, 1306, 631, 450, 5295, 257, 1306, 465, 37496, 17054, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11907783401346653, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.12603206932544708}, {"id": 29, "seek": 20968, "start": 215.68, "end": 225.68, "text": " El m\u00e9todo de este correcci\u00f3n utilizaba fuertemente la regla de Valles para modelar la situaci\u00f3n.", "tokens": [50664, 2699, 20275, 17423, 368, 4065, 29731, 14735, 19906, 5509, 8536, 911, 16288, 635, 1121, 875, 368, 691, 37927, 1690, 2316, 289, 635, 29343, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11907783401346653, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.12603206932544708}, {"id": 30, "seek": 20968, "start": 225.68, "end": 230.68, "text": " Hasta ahora hemos hablado en todas las cosas que hemos tratado de palabras aisladas.", "tokens": [51164, 45027, 9923, 15396, 26280, 1573, 465, 10906, 2439, 12218, 631, 15396, 21507, 1573, 368, 35240, 257, 5788, 6872, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11907783401346653, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.12603206932544708}, {"id": 31, "seek": 20968, "start": 230.68, "end": 233.68, "text": " La morfolog\u00eda estudia, primero hablamos de c\u00f3mo separar las palabras", "tokens": [51414, 2369, 1896, 69, 29987, 13542, 654, 11, 21289, 26280, 2151, 368, 12826, 3128, 289, 2439, 35240, 51564], "temperature": 0.0, "avg_logprob": -0.11907783401346653, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.12603206932544708}, {"id": 32, "seek": 20968, "start": 233.68, "end": 238.68, "text": " y despu\u00e9s vimos c\u00f3mo analizarla internamente, pero siempre habl\u00e1bamos de palabras aisladas.", "tokens": [51564, 288, 15283, 49266, 12826, 2624, 9736, 875, 2154, 3439, 11, 4768, 12758, 26280, 27879, 2151, 368, 35240, 257, 5788, 6872, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11907783401346653, "compression_ratio": 1.6349206349206349, "no_speech_prob": 0.12603206932544708}, {"id": 33, "seek": 23868, "start": 238.68, "end": 247.68, "text": " Ac\u00e1 lo que vamos a empezar a mirar es qu\u00e9 pasa cuando las palabras aparecen juntas.", "tokens": [50364, 5097, 842, 450, 631, 5295, 257, 31168, 257, 3149, 289, 785, 8057, 20260, 7767, 2439, 35240, 15004, 13037, 22739, 296, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1847332763671875, "compression_ratio": 1.0493827160493827, "no_speech_prob": 0.010617061518132687}, {"id": 34, "seek": 24768, "start": 247.68, "end": 266.68, "text": " Es decir, nosotros lo que vamos a hablar es de la probabilidad de una secuencia de palabras.", "tokens": [50364, 2313, 10235, 11, 13863, 450, 631, 5295, 257, 21014, 785, 368, 635, 31959, 4580, 368, 2002, 907, 47377, 368, 35240, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20657806396484374, "compression_ratio": 1.15, "no_speech_prob": 0.13539081811904907}, {"id": 35, "seek": 26668, "start": 266.68, "end": 275.68, "text": " \u00bfPor qu\u00e9 esto importa?", "tokens": [50364, 3841, 24907, 8057, 7433, 33218, 30, 50814], "temperature": 0.0, "avg_logprob": -0.14003554979960123, "compression_ratio": 1.4382022471910112, "no_speech_prob": 0.11360733956098557}, {"id": 36, "seek": 26668, "start": 275.68, "end": 281.68, "text": " Porque, como ustedes bien sabr\u00e1n, las palabras en el idioma pa\u00f1\u00f3n las aparecen solas.", "tokens": [50814, 11287, 11, 2617, 17110, 3610, 5560, 81, 7200, 11, 2439, 35240, 465, 806, 18014, 6440, 2502, 2791, 1801, 2439, 15004, 13037, 1404, 296, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14003554979960123, "compression_ratio": 1.4382022471910112, "no_speech_prob": 0.11360733956098557}, {"id": 37, "seek": 26668, "start": 281.68, "end": 283.68, "text": " Y no cualquier palabra sigue a otra palabra.", "tokens": [51114, 398, 572, 21004, 31702, 34532, 257, 13623, 31702, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14003554979960123, "compression_ratio": 1.4382022471910112, "no_speech_prob": 0.11360733956098557}, {"id": 38, "seek": 26668, "start": 283.68, "end": 293.68, "text": " Nosotros tenemos una cantidad de reglas para expresar en el idioma que hace que el orden importe.", "tokens": [51214, 18749, 11792, 9914, 2002, 33757, 368, 1121, 7743, 1690, 33397, 289, 465, 806, 18014, 6440, 631, 10032, 631, 806, 28615, 974, 68, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14003554979960123, "compression_ratio": 1.4382022471910112, "no_speech_prob": 0.11360733956098557}, {"id": 39, "seek": 29368, "start": 293.68, "end": 303.68, "text": " Es decir, lo que se trata es de ver c\u00f3mo tener en cuenta ese orden nos puede ayudar a otra estaria.", "tokens": [50364, 2313, 10235, 11, 450, 631, 369, 31920, 785, 368, 1306, 12826, 11640, 465, 17868, 10167, 28615, 3269, 8919, 38759, 257, 13623, 8755, 654, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1956571910692298, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0772147923707962}, {"id": 40, "seek": 29368, "start": 303.68, "end": 306.68, "text": " Creo que con alg\u00fan ejemplo lo vamos a ver m\u00e1s claro.", "tokens": [50864, 40640, 631, 416, 26300, 13358, 450, 5295, 257, 1306, 3573, 16742, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1956571910692298, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0772147923707962}, {"id": 41, "seek": 29368, "start": 306.68, "end": 315.68, "text": " Primero que nada, vamos a recordar a Chonky, que esto yo lo comentaba en la primera clase,", "tokens": [51014, 19671, 2032, 631, 8096, 11, 5295, 257, 2136, 289, 257, 761, 266, 4133, 11, 631, 7433, 5290, 450, 14541, 5509, 465, 635, 17382, 44578, 11, 51464], "temperature": 0.0, "avg_logprob": -0.1956571910692298, "compression_ratio": 1.455621301775148, "no_speech_prob": 0.0772147923707962}, {"id": 42, "seek": 31568, "start": 315.68, "end": 325.68, "text": " aquello de que Chonky dijo la noci\u00f3n de probabilidad de una oraci\u00f3n es completamente in\u00fatil bajo cualquier interpretaci\u00f3n de este t\u00e9rmino.", "tokens": [50364, 2373, 11216, 368, 631, 761, 266, 4133, 27024, 635, 572, 5687, 368, 31959, 4580, 368, 2002, 420, 3482, 785, 28381, 294, 2481, 20007, 30139, 21004, 7302, 3482, 368, 4065, 45198, 78, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2302169215922453, "compression_ratio": 1.537190082644628, "no_speech_prob": 0.7509516477584839}, {"id": 43, "seek": 31568, "start": 325.68, "end": 329.68, "text": " Y tranc\u00f3 por 20 a\u00f1os, la investigaci\u00f3n hasta ac\u00e1 apareci\u00f3.", "tokens": [50864, 398, 504, 4463, 812, 1515, 945, 11424, 11, 635, 48919, 10764, 23496, 15004, 19609, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2302169215922453, "compression_ratio": 1.537190082644628, "no_speech_prob": 0.7509516477584839}, {"id": 44, "seek": 31568, "start": 329.68, "end": 342.68, "text": " Chelline, que volvi\u00f3 a revivir el tema de los m\u00e9todos probabilistas o basados en conteos para aproximarse a los problemas de procesamiento en el lenguaje natural.", "tokens": [51064, 3351, 285, 533, 11, 631, 1996, 4917, 812, 257, 3698, 592, 347, 806, 15854, 368, 1750, 20275, 378, 329, 31959, 14858, 277, 987, 4181, 465, 34444, 329, 1690, 31270, 11668, 257, 1750, 20720, 368, 17565, 16971, 465, 806, 35044, 84, 11153, 3303, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2302169215922453, "compression_ratio": 1.537190082644628, "no_speech_prob": 0.7509516477584839}, {"id": 45, "seek": 34268, "start": 343.68, "end": 350.68, "text": " Chonky lo que dec\u00eda esencialmente es cuando nosotros lo hacemos con teo y sacamos conclusi\u00f3n en base a cuenta, en base a n\u00fameros, en base a experiencia,", "tokens": [50414, 761, 266, 4133, 450, 631, 37599, 785, 26567, 4082, 785, 7767, 13863, 450, 33839, 416, 535, 78, 288, 4899, 2151, 18646, 2560, 465, 3096, 257, 17868, 11, 465, 3096, 257, 36545, 11, 465, 3096, 257, 36489, 11, 50764], "temperature": 0.0, "avg_logprob": -0.16942739880774632, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.03288542851805687}, {"id": 46, "seek": 34268, "start": 350.68, "end": 353.68, "text": " que es t\u00edpicamente lo que vamos a ver en este caso de los enegramas,", "tokens": [50764, 631, 785, 256, 28236, 23653, 450, 631, 5295, 257, 1306, 465, 4065, 9666, 368, 1750, 465, 1146, 2356, 296, 11, 50914], "temperature": 0.0, "avg_logprob": -0.16942739880774632, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.03288542851805687}, {"id": 47, "seek": 34268, "start": 353.68, "end": 358.68, "text": " estamos obteniendo soluciones a problemas, pero no estamos entendiendo qu\u00e9 es lo que est\u00e1 pasando.", "tokens": [50914, 10382, 28326, 7304, 1404, 46649, 257, 20720, 11, 4768, 572, 10382, 16612, 7304, 8057, 785, 450, 631, 3192, 45412, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16942739880774632, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.03288542851805687}, {"id": 48, "seek": 34268, "start": 358.68, "end": 360.68, "text": " Y eso es una discusi\u00f3n catalida de hoy s\u00ed.", "tokens": [51164, 398, 7287, 785, 2002, 717, 1149, 2560, 13192, 2887, 368, 13775, 8600, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16942739880774632, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.03288542851805687}, {"id": 49, "seek": 34268, "start": 360.68, "end": 366.68, "text": " Es decir, hay una famosa discusi\u00f3n por ah\u00ed en internet entre Chonky,", "tokens": [51264, 2313, 10235, 11, 4842, 2002, 1087, 6447, 717, 1149, 2560, 1515, 12571, 465, 4705, 3962, 761, 266, 4133, 11, 51564], "temperature": 0.0, "avg_logprob": -0.16942739880774632, "compression_ratio": 1.7065637065637065, "no_speech_prob": 0.03288542851805687}, {"id": 50, "seek": 36668, "start": 366.68, "end": 372.68, "text": " esto te hablando hace dos o tres a\u00f1os, o cinco a\u00f1os, entre Chonky y Peter Norby,", "tokens": [50364, 7433, 535, 29369, 10032, 4491, 277, 15890, 11424, 11, 277, 21350, 11424, 11, 3962, 761, 266, 4133, 288, 6508, 6966, 2322, 11, 50664], "temperature": 0.0, "avg_logprob": -0.189453125, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.03792450577020645}, {"id": 51, "seek": 36668, "start": 375.68, "end": 377.68, "text": " que discute un poco esto.", "tokens": [50814, 631, 2983, 1169, 517, 10639, 7433, 13, 50914], "temperature": 0.0, "avg_logprob": -0.189453125, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.03792450577020645}, {"id": 52, "seek": 36668, "start": 377.68, "end": 386.68, "text": " Es decir, si esto que estamos haciendo ahora y que ha tenido tan buenos resultados desde el punto de vista de reconocimiento del habla y el procesamiento del lenguaje natural", "tokens": [50914, 2313, 10235, 11, 1511, 7433, 631, 10382, 20509, 9923, 288, 631, 324, 33104, 7603, 49617, 36796, 10188, 806, 14326, 368, 22553, 368, 43838, 14007, 1103, 42135, 288, 806, 17565, 16971, 1103, 35044, 84, 11153, 3303, 51364], "temperature": 0.0, "avg_logprob": -0.189453125, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.03792450577020645}, {"id": 53, "seek": 36668, "start": 386.68, "end": 391.68, "text": " es en realidad inteligencia artificial o es solamente number crunching que no nos aporta mucho.", "tokens": [51364, 785, 465, 25635, 24777, 3213, 2755, 11677, 277, 785, 27814, 1230, 13386, 278, 631, 572, 3269, 1882, 36403, 9824, 13, 51614], "temperature": 0.0, "avg_logprob": -0.189453125, "compression_ratio": 1.5857740585774058, "no_speech_prob": 0.03792450577020645}, {"id": 54, "seek": 39168, "start": 392.68, "end": 396.68, "text": " Norby lo que le dice es bueno, de hecho la ciencia es siempre m\u00e1s o menos funcionada as\u00ed.", "tokens": [50414, 6966, 2322, 450, 631, 476, 10313, 785, 11974, 11, 368, 13064, 635, 269, 30592, 785, 12758, 3573, 277, 8902, 14186, 1538, 8582, 13, 50614], "temperature": 0.0, "avg_logprob": -0.19709760006343094, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0028853679541498423}, {"id": 55, "seek": 39168, "start": 399.68, "end": 402.68, "text": " Bueno, entonces \u00bfcu\u00e1l es el objetivo de lo que vamos a hablar ac\u00e1?", "tokens": [50764, 16046, 11, 13003, 3841, 12032, 11447, 785, 806, 29809, 368, 450, 631, 5295, 257, 21014, 23496, 30, 50914], "temperature": 0.0, "avg_logprob": -0.19709760006343094, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0028853679541498423}, {"id": 56, "seek": 39168, "start": 402.68, "end": 403.68, "text": " Son de modelos del lenguaje.", "tokens": [50914, 5185, 368, 2316, 329, 1103, 35044, 84, 11153, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19709760006343094, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0028853679541498423}, {"id": 57, "seek": 39168, "start": 403.68, "end": 408.68, "text": " El objetivo del modelo del lenguaje es calcular la probabilidad de una secuencia de palabra.", "tokens": [50964, 2699, 29809, 1103, 27825, 1103, 35044, 84, 11153, 785, 2104, 17792, 635, 31959, 4580, 368, 2002, 907, 47377, 368, 31702, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19709760006343094, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0028853679541498423}, {"id": 58, "seek": 39168, "start": 409.68, "end": 415.68, "text": " Es decir, qu\u00e9 tan probable es en mil lenguajes que una secuencia se d\u00e9.", "tokens": [51264, 2313, 10235, 11, 8057, 7603, 21759, 785, 465, 1962, 35044, 84, 29362, 631, 2002, 907, 47377, 369, 2795, 13, 51564], "temperature": 0.0, "avg_logprob": -0.19709760006343094, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0028853679541498423}, {"id": 59, "seek": 39168, "start": 416.68, "end": 417.68, "text": " \u00bfDe acuerdo?", "tokens": [51614, 3841, 11089, 28113, 30, 51664], "temperature": 0.0, "avg_logprob": -0.19709760006343094, "compression_ratio": 1.5991379310344827, "no_speech_prob": 0.0028853679541498423}, {"id": 60, "seek": 41768, "start": 418.68, "end": 420.68, "text": " \u00bfPara qu\u00e9 nos puede servir eso?", "tokens": [50414, 3841, 47, 2419, 8057, 3269, 8919, 29463, 7287, 30, 50514], "temperature": 0.0, "avg_logprob": -0.207788909353861, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.003092232160270214}, {"id": 61, "seek": 41768, "start": 420.68, "end": 427.68, "text": " Bueno, imag\u00ednense ustedes que, y ac\u00e1 vamos a recordar otra vez el modelo del canal ruidoso, de la otra vez,", "tokens": [50514, 16046, 11, 2576, 10973, 1288, 17110, 631, 11, 288, 23496, 5295, 257, 2136, 289, 13623, 5715, 806, 27825, 1103, 9911, 5420, 7895, 78, 11, 368, 635, 13623, 5715, 11, 50864], "temperature": 0.0, "avg_logprob": -0.207788909353861, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.003092232160270214}, {"id": 62, "seek": 41768, "start": 427.68, "end": 438.68, "text": " imag\u00ednense ustedes que tengo este texto escrito y por medio de un m\u00e9todo que no s\u00e9 cu\u00e1l es.", "tokens": [50864, 2576, 10973, 1288, 17110, 631, 13989, 4065, 35503, 49451, 288, 1515, 22123, 368, 517, 20275, 17423, 631, 572, 7910, 44318, 785, 13, 51414], "temperature": 0.0, "avg_logprob": -0.207788909353861, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.003092232160270214}, {"id": 63, "seek": 41768, "start": 439.68, "end": 444.68, "text": " Tengo dos oraciones candidatas, \u00bfde acuerdo?", "tokens": [51464, 314, 30362, 4491, 420, 9188, 6268, 37892, 11, 3841, 1479, 28113, 30, 51714], "temperature": 0.0, "avg_logprob": -0.207788909353861, "compression_ratio": 1.4766839378238341, "no_speech_prob": 0.003092232160270214}, {"id": 64, "seek": 44468, "start": 444.68, "end": 445.68, "text": " Dos textos candidatos.", "tokens": [50364, 33474, 2487, 329, 6268, 26818, 13, 50414], "temperature": 0.0, "avg_logprob": -0.13583655637853287, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.0016579164657741785}, {"id": 65, "seek": 44468, "start": 448.68, "end": 454.68, "text": " Uno que es PRNEVA para el curso de PLN y PREVA para el curso de PLN.", "tokens": [50564, 37468, 631, 785, 11568, 15988, 20914, 1690, 806, 31085, 368, 6999, 45, 288, 11568, 36, 20914, 1690, 806, 31085, 368, 6999, 45, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13583655637853287, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.0016579164657741785}, {"id": 66, "seek": 44468, "start": 455.68, "end": 456.68, "text": " \u00bfDe acuerdo?", "tokens": [50914, 3841, 11089, 28113, 30, 50964], "temperature": 0.0, "avg_logprob": -0.13583655637853287, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.0016579164657741785}, {"id": 67, "seek": 44468, "start": 456.68, "end": 464.68, "text": " Y adem\u00e1s supongamos que el m\u00e9todo que utilic\u00e9 para reconocer la escritura", "tokens": [50964, 398, 21251, 9331, 556, 2151, 631, 806, 20275, 17423, 631, 4976, 299, 526, 1690, 43838, 260, 635, 4721, 3210, 2991, 51364], "temperature": 0.0, "avg_logprob": -0.13583655637853287, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.0016579164657741785}, {"id": 68, "seek": 44468, "start": 465.68, "end": 467.68, "text": " me dice que este es m\u00e1s probable que este.", "tokens": [51414, 385, 10313, 631, 4065, 785, 3573, 21759, 631, 4065, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13583655637853287, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.0016579164657741785}, {"id": 69, "seek": 44468, "start": 469.68, "end": 470.68, "text": " \u00bfCu\u00e1l vamos a elegir?", "tokens": [51614, 3841, 35222, 11447, 5295, 257, 14459, 347, 30, 51664], "temperature": 0.0, "avg_logprob": -0.13583655637853287, "compression_ratio": 1.4367816091954022, "no_speech_prob": 0.0016579164657741785}, {"id": 70, "seek": 47068, "start": 470.68, "end": 471.68, "text": " \u00bfCu\u00e1l vamos a elegir?", "tokens": [50364, 3841, 35222, 11447, 5295, 257, 14459, 347, 30, 50414], "temperature": 0.0, "avg_logprob": -0.1707428757862378, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.003376181935891509}, {"id": 71, "seek": 47068, "start": 473.68, "end": 474.68, "text": " Vamos a elegir el de abajo.", "tokens": [50514, 10894, 257, 14459, 347, 806, 368, 30613, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1707428757862378, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.003376181935891509}, {"id": 72, "seek": 47068, "start": 477.68, "end": 478.68, "text": " \u00bfPor qu\u00e9?", "tokens": [50714, 3841, 24907, 8057, 30, 50764], "temperature": 0.0, "avg_logprob": -0.1707428757862378, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.003376181935891509}, {"id": 73, "seek": 47068, "start": 478.68, "end": 480.68, "text": " \u00bfPor qu\u00e9 esto no es una palabra v\u00e1lida?", "tokens": [50764, 3841, 24907, 8057, 7433, 572, 785, 2002, 31702, 371, 11447, 2887, 30, 50864], "temperature": 0.0, "avg_logprob": -0.1707428757862378, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.003376181935891509}, {"id": 74, "seek": 47068, "start": 481.68, "end": 487.68, "text": " Pero a\u00fan siendo una palabra v\u00e1lida, o a\u00fan suponiendo que fuera una palabra v\u00e1lida,", "tokens": [50914, 9377, 31676, 31423, 2002, 31702, 371, 11447, 2887, 11, 277, 31676, 9331, 266, 7304, 631, 24818, 2002, 31702, 371, 11447, 2887, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1707428757862378, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.003376181935891509}, {"id": 75, "seek": 47068, "start": 489.68, "end": 492.68, "text": " podr\u00eda darse un caso donde yo identifico una palabra v\u00e1lida, \u00bfse acuerdan lo correcci\u00f3n?", "tokens": [51314, 27246, 4072, 405, 517, 9666, 10488, 5290, 49456, 78, 2002, 31702, 371, 11447, 2887, 11, 3841, 405, 696, 5486, 10312, 450, 1181, 13867, 5687, 30, 51464], "temperature": 0.0, "avg_logprob": -0.1707428757862378, "compression_ratio": 1.653179190751445, "no_speech_prob": 0.003376181935891509}, {"id": 76, "seek": 49268, "start": 493.68, "end": 496.68, "text": " A\u00fan as\u00ed, yo podr\u00eda decir, bueno, pero en este lugar,", "tokens": [50414, 316, 9453, 8582, 11, 5290, 27246, 10235, 11, 11974, 11, 4768, 465, 4065, 11467, 11, 50564], "temperature": 0.0, "avg_logprob": -0.1656327928815569, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.09075698256492615}, {"id": 77, "seek": 49268, "start": 501.68, "end": 504.68, "text": " esa palabra no calza, digan.", "tokens": [50814, 11342, 31702, 572, 2104, 2394, 11, 2528, 282, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1656327928815569, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.09075698256492615}, {"id": 78, "seek": 49268, "start": 505.68, "end": 506.68, "text": " Si de alguna forma yo s\u00e9.", "tokens": [51014, 4909, 368, 20651, 8366, 5290, 7910, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1656327928815569, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.09075698256492615}, {"id": 79, "seek": 49268, "start": 507.68, "end": 511.68, "text": " Es decir, si yo logro detectar que esta oraci\u00f3n es m\u00e1s probable que esta,", "tokens": [51114, 2313, 10235, 11, 1511, 5290, 3565, 340, 5531, 289, 631, 5283, 420, 3482, 785, 3573, 21759, 631, 5283, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1656327928815569, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.09075698256492615}, {"id": 80, "seek": 49268, "start": 511.68, "end": 515.6800000000001, "text": " de alguna forma, eso me va a ayudar en la tarea de reconocimiento.", "tokens": [51314, 368, 20651, 8366, 11, 7287, 385, 2773, 257, 38759, 465, 635, 256, 35425, 368, 43838, 14007, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1656327928815569, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.09075698256492615}, {"id": 81, "seek": 49268, "start": 516.6800000000001, "end": 518.6800000000001, "text": " Lo mismo pasa con el reconocimiento del habla,", "tokens": [51564, 6130, 12461, 20260, 416, 806, 43838, 14007, 1103, 42135, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1656327928815569, "compression_ratio": 1.4975124378109452, "no_speech_prob": 0.09075698256492615}, {"id": 82, "seek": 51868, "start": 518.68, "end": 521.68, "text": " de lo que hablamos el otro d\u00eda con el speed recognition,", "tokens": [50364, 368, 450, 631, 26280, 2151, 806, 11921, 12271, 416, 806, 3073, 11150, 11, 50514], "temperature": 0.0, "avg_logprob": -0.17372554110497543, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0032791716512292624}, {"id": 83, "seek": 51868, "start": 521.68, "end": 523.68, "text": " y cuando yo hablo y digo una palabra, ustedes me escuchan.", "tokens": [50514, 288, 7767, 5290, 3025, 752, 288, 22990, 2002, 31702, 11, 17110, 385, 22483, 282, 13, 50614], "temperature": 0.0, "avg_logprob": -0.17372554110497543, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0032791716512292624}, {"id": 84, "seek": 51868, "start": 525.68, "end": 529.68, "text": " Entonces, los modelos de lenguaje sirven para ayudar en este tipo de tarea.", "tokens": [50714, 15097, 11, 1750, 2316, 329, 368, 35044, 84, 11153, 4735, 553, 1690, 38759, 465, 4065, 9746, 368, 256, 35425, 13, 50914], "temperature": 0.0, "avg_logprob": -0.17372554110497543, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0032791716512292624}, {"id": 85, "seek": 51868, "start": 529.68, "end": 532.68, "text": " T\u00edpicamente los modelos de lenguaje ayudan en otra tarea.", "tokens": [50914, 314, 28236, 23653, 1750, 2316, 329, 368, 35044, 84, 11153, 20333, 282, 465, 13623, 256, 35425, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17372554110497543, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0032791716512292624}, {"id": 86, "seek": 51868, "start": 534.68, "end": 535.68, "text": " Nos agregan mucha informaci\u00f3n.", "tokens": [51164, 18749, 4554, 1275, 25248, 21660, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17372554110497543, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0032791716512292624}, {"id": 87, "seek": 51868, "start": 538.68, "end": 542.68, "text": " Entonces, cuando nosotros hacemos reconocimiento de escritura,", "tokens": [51364, 15097, 11, 7767, 13863, 33839, 43838, 14007, 368, 4721, 3210, 2991, 11, 51564], "temperature": 0.0, "avg_logprob": -0.17372554110497543, "compression_ratio": 1.5944700460829493, "no_speech_prob": 0.0032791716512292624}, {"id": 88, "seek": 54268, "start": 543.68, "end": 545.68, "text": " un poco lo que decimos es,", "tokens": [50414, 517, 10639, 450, 631, 979, 8372, 785, 11, 50514], "temperature": 0.0, "avg_logprob": -0.1603105375082186, "compression_ratio": 1.975, "no_speech_prob": 0.027522506192326546}, {"id": 89, "seek": 54268, "start": 547.68, "end": 552.68, "text": " \u00bfCu\u00e1l es la probabilidad de la oraci\u00f3n origen dada la observaci\u00f3n que tengo?", "tokens": [50614, 3841, 35222, 11447, 785, 635, 31959, 4580, 368, 635, 420, 3482, 2349, 268, 274, 1538, 635, 9951, 3482, 631, 13989, 30, 50864], "temperature": 0.0, "avg_logprob": -0.1603105375082186, "compression_ratio": 1.975, "no_speech_prob": 0.027522506192326546}, {"id": 90, "seek": 54268, "start": 552.68, "end": 555.68, "text": " Yo tengo una observaci\u00f3n, \u00bfs\u00ed?", "tokens": [50864, 7616, 13989, 2002, 9951, 3482, 11, 3841, 82, 870, 30, 51014], "temperature": 0.0, "avg_logprob": -0.1603105375082186, "compression_ratio": 1.975, "no_speech_prob": 0.027522506192326546}, {"id": 91, "seek": 54268, "start": 556.68, "end": 558.68, "text": " \u00bfCu\u00e1l es la probabilidad de una oraci\u00f3n origen?", "tokens": [51064, 3841, 35222, 11447, 785, 635, 31959, 4580, 368, 2002, 420, 3482, 2349, 268, 30, 51164], "temperature": 0.0, "avg_logprob": -0.1603105375082186, "compression_ratio": 1.975, "no_speech_prob": 0.027522506192326546}, {"id": 92, "seek": 54268, "start": 559.68, "end": 565.68, "text": " Es proporcionar a la probabilidad de la observaci\u00f3n dada la oraci\u00f3n", "tokens": [51214, 2313, 41516, 10015, 289, 257, 635, 31959, 4580, 368, 635, 9951, 3482, 274, 1538, 635, 420, 3482, 51514], "temperature": 0.0, "avg_logprob": -0.1603105375082186, "compression_ratio": 1.975, "no_speech_prob": 0.027522506192326546}, {"id": 93, "seek": 54268, "start": 565.68, "end": 567.68, "text": " por la probabilidad de la oraci\u00f3n.", "tokens": [51514, 1515, 635, 31959, 4580, 368, 635, 420, 3482, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1603105375082186, "compression_ratio": 1.975, "no_speech_prob": 0.027522506192326546}, {"id": 94, "seek": 54268, "start": 568.68, "end": 569.68, "text": " \u00bfY esto qu\u00e9 es?", "tokens": [51664, 3841, 56, 7433, 8057, 785, 30, 51714], "temperature": 0.0, "avg_logprob": -0.1603105375082186, "compression_ratio": 1.975, "no_speech_prob": 0.027522506192326546}, {"id": 95, "seek": 56968, "start": 569.68, "end": 572.68, "text": " Eso es valles, es la regla de valles.", "tokens": [50364, 27795, 785, 371, 37927, 11, 785, 635, 1121, 875, 368, 371, 37927, 13, 50514], "temperature": 0.0, "avg_logprob": -0.20764039597421322, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.005076561588793993}, {"id": 96, "seek": 56968, "start": 573.68, "end": 576.68, "text": " Entonces, nosotros por valles sabemos eso,", "tokens": [50564, 15097, 11, 13863, 1515, 371, 37927, 27200, 7287, 11, 50714], "temperature": 0.0, "avg_logprob": -0.20764039597421322, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.005076561588793993}, {"id": 97, "seek": 56968, "start": 576.68, "end": 580.68, "text": " y como ven, ac\u00e1 aparece la noci\u00f3n de probabilidad de la oraci\u00f3n.", "tokens": [50714, 288, 2617, 6138, 11, 23496, 37863, 635, 572, 5687, 368, 31959, 4580, 368, 635, 420, 3482, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20764039597421322, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.005076561588793993}, {"id": 98, "seek": 56968, "start": 580.68, "end": 583.68, "text": " Por eso es que nos interesa conocer la probabilidad de la oraci\u00f3n.", "tokens": [50914, 5269, 7287, 785, 631, 3269, 728, 13708, 35241, 635, 31959, 4580, 368, 635, 420, 3482, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20764039597421322, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.005076561588793993}, {"id": 99, "seek": 56968, "start": 586.68, "end": 590.68, "text": " Ahora, \u00bfC\u00f3mo calculamos la probabilidad de la oraci\u00f3n?", "tokens": [51214, 18840, 11, 3841, 28342, 4322, 2151, 635, 31959, 4580, 368, 635, 420, 3482, 30, 51414], "temperature": 0.0, "avg_logprob": -0.20764039597421322, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.005076561588793993}, {"id": 100, "seek": 56968, "start": 591.68, "end": 593.68, "text": " Bueno, hay alg\u00fan ejemplo m\u00e1s, \u00bfno?", "tokens": [51464, 16046, 11, 4842, 26300, 13358, 3573, 11, 3841, 1771, 30, 51564], "temperature": 0.0, "avg_logprob": -0.20764039597421322, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.005076561588793993}, {"id": 101, "seek": 56968, "start": 594.68, "end": 596.68, "text": " Por ejemplo, en la traducci\u00f3n auton\u00e1tica,", "tokens": [51614, 5269, 13358, 11, 465, 635, 2479, 1311, 5687, 1476, 266, 23432, 11, 51714], "temperature": 0.0, "avg_logprob": -0.20764039597421322, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.005076561588793993}, {"id": 102, "seek": 59968, "start": 599.68, "end": 603.68, "text": " en la traducci\u00f3n auton\u00e1tica, si tenemos estos tres candidatos,", "tokens": [50364, 465, 635, 2479, 1311, 5687, 1476, 266, 23432, 11, 1511, 9914, 12585, 15890, 6268, 26818, 11, 50564], "temperature": 0.0, "avg_logprob": -0.13835431159810818, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.000602104002609849}, {"id": 103, "seek": 59968, "start": 603.68, "end": 606.68, "text": " nuevamente a m\u00ed me va a ayudar conocer el orden", "tokens": [50564, 10412, 85, 3439, 257, 14692, 385, 2773, 257, 38759, 35241, 806, 28615, 50714], "temperature": 0.0, "avg_logprob": -0.13835431159810818, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.000602104002609849}, {"id": 104, "seek": 59968, "start": 607.68, "end": 610.68, "text": " o saber cu\u00e1l es la m\u00e1s probable en mi lenguaje.", "tokens": [50764, 277, 12489, 44318, 785, 635, 3573, 21759, 465, 2752, 35044, 84, 11153, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13835431159810818, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.000602104002609849}, {"id": 105, "seek": 59968, "start": 616.68, "end": 619.68, "text": " En la correcci\u00f3n de errores, como vimos en la vez pasada,", "tokens": [51214, 2193, 635, 1181, 13867, 5687, 368, 45935, 495, 11, 2617, 49266, 465, 635, 5715, 1736, 1538, 11, 51364], "temperature": 0.0, "avg_logprob": -0.13835431159810818, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.000602104002609849}, {"id": 106, "seek": 59968, "start": 620.68, "end": 624.68, "text": " hordas de botero es una secuencia muy de poca probabilidad,", "tokens": [51414, 276, 765, 296, 368, 10592, 2032, 785, 2002, 907, 47377, 5323, 368, 714, 496, 31959, 4580, 11, 51614], "temperature": 0.0, "avg_logprob": -0.13835431159810818, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.000602104002609849}, {"id": 107, "seek": 59968, "start": 624.68, "end": 626.68, "text": " y pensemos un poquito,", "tokens": [51614, 288, 11209, 3415, 517, 28229, 11, 51714], "temperature": 0.0, "avg_logprob": -0.13835431159810818, "compression_ratio": 1.4878048780487805, "no_speech_prob": 0.000602104002609849}, {"id": 108, "seek": 62968, "start": 630.68, "end": 632.68, "text": " \u00bfPreguntemos no?", "tokens": [50414, 3841, 47, 3375, 2760, 4485, 572, 30, 50514], "temperature": 0.0, "avg_logprob": -0.20303646353788155, "compression_ratio": 1.3153153153153154, "no_speech_prob": 0.0009206223767250776}, {"id": 109, "seek": 62968, "start": 636.68, "end": 643.68, "text": " \u00bfPor qu\u00e9 esta oraci\u00f3n no les parece que sea muy probable?", "tokens": [50714, 3841, 24907, 8057, 5283, 420, 3482, 572, 1512, 14120, 631, 4158, 5323, 21759, 30, 51064], "temperature": 0.0, "avg_logprob": -0.20303646353788155, "compression_ratio": 1.3153153153153154, "no_speech_prob": 0.0009206223767250776}, {"id": 110, "seek": 62968, "start": 644.68, "end": 649.68, "text": " \u00bfQu\u00e9 nos podr\u00eda determinar que esta oraci\u00f3n no es muy probable?", "tokens": [51114, 3841, 15137, 3269, 27246, 3618, 6470, 631, 5283, 420, 3482, 572, 785, 5323, 21759, 30, 51364], "temperature": 0.0, "avg_logprob": -0.20303646353788155, "compression_ratio": 1.3153153153153154, "no_speech_prob": 0.0009206223767250776}, {"id": 111, "seek": 65968, "start": 659.68, "end": 663.68, "text": " \u00bfO esta? \u00bfImplementaci\u00f3n a la educaci\u00f3n ley?", "tokens": [50364, 3841, 46, 5283, 30, 3841, 31128, 43704, 3482, 257, 635, 48861, 27786, 30, 50564], "temperature": 0.0, "avg_logprob": -0.1894536629701272, "compression_ratio": 1.3631284916201116, "no_speech_prob": 0.011920178309082985}, {"id": 112, "seek": 65968, "start": 664.68, "end": 667.68, "text": " \u00bfPor qu\u00e9 podemos suponer que esa no es probable?", "tokens": [50614, 3841, 24907, 8057, 12234, 9331, 32949, 631, 11342, 572, 785, 21759, 30, 50764], "temperature": 0.0, "avg_logprob": -0.1894536629701272, "compression_ratio": 1.3631284916201116, "no_speech_prob": 0.011920178309082985}, {"id": 113, "seek": 65968, "start": 675.68, "end": 677.68, "text": " Bueno, a m\u00ed se me ocurren dos razones principales,", "tokens": [51164, 16046, 11, 257, 14692, 369, 385, 26430, 1095, 4491, 9639, 2213, 6959, 4229, 11, 51264], "temperature": 0.0, "avg_logprob": -0.1894536629701272, "compression_ratio": 1.3631284916201116, "no_speech_prob": 0.011920178309082985}, {"id": 114, "seek": 65968, "start": 677.68, "end": 680.68, "text": " dos aproximaciones, una es por la sintaxis, \u00bfno?", "tokens": [51264, 4491, 31270, 9188, 11, 2002, 785, 1515, 635, 41259, 24633, 11, 3841, 1771, 30, 51414], "temperature": 0.0, "avg_logprob": -0.1894536629701272, "compression_ratio": 1.3631284916201116, "no_speech_prob": 0.011920178309082985}, {"id": 115, "seek": 65968, "start": 681.68, "end": 684.68, "text": " La sintaxis del idioma pa\u00f1\u00f3n no es as\u00ed.", "tokens": [51464, 2369, 41259, 24633, 1103, 18014, 6440, 2502, 2791, 1801, 572, 785, 8582, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1894536629701272, "compression_ratio": 1.3631284916201116, "no_speech_prob": 0.011920178309082985}, {"id": 116, "seek": 68468, "start": 685.68, "end": 689.68, "text": " \u00bfNos decimos educaci\u00f3n ley, educaci\u00f3n que...?", "tokens": [50414, 3841, 45, 329, 979, 8372, 48861, 27786, 11, 48861, 631, 8964, 50614], "temperature": 0.0, "avg_logprob": -0.23784982173814687, "compression_ratio": 1.5529953917050692, "no_speech_prob": 0.0030445775482803583}, {"id": 117, "seek": 68468, "start": 690.68, "end": 692.68, "text": " En la segunda, porque no publicamos la procesi\u00f3n.", "tokens": [50664, 2193, 635, 21978, 11, 4021, 572, 1908, 2151, 635, 447, 887, 2560, 13, 50764], "temperature": 0.0, "avg_logprob": -0.23784982173814687, "compression_ratio": 1.5529953917050692, "no_speech_prob": 0.0030445775482803583}, {"id": 118, "seek": 68468, "start": 692.68, "end": 693.68, "text": " \u00bfPor qu\u00e9 no qu\u00e9?", "tokens": [50764, 3841, 24907, 8057, 572, 8057, 30, 50814], "temperature": 0.0, "avg_logprob": -0.23784982173814687, "compression_ratio": 1.5529953917050692, "no_speech_prob": 0.0030445775482803583}, {"id": 119, "seek": 68468, "start": 693.68, "end": 696.68, "text": " En la procesi\u00f3n, porque si tenemos sus y de botero,", "tokens": [50814, 2193, 635, 447, 887, 2560, 11, 4021, 1511, 9914, 3291, 288, 368, 10592, 2032, 11, 50964], "temperature": 0.0, "avg_logprob": -0.23784982173814687, "compression_ratio": 1.5529953917050692, "no_speech_prob": 0.0030445775482803583}, {"id": 120, "seek": 68468, "start": 696.68, "end": 698.68, "text": " estamos publicando, \u00bfverdad?", "tokens": [50964, 10382, 1908, 1806, 11, 3841, 331, 20034, 30, 51064], "temperature": 0.0, "avg_logprob": -0.23784982173814687, "compression_ratio": 1.5529953917050692, "no_speech_prob": 0.0030445775482803583}, {"id": 121, "seek": 68468, "start": 701.68, "end": 704.68, "text": " Ah, bueno, pero ese podr\u00eda ser un sus de un tercero, \u00bfno?", "tokens": [51214, 2438, 11, 11974, 11, 4768, 10167, 27246, 816, 517, 3291, 368, 517, 38103, 78, 11, 3841, 1771, 30, 51364], "temperature": 0.0, "avg_logprob": -0.23784982173814687, "compression_ratio": 1.5529953917050692, "no_speech_prob": 0.0030445775482803583}, {"id": 122, "seek": 68468, "start": 705.68, "end": 709.68, "text": " Ac\u00e1 seguramente lo que hay es un error tor\u00e1fico de sus hordas de botero.", "tokens": [51414, 5097, 842, 22179, 3439, 450, 631, 4842, 785, 517, 6713, 3930, 23858, 78, 368, 3291, 276, 765, 296, 368, 10592, 2032, 13, 51614], "temperature": 0.0, "avg_logprob": -0.23784982173814687, "compression_ratio": 1.5529953917050692, "no_speech_prob": 0.0030445775482803583}, {"id": 123, "seek": 70968, "start": 710.68, "end": 712.68, "text": " O sea, ac\u00e1 tenemos un tema de sintaxis.", "tokens": [50414, 422, 4158, 11, 23496, 9914, 517, 15854, 368, 41259, 24633, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16181232422355593, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.01249667163938284}, {"id": 124, "seek": 70968, "start": 712.68, "end": 714.68, "text": " Ac\u00e1 no tenemos un tema de sintaxis.", "tokens": [50514, 5097, 842, 572, 9914, 517, 15854, 368, 41259, 24633, 13, 50614], "temperature": 0.0, "avg_logprob": -0.16181232422355593, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.01249667163938284}, {"id": 125, "seek": 70968, "start": 717.68, "end": 720.68, "text": " Deber\u00edamos conocer un poco de sem\u00e1ntica para asociar botero,", "tokens": [50764, 1346, 607, 16275, 35241, 517, 10639, 368, 4361, 27525, 2262, 1690, 382, 78, 537, 289, 10592, 2032, 11, 50914], "temperature": 0.0, "avg_logprob": -0.16181232422355593, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.01249667163938284}, {"id": 126, "seek": 70968, "start": 720.68, "end": 722.68, "text": " que pintaba mujeres gordas y entonces...", "tokens": [50914, 631, 23924, 5509, 31683, 42443, 296, 288, 13003, 485, 51014], "temperature": 0.0, "avg_logprob": -0.16181232422355593, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.01249667163938284}, {"id": 127, "seek": 70968, "start": 723.68, "end": 725.68, "text": " O una aproximaci\u00f3n un poco m\u00e1s humilde,", "tokens": [51064, 422, 2002, 31270, 3482, 517, 10639, 3573, 1484, 15956, 11, 51164], "temperature": 0.0, "avg_logprob": -0.16181232422355593, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.01249667163938284}, {"id": 128, "seek": 70968, "start": 725.68, "end": 728.68, "text": " que es la segunda, es la una aproximaci\u00f3n m\u00e1s detal\u00edstica,", "tokens": [51164, 631, 785, 635, 21978, 11, 785, 635, 2002, 31270, 3482, 3573, 33185, 19512, 2262, 11, 51314], "temperature": 0.0, "avg_logprob": -0.16181232422355593, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.01249667163938284}, {"id": 129, "seek": 70968, "start": 728.68, "end": 730.68, "text": " porque si nosotros...", "tokens": [51314, 4021, 1511, 13863, 485, 51414], "temperature": 0.0, "avg_logprob": -0.16181232422355593, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.01249667163938284}, {"id": 130, "seek": 70968, "start": 730.68, "end": 733.68, "text": " y que juega con el hecho de que tenemos grandes vol\u00famenes de texto", "tokens": [51414, 288, 631, 27833, 3680, 416, 806, 13064, 368, 631, 9914, 16640, 1996, 2481, 2558, 279, 368, 35503, 51564], "temperature": 0.0, "avg_logprob": -0.16181232422355593, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.01249667163938284}, {"id": 131, "seek": 70968, "start": 733.68, "end": 736.68, "text": " y de ah\u00ed el cambio de los modelos probabil\u00edsticos,", "tokens": [51564, 288, 368, 12571, 806, 28731, 368, 1750, 2316, 329, 31959, 19512, 9940, 11, 51714], "temperature": 0.0, "avg_logprob": -0.16181232422355593, "compression_ratio": 1.6984126984126984, "no_speech_prob": 0.01249667163938284}, {"id": 132, "seek": 73668, "start": 736.68, "end": 740.68, "text": " es que sus gordas de botero seguramente apareci\u00f3", "tokens": [50364, 785, 631, 3291, 42443, 296, 368, 10592, 2032, 22179, 3439, 15004, 19609, 50564], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 133, "seek": 73668, "start": 740.68, "end": 742.68, "text": " antes en mis corpus de texto", "tokens": [50564, 11014, 465, 3346, 1181, 31624, 368, 35503, 50664], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 134, "seek": 73668, "start": 743.68, "end": 745.68, "text": " y hordas de botero, no.", "tokens": [50714, 288, 276, 765, 296, 368, 10592, 2032, 11, 572, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 135, "seek": 73668, "start": 747.68, "end": 749.68, "text": " Es una aproximaci\u00f3n mucho m\u00e1s detal\u00edstica.", "tokens": [50914, 2313, 2002, 31270, 3482, 9824, 3573, 33185, 19512, 2262, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 136, "seek": 73668, "start": 749.68, "end": 752.68, "text": " Eso es lo que vamos a hacer en los modelos de diagrama, justamente.", "tokens": [51014, 27795, 785, 450, 631, 5295, 257, 6720, 465, 1750, 2316, 329, 368, 10686, 64, 11, 41056, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 137, "seek": 73668, "start": 752.68, "end": 754.68, "text": " A partir de grandes vol\u00famenes de texto,", "tokens": [51164, 316, 13906, 368, 16640, 1996, 2481, 2558, 279, 368, 35503, 11, 51264], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 138, "seek": 73668, "start": 754.68, "end": 756.68, "text": " detectar e calcular las probabilidades.", "tokens": [51264, 5531, 289, 308, 2104, 17792, 2439, 31959, 10284, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 139, "seek": 73668, "start": 757.68, "end": 759.68, "text": " Es una aproximaci\u00f3n puramente estad\u00edstica,", "tokens": [51414, 2313, 2002, 31270, 3482, 1864, 3439, 39160, 19512, 2262, 11, 51514], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 140, "seek": 73668, "start": 759.68, "end": 761.68, "text": " es bien salvaje, es.", "tokens": [51514, 785, 3610, 26858, 11153, 11, 785, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 141, "seek": 73668, "start": 761.68, "end": 763.68, "text": " Yo no s\u00e9 qu\u00e9 estructura tiene esto,", "tokens": [51614, 7616, 572, 7910, 8057, 43935, 2991, 7066, 7433, 11, 51714], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 142, "seek": 73668, "start": 763.68, "end": 765.68, "text": " pero s\u00e9 que esto no se dio nunca", "tokens": [51714, 4768, 7910, 631, 7433, 572, 369, 31965, 13768, 51814], "temperature": 0.0, "avg_logprob": -0.12440361193756559, "compression_ratio": 1.6795366795366795, "no_speech_prob": 0.0006852821097709239}, {"id": 143, "seek": 76568, "start": 765.68, "end": 767.68, "text": " de botero, s\u00ed, muchas veces.", "tokens": [50364, 368, 10592, 2032, 11, 8600, 11, 16072, 17054, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14033075741359166, "compression_ratio": 1.484375, "no_speech_prob": 0.003551444038748741}, {"id": 144, "seek": 76568, "start": 767.68, "end": 769.68, "text": " Entonces, es m\u00e1s probable que me haya equivocado.", "tokens": [50464, 15097, 11, 785, 3573, 21759, 631, 385, 24693, 48726, 21636, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14033075741359166, "compression_ratio": 1.484375, "no_speech_prob": 0.003551444038748741}, {"id": 145, "seek": 76568, "start": 777.68, "end": 779.68, "text": " A ver, relacionado con esto,", "tokens": [50964, 316, 1306, 11, 27189, 1573, 416, 7433, 11, 51064], "temperature": 0.0, "avg_logprob": -0.14033075741359166, "compression_ratio": 1.484375, "no_speech_prob": 0.003551444038748741}, {"id": 146, "seek": 76568, "start": 779.68, "end": 781.68, "text": " ahora vamos a ver por qu\u00e9 est\u00e1 relacionado.", "tokens": [51064, 9923, 5295, 257, 1306, 1515, 8057, 3192, 27189, 1573, 13, 51164], "temperature": 0.0, "avg_logprob": -0.14033075741359166, "compression_ratio": 1.484375, "no_speech_prob": 0.003551444038748741}, {"id": 147, "seek": 76568, "start": 781.68, "end": 784.68, "text": " Est\u00e1 el tema de la predicci\u00f3n de la siguiente palabra.", "tokens": [51164, 27304, 806, 15854, 368, 635, 47336, 5687, 368, 635, 25666, 31702, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14033075741359166, "compression_ratio": 1.484375, "no_speech_prob": 0.003551444038748741}, {"id": 148, "seek": 76568, "start": 786.68, "end": 789.68, "text": " \u00bfCu\u00e1les se imaginan que es la siguiente palabra", "tokens": [51414, 3841, 35222, 842, 904, 369, 23427, 282, 631, 785, 635, 25666, 31702, 51564], "temperature": 0.0, "avg_logprob": -0.14033075741359166, "compression_ratio": 1.484375, "no_speech_prob": 0.003551444038748741}, {"id": 149, "seek": 76568, "start": 789.68, "end": 791.68, "text": " a la primera oraci\u00f3n?", "tokens": [51564, 257, 635, 17382, 420, 3482, 30, 51664], "temperature": 0.0, "avg_logprob": -0.14033075741359166, "compression_ratio": 1.484375, "no_speech_prob": 0.003551444038748741}, {"id": 150, "seek": 79168, "start": 792.68, "end": 794.68, "text": " \u00bfCu\u00e1l puede ser la siguiente palabra?", "tokens": [50414, 3841, 35222, 11447, 8919, 816, 635, 25666, 31702, 30, 50514], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 151, "seek": 79168, "start": 797.68, "end": 798.68, "text": " \u00bfPara?", "tokens": [50664, 3841, 47, 2419, 30, 50714], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 152, "seek": 79168, "start": 799.68, "end": 800.68, "text": " \u00bfPara?", "tokens": [50764, 3841, 47, 2419, 30, 50814], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 153, "seek": 79168, "start": 800.68, "end": 801.68, "text": " Para.", "tokens": [50814, 11107, 13, 50864], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 154, "seek": 79168, "start": 801.68, "end": 802.68, "text": " \u00bfPara?", "tokens": [50864, 3841, 47, 2419, 30, 50914], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 155, "seek": 79168, "start": 802.68, "end": 803.68, "text": " \u00bfPara?", "tokens": [50914, 3841, 47, 2419, 30, 50964], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 156, "seek": 79168, "start": 803.68, "end": 804.68, "text": " \u00bfPara?", "tokens": [50964, 3841, 47, 2419, 30, 51014], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 157, "seek": 79168, "start": 804.68, "end": 805.68, "text": " \u00bfPara?", "tokens": [51014, 3841, 47, 2419, 30, 51064], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 158, "seek": 79168, "start": 805.68, "end": 807.68, "text": " \u00bfPara es una preposici\u00f3n, no?", "tokens": [51064, 3841, 47, 2419, 785, 2002, 2666, 329, 15534, 11, 572, 30, 51164], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 159, "seek": 79168, "start": 813.68, "end": 814.68, "text": " \u00bfQu\u00e9 m\u00e1s?", "tokens": [51464, 3841, 15137, 3573, 30, 51514], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 160, "seek": 79168, "start": 814.68, "end": 816.68, "text": " \u00bfQu\u00e9 otra cosa puede decir ah\u00ed?", "tokens": [51514, 3841, 15137, 13623, 10163, 8919, 10235, 12571, 30, 51614], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 161, "seek": 79168, "start": 816.68, "end": 818.68, "text": " \u00bfCu\u00e1l, por ejemplo?", "tokens": [51614, 3841, 35222, 11447, 11, 1515, 13358, 30, 51714], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 162, "seek": 79168, "start": 818.68, "end": 820.68, "text": " \u00bfUn pron\u00f3stico alentador?", "tokens": [51714, 3841, 12405, 7569, 45052, 2789, 419, 317, 5409, 30, 51814], "temperature": 0.0, "avg_logprob": -0.27915214089786305, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.000576989259570837}, {"id": 163, "seek": 82068, "start": 820.68, "end": 822.68, "text": " \u00bfUn pron\u00f3stico alentador?", "tokens": [50364, 3841, 12405, 7569, 45052, 2789, 419, 317, 5409, 30, 50464], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 164, "seek": 82068, "start": 823.68, "end": 825.68, "text": " \u00bfO puede decir un pron\u00f3stico terrible?", "tokens": [50514, 3841, 46, 8919, 10235, 517, 7569, 45052, 2789, 6237, 30, 50614], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 165, "seek": 82068, "start": 825.68, "end": 827.68, "text": " \u00bfUn pron\u00f3stico...", "tokens": [50614, 3841, 12405, 7569, 45052, 2789, 485, 50714], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 166, "seek": 82068, "start": 828.68, "end": 829.68, "text": " \u00bfO qu\u00e9 otra cosa m\u00e1s?", "tokens": [50764, 3841, 46, 8057, 13623, 10163, 3573, 30, 50814], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 167, "seek": 82068, "start": 829.68, "end": 831.68, "text": " Hay uno m\u00e1s com\u00fan para m\u00ed.", "tokens": [50814, 8721, 8526, 3573, 45448, 1690, 14692, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 168, "seek": 82068, "start": 831.68, "end": 834.68, "text": " Elmiti\u00f3 un pron\u00f3stico meteorol\u00f3gico, no?", "tokens": [50914, 2699, 3508, 7138, 517, 7569, 45052, 2789, 25313, 27629, 2789, 11, 572, 30, 51064], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 169, "seek": 82068, "start": 835.68, "end": 838.68, "text": " \u00bfA ra\u00edz de este fen\u00f3meno se suceder\u00e1n tormentas?", "tokens": [51114, 3841, 32, 3342, 44551, 368, 4065, 26830, 812, 43232, 369, 41928, 260, 7200, 36662, 296, 30, 51264], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 170, "seek": 82068, "start": 838.68, "end": 840.68, "text": " Fuertes,", "tokens": [51264, 12807, 911, 279, 11, 51364], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 171, "seek": 82068, "start": 840.68, "end": 842.68, "text": " importantes,", "tokens": [51364, 27963, 11, 51464], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 172, "seek": 82068, "start": 842.68, "end": 844.68, "text": " muy.", "tokens": [51464, 5323, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 173, "seek": 82068, "start": 844.68, "end": 846.68, "text": " No creo que hay diga tormentas", "tokens": [51564, 883, 14336, 631, 4842, 2528, 64, 36662, 296, 51664], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 174, "seek": 82068, "start": 846.68, "end": 848.68, "text": " gatito, \u00bfno?", "tokens": [51664, 44092, 3528, 11, 3841, 1771, 30, 51764], "temperature": 0.0, "avg_logprob": -0.2138268700961409, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.0011490026954561472}, {"id": 175, "seek": 84868, "start": 848.68, "end": 851.68, "text": " Esto no es muy probable que sea la palabra siguiente.", "tokens": [50364, 20880, 572, 785, 5323, 21759, 631, 4158, 635, 31702, 25666, 13, 50514], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 176, "seek": 84868, "start": 851.68, "end": 853.68, "text": " Nuevamente, \u00bfpor qu\u00e9 sabemos esto?", "tokens": [50514, 47970, 85, 3439, 11, 3841, 2816, 8057, 27200, 7433, 30, 50614], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 177, "seek": 84868, "start": 853.68, "end": 856.68, "text": " Porque es muy raro que hay un d\u00eda tormentas gatito, digamos, \u00bfno?", "tokens": [50614, 11287, 785, 5323, 367, 9708, 631, 4842, 517, 12271, 36662, 296, 44092, 3528, 11, 36430, 11, 3841, 1771, 30, 50764], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 178, "seek": 84868, "start": 857.68, "end": 859.68, "text": " Entonces,", "tokens": [50814, 15097, 11, 50914], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 179, "seek": 84868, "start": 859.68, "end": 861.68, "text": " esto que tenemos ac\u00e1", "tokens": [50914, 7433, 631, 9914, 23496, 51014], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 180, "seek": 84868, "start": 861.68, "end": 863.68, "text": " es", "tokens": [51014, 785, 51114], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 181, "seek": 84868, "start": 863.68, "end": 866.68, "text": " las posibilidades que hay de siguiente palabra.", "tokens": [51114, 2439, 1366, 11607, 10284, 631, 4842, 368, 25666, 31702, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 182, "seek": 84868, "start": 866.68, "end": 869.68, "text": " Dadas todas las anteriores.", "tokens": [51264, 413, 6872, 10906, 2439, 364, 34345, 2706, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 183, "seek": 84868, "start": 869.68, "end": 872.68, "text": " Es decir, yo tengo todo el contexto, lo que se llama contexto,", "tokens": [51414, 2313, 10235, 11, 5290, 13989, 5149, 806, 47685, 11, 450, 631, 369, 23272, 47685, 11, 51564], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 184, "seek": 84868, "start": 872.68, "end": 875.68, "text": " dado el contexto de la palabra que sigue ac\u00e1.", "tokens": [51564, 29568, 806, 47685, 368, 635, 31702, 631, 34532, 23496, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 185, "seek": 84868, "start": 875.68, "end": 877.68, "text": " \u00bfS\u00ed?", "tokens": [51714, 3841, 30463, 30, 51814], "temperature": 0.0, "avg_logprob": -0.16155761327499, "compression_ratio": 1.6218487394957983, "no_speech_prob": 0.010339304804801941}, {"id": 186, "seek": 87768, "start": 877.68, "end": 879.68, "text": " Una de las, lo que nosotros vamos a querer hacer", "tokens": [50364, 15491, 368, 2439, 11, 450, 631, 13863, 5295, 257, 39318, 6720, 50464], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 187, "seek": 87768, "start": 879.68, "end": 881.68, "text": " en un modelo de lenguaje,", "tokens": [50464, 465, 517, 27825, 368, 35044, 84, 11153, 11, 50564], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 188, "seek": 87768, "start": 881.68, "end": 883.68, "text": " como camino para calcular la probabilidad de una oraci\u00f3n,", "tokens": [50564, 2617, 34124, 1690, 2104, 17792, 635, 31959, 4580, 368, 2002, 420, 3482, 11, 50664], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 189, "seek": 87768, "start": 883.68, "end": 885.68, "text": " es dado el contexto", "tokens": [50664, 785, 29568, 806, 47685, 50764], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 190, "seek": 87768, "start": 885.68, "end": 887.68, "text": " calcular la palabra.", "tokens": [50764, 2104, 17792, 635, 31702, 13, 50864], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 191, "seek": 87768, "start": 887.68, "end": 889.68, "text": " Siguiente.", "tokens": [50864, 318, 16397, 8413, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 192, "seek": 87768, "start": 889.68, "end": 891.68, "text": " \u00bfS\u00ed?", "tokens": [50964, 3841, 30463, 30, 51064], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 193, "seek": 87768, "start": 893.68, "end": 895.68, "text": " Rachas de viento fuerte de componente.", "tokens": [51164, 497, 608, 296, 368, 371, 7814, 37129, 368, 4026, 1576, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 194, "seek": 87768, "start": 897.68, "end": 899.68, "text": " Veremos que.", "tokens": [51364, 691, 19065, 631, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 195, "seek": 87768, "start": 901.68, "end": 903.68, "text": " Bueno, resulta ser que de los ejemplos que yo tom\u00e9,", "tokens": [51564, 16046, 11, 1874, 64, 816, 631, 368, 1750, 10012, 5895, 329, 631, 5290, 2916, 526, 11, 51664], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 196, "seek": 87768, "start": 903.68, "end": 906.68, "text": " ah bueno, puse viento fuerte de componente,", "tokens": [51664, 3716, 11974, 11, 280, 438, 371, 7814, 37129, 368, 4026, 1576, 11, 51814], "temperature": 0.0, "avg_logprob": -0.19414082402768343, "compression_ratio": 1.5642201834862386, "no_speech_prob": 0.0008860921952873468}, {"id": 197, "seek": 90668, "start": 906.68, "end": 908.68, "text": " el lin\u00f3mede emiti\u00f3 pron\u00f3tico especial,", "tokens": [50364, 806, 22896, 812, 1912, 68, 32084, 7138, 7569, 34712, 2789, 15342, 11, 50464], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 198, "seek": 90668, "start": 908.68, "end": 910.68, "text": " o sea que le ramos,", "tokens": [50464, 277, 4158, 631, 476, 367, 2151, 11, 50564], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 199, "seek": 90668, "start": 910.68, "end": 912.68, "text": " se sonan tormentas fuertes,", "tokens": [50564, 369, 1872, 282, 36662, 296, 8536, 911, 279, 11, 50664], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 200, "seek": 90668, "start": 912.68, "end": 914.68, "text": " viento fuerte de componente sudo este,", "tokens": [50664, 371, 7814, 37129, 368, 4026, 1576, 459, 2595, 4065, 11, 50764], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 201, "seek": 90668, "start": 914.68, "end": 916.68, "text": " ejemplo, predicci\u00f3n.", "tokens": [50764, 13358, 11, 47336, 5687, 13, 50864], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 202, "seek": 90668, "start": 919.68, "end": 921.68, "text": " Vamos a poner un poquito de notaci\u00f3n", "tokens": [51014, 10894, 257, 19149, 517, 28229, 368, 406, 3482, 51114], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 203, "seek": 90668, "start": 921.68, "end": 923.68, "text": " antes de que,", "tokens": [51114, 11014, 368, 631, 11, 51214], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 204, "seek": 90668, "start": 923.68, "end": 925.68, "text": " antes de seguir,", "tokens": [51214, 11014, 368, 18584, 11, 51314], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 205, "seek": 90668, "start": 925.68, "end": 927.68, "text": " porque vamos a ver c\u00f3mo enfrentamos este problema,", "tokens": [51314, 4021, 5295, 257, 1306, 12826, 33771, 2151, 4065, 12395, 11, 51414], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 206, "seek": 90668, "start": 927.68, "end": 929.68, "text": " es decir, \u00bfc\u00f3mo calculamos esa probabilidad?", "tokens": [51414, 785, 10235, 11, 3841, 46614, 4322, 2151, 11342, 31959, 4580, 30, 51514], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 207, "seek": 90668, "start": 929.68, "end": 931.68, "text": " Un poco de notaci\u00f3n para seguir,", "tokens": [51514, 1156, 10639, 368, 406, 3482, 1690, 18584, 11, 51614], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 208, "seek": 90668, "start": 931.68, "end": 933.68, "text": " yo lo que estoy diciendo es", "tokens": [51614, 5290, 450, 631, 15796, 42797, 785, 51714], "temperature": 0.0, "avg_logprob": -0.20403207143147786, "compression_ratio": 1.6593886462882097, "no_speech_prob": 0.00037596930633299053}, {"id": 209, "seek": 93368, "start": 934.68, "end": 936.68, "text": " la probabilidad de que una variable aleatoria", "tokens": [50414, 635, 31959, 4580, 368, 631, 2002, 7006, 6775, 1639, 654, 50514], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 210, "seek": 93368, "start": 936.68, "end": 938.68, "text": " ah\u00ed", "tokens": [50514, 12571, 50614], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 211, "seek": 93368, "start": 938.68, "end": 940.68, "text": " valga,", "tokens": [50614, 1323, 3680, 11, 50714], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 212, "seek": 93368, "start": 940.68, "end": 942.68, "text": " tome el valor conocimiento,", "tokens": [50714, 281, 1398, 806, 15367, 15871, 14007, 11, 50814], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 213, "seek": 93368, "start": 942.68, "end": 944.68, "text": " en este caso tendr\u00eda una variable aleatoria", "tokens": [50814, 465, 4065, 9666, 3928, 37183, 2002, 7006, 6775, 1639, 654, 50914], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 214, "seek": 93368, "start": 944.68, "end": 946.68, "text": " por cada posici\u00f3n en el texto,", "tokens": [50914, 1515, 8411, 46595, 465, 806, 35503, 11, 51014], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 215, "seek": 93368, "start": 946.68, "end": 948.68, "text": " \u00bfverdad?", "tokens": [51014, 3841, 331, 20034, 30, 51114], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 216, "seek": 93368, "start": 948.68, "end": 950.68, "text": " Tengo una X1, que es la primera palabra, aqu\u00ed dos,", "tokens": [51114, 314, 30362, 2002, 1783, 16, 11, 631, 785, 635, 17382, 31702, 11, 6661, 4491, 11, 51214], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 217, "seek": 93368, "start": 950.68, "end": 952.68, "text": " que es la segunda, aqu\u00ed tres.", "tokens": [51214, 631, 785, 635, 21978, 11, 6661, 15890, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 218, "seek": 93368, "start": 952.68, "end": 954.68, "text": " Son variables aleatorias, que la variable aleatoria", "tokens": [51314, 5185, 9102, 6775, 1639, 4609, 11, 631, 635, 7006, 6775, 1639, 654, 51414], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 219, "seek": 93368, "start": 954.68, "end": 956.68, "text": " es un mapeo, es una funci\u00f3n que mapea", "tokens": [51414, 785, 517, 463, 494, 78, 11, 785, 2002, 43735, 631, 463, 494, 64, 51514], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 220, "seek": 93368, "start": 956.68, "end": 958.68, "text": " de un evento, un n\u00famero entre 0 y 1.", "tokens": [51514, 368, 517, 40655, 11, 517, 14959, 3962, 1958, 288, 502, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17675395274725486, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.005415450781583786}, {"id": 221, "seek": 95868, "start": 958.68, "end": 960.68, "text": " \u00bfLa probabilidad de una?", "tokens": [50364, 3841, 5478, 31959, 4580, 368, 2002, 30, 50464], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 222, "seek": 95868, "start": 960.68, "end": 962.68, "text": " La probabilidad de una.", "tokens": [50464, 2369, 31959, 4580, 368, 2002, 13, 50564], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 223, "seek": 95868, "start": 964.68, "end": 966.68, "text": " Perd\u00f3n.", "tokens": [50664, 47633, 1801, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 224, "seek": 95868, "start": 966.68, "end": 968.68, "text": " Perd\u00f3n.", "tokens": [50764, 47633, 1801, 13, 50864], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 225, "seek": 95868, "start": 968.68, "end": 970.68, "text": " Bueno, no vamos a entrar en definiciones,", "tokens": [50864, 16046, 11, 572, 5295, 257, 20913, 465, 1561, 29719, 11, 50964], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 226, "seek": 95868, "start": 970.68, "end": 972.68, "text": " mapea con un real y la probabilidad", "tokens": [50964, 463, 494, 64, 416, 517, 957, 288, 635, 31959, 4580, 51064], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 227, "seek": 95868, "start": 972.68, "end": 974.68, "text": " me devuelve un n\u00famero entre 0 y 1,", "tokens": [51064, 385, 1905, 3483, 303, 517, 14959, 3962, 1958, 288, 502, 11, 51164], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 228, "seek": 95868, "start": 974.68, "end": 976.68, "text": " es decir, yo defino la probabilidad", "tokens": [51164, 785, 10235, 11, 5290, 1561, 78, 635, 31959, 4580, 51264], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 229, "seek": 95868, "start": 976.68, "end": 978.68, "text": " de una variable aleatoria,", "tokens": [51264, 368, 2002, 7006, 6775, 1639, 654, 11, 51364], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 230, "seek": 95868, "start": 978.68, "end": 980.68, "text": " como", "tokens": [51364, 2617, 51464], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 231, "seek": 95868, "start": 980.68, "end": 982.68, "text": " la distribuci\u00f3n", "tokens": [51464, 635, 4400, 30813, 51564], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 232, "seek": 95868, "start": 982.68, "end": 984.68, "text": " de probabilidad de una variable aleatoria,", "tokens": [51564, 368, 31959, 4580, 368, 2002, 7006, 6775, 1639, 654, 11, 51664], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 233, "seek": 95868, "start": 984.68, "end": 986.68, "text": " dado los diferentes valores que puede tomar", "tokens": [51664, 29568, 1750, 17686, 38790, 631, 8919, 22048, 51764], "temperature": 0.0, "avg_logprob": -0.17287864685058593, "compression_ratio": 1.7828282828282829, "no_speech_prob": 0.005278267432004213}, {"id": 234, "seek": 98668, "start": 987.68, "end": 989.68, "text": " \u00bfCu\u00e1l es el valor de cada uno de ellos?", "tokens": [50414, 3841, 35222, 11447, 785, 806, 15367, 368, 8411, 8526, 368, 16353, 30, 50514], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 235, "seek": 98668, "start": 989.68, "end": 991.68, "text": " \u00bfS\u00ed?", "tokens": [50514, 3841, 30463, 30, 50614], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 236, "seek": 98668, "start": 991.68, "end": 993.68, "text": " Y esto, \u00bfcu\u00e1l es el rango?", "tokens": [50614, 398, 7433, 11, 3841, 12032, 11447, 785, 806, 367, 17150, 30, 50714], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 237, "seek": 98668, "start": 993.68, "end": 995.68, "text": " \u00bfQu\u00e9 valor es probable que tiene ac\u00e1", "tokens": [50714, 3841, 15137, 15367, 785, 21759, 631, 7066, 23496, 50814], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 238, "seek": 98668, "start": 995.68, "end": 997.68, "text": " una variable aleatoria", "tokens": [50814, 2002, 7006, 6775, 1639, 654, 50914], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 239, "seek": 98668, "start": 997.68, "end": 999.68, "text": " que refiera palabras?", "tokens": [50914, 631, 1895, 10609, 35240, 30, 51014], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 240, "seek": 98668, "start": 1002.68, "end": 1004.68, "text": " Todo el vocabulario, \u00bfno?", "tokens": [51164, 26466, 806, 2329, 455, 1040, 1004, 11, 3841, 1771, 30, 51264], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 241, "seek": 98668, "start": 1004.68, "end": 1006.68, "text": " Todas las palabras diferentes que yo puedo tener.", "tokens": [51264, 2465, 296, 2439, 35240, 17686, 631, 5290, 21612, 11640, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 242, "seek": 98668, "start": 1006.68, "end": 1008.68, "text": " \u00bfDe acuerdo?", "tokens": [51364, 3841, 11089, 28113, 30, 51464], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 243, "seek": 98668, "start": 1008.68, "end": 1010.68, "text": " Entonces nosotros vamos a poner", "tokens": [51464, 15097, 13863, 5295, 257, 19149, 51564], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 244, "seek": 98668, "start": 1010.68, "end": 1012.68, "text": " en notaci\u00f3n probabilidad de conocimiento,", "tokens": [51564, 465, 406, 3482, 31959, 4580, 368, 15871, 14007, 11, 51664], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 245, "seek": 98668, "start": 1012.68, "end": 1014.68, "text": " de que la palabra sea conocimiento.", "tokens": [51664, 368, 631, 635, 31702, 4158, 15871, 14007, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17050789542820144, "compression_ratio": 1.5757575757575757, "no_speech_prob": 0.0010787101928144693}, {"id": 246, "seek": 101668, "start": 1016.68, "end": 1018.68, "text": " Vamos a denotar W1n", "tokens": [50364, 10894, 257, 1441, 310, 289, 343, 16, 77, 50464], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 247, "seek": 101668, "start": 1018.68, "end": 1020.68, "text": " 1n", "tokens": [50464, 502, 77, 50564], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 248, "seek": 101668, "start": 1020.68, "end": 1022.68, "text": " a la secuencia", "tokens": [50564, 257, 635, 907, 47377, 50664], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 249, "seek": 101668, "start": 1022.68, "end": 1024.6799999999998, "text": " de palabras W1", "tokens": [50664, 368, 35240, 343, 16, 50764], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 250, "seek": 101668, "start": 1024.6799999999998, "end": 1026.6799999999998, "text": " W2", "tokens": [50764, 343, 17, 50864], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 251, "seek": 101668, "start": 1026.6799999999998, "end": 1028.6799999999998, "text": " Wn, por ejemplo, en una eraci\u00f3n", "tokens": [50864, 343, 77, 11, 1515, 13358, 11, 465, 2002, 1189, 3482, 50964], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 252, "seek": 101668, "start": 1028.6799999999998, "end": 1030.6799999999998, "text": " y vamos a decir", "tokens": [50964, 288, 5295, 257, 10235, 51064], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 253, "seek": 101668, "start": 1030.6799999999998, "end": 1032.6799999999998, "text": " vamos a decir que vamos a hablar", "tokens": [51064, 5295, 257, 10235, 631, 5295, 257, 21014, 51164], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 254, "seek": 101668, "start": 1032.6799999999998, "end": 1034.6799999999998, "text": " de la probabilidad de", "tokens": [51164, 368, 635, 31959, 4580, 368, 51264], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 255, "seek": 101668, "start": 1034.6799999999998, "end": 1036.6799999999998, "text": " la secuencia de palabras queriendo decir, bueno,", "tokens": [51264, 635, 907, 47377, 368, 35240, 7083, 7304, 10235, 11, 11974, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 256, "seek": 101668, "start": 1036.6799999999998, "end": 1038.6799999999998, "text": " la probabilidad de la que la primera sea W1", "tokens": [51364, 635, 31959, 4580, 368, 635, 631, 635, 17382, 4158, 343, 16, 51464], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 257, "seek": 101668, "start": 1038.6799999999998, "end": 1040.6799999999998, "text": " que la segunda sea W2, etc.", "tokens": [51464, 631, 635, 21978, 4158, 343, 17, 11, 5183, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 258, "seek": 101668, "start": 1040.6799999999998, "end": 1042.6799999999998, "text": " \u00bfDe acuerdo?", "tokens": [51564, 3841, 11089, 28113, 30, 51664], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 259, "seek": 101668, "start": 1042.6799999999998, "end": 1044.6799999999998, "text": " O sea que esta distribuci\u00f3n de probabilidad", "tokens": [51664, 422, 4158, 631, 5283, 4400, 30813, 368, 31959, 4580, 51764], "temperature": 0.0, "avg_logprob": -0.1595138581860967, "compression_ratio": 1.7474226804123711, "no_speech_prob": 0.0042483690194785595}, {"id": 260, "seek": 104668, "start": 1046.68, "end": 1048.68, "text": " tiene como rango", "tokens": [50364, 7066, 2617, 367, 17150, 50464], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 261, "seek": 104668, "start": 1048.68, "end": 1050.68, "text": " todas las secuencias posibles de palabras.", "tokens": [50464, 10906, 2439, 907, 7801, 12046, 1366, 14428, 368, 35240, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 262, "seek": 104668, "start": 1050.68, "end": 1052.68, "text": " \u00bfS\u00ed?", "tokens": [50564, 3841, 30463, 30, 50664], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 263, "seek": 104668, "start": 1052.68, "end": 1054.68, "text": " O sea que si mi vocabulario es B", "tokens": [50664, 422, 4158, 631, 1511, 2752, 2329, 455, 1040, 1004, 785, 363, 50764], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 264, "seek": 104668, "start": 1054.68, "end": 1056.68, "text": " tengo", "tokens": [50764, 13989, 50864], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 265, "seek": 104668, "start": 1056.68, "end": 1058.68, "text": " N a la B", "tokens": [50864, 426, 257, 635, 363, 50964], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 266, "seek": 104668, "start": 1060.68, "end": 1062.68, "text": " V a la N", "tokens": [51064, 691, 257, 635, 426, 51164], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 267, "seek": 104668, "start": 1062.68, "end": 1064.68, "text": " V a la N", "tokens": [51164, 691, 257, 635, 426, 51264], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 268, "seek": 104668, "start": 1064.68, "end": 1066.68, "text": " Posita V a la N", "tokens": [51264, 25906, 2786, 691, 257, 635, 426, 51364], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 269, "seek": 104668, "start": 1066.68, "end": 1068.68, "text": " O sea que es enorme", "tokens": [51364, 422, 4158, 631, 785, 33648, 51464], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 270, "seek": 104668, "start": 1068.68, "end": 1070.68, "text": " esencialmente, \u00bfno?", "tokens": [51464, 785, 26567, 4082, 11, 3841, 1771, 30, 51564], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 271, "seek": 104668, "start": 1070.68, "end": 1072.68, "text": " Todas las posibles secuencias.", "tokens": [51564, 2465, 296, 2439, 1366, 14428, 907, 7801, 12046, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14557963229240256, "compression_ratio": 1.5068493150684932, "no_speech_prob": 0.0012808616738766432}, {"id": 272, "seek": 107268, "start": 1072.68, "end": 1074.68, "text": " Y", "tokens": [50364, 398, 50464], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 273, "seek": 107268, "start": 1074.68, "end": 1076.68, "text": " vamos a recordar", "tokens": [50464, 5295, 257, 2136, 289, 50564], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 274, "seek": 107268, "start": 1076.68, "end": 1078.68, "text": " la chain rule", "tokens": [50564, 635, 5021, 4978, 50664], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 275, "seek": 107268, "start": 1078.68, "end": 1080.68, "text": " la regla de multiplicaci\u00f3n", "tokens": [50664, 635, 1121, 875, 368, 17596, 3482, 50764], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 276, "seek": 107268, "start": 1080.68, "end": 1082.68, "text": " de las probabilidades", "tokens": [50764, 368, 2439, 31959, 10284, 50864], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 277, "seek": 107268, "start": 1082.68, "end": 1084.68, "text": " que es, si yo tengo la probabilidad de una", "tokens": [50864, 631, 785, 11, 1511, 5290, 13989, 635, 31959, 4580, 368, 2002, 50964], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 278, "seek": 107268, "start": 1084.68, "end": 1086.68, "text": " secuencia de palabras", "tokens": [50964, 907, 47377, 368, 35240, 51064], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 279, "seek": 107268, "start": 1086.68, "end": 1088.68, "text": " W1, Wn", "tokens": [51064, 343, 16, 11, 343, 77, 51164], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 280, "seek": 107268, "start": 1088.68, "end": 1090.68, "text": " esto es", "tokens": [51164, 7433, 785, 51264], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 281, "seek": 107268, "start": 1090.68, "end": 1092.68, "text": " la probabilidad de la primera", "tokens": [51264, 635, 31959, 4580, 368, 635, 17382, 51364], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 282, "seek": 107268, "start": 1092.68, "end": 1094.68, "text": " palabra, de alguna forma", "tokens": [51364, 31702, 11, 368, 20651, 8366, 51464], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 283, "seek": 107268, "start": 1094.68, "end": 1096.68, "text": " la calculo", "tokens": [51464, 635, 4322, 78, 51564], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 284, "seek": 107268, "start": 1096.68, "end": 1098.68, "text": " por la probabilidad de la segunda dada", "tokens": [51564, 1515, 635, 31959, 4580, 368, 635, 21978, 274, 1538, 51664], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 285, "seek": 107268, "start": 1098.68, "end": 1100.68, "text": " la primera, dado que la primera", "tokens": [51664, 635, 17382, 11, 29568, 631, 635, 17382, 51764], "temperature": 0.0, "avg_logprob": -0.12831381797790528, "compression_ratio": 1.7797619047619047, "no_speech_prob": 0.003737933235242963}, {"id": 286, "seek": 110068, "start": 1100.68, "end": 1102.68, "text": " fue W1", "tokens": [50364, 9248, 343, 16, 50464], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 287, "seek": 110068, "start": 1102.68, "end": 1104.68, "text": " observen ac\u00e1 que", "tokens": [50464, 9951, 268, 23496, 631, 50564], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 288, "seek": 110068, "start": 1104.68, "end": 1106.68, "text": " no son independientes", "tokens": [50564, 572, 1872, 4819, 20135, 50664], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 289, "seek": 110068, "start": 1106.68, "end": 1108.68, "text": " es decir, las palabras por definici\u00f3n", "tokens": [50664, 785, 10235, 11, 2439, 35240, 1515, 1561, 15534, 50764], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 290, "seek": 110068, "start": 1108.68, "end": 1110.68, "text": " ac\u00e1, no son eventos independientes", "tokens": [50764, 23496, 11, 572, 1872, 2280, 329, 4819, 20135, 50864], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 291, "seek": 110068, "start": 1110.68, "end": 1112.68, "text": " es decir", "tokens": [50864, 785, 10235, 50964], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 292, "seek": 110068, "start": 1112.68, "end": 1114.68, "text": " tengo una cierta probabilidad de que", "tokens": [50964, 13989, 2002, 39769, 1328, 31959, 4580, 368, 631, 51064], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 293, "seek": 110068, "start": 1114.68, "end": 1116.68, "text": " empiece con W1", "tokens": [51064, 4012, 46566, 416, 343, 16, 51164], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 294, "seek": 110068, "start": 1116.68, "end": 1118.68, "text": " la multiplico por la probabilidad de que", "tokens": [51164, 635, 12788, 2789, 1515, 635, 31959, 4580, 368, 631, 51264], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 295, "seek": 110068, "start": 1118.68, "end": 1120.68, "text": " la segunda sea W2, dado que la primera", "tokens": [51264, 635, 21978, 4158, 343, 17, 11, 29568, 631, 635, 17382, 51364], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 296, "seek": 110068, "start": 1120.68, "end": 1122.68, "text": " fue W1", "tokens": [51364, 9248, 343, 16, 51464], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 297, "seek": 110068, "start": 1122.68, "end": 1124.68, "text": " por la probabilidad que", "tokens": [51464, 1515, 635, 31959, 4580, 631, 51564], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 298, "seek": 110068, "start": 1124.68, "end": 1126.68, "text": " la tercera sea W3, dado que las dos primeras", "tokens": [51564, 635, 1796, 41034, 4158, 343, 18, 11, 29568, 631, 2439, 4491, 2886, 6985, 51664], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 299, "seek": 110068, "start": 1126.68, "end": 1128.68, "text": " fueron uno de hoy as\u00ed", "tokens": [51664, 28739, 8526, 368, 13775, 8582, 51764], "temperature": 0.0, "avg_logprob": -0.10762380942320213, "compression_ratio": 1.7871287128712872, "no_speech_prob": 0.027046674862504005}, {"id": 300, "seek": 112868, "start": 1128.68, "end": 1130.68, "text": " de acuerdo", "tokens": [50364, 368, 28113, 50464], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 301, "seek": 112868, "start": 1130.68, "end": 1132.68, "text": " de esa forma con esta regla yo", "tokens": [50464, 368, 11342, 8366, 416, 5283, 1121, 875, 5290, 50564], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 302, "seek": 112868, "start": 1132.68, "end": 1134.68, "text": " y al final Wn", "tokens": [50564, 288, 419, 2572, 343, 77, 50664], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 303, "seek": 112868, "start": 1134.68, "end": 1136.68, "text": " la \u00faltima dada todas las anteriores", "tokens": [50664, 635, 28118, 274, 1538, 10906, 2439, 364, 34345, 2706, 50764], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 304, "seek": 112868, "start": 1136.68, "end": 1138.68, "text": " esto se llama", "tokens": [50764, 7433, 369, 23272, 50864], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 305, "seek": 112868, "start": 1138.68, "end": 1140.68, "text": " regla de la cadena", "tokens": [50864, 1121, 875, 368, 635, 12209, 4118, 50964], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 306, "seek": 112868, "start": 1140.68, "end": 1142.68, "text": " yo con la regla de la cadena", "tokens": [50964, 5290, 416, 635, 1121, 875, 368, 635, 12209, 4118, 51064], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 307, "seek": 112868, "start": 1142.68, "end": 1144.68, "text": " puedo calcular la probabilidad", "tokens": [51064, 21612, 2104, 17792, 635, 31959, 4580, 51164], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 308, "seek": 112868, "start": 1146.68, "end": 1148.68, "text": " de una secuencia o de una oraci\u00f3n", "tokens": [51264, 368, 2002, 907, 47377, 277, 368, 2002, 420, 3482, 51364], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 309, "seek": 112868, "start": 1148.68, "end": 1150.68, "text": " dada la secuencia", "tokens": [51364, 274, 1538, 635, 907, 47377, 51464], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 310, "seek": 112868, "start": 1150.68, "end": 1152.68, "text": " si logro calcular estas", "tokens": [51464, 1511, 3565, 340, 2104, 17792, 13897, 51564], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 311, "seek": 112868, "start": 1152.68, "end": 1154.68, "text": " probabilidades, o sea", "tokens": [51564, 31959, 10284, 11, 277, 4158, 51664], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 312, "seek": 112868, "start": 1154.68, "end": 1156.68, "text": " si logro calcular", "tokens": [51664, 1511, 3565, 340, 2104, 17792, 51764], "temperature": 0.0, "avg_logprob": -0.08328752697638746, "compression_ratio": 1.8303030303030303, "no_speech_prob": 0.005887374747544527}, {"id": 313, "seek": 115668, "start": 1156.68, "end": 1158.68, "text": " predecir las palabras", "tokens": [50364, 24874, 23568, 2439, 35240, 50464], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 314, "seek": 115668, "start": 1158.68, "end": 1160.68, "text": " correctamente", "tokens": [50464, 3006, 3439, 50564], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 315, "seek": 115668, "start": 1160.68, "end": 1162.68, "text": " voy a", "tokens": [50564, 7552, 257, 50664], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 316, "seek": 115668, "start": 1162.68, "end": 1164.68, "text": " poder predecir la secuencia", "tokens": [50664, 8152, 24874, 23568, 635, 907, 47377, 50764], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 317, "seek": 115668, "start": 1164.68, "end": 1166.68, "text": " de esa forma paso de la predicci\u00f3n al c\u00e1lculo", "tokens": [50764, 368, 11342, 8366, 29212, 368, 635, 47336, 5687, 419, 6476, 75, 25436, 50864], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 318, "seek": 115668, "start": 1166.68, "end": 1168.68, "text": " de toda la probabilidad de la oraci\u00f3n \u00bfse entienden?", "tokens": [50864, 368, 11687, 635, 31959, 4580, 368, 635, 420, 3482, 3841, 405, 948, 1174, 268, 30, 50964], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 319, "seek": 115668, "start": 1172.68, "end": 1174.68, "text": " bien", "tokens": [51164, 3610, 51264], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 320, "seek": 115668, "start": 1174.68, "end": 1176.68, "text": " entonces vamos a quedarnos con esa notaci\u00f3n", "tokens": [51264, 13003, 5295, 257, 13617, 24979, 416, 11342, 406, 3482, 51364], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 321, "seek": 115668, "start": 1178.68, "end": 1180.68, "text": " entonces yo digo bueno", "tokens": [51464, 13003, 5290, 22990, 11974, 51564], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 322, "seek": 115668, "start": 1180.68, "end": 1182.68, "text": " un ejemplo no, si yo quiero saber", "tokens": [51564, 517, 13358, 572, 11, 1511, 5290, 16811, 12489, 51664], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 323, "seek": 115668, "start": 1182.68, "end": 1184.68, "text": " la probabilidad de", "tokens": [51664, 635, 31959, 4580, 368, 51764], "temperature": 0.0, "avg_logprob": -0.1306736809866769, "compression_ratio": 1.6195652173913044, "no_speech_prob": 0.002514217747375369}, {"id": 324, "seek": 118468, "start": 1184.68, "end": 1186.68, "text": " viento fuerte de componente sudoeste", "tokens": [50364, 371, 7814, 37129, 368, 4026, 1576, 459, 2595, 8887, 50464], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 325, "seek": 118468, "start": 1186.68, "end": 1188.68, "text": " como el que est\u00e1 soplando", "tokens": [50464, 2617, 806, 631, 3192, 370, 564, 1806, 50564], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 326, "seek": 118468, "start": 1188.68, "end": 1190.68, "text": " no s\u00e9 si de componente sudoeste pero es fuerte", "tokens": [50564, 572, 7910, 1511, 368, 4026, 1576, 459, 2595, 8887, 4768, 785, 37129, 50664], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 327, "seek": 118468, "start": 1190.68, "end": 1192.68, "text": " es la probabilidad de viento", "tokens": [50664, 785, 635, 31959, 4580, 368, 371, 7814, 50764], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 328, "seek": 118468, "start": 1192.68, "end": 1194.68, "text": " por la probabilidad de fuerte", "tokens": [50764, 1515, 635, 31959, 4580, 368, 37129, 50864], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 329, "seek": 118468, "start": 1194.68, "end": 1196.68, "text": " dado viento por la probabilidad de", "tokens": [50864, 29568, 371, 7814, 1515, 635, 31959, 4580, 368, 50964], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 330, "seek": 118468, "start": 1196.68, "end": 1198.68, "text": " dado viento fuerte etc.", "tokens": [50964, 29568, 371, 7814, 37129, 5183, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 331, "seek": 118468, "start": 1198.68, "end": 1200.68, "text": " nada menos que la regla de la cadena", "tokens": [51064, 8096, 8902, 631, 635, 1121, 875, 368, 635, 12209, 4118, 51164], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 332, "seek": 118468, "start": 1206.68, "end": 1208.68, "text": " entonces", "tokens": [51464, 13003, 51564], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 333, "seek": 118468, "start": 1208.68, "end": 1210.68, "text": " yo quiero saber la \u00faltima", "tokens": [51564, 5290, 16811, 12489, 635, 28118, 51664], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 334, "seek": 118468, "start": 1210.68, "end": 1212.68, "text": " p de sudoeste dado viento fuerte", "tokens": [51664, 280, 368, 459, 2595, 8887, 29568, 371, 7814, 37129, 51764], "temperature": 0.0, "avg_logprob": -0.1509079933166504, "compression_ratio": 2.042682926829268, "no_speech_prob": 0.03289766609668732}, {"id": 335, "seek": 121268, "start": 1212.68, "end": 1214.68, "text": " de componente", "tokens": [50364, 368, 4026, 1576, 50464], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 336, "seek": 121268, "start": 1214.68, "end": 1216.68, "text": " y vos con google por ejemplo y digo", "tokens": [50464, 288, 13845, 416, 20742, 1515, 13358, 288, 22990, 50564], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 337, "seek": 121268, "start": 1216.68, "end": 1218.68, "text": " bueno viento fuerte de componente", "tokens": [50564, 11974, 371, 7814, 37129, 368, 4026, 1576, 50664], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 338, "seek": 121268, "start": 1218.68, "end": 1220.68, "text": " aparece 9.230 veces", "tokens": [50664, 37863, 1722, 13, 17, 3446, 17054, 50764], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 339, "seek": 121268, "start": 1222.68, "end": 1224.68, "text": " viento fuerte de componente sudoeste", "tokens": [50864, 371, 7814, 37129, 368, 4026, 1576, 459, 2595, 8887, 50964], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 340, "seek": 121268, "start": 1224.68, "end": 1226.68, "text": " aparece 347 veces", "tokens": [50964, 37863, 12790, 22, 17054, 51064], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 341, "seek": 121268, "start": 1226.68, "end": 1228.68, "text": " y yo entonces", "tokens": [51064, 288, 5290, 13003, 51164], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 342, "seek": 121268, "start": 1228.68, "end": 1230.68, "text": " voy a estimar la probabilidad de esa", "tokens": [51164, 7552, 257, 8017, 289, 635, 31959, 4580, 368, 11342, 51264], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 343, "seek": 121268, "start": 1230.68, "end": 1232.68, "text": " por medio de conteos", "tokens": [51264, 1515, 22123, 368, 34444, 329, 51364], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 344, "seek": 121268, "start": 1232.68, "end": 1234.68, "text": " la cantidad de veces que apareci\u00f3", "tokens": [51364, 635, 33757, 368, 17054, 631, 15004, 19609, 51464], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 345, "seek": 121268, "start": 1234.68, "end": 1236.68, "text": " viento fuerte de componente sudoeste", "tokens": [51464, 371, 7814, 37129, 368, 4026, 1576, 459, 2595, 8887, 51564], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 346, "seek": 121268, "start": 1236.68, "end": 1238.68, "text": " dividido la cantidad de veces que aparece", "tokens": [51564, 4996, 2925, 635, 33757, 368, 17054, 631, 37863, 51664], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 347, "seek": 121268, "start": 1238.68, "end": 1240.68, "text": " fuerte de componente", "tokens": [51664, 37129, 368, 4026, 1576, 51764], "temperature": 0.0, "avg_logprob": -0.1392384182323109, "compression_ratio": 2.134502923976608, "no_speech_prob": 0.041580427438020706}, {"id": 348, "seek": 124068, "start": 1240.68, "end": 1242.68, "text": " dividido 9.230", "tokens": [50364, 4996, 2925, 1722, 13, 17, 3446, 50464], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 349, "seek": 124068, "start": 1242.68, "end": 1244.68, "text": " \u00bfAguardo?", "tokens": [50464, 3841, 32, 2794, 12850, 30, 50564], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 350, "seek": 124068, "start": 1244.68, "end": 1246.68, "text": " y esta es la probabilidad", "tokens": [50564, 288, 5283, 785, 635, 31959, 4580, 50664], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 351, "seek": 124068, "start": 1246.68, "end": 1248.68, "text": " de que la siguiente palabra", "tokens": [50664, 368, 631, 635, 25666, 31702, 50764], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 352, "seek": 124068, "start": 1248.68, "end": 1250.68, "text": " sea sudoeste en mi estimaci\u00f3n", "tokens": [50764, 4158, 459, 2595, 8887, 465, 2752, 8017, 3482, 50864], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 353, "seek": 124068, "start": 1250.68, "end": 1252.68, "text": " si ustedes se fijan", "tokens": [50864, 1511, 17110, 369, 42001, 282, 50964], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 354, "seek": 124068, "start": 1252.68, "end": 1254.68, "text": " esto es una probabilidad", "tokens": [50964, 7433, 785, 2002, 31959, 4580, 51064], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 355, "seek": 124068, "start": 1254.68, "end": 1256.68, "text": " porque", "tokens": [51064, 4021, 51164], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 356, "seek": 124068, "start": 1256.68, "end": 1258.68, "text": " contando", "tokens": [51164, 660, 1806, 51264], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 357, "seek": 124068, "start": 1258.68, "end": 1260.68, "text": " todas las palabras posibles que pueden seguir", "tokens": [51264, 10906, 2439, 35240, 1366, 14428, 631, 14714, 18584, 51364], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 358, "seek": 124068, "start": 1260.68, "end": 1262.68, "text": " ac\u00e1 si yo logro determinar cu\u00e1les son", "tokens": [51364, 23496, 1511, 5290, 3565, 340, 3618, 6470, 2702, 842, 904, 1872, 51464], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 359, "seek": 124068, "start": 1264.68, "end": 1266.68, "text": " yo s\u00e9 que van a ver 9.230", "tokens": [51564, 5290, 7910, 631, 3161, 257, 1306, 1722, 13, 17, 3446, 51664], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 360, "seek": 124068, "start": 1266.68, "end": 1268.68, "text": " van a sumar 9.230", "tokens": [51664, 3161, 257, 2408, 289, 1722, 13, 17, 3446, 51764], "temperature": 0.0, "avg_logprob": -0.11213893360561794, "compression_ratio": 1.4876847290640394, "no_speech_prob": 0.04980640113353729}, {"id": 361, "seek": 126868, "start": 1268.68, "end": 1270.68, "text": " \u00bfno?", "tokens": [50364, 3841, 1771, 30, 50464], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 362, "seek": 126868, "start": 1270.68, "end": 1272.68, "text": " es decir todos los casos posibles", "tokens": [50464, 785, 10235, 6321, 1750, 25135, 1366, 14428, 50564], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 363, "seek": 126868, "start": 1272.68, "end": 1274.68, "text": " miro todos los casos junto a lo que es la siguiente palabra", "tokens": [50564, 2752, 340, 6321, 1750, 25135, 24663, 257, 450, 631, 785, 635, 25666, 31702, 50664], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 364, "seek": 126868, "start": 1276.68, "end": 1278.68, "text": " eso hace que como esto me va a dar", "tokens": [50764, 7287, 10032, 631, 2617, 7433, 385, 2773, 257, 4072, 50864], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 365, "seek": 126868, "start": 1278.68, "end": 1280.68, "text": " 9.230 a la suma de todas las cantidades", "tokens": [50864, 1722, 13, 17, 3446, 257, 635, 2408, 64, 368, 10906, 2439, 11223, 10284, 50964], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 366, "seek": 126868, "start": 1280.68, "end": 1282.68, "text": " esto va a dar uno", "tokens": [50964, 7433, 2773, 257, 4072, 8526, 51064], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 367, "seek": 126868, "start": 1282.68, "end": 1284.68, "text": " el total", "tokens": [51064, 806, 3217, 51164], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 368, "seek": 126868, "start": 1284.68, "end": 1286.68, "text": " entonces esto s\u00ed es una distribuci\u00f3n de probabilidad", "tokens": [51164, 13003, 7433, 8600, 785, 2002, 4400, 30813, 368, 31959, 4580, 51264], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 369, "seek": 126868, "start": 1286.68, "end": 1288.68, "text": " entonces que estamos bien", "tokens": [51264, 13003, 631, 10382, 3610, 51364], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 370, "seek": 126868, "start": 1288.68, "end": 1290.68, "text": " efectivamente aquello es una probabilidad", "tokens": [51364, 22565, 23957, 2373, 11216, 785, 2002, 31959, 4580, 51464], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 371, "seek": 126868, "start": 1290.68, "end": 1292.68, "text": " \u00bfde acuerdo? esto lo que me dice es", "tokens": [51464, 3841, 1479, 28113, 30, 7433, 450, 631, 385, 10313, 785, 51564], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 372, "seek": 126868, "start": 1292.68, "end": 1294.68, "text": " bueno", "tokens": [51564, 11974, 51664], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 373, "seek": 126868, "start": 1294.68, "end": 1296.68, "text": " el 3,76% de las veces", "tokens": [51664, 806, 805, 11, 25026, 4, 368, 2439, 17054, 51764], "temperature": 0.0, "avg_logprob": -0.16844907082802008, "compression_ratio": 1.6695278969957081, "no_speech_prob": 0.002706363331526518}, {"id": 374, "seek": 129668, "start": 1296.68, "end": 1298.68, "text": " es sudoeste la siguiente palabra", "tokens": [50364, 785, 459, 2595, 8887, 635, 25666, 31702, 50464], "temperature": 0.0, "avg_logprob": -0.14585613012313842, "compression_ratio": 1.6, "no_speech_prob": 0.006182126700878143}, {"id": 375, "seek": 129668, "start": 1308.68, "end": 1310.68, "text": " eso que acabamos de hacer", "tokens": [50964, 7287, 631, 13281, 2151, 368, 6720, 51064], "temperature": 0.0, "avg_logprob": -0.14585613012313842, "compression_ratio": 1.6, "no_speech_prob": 0.006182126700878143}, {"id": 376, "seek": 129668, "start": 1310.68, "end": 1312.68, "text": " es estimar la probabilidad", "tokens": [51064, 785, 8017, 289, 635, 31959, 4580, 51164], "temperature": 0.0, "avg_logprob": -0.14585613012313842, "compression_ratio": 1.6, "no_speech_prob": 0.006182126700878143}, {"id": 377, "seek": 129668, "start": 1312.68, "end": 1314.68, "text": " a partir de la frecuencia", "tokens": [51164, 257, 13906, 368, 635, 2130, 66, 47377, 51264], "temperature": 0.0, "avg_logprob": -0.14585613012313842, "compression_ratio": 1.6, "no_speech_prob": 0.006182126700878143}, {"id": 378, "seek": 129668, "start": 1314.68, "end": 1316.68, "text": " de ocurrencia en un cuerpo grande", "tokens": [51264, 368, 26430, 1095, 2755, 465, 517, 20264, 8883, 51364], "temperature": 0.0, "avg_logprob": -0.14585613012313842, "compression_ratio": 1.6, "no_speech_prob": 0.006182126700878143}, {"id": 379, "seek": 129668, "start": 1316.68, "end": 1318.68, "text": " eso Google es un cuerpo grande", "tokens": [51364, 7287, 3329, 785, 517, 20264, 8883, 51464], "temperature": 0.0, "avg_logprob": -0.14585613012313842, "compression_ratio": 1.6, "no_speech_prob": 0.006182126700878143}, {"id": 380, "seek": 129668, "start": 1318.68, "end": 1320.68, "text": " muy grande", "tokens": [51464, 5323, 8883, 51564], "temperature": 0.0, "avg_logprob": -0.14585613012313842, "compression_ratio": 1.6, "no_speech_prob": 0.006182126700878143}, {"id": 381, "seek": 129668, "start": 1320.68, "end": 1322.68, "text": " y eso se llama principio m\u00e1ximo", "tokens": [51564, 288, 7287, 369, 23272, 34308, 38876, 51664], "temperature": 0.0, "avg_logprob": -0.14585613012313842, "compression_ratio": 1.6, "no_speech_prob": 0.006182126700878143}, {"id": 382, "seek": 129668, "start": 1322.68, "end": 1324.68, "text": " pero similitud que lo vimos en la de pasada", "tokens": [51664, 4768, 1034, 388, 21875, 631, 450, 49266, 465, 635, 368, 1736, 1538, 51764], "temperature": 0.0, "avg_logprob": -0.14585613012313842, "compression_ratio": 1.6, "no_speech_prob": 0.006182126700878143}, {"id": 383, "seek": 132468, "start": 1324.68, "end": 1326.68, "text": " es trato de hacer", "tokens": [50364, 785, 504, 2513, 368, 6720, 50464], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 384, "seek": 132468, "start": 1326.68, "end": 1328.68, "text": " calcular la probabilidad en base", "tokens": [50464, 2104, 17792, 635, 31959, 4580, 465, 3096, 50564], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 385, "seek": 132468, "start": 1328.68, "end": 1330.68, "text": " a lo mejor posible", "tokens": [50564, 257, 450, 11479, 26644, 50664], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 386, "seek": 132468, "start": 1330.68, "end": 1332.68, "text": " a los datos que tengo", "tokens": [50664, 257, 1750, 27721, 631, 13989, 50764], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 387, "seek": 132468, "start": 1332.68, "end": 1334.68, "text": " es decir considero", "tokens": [50764, 785, 10235, 1949, 78, 50864], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 388, "seek": 132468, "start": 1334.68, "end": 1336.68, "text": " yo estoy considerando que los datos que tengo", "tokens": [50864, 5290, 15796, 1949, 1806, 631, 1750, 27721, 631, 13989, 50964], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 389, "seek": 132468, "start": 1336.68, "end": 1338.68, "text": " es decir el corpo de Google", "tokens": [50964, 785, 10235, 806, 23257, 368, 3329, 51064], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 390, "seek": 132468, "start": 1338.68, "end": 1340.68, "text": " es una buena aproximaci\u00f3n", "tokens": [51064, 785, 2002, 25710, 31270, 3482, 51164], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 391, "seek": 132468, "start": 1340.68, "end": 1342.68, "text": " del mundo de lenguaje", "tokens": [51164, 1103, 7968, 368, 35044, 84, 11153, 51264], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 392, "seek": 132468, "start": 1342.68, "end": 1344.68, "text": " en realidad", "tokens": [51264, 465, 25635, 51364], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 393, "seek": 132468, "start": 1344.68, "end": 1346.68, "text": " yo no s\u00e9 si en realidad", "tokens": [51364, 5290, 572, 7910, 1511, 465, 25635, 51464], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 394, "seek": 132468, "start": 1346.68, "end": 1348.68, "text": " efectivamente cuando los seres humanos hablamos", "tokens": [51464, 22565, 23957, 7767, 1750, 44721, 34555, 26280, 2151, 51564], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 395, "seek": 132468, "start": 1348.68, "end": 1350.68, "text": " hay un 3,76%", "tokens": [51564, 4842, 517, 805, 11, 25026, 4, 51664], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 396, "seek": 132468, "start": 1350.68, "end": 1352.68, "text": " de probabilidad", "tokens": [51664, 368, 31959, 4580, 51764], "temperature": 0.0, "avg_logprob": -0.08845001796506485, "compression_ratio": 1.6523809523809523, "no_speech_prob": 0.04895779490470886}, {"id": 397, "seek": 135268, "start": 1352.68, "end": 1354.68, "text": " de que despu\u00e9s de decir", "tokens": [50364, 368, 631, 15283, 368, 10235, 50464], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 398, "seek": 135268, "start": 1354.68, "end": 1356.68, "text": " viento fuerte componente", "tokens": [50464, 371, 7814, 37129, 4026, 1576, 50564], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 399, "seek": 135268, "start": 1356.68, "end": 1358.68, "text": " viene sudoeste", "tokens": [50564, 19561, 459, 2595, 8887, 50664], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 400, "seek": 135268, "start": 1358.68, "end": 1360.68, "text": " pero el corpo de Google", "tokens": [50664, 4768, 806, 23257, 368, 3329, 50764], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 401, "seek": 135268, "start": 1360.68, "end": 1362.68, "text": " que es lo mejor que tengo como aproximaci\u00f3n", "tokens": [50764, 631, 785, 450, 11479, 631, 13989, 2617, 31270, 3482, 50864], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 402, "seek": 135268, "start": 1362.68, "end": 1364.68, "text": " me dice eso y eso es lo que yo utilizo", "tokens": [50864, 385, 10313, 7287, 288, 7287, 785, 450, 631, 5290, 4976, 19055, 50964], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 403, "seek": 135268, "start": 1364.68, "end": 1366.68, "text": " como un estimador de m\u00e1xima verosimilitud", "tokens": [50964, 2617, 517, 8017, 5409, 368, 31031, 64, 1306, 329, 332, 388, 21875, 51064], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 404, "seek": 135268, "start": 1366.68, "end": 1368.68, "text": " es lo mejor que puedo acercarme", "tokens": [51064, 785, 450, 11479, 631, 21612, 696, 2869, 35890, 51164], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 405, "seek": 135268, "start": 1368.68, "end": 1370.68, "text": " con el cuerpo que tengo", "tokens": [51164, 416, 806, 20264, 631, 13989, 51264], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 406, "seek": 135268, "start": 1370.68, "end": 1372.68, "text": " eso es lo que vamos a hacer todo el tiempo ac\u00e1", "tokens": [51264, 7287, 785, 450, 631, 5295, 257, 6720, 5149, 806, 11772, 23496, 51364], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 407, "seek": 135268, "start": 1372.68, "end": 1374.68, "text": " calcular", "tokens": [51364, 2104, 17792, 51464], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 408, "seek": 135268, "start": 1374.68, "end": 1376.68, "text": " componentes de m\u00e1xima verosimilitud", "tokens": [51464, 6542, 279, 368, 31031, 64, 1306, 329, 332, 388, 21875, 51564], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 409, "seek": 135268, "start": 1378.68, "end": 1380.68, "text": " pero tenemos alg\u00fan problema", "tokens": [51664, 4768, 9914, 26300, 12395, 51764], "temperature": 0.0, "avg_logprob": -0.08782453296565208, "compression_ratio": 1.8073394495412844, "no_speech_prob": 0.026651114225387573}, {"id": 410, "seek": 138068, "start": 1380.68, "end": 1382.68, "text": " y es", "tokens": [50364, 288, 785, 50464], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 411, "seek": 138068, "start": 1382.68, "end": 1384.68, "text": " en el otro casos", "tokens": [50464, 465, 806, 11921, 25135, 50564], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 412, "seek": 138068, "start": 1384.68, "end": 1386.68, "text": " dice, a ra\u00edz de estos fen\u00f3menos se producir\u00e1n", "tokens": [50564, 10313, 11, 257, 3342, 44551, 368, 12585, 26830, 812, 2558, 329, 369, 1082, 23568, 7200, 50664], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 413, "seek": 138068, "start": 1386.68, "end": 1388.68, "text": " tormentas fuertes", "tokens": [50664, 36662, 296, 8536, 911, 279, 50764], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 414, "seek": 138068, "start": 1388.68, "end": 1390.68, "text": " la probabilidad de fuertes", "tokens": [50764, 635, 31959, 4580, 368, 8536, 911, 279, 50864], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 415, "seek": 138068, "start": 1390.68, "end": 1392.68, "text": " y a ra\u00edz de estos fen\u00f3menos se producir\u00e1n tormentas", "tokens": [50864, 288, 257, 3342, 44551, 368, 12585, 26830, 812, 2558, 329, 369, 1082, 23568, 7200, 36662, 296, 50964], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 416, "seek": 138068, "start": 1394.68, "end": 1396.68, "text": " tienen un problema y es que", "tokens": [51064, 12536, 517, 12395, 288, 785, 631, 51164], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 417, "seek": 138068, "start": 1396.68, "end": 1398.68, "text": " nunca apareci\u00f3 en mi corpus", "tokens": [51164, 13768, 15004, 19609, 465, 2752, 1181, 31624, 51264], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 418, "seek": 138068, "start": 1398.68, "end": 1400.68, "text": " a ra\u00edz de estos fen\u00f3menos", "tokens": [51264, 257, 3342, 44551, 368, 12585, 26830, 812, 2558, 329, 51364], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 419, "seek": 138068, "start": 1400.68, "end": 1402.68, "text": " se producir\u00e1n tormentas", "tokens": [51364, 369, 1082, 23568, 7200, 36662, 296, 51464], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 420, "seek": 138068, "start": 1402.68, "end": 1404.68, "text": " y nunca apareci\u00f3 en mi corpus", "tokens": [51464, 288, 13768, 15004, 19609, 465, 2752, 1181, 31624, 51564], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 421, "seek": 138068, "start": 1404.68, "end": 1406.68, "text": " a ra\u00edz de estos fen\u00f3menos se producir\u00e1n tormentas fuertes", "tokens": [51564, 257, 3342, 44551, 368, 12585, 26830, 812, 2558, 329, 369, 1082, 23568, 7200, 36662, 296, 8536, 911, 279, 51664], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 422, "seek": 138068, "start": 1406.68, "end": 1408.68, "text": " y", "tokens": [51664, 288, 51764], "temperature": 1.0, "avg_logprob": -0.23403637336962152, "compression_ratio": 2.5616438356164384, "no_speech_prob": 0.030691692605614662}, {"id": 423, "seek": 140868, "start": 1408.68, "end": 1416.44, "text": " Y eso nos da una horrible visi\u00f3n por cero, que queremos evitar, o sea que nuestra probabilidad", "tokens": [50364, 398, 7287, 3269, 1120, 2002, 9263, 1452, 2560, 1515, 269, 2032, 11, 631, 26813, 31326, 11, 277, 4158, 631, 16825, 31959, 4580, 50752], "temperature": 0.0, "avg_logprob": -0.2713117928340517, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.3110971450805664}, {"id": 424, "seek": 140868, "start": 1416.44, "end": 1421.72, "text": " da infinito, no s\u00e9, no est\u00e1 definida.", "tokens": [50752, 1120, 7193, 3528, 11, 572, 7910, 11, 572, 3192, 1561, 2887, 13, 51016], "temperature": 0.0, "avg_logprob": -0.2713117928340517, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.3110971450805664}, {"id": 425, "seek": 140868, "start": 1421.72, "end": 1426.5600000000002, "text": " Esto, una pregunta, \u00bfesto les parece que es un fen\u00f3meno com\u00fan o no, que nos puede", "tokens": [51016, 20880, 11, 2002, 24252, 11, 3841, 18465, 1512, 14120, 631, 785, 517, 26830, 812, 43232, 45448, 277, 572, 11, 631, 3269, 8919, 51258], "temperature": 0.0, "avg_logprob": -0.2713117928340517, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.3110971450805664}, {"id": 426, "seek": 140868, "start": 1426.5600000000002, "end": 1434.1200000000001, "text": " pasar cuando estemos estimando todo el tiempo, porque por m\u00e1s grande que sea el corpus, el", "tokens": [51258, 25344, 7767, 871, 4485, 8017, 1806, 5149, 806, 11772, 11, 4021, 1515, 3573, 8883, 631, 4158, 806, 1181, 31624, 11, 806, 51636], "temperature": 0.0, "avg_logprob": -0.2713117928340517, "compression_ratio": 1.5072463768115942, "no_speech_prob": 0.3110971450805664}, {"id": 427, "seek": 143412, "start": 1434.12, "end": 1438.9599999999998, "text": " lenguaje es muy creativo?", "tokens": [50364, 35044, 84, 11153, 785, 5323, 1428, 6340, 30, 50606], "temperature": 0.0, "avg_logprob": -0.2508923379998458, "compression_ratio": 1.6150793650793651, "no_speech_prob": 0.13588759303092957}, {"id": 428, "seek": 143412, "start": 1438.9599999999998, "end": 1445.08, "text": " Entonces tenemos que buscar forma y adem\u00e1s porque estamos haciendo un conteo de palabras", "tokens": [50606, 15097, 9914, 631, 26170, 8366, 288, 21251, 4021, 10382, 20509, 517, 34444, 78, 368, 35240, 50912], "temperature": 0.0, "avg_logprob": -0.2508923379998458, "compression_ratio": 1.6150793650793651, "no_speech_prob": 0.13588759303092957}, {"id": 429, "seek": 143412, "start": 1445.08, "end": 1446.08, "text": " de oraciones muy largas.", "tokens": [50912, 368, 420, 9188, 5323, 1613, 10549, 13, 50962], "temperature": 0.0, "avg_logprob": -0.2508923379998458, "compression_ratio": 1.6150793650793651, "no_speech_prob": 0.13588759303092957}, {"id": 430, "seek": 143412, "start": 1446.08, "end": 1452.32, "text": " O sea que la regla de la cadena no resuelve mi problema, porque yo, una aproximaci\u00f3n", "tokens": [50962, 422, 4158, 631, 635, 1121, 875, 368, 635, 12209, 4118, 572, 725, 3483, 303, 2752, 12395, 11, 4021, 5290, 11, 2002, 31270, 3482, 51274], "temperature": 0.0, "avg_logprob": -0.2508923379998458, "compression_ratio": 1.6150793650793651, "no_speech_prob": 0.13588759303092957}, {"id": 431, "seek": 143412, "start": 1452.32, "end": 1457.6, "text": " bien naif para calcular la probabilidad de calcular toda la secuencia posible, \u00bfs\u00ed?", "tokens": [51274, 3610, 1667, 351, 1690, 2104, 17792, 635, 31959, 4580, 368, 2104, 17792, 11687, 635, 907, 47377, 26644, 11, 3841, 82, 870, 30, 51538], "temperature": 0.0, "avg_logprob": -0.2508923379998458, "compression_ratio": 1.6150793650793651, "no_speech_prob": 0.13588759303092957}, {"id": 432, "seek": 143412, "start": 1457.6, "end": 1461.12, "text": " \u00bfcu\u00e1ntas veces aparece la secuencia que quiero calcular, la oraci\u00f3n del total de", "tokens": [51538, 3841, 12032, 27525, 296, 17054, 37863, 635, 907, 47377, 631, 16811, 2104, 17792, 11, 635, 420, 3482, 1103, 3217, 368, 51714], "temperature": 0.0, "avg_logprob": -0.2508923379998458, "compression_ratio": 1.6150793650793651, "no_speech_prob": 0.13588759303092957}, {"id": 433, "seek": 143412, "start": 1461.12, "end": 1462.12, "text": " oraciones?", "tokens": [51714, 420, 9188, 30, 51764], "temperature": 0.0, "avg_logprob": -0.2508923379998458, "compression_ratio": 1.6150793650793651, "no_speech_prob": 0.13588759303092957}, {"id": 434, "seek": 146212, "start": 1462.12, "end": 1465.56, "text": " Bueno, tengo un corpus evidentemente grande, pero esta aproximaci\u00f3n tampoco nos ayuda", "tokens": [50364, 16046, 11, 13989, 517, 1181, 31624, 16371, 16288, 8883, 11, 4768, 5283, 31270, 3482, 36838, 3269, 30737, 50536], "temperature": 0.0, "avg_logprob": -0.23227513631184896, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.14476408064365387}, {"id": 435, "seek": 146212, "start": 1465.56, "end": 1471.9599999999998, "text": " mucho porque sigo teniendo contestos muy largos, porque si ustedes se fijan en la regla de", "tokens": [50536, 9824, 4021, 4556, 78, 2064, 7304, 10287, 329, 5323, 11034, 329, 11, 4021, 1511, 17110, 369, 42001, 282, 465, 635, 1121, 875, 368, 50856], "temperature": 0.0, "avg_logprob": -0.23227513631184896, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.14476408064365387}, {"id": 436, "seek": 146212, "start": 1471.9599999999998, "end": 1477.56, "text": " la cadena, bueno, en lo que acabamos de hacer, la \u00faltima probabilidad es casi la misma", "tokens": [50856, 635, 12209, 4118, 11, 11974, 11, 465, 450, 631, 13281, 2151, 368, 6720, 11, 635, 28118, 31959, 4580, 785, 22567, 635, 24946, 51136], "temperature": 0.0, "avg_logprob": -0.23227513631184896, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.14476408064365387}, {"id": 437, "seek": 146212, "start": 1477.56, "end": 1491.04, "text": " que la primera, menos una palabra, tengo que buscar una forma de achicar eso.", "tokens": [51136, 631, 635, 17382, 11, 8902, 2002, 31702, 11, 13989, 631, 26170, 2002, 8366, 368, 2800, 7953, 7287, 13, 51810], "temperature": 0.0, "avg_logprob": -0.23227513631184896, "compression_ratio": 1.5520361990950227, "no_speech_prob": 0.14476408064365387}, {"id": 438, "seek": 149104, "start": 1491.04, "end": 1500.68, "text": " Entonces, una de las ideas fuerzas para computar esta probabilidad es en lugar de tomar todas", "tokens": [50364, 15097, 11, 2002, 368, 2439, 3487, 17669, 24561, 1690, 2807, 289, 5283, 31959, 4580, 785, 465, 11467, 368, 22048, 10906, 50846], "temperature": 0.0, "avg_logprob": -0.2571500937143962, "compression_ratio": 1.4173228346456692, "no_speech_prob": 0.29798486828804016}, {"id": 439, "seek": 149104, "start": 1500.68, "end": 1508.12, "text": " las palabras, tomar sobre las \u00faltimas, es decir, yo me quedo con las \u00faltimas n menos", "tokens": [50846, 2439, 35240, 11, 22048, 5473, 2439, 11499, 17957, 11, 785, 10235, 11, 5290, 385, 13617, 78, 416, 2439, 11499, 17957, 297, 8902, 51218], "temperature": 0.0, "avg_logprob": -0.2571500937143962, "compression_ratio": 1.4173228346456692, "no_speech_prob": 0.29798486828804016}, {"id": 440, "seek": 150812, "start": 1508.12, "end": 1520.3999999999999, "text": " un palabras, \u00bfs\u00ed? n menos n, bueno. \u00bfs\u00ed? En esto es enigrante, \u00bfno? Y las otras no", "tokens": [50364, 517, 35240, 11, 3841, 82, 870, 30, 297, 8902, 297, 11, 11974, 13, 3841, 82, 870, 30, 2193, 7433, 785, 465, 328, 81, 2879, 11, 3841, 1771, 30, 398, 2439, 20244, 572, 50978], "temperature": 0.0, "avg_logprob": -0.27386960750672873, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5840741991996765}, {"id": 441, "seek": 150812, "start": 1520.3999999999999, "end": 1529.12, "text": " las considero, digo, bueno, mi humilde aproximaci\u00f3n para que esto se pueda volver manejable es", "tokens": [50978, 2439, 1949, 78, 11, 22990, 11, 11974, 11, 2752, 1484, 15956, 31270, 3482, 1690, 631, 7433, 369, 31907, 33998, 12743, 73, 712, 785, 51414], "temperature": 0.0, "avg_logprob": -0.27386960750672873, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5840741991996765}, {"id": 442, "seek": 150812, "start": 1529.12, "end": 1532.9599999999998, "text": " decir, bueno, yo en realidad solamente me importan las \u00faltimas palabras afectan a la", "tokens": [51414, 10235, 11, 11974, 11, 5290, 465, 25635, 27814, 385, 974, 282, 2439, 11499, 17957, 35240, 30626, 282, 257, 635, 51606], "temperature": 0.0, "avg_logprob": -0.27386960750672873, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.5840741991996765}, {"id": 443, "seek": 153296, "start": 1532.96, "end": 1542.32, "text": " que voy a predecir, son las \u00faltimas. Y de eso se tratan los modelos enigramas que", "tokens": [50364, 631, 7552, 257, 24874, 23568, 11, 1872, 2439, 11499, 17957, 13, 398, 368, 7287, 369, 21507, 282, 1750, 2316, 329, 465, 328, 2356, 296, 631, 50832], "temperature": 0.0, "avg_logprob": -0.2245809555053711, "compression_ratio": 1.5170454545454546, "no_speech_prob": 0.6084570288658142}, {"id": 444, "seek": 153296, "start": 1542.32, "end": 1545.8400000000001, "text": " utilizan lo que se llama, eso que acabo de decir, yo llamo hip\u00f3tesis de marcovo, hip\u00f3tesis", "tokens": [50832, 19906, 282, 450, 631, 369, 23272, 11, 7287, 631, 13281, 78, 368, 10235, 11, 5290, 4849, 10502, 8103, 812, 7269, 271, 368, 1849, 1291, 3080, 11, 8103, 812, 7269, 271, 51008], "temperature": 0.0, "avg_logprob": -0.2245809555053711, "compression_ratio": 1.5170454545454546, "no_speech_prob": 0.6084570288658142}, {"id": 445, "seek": 153296, "start": 1545.8400000000001, "end": 1556.88, "text": " marcoviana. Solamente las \u00faltimas palabras afectan a siguiente, hay un l\u00edmite. Y f\u00edjense", "tokens": [51008, 1849, 1291, 85, 8497, 13, 7026, 3439, 2439, 11499, 17957, 35240, 30626, 282, 257, 25666, 11, 4842, 517, 287, 14569, 642, 13, 398, 283, 870, 73, 1288, 51560], "temperature": 0.0, "avg_logprob": -0.2245809555053711, "compression_ratio": 1.5170454545454546, "no_speech_prob": 0.6084570288658142}, {"id": 446, "seek": 155688, "start": 1556.88, "end": 1564.0800000000002, "text": " que en la hip\u00f3tesis de bigrama, yo digo, cada palabra es la pr\u00f3xima por la anterior,", "tokens": [50364, 631, 465, 635, 8103, 812, 7269, 271, 368, 955, 29762, 11, 5290, 22990, 11, 8411, 31702, 785, 635, 24096, 1515, 635, 22272, 11, 50724], "temperature": 0.0, "avg_logprob": -0.2775375865330206, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.6585492491722107}, {"id": 447, "seek": 155688, "start": 1564.0800000000002, "end": 1569.64, "text": " simplemente, estoy diciendo una cosa tan sencilla como la \u00faltima, la \u00faltima palabra es la", "tokens": [50724, 33190, 11, 15796, 42797, 2002, 10163, 7603, 3151, 66, 5291, 2617, 635, 28118, 11, 635, 28118, 31702, 785, 635, 51002], "temperature": 0.0, "avg_logprob": -0.2775375865330206, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.6585492491722107}, {"id": 448, "seek": 155688, "start": 1569.64, "end": 1575.5200000000002, "text": " \u00fanica, cada palabra es la siguiente, pero las anteriores no. Es muy fuerte, \u00bfno? Y", "tokens": [51002, 30104, 11, 8411, 31702, 785, 635, 25666, 11, 4768, 2439, 364, 34345, 2706, 572, 13, 2313, 5323, 37129, 11, 3841, 1771, 30, 398, 51296], "temperature": 0.0, "avg_logprob": -0.2775375865330206, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.6585492491722107}, {"id": 449, "seek": 155688, "start": 1575.5200000000002, "end": 1584.44, "text": " de trigramas son dos y con n, con n son n. \u00bfs\u00ed? Con la hip\u00f3tesis de bigrama, mi probabilidad", "tokens": [51296, 368, 35386, 2356, 296, 1872, 4491, 288, 416, 297, 11, 416, 297, 1872, 297, 13, 3841, 82, 870, 30, 2656, 635, 8103, 812, 7269, 271, 368, 955, 29762, 11, 2752, 31959, 4580, 51742], "temperature": 0.0, "avg_logprob": -0.2775375865330206, "compression_ratio": 1.6854460093896713, "no_speech_prob": 0.6585492491722107}, {"id": 450, "seek": 158444, "start": 1584.44, "end": 1591.16, "text": " es mucho m\u00e1s sencilla que antes, porque es como cada palabra solo depende, vamos a mirar,", "tokens": [50364, 785, 9824, 3573, 3151, 66, 5291, 631, 11014, 11, 4021, 785, 2617, 8411, 31702, 6944, 47091, 11, 5295, 257, 3149, 289, 11, 50700], "temperature": 0.0, "avg_logprob": -0.22780388944289265, "compression_ratio": 1.7388535031847134, "no_speech_prob": 0.3300141394138336}, {"id": 451, "seek": 158444, "start": 1591.16, "end": 1600.0800000000002, "text": " uno bueno uno no est\u00e1 m\u00e1s, pero cada palabra depende de la anterior, simplemente me queda", "tokens": [50700, 8526, 11974, 8526, 572, 3192, 3573, 11, 4768, 8411, 31702, 47091, 368, 635, 22272, 11, 33190, 385, 23314, 51146], "temperature": 0.0, "avg_logprob": -0.22780388944289265, "compression_ratio": 1.7388535031847134, "no_speech_prob": 0.3300141394138336}, {"id": 452, "seek": 158444, "start": 1600.0800000000002, "end": 1612.4, "text": " que la probabilidad de una secuencia es la probabilidad de la primera, por la probabilidad", "tokens": [51146, 631, 635, 31959, 4580, 368, 2002, 907, 47377, 785, 635, 31959, 4580, 368, 635, 17382, 11, 1515, 635, 31959, 4580, 51762], "temperature": 0.0, "avg_logprob": -0.22780388944289265, "compression_ratio": 1.7388535031847134, "no_speech_prob": 0.3300141394138336}, {"id": 453, "seek": 161240, "start": 1612.4, "end": 1632.2, "text": " de la segunda a la primera, por la probabilidad de la tercera a la segunda, etc\u00e9tera. \u00bfLe", "tokens": [50364, 368, 635, 21978, 257, 635, 17382, 11, 1515, 635, 31959, 4580, 368, 635, 1796, 41034, 257, 635, 21978, 11, 5183, 526, 23833, 13, 3841, 11020, 51354], "temperature": 0.0, "avg_logprob": -0.2692566754525168, "compression_ratio": 1.2816901408450705, "no_speech_prob": 0.222614586353302}, {"id": 454, "seek": 161240, "start": 1632.2, "end": 1639.88, "text": " guardo? Ac\u00e1 nos falta este PW1 en esa f\u00f3rmula, pero no nos preocupa demasiado porque eso", "tokens": [51354, 6290, 78, 30, 5097, 842, 3269, 22111, 4065, 430, 54, 16, 465, 11342, 283, 15614, 76, 3780, 11, 4768, 572, 3269, 23080, 64, 39820, 4021, 7287, 51738], "temperature": 0.0, "avg_logprob": -0.2692566754525168, "compression_ratio": 1.2816901408450705, "no_speech_prob": 0.222614586353302}, {"id": 455, "seek": 163988, "start": 1639.88, "end": 1644.7600000000002, "text": " lo resolvemos poniendo una marca al comienzo de la secuencia que siempre vale uno, su probabilidad,", "tokens": [50364, 450, 14151, 3415, 9224, 7304, 2002, 30582, 419, 395, 1053, 4765, 368, 635, 907, 47377, 631, 12758, 15474, 8526, 11, 459, 31959, 4580, 11, 50608], "temperature": 0.0, "avg_logprob": -0.262940201104856, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.4698336124420166}, {"id": 456, "seek": 163988, "start": 1644.7600000000002, "end": 1651.1200000000001, "text": " es decir, que toda la graci\u00f3n empieza con una marca. Y si no, multiplico ac\u00e1, \u00bfno? Si", "tokens": [50608, 785, 10235, 11, 631, 11687, 635, 677, 3482, 44577, 416, 2002, 30582, 13, 398, 1511, 572, 11, 12788, 2789, 23496, 11, 3841, 1771, 30, 4909, 50926], "temperature": 0.0, "avg_logprob": -0.262940201104856, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.4698336124420166}, {"id": 457, "seek": 163988, "start": 1651.1200000000001, "end": 1656.92, "text": " no, si lo quiero hacer de otra forma, agregue un PW0 ac\u00e1 y lo mismo. Pero esencialmente", "tokens": [50926, 572, 11, 1511, 450, 16811, 6720, 368, 13623, 8366, 11, 623, 3375, 622, 517, 430, 54, 15, 23496, 288, 450, 12461, 13, 9377, 785, 26567, 4082, 51216], "temperature": 0.0, "avg_logprob": -0.262940201104856, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.4698336124420166}, {"id": 458, "seek": 163988, "start": 1656.92, "end": 1661.2, "text": " lo importante ac\u00e1 es que esto se transforma en una simple multiplicaci\u00f3n de probabilidades", "tokens": [51216, 450, 9416, 23496, 785, 631, 7433, 369, 4088, 64, 465, 2002, 2199, 17596, 3482, 368, 31959, 10284, 51430], "temperature": 0.0, "avg_logprob": -0.262940201104856, "compression_ratio": 1.5677966101694916, "no_speech_prob": 0.4698336124420166}, {"id": 459, "seek": 166120, "start": 1661.2, "end": 1669.0, "text": " de una palabra dada en anterior. \u00bfY c\u00f3mo hago para calcular esto? \u00bfC\u00f3mo puedo calcular", "tokens": [50364, 368, 2002, 31702, 274, 1538, 465, 22272, 13, 3841, 56, 12826, 38721, 1690, 2104, 17792, 7433, 30, 3841, 28342, 21612, 2104, 17792, 50754], "temperature": 0.0, "avg_logprob": -0.24282845088413785, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.691729724407196}, {"id": 460, "seek": 166120, "start": 1669.0, "end": 1680.4, "text": " esto ac\u00e1? \u00bfC\u00f3mo calcula la probabilidad de una palabra dada en anterior? Contando,", "tokens": [50754, 7433, 23496, 30, 3841, 28342, 4322, 64, 635, 31959, 4580, 368, 2002, 31702, 274, 1538, 465, 22272, 30, 4839, 1806, 11, 51324], "temperature": 0.0, "avg_logprob": -0.24282845088413785, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.691729724407196}, {"id": 461, "seek": 166120, "start": 1680.4, "end": 1684.1200000000001, "text": " pero solamente tienen cuenta dos, lo cual lo vuelve un problema mucho m\u00e1s manejable.", "tokens": [51324, 4768, 27814, 12536, 17868, 4491, 11, 450, 10911, 450, 20126, 303, 517, 12395, 9824, 3573, 12743, 73, 712, 13, 51510], "temperature": 0.0, "avg_logprob": -0.24282845088413785, "compression_ratio": 1.5783132530120483, "no_speech_prob": 0.691729724407196}, {"id": 462, "seek": 168412, "start": 1684.12, "end": 1693.4399999999998, "text": " Y eso es justo lo que vamos a hacer. Un modelo de lenguaje intenta predecir la pr\u00f3xima palabra", "tokens": [50364, 398, 7287, 785, 40534, 450, 631, 5295, 257, 6720, 13, 1156, 27825, 368, 35044, 84, 11153, 8446, 64, 24874, 23568, 635, 24096, 31702, 50830], "temperature": 0.0, "avg_logprob": -0.2293437405636436, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.007315601222217083}, {"id": 463, "seek": 168412, "start": 1693.4399999999998, "end": 1698.6799999999998, "text": " de una oraci\u00f3n a partir de las n menos una anteriores y, por supuesto, que importa el", "tokens": [50830, 368, 2002, 420, 3482, 257, 13906, 368, 2439, 297, 8902, 2002, 364, 34345, 2706, 288, 11, 1515, 34177, 11, 631, 33218, 806, 51092], "temperature": 0.0, "avg_logprob": -0.2293437405636436, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.007315601222217083}, {"id": 464, "seek": 168412, "start": 1698.6799999999998, "end": 1709.32, "text": " orden en ese c\u00e1lculo, \u00bfno? Tambi\u00e9n tenemos que plantearnos cuando hagamos los engramas,", "tokens": [51092, 28615, 465, 10167, 6476, 75, 25436, 11, 3841, 1771, 30, 25682, 9914, 631, 36829, 24979, 7767, 42386, 2151, 1750, 465, 1342, 296, 11, 51624], "temperature": 0.0, "avg_logprob": -0.2293437405636436, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.007315601222217083}, {"id": 465, "seek": 170932, "start": 1710.32, "end": 1714.9199999999998, "text": " cuando calculemos la probabilidad de una palabra, bueno, cosas que ya hemos conversado. \u00bfQu\u00e9", "tokens": [50414, 7767, 2104, 66, 2271, 3415, 635, 31959, 4580, 368, 2002, 31702, 11, 11974, 11, 12218, 631, 2478, 15396, 2615, 1573, 13, 3841, 15137, 50644], "temperature": 0.0, "avg_logprob": -0.22363177708217075, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.014844448305666447}, {"id": 466, "seek": 170932, "start": 1714.9199999999998, "end": 1724.72, "text": " elemento vamos a contar? Por ejemplo, tengo un tema de toquenizaci\u00f3n, esta coma, \u00bfla tengo", "tokens": [50644, 47961, 5295, 257, 27045, 30, 5269, 13358, 11, 13989, 517, 15854, 368, 281, 358, 268, 27603, 11, 5283, 35106, 11, 3841, 875, 13989, 51134], "temperature": 0.0, "avg_logprob": -0.22363177708217075, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.014844448305666447}, {"id": 467, "seek": 170932, "start": 1724.72, "end": 1730.04, "text": " que considerar un engrama o no la tengo que considerar un engrama? \u00bfLa tengo que considerar", "tokens": [51134, 631, 1949, 289, 517, 465, 1342, 64, 277, 572, 635, 13989, 631, 1949, 289, 517, 465, 1342, 64, 30, 3841, 5478, 13989, 631, 1949, 289, 51400], "temperature": 0.0, "avg_logprob": -0.22363177708217075, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.014844448305666447}, {"id": 468, "seek": 170932, "start": 1730.04, "end": 1734.76, "text": " un token o no la tengo que considerar un token? \u00bfMe interesa? Bueno, eso seguramente va a", "tokens": [51400, 517, 14862, 277, 572, 635, 13989, 631, 1949, 289, 517, 14862, 30, 3841, 12671, 728, 13708, 30, 16046, 11, 7287, 22179, 3439, 2773, 257, 51636], "temperature": 0.0, "avg_logprob": -0.22363177708217075, "compression_ratio": 1.8009708737864079, "no_speech_prob": 0.014844448305666447}, {"id": 469, "seek": 173476, "start": 1734.76, "end": 1737.8799999999999, "text": " depender un poco de la aplicaci\u00f3n en la que lo estoy aplicando, a lo que lo estoy utilizando.", "tokens": [50364, 1367, 3216, 517, 10639, 368, 635, 18221, 3482, 465, 635, 631, 450, 15796, 18221, 1806, 11, 257, 450, 631, 450, 15796, 19906, 1806, 13, 50520], "temperature": 0.0, "avg_logprob": -0.21075094447416418, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.10386186093091965}, {"id": 470, "seek": 173476, "start": 1737.8799999999999, "end": 1747.64, "text": " O tengo un cuerpo oral donde tengo disfluencias, disfluencias, creo que se llama esto. \u00bfQu\u00e9 tengo", "tokens": [50520, 422, 13989, 517, 20264, 19338, 10488, 13989, 717, 49253, 37246, 11, 717, 49253, 37246, 11, 14336, 631, 369, 23272, 7433, 13, 3841, 15137, 13989, 51008], "temperature": 0.0, "avg_logprob": -0.21075094447416418, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.10386186093091965}, {"id": 471, "seek": 173476, "start": 1747.64, "end": 1752.12, "text": " que hacer con las may\u00fasculas? \u00bfQu\u00e9 hago con la forma flexionada? Todo los problemas de la", "tokens": [51008, 631, 6720, 416, 2439, 815, 10227, 2444, 296, 30, 3841, 15137, 38721, 416, 635, 8366, 5896, 313, 1538, 30, 26466, 1750, 20720, 368, 635, 51232], "temperature": 0.0, "avg_logprob": -0.21075094447416418, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.10386186093091965}, {"id": 472, "seek": 173476, "start": 1752.12, "end": 1756.92, "text": " toquenizaci\u00f3n me parecen en los engramas, es decir, estos son cascadas, digamos, \u00bfno? Yo acabo", "tokens": [51232, 281, 358, 268, 27603, 385, 7448, 13037, 465, 1750, 465, 1342, 296, 11, 785, 10235, 11, 12585, 1872, 3058, 66, 6872, 11, 36430, 11, 3841, 1771, 30, 7616, 13281, 78, 51472], "temperature": 0.0, "avg_logprob": -0.21075094447416418, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.10386186093091965}, {"id": 473, "seek": 173476, "start": 1756.92, "end": 1761.4, "text": " de tener la toquenizaci\u00f3n realizada. En realidad no hay respuesta universal, depende de la tarea", "tokens": [51472, 368, 11640, 635, 281, 358, 268, 27603, 22828, 1538, 13, 2193, 25635, 572, 4842, 40585, 11455, 11, 47091, 368, 635, 256, 35425, 51696], "temperature": 0.0, "avg_logprob": -0.21075094447416418, "compression_ratio": 1.7338129496402879, "no_speech_prob": 0.10386186093091965}, {"id": 474, "seek": 176140, "start": 1761.48, "end": 1767.24, "text": " que estamos haciendo. Por ejemplo, t\u00edpicamente los corporeales est\u00e1n todos pasados a may\u00fasculas,", "tokens": [50368, 631, 10382, 20509, 13, 5269, 13358, 11, 256, 28236, 23653, 1750, 1181, 79, 418, 4229, 10368, 6321, 1736, 4181, 257, 815, 10227, 2444, 296, 11, 50656], "temperature": 0.0, "avg_logprob": -0.2322829564412435, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036300551146268845}, {"id": 475, "seek": 176140, "start": 1767.24, "end": 1779.8000000000002, "text": " porque como son m\u00e1s continuos, la identificaci\u00f3n de oraciones no es tan importante. Si yo voy", "tokens": [50656, 4021, 2617, 1872, 3573, 2993, 329, 11, 635, 2473, 40802, 368, 420, 9188, 572, 785, 7603, 9416, 13, 4909, 5290, 7552, 51284], "temperature": 0.0, "avg_logprob": -0.2322829564412435, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036300551146268845}, {"id": 476, "seek": 176140, "start": 1779.8000000000002, "end": 1785.44, "text": " a hacer an\u00e1lisis, si estoy haciendo un an\u00e1lisis de c\u00f3mo se usan los signos de puntuaci\u00f3n", "tokens": [51284, 257, 6720, 44113, 28436, 11, 1511, 15796, 20509, 517, 44113, 28436, 368, 12826, 369, 505, 282, 1750, 1465, 329, 368, 18212, 84, 3482, 51566], "temperature": 0.0, "avg_logprob": -0.2322829564412435, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036300551146268845}, {"id": 477, "seek": 176140, "start": 1785.44, "end": 1788.96, "text": " en mi lenguaje, obviamente la coma la tengo que identificar, sino capaz que no me interese.", "tokens": [51566, 465, 2752, 35044, 84, 11153, 11, 36325, 635, 35106, 635, 13989, 631, 2473, 25625, 11, 18108, 35453, 631, 572, 385, 728, 1130, 13, 51742], "temperature": 0.0, "avg_logprob": -0.2322829564412435, "compression_ratio": 1.5899581589958158, "no_speech_prob": 0.036300551146268845}, {"id": 478, "seek": 178896, "start": 1789.96, "end": 1796.44, "text": " O me puede interesar todo estos, mapearlos a una cosa sola que se llama signo de puntuaci\u00f3n", "tokens": [50414, 422, 385, 8919, 728, 18876, 5149, 12585, 11, 463, 494, 39734, 257, 2002, 10163, 34162, 631, 369, 23272, 1465, 78, 368, 18212, 84, 3482, 50738], "temperature": 0.0, "avg_logprob": -0.2869076242252272, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.005631076171994209}, {"id": 479, "seek": 178896, "start": 1798.04, "end": 1803.88, "text": " y juntar los puntos con las comas. Van a tener que hacer eso en el laboratorio.", "tokens": [50818, 288, 22739, 289, 1750, 34375, 416, 2439, 395, 296, 13, 8979, 257, 11640, 631, 6720, 7287, 465, 806, 5938, 48028, 13, 51110], "temperature": 0.0, "avg_logprob": -0.2869076242252272, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.005631076171994209}, {"id": 480, "seek": 178896, "start": 1803.88, "end": 1809.24, "text": " Ya se van a colar. Bueno, nada, se necesita un pretratamiento disponible al menos para", "tokens": [51110, 6080, 369, 3161, 257, 1173, 289, 13, 16046, 11, 8096, 11, 369, 45485, 517, 1162, 4481, 16971, 23311, 964, 419, 8902, 1690, 51378], "temperature": 0.0, "avg_logprob": -0.2869076242252272, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.005631076171994209}, {"id": 481, "seek": 178896, "start": 1809.24, "end": 1816.48, "text": " las oraciones y el modelo no hay modelos generales. Tambi\u00e9n va a depender un poco,", "tokens": [51378, 2439, 420, 9188, 288, 806, 27825, 572, 4842, 2316, 329, 2674, 279, 13, 25682, 2773, 257, 1367, 3216, 517, 10639, 11, 51740], "temperature": 0.0, "avg_logprob": -0.2869076242252272, "compression_ratio": 1.5176991150442478, "no_speech_prob": 0.005631076171994209}, {"id": 482, "seek": 181648, "start": 1816.96, "end": 1823.68, "text": " nuestros n\u00fameros van a depender de la cantidad de palabras. El dictionary,", "tokens": [50388, 24099, 36545, 3161, 257, 1367, 3216, 368, 635, 33757, 368, 35240, 13, 2699, 25890, 11, 50724], "temperature": 0.0, "avg_logprob": -0.3429831685246648, "compression_ratio": 1.3854166666666667, "no_speech_prob": 0.03596698120236397}, {"id": 483, "seek": 181648, "start": 1823.68, "end": 1831.8, "text": " el Oxford English dictionary tiene 290.000 entradas, el Tresor de la langue franc\u00e9s tiene 54.000", "tokens": [50724, 806, 24786, 3669, 25890, 7066, 568, 7771, 13, 1360, 948, 48906, 11, 806, 314, 495, 284, 368, 635, 40318, 30514, 2191, 7066, 20793, 13, 1360, 51130], "temperature": 0.0, "avg_logprob": -0.3429831685246648, "compression_ratio": 1.3854166666666667, "no_speech_prob": 0.03596698120236397}, {"id": 484, "seek": 181648, "start": 1831.8, "end": 1838.88, "text": " y el dicionario de la radio 88.000. \u00bfPor qu\u00e9 les parece que hay tantas m\u00e1s ac\u00e1 que ac\u00e1?", "tokens": [51130, 288, 806, 14285, 313, 4912, 368, 635, 6477, 24587, 13, 1360, 13, 3841, 24907, 8057, 1512, 14120, 631, 4842, 12095, 296, 3573, 23496, 631, 23496, 30, 51484], "temperature": 0.0, "avg_logprob": -0.3429831685246648, "compression_ratio": 1.3854166666666667, "no_speech_prob": 0.03596698120236397}, {"id": 485, "seek": 183888, "start": 1839.88, "end": 1847.6000000000001, "text": " Porque el dicionario no parece en la forma flexionada y el espa\u00f1ol est\u00e1 mucho m\u00e1s flexionado.", "tokens": [50414, 11287, 806, 14285, 313, 4912, 572, 14120, 465, 635, 8366, 5896, 313, 1538, 288, 806, 31177, 3192, 9824, 3573, 5896, 313, 1573, 13, 50800], "temperature": 0.0, "avg_logprob": -0.3213644154866536, "compression_ratio": 1.441025641025641, "no_speech_prob": 0.004286219831556082}, {"id": 486, "seek": 183888, "start": 1851.68, "end": 1857.3600000000001, "text": " O sea que el ingl\u00e9s la tiene que arreglar m\u00e1s solito. Bueno, y despu\u00e9s tenemos corpus.", "tokens": [51004, 422, 4158, 631, 806, 49766, 635, 7066, 631, 594, 3375, 2200, 3573, 1404, 3528, 13, 16046, 11, 288, 15283, 9914, 1181, 31624, 13, 51288], "temperature": 0.0, "avg_logprob": -0.3213644154866536, "compression_ratio": 1.441025641025641, "no_speech_prob": 0.004286219831556082}, {"id": 487, "seek": 183888, "start": 1858.8000000000002, "end": 1863.8000000000002, "text": " Esto ya hablamos un poco y aquello distinguir entre el n\u00famero de token que son la cantidad de", "tokens": [51360, 20880, 2478, 26280, 2151, 517, 10639, 288, 2373, 11216, 11365, 347, 3962, 806, 14959, 368, 14862, 631, 1872, 635, 33757, 368, 51610], "temperature": 0.0, "avg_logprob": -0.3213644154866536, "compression_ratio": 1.441025641025641, "no_speech_prob": 0.004286219831556082}, {"id": 488, "seek": 186380, "start": 1863.8, "end": 1867.56, "text": " ocurrencias que hay en el texto y el n\u00famero de palabras distintas, el vocabular.", "tokens": [50364, 26430, 1095, 12046, 631, 4842, 465, 806, 35503, 288, 806, 14959, 368, 35240, 31489, 296, 11, 806, 2329, 455, 1040, 13, 50552], "temperature": 0.0, "avg_logprob": -0.28747006052548124, "compression_ratio": 1.4776785714285714, "no_speech_prob": 0.010357575491070747}, {"id": 489, "seek": 186380, "start": 1874.04, "end": 1877.36, "text": " Ac\u00e1 est\u00e1 la respuesta a la pregunta que hac\u00edamos antes. \u00bfC\u00f3mo estimamos los", "tokens": [50876, 5097, 842, 3192, 635, 40585, 257, 635, 24252, 631, 46093, 16275, 11014, 13, 3841, 28342, 8017, 2151, 1750, 51042], "temperature": 0.0, "avg_logprob": -0.28747006052548124, "compression_ratio": 1.4776785714285714, "no_speech_prob": 0.010357575491070747}, {"id": 490, "seek": 186380, "start": 1877.36, "end": 1881.6399999999999, "text": " bigramas utilizando otra vez lo que se llama un estimador de m\u00e1xima verosimilituos,", "tokens": [51042, 955, 2356, 296, 19906, 1806, 13623, 5715, 450, 631, 369, 23272, 517, 8017, 5409, 368, 31031, 64, 1306, 329, 332, 388, 6380, 329, 11, 51256], "temperature": 0.0, "avg_logprob": -0.28747006052548124, "compression_ratio": 1.4776785714285714, "no_speech_prob": 0.010357575491070747}, {"id": 491, "seek": 186380, "start": 1881.6399999999999, "end": 1886.12, "text": " lo que se llama m\u00e9todos de frecuencias relativas? Que es, cuento las cantidades de", "tokens": [51256, 450, 631, 369, 23272, 20275, 378, 329, 368, 2130, 66, 7801, 12046, 21960, 296, 30, 4493, 785, 11, 2702, 15467, 2439, 11223, 10284, 368, 51480], "temperature": 0.0, "avg_logprob": -0.28747006052548124, "compression_ratio": 1.4776785714285714, "no_speech_prob": 0.010357575491070747}, {"id": 492, "seek": 188612, "start": 1886.28, "end": 1895.84, "text": " la cantidad de veces que apareci\u00f3 una palabra con, por ejemplo, la probabilidad de fuerte,", "tokens": [50372, 635, 33757, 368, 17054, 631, 15004, 19609, 2002, 31702, 416, 11, 1515, 13358, 11, 635, 31959, 4580, 368, 37129, 11, 50850], "temperature": 0.0, "avg_logprob": -0.38009864633733575, "compression_ratio": 1.5225225225225225, "no_speech_prob": 0.3036929965019226}, {"id": 493, "seek": 188612, "start": 1895.84, "end": 1904.7199999999998, "text": " dado viento, se aproxima como la cantidad de veces que aparece viento fuerte.", "tokens": [50850, 29568, 371, 7814, 11, 369, 31270, 64, 2617, 635, 33757, 368, 17054, 631, 37863, 371, 7814, 37129, 13, 51294], "temperature": 0.0, "avg_logprob": -0.38009864633733575, "compression_ratio": 1.5225225225225225, "no_speech_prob": 0.3036929965019226}, {"id": 494, "seek": 190472, "start": 1904.96, "end": 1917.32, "text": " Por la dividida de la cantidad de veces que apareci\u00f3", "tokens": [50376, 5269, 635, 4996, 2887, 368, 635, 33757, 368, 17054, 631, 15004, 19609, 50994], "temperature": 0.0, "avg_logprob": -0.25901331901550295, "compression_ratio": 1.2173913043478262, "no_speech_prob": 0.034703757613897324}, {"id": 495, "seek": 190472, "start": 1926.4, "end": 1934.3600000000001, "text": " dividido todas las posibles continuaciones. \u00bfDe acuerdo? Viento fuerte, viento calmo,", "tokens": [51448, 4996, 2925, 10906, 2439, 1366, 14428, 2993, 9188, 13, 3841, 11089, 28113, 30, 691, 7814, 37129, 11, 371, 7814, 2104, 3280, 11, 51846], "temperature": 0.0, "avg_logprob": -0.25901331901550295, "compression_ratio": 1.2173913043478262, "no_speech_prob": 0.034703757613897324}, {"id": 496, "seek": 193436, "start": 1934.36, "end": 1942.76, "text": " viento, viento dile, viento, no s\u00e9, lo que quieras. Y sumo todas las posibles,", "tokens": [50364, 371, 7814, 11, 371, 7814, 25623, 11, 371, 7814, 11, 572, 7910, 11, 450, 631, 23572, 296, 13, 398, 2408, 78, 10906, 2439, 1366, 14428, 11, 50784], "temperature": 0.0, "avg_logprob": -0.2914006404387645, "compression_ratio": 1.441988950276243, "no_speech_prob": 0.0021325289271771908}, {"id": 497, "seek": 193436, "start": 1942.76, "end": 1945.9199999999998, "text": " lo que estoy haciendo es normalizando, como hablamos al principio de, como hablamos ac\u00e1,", "tokens": [50784, 450, 631, 15796, 20509, 785, 2710, 590, 1806, 11, 2617, 26280, 2151, 419, 34308, 368, 11, 2617, 26280, 2151, 23496, 11, 50942], "temperature": 0.0, "avg_logprob": -0.2914006404387645, "compression_ratio": 1.441988950276243, "no_speech_prob": 0.0021325289271771908}, {"id": 498, "seek": 193436, "start": 1945.9199999999998, "end": 1952.08, "text": " \u00bfno? Estoy normalizando. Ahora, esto aqu\u00ed es equivalente. \u00bfC\u00f3mo puedo simplificar esto?", "tokens": [50942, 3841, 1771, 30, 49651, 2710, 590, 1806, 13, 18840, 11, 7433, 6661, 785, 9052, 1576, 13, 3841, 28342, 21612, 6883, 25625, 7433, 30, 51250], "temperature": 0.0, "avg_logprob": -0.2914006404387645, "compression_ratio": 1.441988950276243, "no_speech_prob": 0.0021325289271771908}, {"id": 499, "seek": 195208, "start": 1952.32, "end": 1962.0, "text": " Si yo tengo todas las veces que aparecen viento fuerte,", "tokens": [50376, 4909, 5290, 13989, 10906, 2439, 17054, 631, 15004, 13037, 371, 7814, 37129, 11, 50860], "temperature": 0.0, "avg_logprob": -0.27812845953579607, "compression_ratio": 1.3984962406015038, "no_speech_prob": 0.02666693925857544}, {"id": 500, "seek": 195208, "start": 1962.0, "end": 1968.0, "text": " viento calmo, no s\u00e9, \u00bfcu\u00e1l es la suma de todo eso?", "tokens": [50860, 371, 7814, 2104, 3280, 11, 572, 7910, 11, 3841, 12032, 11447, 785, 635, 2408, 64, 368, 5149, 7287, 30, 51160], "temperature": 0.0, "avg_logprob": -0.27812845953579607, "compression_ratio": 1.3984962406015038, "no_speech_prob": 0.02666693925857544}, {"id": 501, "seek": 195208, "start": 1973.84, "end": 1977.72, "text": " Es la cantidad de veces que apareci\u00f3 viento. Esto es igual a la cantidad de", "tokens": [51452, 2313, 635, 33757, 368, 17054, 631, 15004, 19609, 371, 7814, 13, 20880, 785, 10953, 257, 635, 33757, 368, 51646], "temperature": 0.0, "avg_logprob": -0.27812845953579607, "compression_ratio": 1.3984962406015038, "no_speech_prob": 0.02666693925857544}, {"id": 502, "seek": 197772, "start": 1977.72, "end": 1981.1200000000001, "text": " secas que aparecen vientos en el cuerpo. \u00bfC\u00f3mo puedo recordar?", "tokens": [50364, 907, 296, 631, 15004, 13037, 371, 20370, 465, 806, 20264, 13, 3841, 28342, 21612, 2136, 289, 30, 50534], "temperature": 0.0, "avg_logprob": -0.26947069631039516, "compression_ratio": 1.528225806451613, "no_speech_prob": 0.01236359216272831}, {"id": 503, "seek": 197772, "start": 1983.24, "end": 1985.28, "text": " Como son todas las posibles ocurrencias.", "tokens": [50640, 11913, 1872, 10906, 2439, 1366, 14428, 26430, 1095, 12046, 13, 50742], "temperature": 0.0, "avg_logprob": -0.26947069631039516, "compression_ratio": 1.528225806451613, "no_speech_prob": 0.01236359216272831}, {"id": 504, "seek": 197772, "start": 1990.68, "end": 1995.72, "text": " Ah\u00ed tenemos la simplificaci\u00f3n y, adem\u00e1s, para tener en cuenta la primera y \u00faltima", "tokens": [51012, 49924, 9914, 635, 6883, 40802, 288, 11, 21251, 11, 1690, 11640, 465, 17868, 635, 17382, 288, 28118, 51264], "temperature": 0.0, "avg_logprob": -0.26947069631039516, "compression_ratio": 1.528225806451613, "no_speech_prob": 0.01236359216272831}, {"id": 505, "seek": 197772, "start": 1995.72, "end": 2000.04, "text": " palabra en oraci\u00f3n, le vamos a agregar siempre los s\u00edmbolos de comienzo y de fin. Eso para", "tokens": [51264, 31702, 465, 420, 3482, 11, 476, 5295, 257, 4554, 2976, 12758, 1750, 8600, 5612, 329, 368, 395, 1053, 4765, 288, 368, 962, 13, 27795, 1690, 51480], "temperature": 0.0, "avg_logprob": -0.26947069631039516, "compression_ratio": 1.528225806451613, "no_speech_prob": 0.01236359216272831}, {"id": 506, "seek": 197772, "start": 2000.04, "end": 2007.08, "text": " asegurarnos de que, para no tener que calcular separada la probabilidad de la primera palabra.", "tokens": [51480, 38174, 374, 24979, 368, 631, 11, 1690, 572, 11640, 631, 2104, 17792, 3128, 1538, 635, 31959, 4580, 368, 635, 17382, 31702, 13, 51832], "temperature": 0.0, "avg_logprob": -0.26947069631039516, "compression_ratio": 1.528225806451613, "no_speech_prob": 0.01236359216272831}, {"id": 507, "seek": 200708, "start": 2007.24, "end": 2014.76, "text": " Yo s\u00e9 que la primera palabra siempre es ese y calculo la probabilidad de la primera en el texto,", "tokens": [50372, 7616, 7910, 631, 635, 17382, 31702, 12758, 785, 10167, 288, 4322, 78, 635, 31959, 4580, 368, 635, 17382, 465, 806, 35503, 11, 50748], "temperature": 0.0, "avg_logprob": -0.22785135594809927, "compression_ratio": 1.455, "no_speech_prob": 0.0012837009271606803}, {"id": 508, "seek": 200708, "start": 2014.76, "end": 2023.08, "text": " digamos, ponerle el dado que la anterior era ese. De acuerdo? Y as\u00ed lo dejo en una sola forma.", "tokens": [50748, 36430, 11, 19149, 306, 806, 29568, 631, 635, 22272, 4249, 10167, 13, 1346, 28113, 30, 398, 8582, 450, 368, 5134, 465, 2002, 34162, 8366, 13, 51164], "temperature": 0.0, "avg_logprob": -0.22785135594809927, "compression_ratio": 1.455, "no_speech_prob": 0.0012837009271606803}, {"id": 509, "seek": 200708, "start": 2024.84, "end": 2033.96, "text": " Por ejemplo, si supongamos que yo tengo ese cuerpo, \u00bfno? Juan abri\u00f3 la puerta, el viento abri\u00f3", "tokens": [51252, 5269, 13358, 11, 1511, 9331, 556, 2151, 631, 5290, 13989, 10167, 20264, 11, 3841, 1771, 30, 17064, 410, 44802, 635, 48597, 11, 806, 371, 7814, 410, 44802, 51708], "temperature": 0.0, "avg_logprob": -0.22785135594809927, "compression_ratio": 1.455, "no_speech_prob": 0.0012837009271606803}, {"id": 510, "seek": 203396, "start": 2034.04, "end": 2040.52, "text": " la puerta, enero abri\u00f3 limones en tus mejillas nuevas, Juan recoge limones. Y quiero saber", "tokens": [50368, 635, 48597, 11, 465, 2032, 410, 44802, 2364, 2213, 465, 20647, 37758, 20243, 42817, 11, 17064, 7759, 432, 2364, 2213, 13, 398, 16811, 12489, 50692], "temperature": 0.0, "avg_logprob": -0.23789794921875, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.007597491145133972}, {"id": 511, "seek": 203396, "start": 2040.52, "end": 2046.6000000000001, "text": " la probabilidad de estas oraciones. Evidentemente no las tengo en el cuerpo, o sea, que no puedo", "tokens": [50692, 635, 31959, 4580, 368, 13897, 420, 9188, 13, 5689, 1078, 16288, 572, 2439, 13989, 465, 806, 20264, 11, 277, 4158, 11, 631, 572, 21612, 50996], "temperature": 0.0, "avg_logprob": -0.23789794921875, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.007597491145133972}, {"id": 512, "seek": 203396, "start": 2046.6000000000001, "end": 2057.32, "text": " contar directamente. Pero quiero utilizar un modelo de diagrama para calcular. Y con lo que", "tokens": [50996, 27045, 46230, 13, 9377, 16811, 24060, 517, 27825, 368, 10686, 64, 1690, 2104, 17792, 13, 398, 416, 450, 631, 51532], "temperature": 0.0, "avg_logprob": -0.23789794921875, "compression_ratio": 1.4893617021276595, "no_speech_prob": 0.007597491145133972}, {"id": 513, "seek": 205732, "start": 2057.4, "end": 2066.44, "text": " sabemos es bastante sencillo. Primero que nada decimos, bueno, la probabilidad de Juan abri\u00f3", "tokens": [50368, 27200, 785, 14651, 46749, 78, 13, 19671, 2032, 631, 8096, 979, 8372, 11, 11974, 11, 635, 31959, 4580, 368, 17064, 410, 44802, 50820], "temperature": 0.0, "avg_logprob": -0.17050278323820267, "compression_ratio": 1.8132530120481927, "no_speech_prob": 0.10220909118652344}, {"id": 514, "seek": 205732, "start": 2066.44, "end": 2075.56, "text": " limones es probabilidad de Juan dado el comienzo. Probabilidad de comienzo siempre es uno. Probabilidad", "tokens": [50820, 2364, 2213, 785, 31959, 4580, 368, 17064, 29568, 806, 395, 1053, 4765, 13, 8736, 5177, 4580, 368, 395, 1053, 4765, 12758, 785, 8526, 13, 8736, 5177, 4580, 51276], "temperature": 0.0, "avg_logprob": -0.17050278323820267, "compression_ratio": 1.8132530120481927, "no_speech_prob": 0.10220909118652344}, {"id": 515, "seek": 205732, "start": 2075.56, "end": 2084.04, "text": " de abri\u00f3 dado Juan, probabilidad de limones dado abri\u00f3, etc\u00e9tera, \u00bfno? F\u00edjense que la probabilidad", "tokens": [51276, 368, 410, 44802, 29568, 17064, 11, 31959, 4580, 368, 2364, 2213, 29568, 410, 44802, 11, 5183, 526, 23833, 11, 3841, 1771, 30, 479, 870, 73, 1288, 631, 635, 31959, 4580, 51700], "temperature": 0.0, "avg_logprob": -0.17050278323820267, "compression_ratio": 1.8132530120481927, "no_speech_prob": 0.10220909118652344}, {"id": 516, "seek": 208404, "start": 2084.04, "end": 2089.24, "text": " Juan dado el comienzo de la cantidad de veces que apareci\u00f3 Juan en la marca de comienzo dividido", "tokens": [50364, 17064, 29568, 806, 395, 1053, 4765, 368, 635, 33757, 368, 17054, 631, 15004, 19609, 17064, 465, 635, 30582, 368, 395, 1053, 4765, 4996, 2925, 50624], "temperature": 0.0, "avg_logprob": -0.27936697594913434, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.03203830122947693}, {"id": 517, "seek": 208404, "start": 2089.24, "end": 2099.96, "text": " de la cantidad de marcas de comienzo que es uno. Entonces esto me da 2 de 4. Ah, porque hay cuatro", "tokens": [50624, 368, 635, 33757, 368, 1849, 16369, 368, 395, 1053, 4765, 631, 785, 8526, 13, 15097, 7433, 385, 1120, 568, 368, 1017, 13, 2438, 11, 4021, 4842, 28795, 51160], "temperature": 0.0, "avg_logprob": -0.27936697594913434, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.03203830122947693}, {"id": 518, "seek": 208404, "start": 2099.96, "end": 2104.68, "text": " oraciones. Perd\u00f3n. Claro, porque yo estoy haciendo contegos directamente. No, no estoy haciendo", "tokens": [51160, 420, 9188, 13, 47633, 1801, 13, 33380, 11, 4021, 5290, 15796, 20509, 660, 1146, 329, 46230, 13, 883, 11, 572, 15796, 20509, 51396], "temperature": 0.0, "avg_logprob": -0.27936697594913434, "compression_ratio": 1.6277777777777778, "no_speech_prob": 0.03203830122947693}, {"id": 519, "seek": 210468, "start": 2105.64, "end": 2116.04, "text": " 2 de 4 veces arranc\u00f3 con Juan, \u00bfs\u00ed? Juan abri\u00f3 es una de dos. Ya hab\u00eda aparecido Juan abri\u00f3", "tokens": [50412, 568, 368, 1017, 17054, 50235, 66, 812, 416, 17064, 11, 3841, 82, 870, 30, 17064, 410, 44802, 785, 2002, 368, 4491, 13, 6080, 16395, 15004, 17994, 17064, 410, 44802, 50932], "temperature": 0.0, "avg_logprob": -0.2319343730967532, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.6370728015899658}, {"id": 520, "seek": 210468, "start": 2117.16, "end": 2123.8799999999997, "text": " en el corpus y Juan aparece dos veces. O sea, de dos veces le apareci\u00f3 Juan y la siguiente apareci\u00f3", "tokens": [50988, 465, 806, 1181, 31624, 288, 17064, 37863, 4491, 17054, 13, 422, 4158, 11, 368, 4491, 17054, 476, 15004, 19609, 17064, 288, 635, 25666, 15004, 19609, 51324], "temperature": 0.0, "avg_logprob": -0.2319343730967532, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.6370728015899658}, {"id": 521, "seek": 210468, "start": 2123.8799999999997, "end": 2131.64, "text": " una vez abri\u00f3. Y as\u00ed sigo multiplicando y como me multiplico la fracci\u00f3n y me da, bueno, 0-0-42.", "tokens": [51324, 2002, 5715, 410, 44802, 13, 398, 8582, 4556, 78, 17596, 1806, 288, 2617, 385, 12788, 2789, 635, 431, 49354, 288, 385, 1120, 11, 11974, 11, 1958, 12, 15, 12, 15628, 13, 51712], "temperature": 0.0, "avg_logprob": -0.2319343730967532, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.6370728015899658}, {"id": 522, "seek": 213164, "start": 2131.64, "end": 2143.0, "text": " Esa es la probabilidad de Juan abri\u00f3 el lim\u00f3n. Enero abri\u00f3 la puerta 0-17. Esto no tiene mucho", "tokens": [50364, 2313, 64, 785, 635, 31959, 4580, 368, 17064, 410, 44802, 806, 2364, 1801, 13, 2193, 2032, 410, 44802, 635, 48597, 1958, 12, 7773, 13, 20880, 572, 7066, 9824, 50932], "temperature": 0.0, "avg_logprob": -0.22240785110828487, "compression_ratio": 1.4258373205741626, "no_speech_prob": 0.010725629515945911}, {"id": 523, "seek": 213164, "start": 2143.0, "end": 2148.2, "text": " sentido, \u00bfno? A ver, justamente el hecho de que sea un ejemplo de juguete le hace perder la gracia", "tokens": [50932, 19850, 11, 3841, 1771, 30, 316, 1306, 11, 41056, 806, 13064, 368, 631, 4158, 517, 13358, 368, 9568, 84, 3498, 476, 10032, 26971, 635, 11625, 654, 51192], "temperature": 0.0, "avg_logprob": -0.22240785110828487, "compression_ratio": 1.4258373205741626, "no_speech_prob": 0.010725629515945911}, {"id": 524, "seek": 213164, "start": 2148.2, "end": 2157.16, "text": " todo esto porque esto funciona porque tengo grandes vol\u00famenes, sino es una pasada. \u00bfY ac\u00e1 que nos", "tokens": [51192, 5149, 7433, 4021, 7433, 26210, 4021, 13989, 16640, 1996, 2481, 2558, 279, 11, 18108, 785, 2002, 1736, 1538, 13, 3841, 56, 23496, 631, 3269, 51640], "temperature": 0.0, "avg_logprob": -0.22240785110828487, "compression_ratio": 1.4258373205741626, "no_speech_prob": 0.010725629515945911}, {"id": 525, "seek": 215716, "start": 2157.16, "end": 2181.64, "text": " pas\u00f3? \u00bfQu\u00e9 puede haber pasado ac\u00e1? La palabra come nunca est\u00e1. \u00bfY en la puerta?", "tokens": [50364, 41382, 30, 3841, 15137, 8919, 15811, 24794, 23496, 30, 2369, 31702, 808, 13768, 3192, 13, 3841, 56, 465, 635, 48597, 30, 51588], "temperature": 0.0, "avg_logprob": -0.3216159057617187, "compression_ratio": 0.9659090909090909, "no_speech_prob": 0.1171456128358841}, {"id": 526, "seek": 218164, "start": 2181.64, "end": 2191.4, "text": " En la puerta est\u00e1. La primera se explica porque come nunca est\u00e1, \u00bfno?", "tokens": [50364, 2193, 635, 48597, 3192, 13, 2369, 17382, 369, 1490, 2262, 4021, 808, 13768, 3192, 11, 3841, 1771, 30, 50852], "temperature": 0.0, "avg_logprob": -0.39299925890835846, "compression_ratio": 0.9863013698630136, "no_speech_prob": 0.0344245545566082}, {"id": 527, "seek": 219140, "start": 2191.4, "end": 2205.64, "text": " Creo que est\u00e1 as\u00ed. Perd\u00f3n. La as\u00ed, la puerta.", "tokens": [50364, 40640, 631, 3192, 8582, 13, 47633, 1801, 13, 2369, 8582, 11, 635, 48597, 13, 51076], "temperature": 0.0, "avg_logprob": -0.50853517320421, "compression_ratio": 0.9074074074074074, "no_speech_prob": 0.08808214217424393}, {"id": 528, "seek": 220564, "start": 2205.64, "end": 2224.04, "text": " \u00bfPor qu\u00e9 da 0? Porque lo que no est\u00e1 es en la. En la no aparece nunca. Si ustedes miran ac\u00e1 la", "tokens": [50364, 3841, 24907, 8057, 1120, 1958, 30, 11287, 450, 631, 572, 3192, 785, 465, 635, 13, 2193, 635, 572, 37863, 13768, 13, 4909, 17110, 3149, 282, 23496, 635, 51284], "temperature": 0.0, "avg_logprob": -0.19369792938232422, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.019794834777712822}, {"id": 529, "seek": 220564, "start": 2224.04, "end": 2229.16, "text": " probabilidad de, perd\u00f3n, la cantidad de, la probabilidad de esto es la probabilidad de que", "tokens": [51284, 31959, 4580, 368, 11, 12611, 1801, 11, 635, 33757, 368, 11, 635, 31959, 4580, 368, 7433, 785, 635, 31959, 4580, 368, 631, 51540], "temperature": 0.0, "avg_logprob": -0.19369792938232422, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.019794834777712822}, {"id": 530, "seek": 222916, "start": 2229.16, "end": 2236.68, "text": " empiece con \u00e9l, ya tenemos un problema con el comienzo con \u00e9l porque creo que no hay ninguna.", "tokens": [50364, 4012, 46566, 416, 11810, 11, 2478, 9914, 517, 12395, 416, 806, 395, 1053, 4765, 416, 11810, 4021, 14336, 631, 572, 4842, 36073, 13, 50740], "temperature": 0.0, "avg_logprob": -0.28036345444716415, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.7047958374023438}, {"id": 531, "seek": 222916, "start": 2236.68, "end": 2244.2, "text": " Ninguna empieza con \u00e9l y eso ya tiene un problema. Y adem\u00e1s en la tampoco est\u00e1. O sea que el", "tokens": [50740, 39417, 5051, 44577, 416, 11810, 288, 7287, 2478, 7066, 517, 12395, 13, 398, 21251, 465, 635, 36838, 3192, 13, 422, 4158, 631, 806, 51116], "temperature": 0.0, "avg_logprob": -0.28036345444716415, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.7047958374023438}, {"id": 532, "seek": 222916, "start": 2244.2, "end": 2252.92, "text": " conteo me va a dar 0. Si el bigrama no aparece en el cuerpo de entrenamiento, siempre mi", "tokens": [51116, 34444, 78, 385, 2773, 257, 4072, 1958, 13, 4909, 806, 955, 29762, 572, 37863, 465, 806, 20264, 368, 45069, 16971, 11, 12758, 2752, 51552], "temperature": 0.0, "avg_logprob": -0.28036345444716415, "compression_ratio": 1.5135135135135136, "no_speech_prob": 0.7047958374023438}, {"id": 533, "seek": 225292, "start": 2253.2400000000002, "end": 2261.0, "text": " problema da 0. Y m\u00e1s interesante a\u00fan, si cualquier bigrama de todos los que aparecen en la oraci\u00f3n", "tokens": [50380, 12395, 1120, 1958, 13, 398, 3573, 36396, 31676, 11, 1511, 21004, 955, 29762, 368, 6321, 1750, 631, 15004, 13037, 465, 635, 420, 3482, 50768], "temperature": 0.0, "avg_logprob": -0.22639412652878535, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.38785475492477417}, {"id": 534, "seek": 225292, "start": 2261.0, "end": 2271.64, "text": " da 0, la probabilidad de la oraci\u00f3n es 0. Eso es un gran problema. Resolver el problema de eso,", "tokens": [50768, 1120, 1958, 11, 635, 31959, 4580, 368, 635, 420, 3482, 785, 1958, 13, 27795, 785, 517, 9370, 12395, 13, 5015, 401, 331, 806, 12395, 368, 7287, 11, 51300], "temperature": 0.0, "avg_logprob": -0.22639412652878535, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.38785475492477417}, {"id": 535, "seek": 225292, "start": 2271.64, "end": 2275.64, "text": " de lo que se llama el suavizado de engramas que vamos a ver c\u00f3mo. Tenemos que buscar alguna forma", "tokens": [51300, 368, 450, 631, 369, 23272, 806, 459, 706, 27441, 368, 465, 861, 19473, 631, 5295, 257, 1306, 12826, 13, 44903, 631, 26170, 20651, 8366, 51500], "temperature": 0.0, "avg_logprob": -0.22639412652878535, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.38785475492477417}, {"id": 536, "seek": 225292, "start": 2275.64, "end": 2281.48, "text": " de resolver eso que nos va a pasar siempre. Es decir, como nuestro cuerpo nunca puede ser tan,", "tokens": [51500, 368, 34480, 7287, 631, 3269, 2773, 257, 25344, 12758, 13, 2313, 10235, 11, 2617, 14726, 20264, 13768, 8919, 816, 7603, 11, 51792], "temperature": 0.0, "avg_logprob": -0.22639412652878535, "compression_ratio": 1.6198347107438016, "no_speech_prob": 0.38785475492477417}, {"id": 537, "seek": 228148, "start": 2281.88, "end": 2286.52, "text": " aunque solo sean dos palabras, igual puede parecerme parezca de palabras que no aparecieron y yo no me", "tokens": [50384, 21962, 6944, 37670, 4491, 35240, 11, 10953, 8919, 7448, 66, 31081, 7448, 89, 496, 368, 35240, 631, 572, 15004, 537, 16308, 288, 5290, 572, 385, 50616], "temperature": 0.0, "avg_logprob": -0.3060543775558472, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.012077858671545982}, {"id": 538, "seek": 228148, "start": 2286.52, "end": 2300.2, "text": " puedo trascar con eso. \u00bfDe acuerdo? Bueno, nos queda ese pendiente de 0 que lo vamos a ver", "tokens": [50616, 21612, 504, 4806, 289, 416, 7287, 13, 3841, 11089, 28113, 30, 16046, 11, 3269, 23314, 10167, 12179, 8413, 368, 1958, 631, 450, 5295, 257, 1306, 51300], "temperature": 0.0, "avg_logprob": -0.3060543775558472, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.012077858671545982}, {"id": 539, "seek": 228148, "start": 2300.2, "end": 2303.56, "text": " despu\u00e9s porque ya te quiero comentar alguna cosa. Pero vamos a acordarnos de eso, que tuvimos este", "tokens": [51300, 15283, 4021, 2478, 535, 16811, 14541, 289, 20651, 10163, 13, 9377, 5295, 257, 38077, 24979, 368, 7287, 11, 631, 38177, 8372, 4065, 51468], "temperature": 0.0, "avg_logprob": -0.3060543775558472, "compression_ratio": 1.5154639175257731, "no_speech_prob": 0.012077858671545982}, {"id": 540, "seek": 230356, "start": 2303.64, "end": 2315.16, "text": " problema pendiente. Bien, en general, ustedes dir\u00e1n, bueno, pero \u00bfcu\u00e1l es el mejor N? \u00bfPor qu\u00e9? \u00bfCu\u00e1l es el tema?", "tokens": [50368, 12395, 12179, 8413, 13, 16956, 11, 465, 2674, 11, 17110, 4746, 7200, 11, 11974, 11, 4768, 3841, 12032, 11447, 785, 806, 11479, 426, 30, 3841, 24907, 8057, 30, 3841, 35222, 11447, 785, 806, 15854, 30, 50944], "temperature": 0.0, "avg_logprob": -0.32844851768180117, "compression_ratio": 1.3526011560693643, "no_speech_prob": 0.2436448335647583}, {"id": 541, "seek": 230356, "start": 2315.16, "end": 2329.32, "text": " \u00bfEs? \u00bfCuanto? \u00bfCuanto m\u00e1s largo sea el tigrama que yo utilizo? M\u00e1s informaci\u00f3n tengo de contexto. Es decir,", "tokens": [50944, 3841, 20442, 30, 3841, 35222, 5857, 30, 3841, 35222, 5857, 3573, 31245, 4158, 806, 256, 328, 29762, 631, 5290, 4976, 19055, 30, 376, 2490, 21660, 13989, 368, 47685, 13, 2313, 10235, 11, 51652], "temperature": 0.0, "avg_logprob": -0.32844851768180117, "compression_ratio": 1.3526011560693643, "no_speech_prob": 0.2436448335647583}, {"id": 542, "seek": 232932, "start": 2329.56, "end": 2333.4, "text": " intuitivamente es mejor estimar con cinco palabras que con una.", "tokens": [50376, 16224, 23957, 785, 11479, 8017, 289, 416, 21350, 35240, 631, 416, 2002, 13, 50568], "temperature": 0.0, "avg_logprob": -0.21337312891863394, "compression_ratio": 1.3920454545454546, "no_speech_prob": 0.013131212443113327}, {"id": 543, "seek": 232932, "start": 2337.4, "end": 2340.52, "text": " Estamos guardados con eso. \u00bfCu\u00e1l es el problema de los tigramas largos?", "tokens": [50768, 34563, 6290, 4181, 416, 7287, 13, 3841, 35222, 11447, 785, 806, 12395, 368, 1750, 256, 328, 2356, 296, 11034, 329, 30, 50924], "temperature": 0.0, "avg_logprob": -0.21337312891863394, "compression_ratio": 1.3920454545454546, "no_speech_prob": 0.013131212443113327}, {"id": 544, "seek": 232932, "start": 2345.0, "end": 2346.6800000000003, "text": " \u00bfPor qu\u00e9 no puedo usar 15?", "tokens": [51148, 3841, 24907, 8057, 572, 21612, 14745, 2119, 30, 51232], "temperature": 0.0, "avg_logprob": -0.21337312891863394, "compression_ratio": 1.3920454545454546, "no_speech_prob": 0.013131212443113327}, {"id": 545, "seek": 232932, "start": 2352.28, "end": 2355.6400000000003, "text": " Porque tenemos el mismo problema por el que llegamos ac\u00e1, que con 15 no tengo", "tokens": [51512, 11287, 9914, 806, 12461, 12395, 1515, 806, 631, 11234, 2151, 23496, 11, 631, 416, 2119, 572, 13989, 51680], "temperature": 0.0, "avg_logprob": -0.21337312891863394, "compression_ratio": 1.3920454545454546, "no_speech_prob": 0.013131212443113327}, {"id": 546, "seek": 235564, "start": 2355.72, "end": 2359.72, "text": " cuerpo suficientemente grande como para que aparezcan esa ocurrencia.", "tokens": [50368, 20264, 459, 1786, 1196, 16288, 8883, 2617, 1690, 631, 15004, 89, 7035, 11342, 26430, 1095, 2755, 13, 50568], "temperature": 0.0, "avg_logprob": -0.22332252439905387, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.10010092705488205}, {"id": 547, "seek": 235564, "start": 2362.04, "end": 2368.04, "text": " Entonces, ese balance entre cantidad de ocurrencia, porque si yo no tengo una buena estimaci\u00f3n de la", "tokens": [50684, 15097, 11, 10167, 4772, 3962, 33757, 368, 26430, 1095, 2755, 11, 4021, 1511, 5290, 572, 13989, 2002, 25710, 8017, 3482, 368, 635, 50984], "temperature": 0.0, "avg_logprob": -0.22332252439905387, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.10010092705488205}, {"id": 548, "seek": 235564, "start": 2368.04, "end": 2372.3599999999997, "text": " cantidad de ocurrencia, no voy a poder estimar bien la probabilidad. Con lo que yo estoy estimando la", "tokens": [50984, 33757, 368, 26430, 1095, 2755, 11, 572, 7552, 257, 8152, 8017, 289, 3610, 635, 31959, 4580, 13, 2656, 450, 631, 5290, 15796, 8017, 1806, 635, 51200], "temperature": 0.0, "avg_logprob": -0.22332252439905387, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.10010092705488205}, {"id": 549, "seek": 235564, "start": 2372.3599999999997, "end": 2377.08, "text": " probabilidad partido en conteos. Si yo tengo una, dos, tres ocurrencias, seguramente esa probabilidad", "tokens": [51200, 31959, 4580, 644, 2925, 465, 34444, 329, 13, 4909, 5290, 13989, 2002, 11, 4491, 11, 15890, 26430, 1095, 12046, 11, 22179, 3439, 11342, 31959, 4580, 51436], "temperature": 0.0, "avg_logprob": -0.22332252439905387, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.10010092705488205}, {"id": 550, "seek": 235564, "start": 2377.08, "end": 2382.44, "text": " sea artificial. Porque si hubo una ocurrencia en un cuerpo de miles de millones de palabras,", "tokens": [51436, 4158, 11677, 13, 11287, 1511, 11838, 78, 2002, 26430, 1095, 2755, 465, 517, 20264, 368, 6193, 368, 22416, 368, 35240, 11, 51704], "temperature": 0.0, "avg_logprob": -0.22332252439905387, "compression_ratio": 1.8645418326693226, "no_speech_prob": 0.10010092705488205}, {"id": 551, "seek": 238244, "start": 2382.92, "end": 2390.04, "text": " no me est\u00e1 diciendo mucho. Generalmente en igualtr\u00e9 se obtienen buenos resultados.", "tokens": [50388, 572, 385, 3192, 42797, 9824, 13, 6996, 4082, 465, 10953, 6903, 526, 369, 7464, 22461, 49617, 36796, 13, 50744], "temperature": 0.0, "avg_logprob": -0.3944645472935268, "compression_ratio": 1.4126984126984128, "no_speech_prob": 0.018470697104930878}, {"id": 552, "seek": 238244, "start": 2394.12, "end": 2402.2000000000003, "text": " Por lo menos para aproximarse da muy bien. Google hace unos a\u00f1os atr\u00e1s sac\u00f3 un cuerpo de negra,", "tokens": [50948, 5269, 450, 8902, 1690, 31270, 11668, 1120, 5323, 3610, 13, 3329, 10032, 17780, 11424, 22906, 4899, 812, 517, 20264, 368, 2485, 424, 11, 51352], "temperature": 0.0, "avg_logprob": -0.3944645472935268, "compression_ratio": 1.4126984126984128, "no_speech_prob": 0.018470697104930878}, {"id": 553, "seek": 238244, "start": 2402.2000000000003, "end": 2409.4, "text": " un s\u00ed, una lista de negramas de hasta cinco. Me acuerdo en esa \u00e9poca bien en ese.", "tokens": [51352, 517, 8600, 11, 2002, 27764, 368, 2485, 2356, 296, 368, 10764, 21350, 13, 1923, 28113, 465, 11342, 25024, 3610, 465, 10167, 13, 51712], "temperature": 0.0, "avg_logprob": -0.3944645472935268, "compression_ratio": 1.4126984126984128, "no_speech_prob": 0.018470697104930878}, {"id": 554, "seek": 241244, "start": 2413.4, "end": 2416.92, "text": " O sea que determinar n va a depender un poco de la tarea y ese se me dio a ojo, digamos, pues una", "tokens": [50412, 422, 4158, 631, 3618, 6470, 297, 2773, 257, 1367, 3216, 517, 10639, 368, 635, 256, 35425, 288, 10167, 369, 385, 31965, 257, 277, 5134, 11, 36430, 11, 11059, 2002, 50588], "temperature": 0.0, "avg_logprob": -0.306837547482468, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0036090679932385683}, {"id": 555, "seek": 241244, "start": 2416.92, "end": 2423.0, "text": " tarea un poco complica. Ahora vamos a ver un poco de evaluaci\u00f3n. Ita y lo que dec\u00edamos, \u00bfno? \u00bfSe", "tokens": [50588, 256, 35425, 517, 10639, 1209, 2262, 13, 18840, 5295, 257, 1306, 517, 10639, 368, 6133, 3482, 13, 467, 64, 288, 450, 631, 979, 16275, 11, 3841, 1771, 30, 3841, 10637, 50892], "temperature": 0.0, "avg_logprob": -0.306837547482468, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0036090679932385683}, {"id": 556, "seek": 241244, "start": 2423.0, "end": 2426.84, "text": " agregan? Como cuando son trigramas, tengo que agregar dos s\u00edmbolos al comienzo de la oraci\u00f3n.", "tokens": [50892, 4554, 1275, 30, 11913, 7767, 1872, 35386, 2356, 296, 11, 13989, 631, 4554, 2976, 4491, 8600, 5612, 329, 419, 395, 1053, 4765, 368, 635, 420, 3482, 13, 51084], "temperature": 0.0, "avg_logprob": -0.306837547482468, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0036090679932385683}, {"id": 557, "seek": 241244, "start": 2428.6, "end": 2429.08, "text": " Tengo a poner.", "tokens": [51172, 314, 30362, 257, 19149, 13, 51196], "temperature": 0.0, "avg_logprob": -0.306837547482468, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0036090679932385683}, {"id": 558, "seek": 241244, "start": 2432.6, "end": 2440.12, "text": " Enero, abri\u00f3. Porque yo necesito dos de contexto para calcular el trigramo en detalle.", "tokens": [51372, 2193, 2032, 11, 410, 44802, 13, 11287, 5290, 11909, 3528, 4491, 368, 47685, 1690, 2104, 17792, 806, 35386, 2356, 78, 465, 1141, 11780, 13, 51748], "temperature": 0.0, "avg_logprob": -0.306837547482468, "compression_ratio": 1.5943775100401607, "no_speech_prob": 0.0036090679932385683}, {"id": 559, "seek": 244012, "start": 2440.8399999999997, "end": 2453.88, "text": " Y bueno, y la pregunta es c\u00f3mo calculamos", "tokens": [50400, 398, 11974, 11, 288, 635, 24252, 785, 12826, 4322, 2151, 51052], "temperature": 0.0, "avg_logprob": -0.34951232651532704, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.0010170751484110951}, {"id": 560, "seek": 244012, "start": 2457.3199999999997, "end": 2460.92, "text": " desde el punto de vista metodol\u00f3gico, c\u00f3mo hacemos para calcular buenas probabilidades.", "tokens": [51224, 10188, 806, 14326, 368, 22553, 1131, 378, 27629, 2789, 11, 12826, 33839, 1690, 2104, 17792, 43852, 31959, 10284, 13, 51404], "temperature": 0.0, "avg_logprob": -0.34951232651532704, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.0010170751484110951}, {"id": 561, "seek": 244012, "start": 2460.92, "end": 2466.68, "text": " Ya vimos c\u00f3mo se hace el conteo. Ya ahora quiero ver c\u00f3mo organiza el corpus y me parece", "tokens": [51404, 6080, 49266, 12826, 369, 10032, 806, 34444, 78, 13, 6080, 9923, 16811, 1306, 12826, 4645, 64, 806, 1181, 31624, 288, 385, 14120, 51692], "temperature": 0.0, "avg_logprob": -0.34951232651532704, "compression_ratio": 1.4866666666666666, "no_speech_prob": 0.0010170751484110951}, {"id": 562, "seek": 246668, "start": 2466.7599999999998, "end": 2471.24, "text": " que es interesante ver esto porque nos va a pasar en muchas cosas, en este tema de", "tokens": [50368, 631, 785, 36396, 1306, 7433, 4021, 3269, 2773, 257, 25344, 465, 16072, 12218, 11, 465, 4065, 15854, 368, 50592], "temperature": 0.0, "avg_logprob": -0.2150671026679907, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.005833988077938557}, {"id": 563, "seek": 246668, "start": 2471.24, "end": 2477.16, "text": " preservamiento del lenguaje natural y que muchas veces induce el mal uso metodol\u00f3gico de estas cosas", "tokens": [50592, 45905, 16971, 1103, 35044, 84, 11153, 3303, 288, 631, 16072, 17054, 41263, 806, 2806, 22728, 1131, 378, 27629, 2789, 368, 13897, 12218, 50888], "temperature": 0.0, "avg_logprob": -0.2150671026679907, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.005833988077938557}, {"id": 564, "seek": 246668, "start": 2479.56, "end": 2484.8399999999997, "text": " lleva error. Entonces me parece que vale la pena comentarlo esto.", "tokens": [51008, 37681, 6713, 13, 15097, 385, 14120, 631, 15474, 635, 29222, 14541, 19457, 7433, 13, 51272], "temperature": 0.0, "avg_logprob": -0.2150671026679907, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.005833988077938557}, {"id": 565, "seek": 246668, "start": 2485.8799999999997, "end": 2492.8399999999997, "text": " Yo. Yo dije que iba a hacer conteo para calcular las probabilidades, \u00bfno? Entonces yo por ac\u00e1 tengo", "tokens": [51324, 7616, 13, 7616, 39414, 631, 33423, 257, 6720, 34444, 78, 1690, 2104, 17792, 2439, 31959, 10284, 11, 3841, 1771, 30, 15097, 5290, 1515, 23496, 13989, 51672], "temperature": 0.0, "avg_logprob": -0.2150671026679907, "compression_ratio": 1.537117903930131, "no_speech_prob": 0.005833988077938557}, {"id": 566, "seek": 249284, "start": 2492.92, "end": 2496.6000000000004, "text": " un corpus, un corpus de texto.", "tokens": [50368, 517, 1181, 31624, 11, 517, 1181, 31624, 368, 35503, 13, 50552], "temperature": 0.0, "avg_logprob": -0.2300710678100586, "compression_ratio": 1.40625, "no_speech_prob": 0.010173655115067959}, {"id": 567, "seek": 249284, "start": 2501.96, "end": 2508.6000000000004, "text": " S\u00ed. Entonces esencialmente lo que tengo son muchos textos, \u00bfno? Obviamente, esencialmente no,", "tokens": [50820, 12375, 13, 15097, 785, 26567, 4082, 450, 631, 13989, 1872, 17061, 2487, 329, 11, 3841, 1771, 30, 4075, 23347, 11, 785, 26567, 4082, 572, 11, 51152], "temperature": 0.0, "avg_logprob": -0.2300710678100586, "compression_ratio": 1.40625, "no_speech_prob": 0.010173655115067959}, {"id": 568, "seek": 249284, "start": 2508.6000000000004, "end": 2511.0, "text": " tengo muchos textos. Esa es la definici\u00f3n de corpus.", "tokens": [51152, 13989, 17061, 2487, 329, 13, 2313, 64, 785, 635, 1561, 15534, 368, 1181, 31624, 13, 51272], "temperature": 0.0, "avg_logprob": -0.2300710678100586, "compression_ratio": 1.40625, "no_speech_prob": 0.010173655115067959}, {"id": 569, "seek": 251100, "start": 2511.32, "end": 2525.64, "text": " Y yo voy a establecer, voy a crear un modelo de una, de un, un modelo de un lenguaje. Es decir,", "tokens": [50380, 398, 5290, 7552, 257, 37444, 1776, 11, 7552, 257, 31984, 517, 27825, 368, 2002, 11, 368, 517, 11, 517, 27825, 368, 517, 35044, 84, 11153, 13, 2313, 10235, 11, 51096], "temperature": 0.0, "avg_logprob": -0.1738072308627042, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.007240649312734604}, {"id": 570, "seek": 251100, "start": 2525.64, "end": 2531.16, "text": " yo lo que quiero construir con esto de las probabilidades de las olaciones es un modelo", "tokens": [51096, 5290, 450, 631, 16811, 38445, 416, 7433, 368, 2439, 31959, 10284, 368, 2439, 2545, 9188, 785, 517, 27825, 51372], "temperature": 0.0, "avg_logprob": -0.1738072308627042, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.007240649312734604}, {"id": 571, "seek": 251100, "start": 2531.16, "end": 2535.56, "text": " del idioma espa\u00f1ol. Yo tengo un corpus de texto en espa\u00f1ol y quiero hacer un modelo del idioma", "tokens": [51372, 1103, 18014, 6440, 31177, 13, 7616, 13989, 517, 1181, 31624, 368, 35503, 465, 31177, 288, 16811, 6720, 517, 27825, 1103, 18014, 6440, 51592], "temperature": 0.0, "avg_logprob": -0.1738072308627042, "compression_ratio": 1.7073170731707317, "no_speech_prob": 0.007240649312734604}, {"id": 572, "seek": 253556, "start": 2536.04, "end": 2541.72, "text": " espa\u00f1ol. Supongo que yo entren un modelo, entrenar el modelo en este caso quiere decir", "tokens": [50388, 31177, 13, 9141, 25729, 631, 5290, 45069, 517, 27825, 11, 45069, 289, 806, 27825, 465, 4065, 9666, 23877, 10235, 50672], "temperature": 0.0, "avg_logprob": -0.24485694305806222, "compression_ratio": 1.458762886597938, "no_speech_prob": 0.032007183879613876}, {"id": 573, "seek": 253556, "start": 2541.72, "end": 2547.7999999999997, "text": " calcular todas esas probabilidades. \u00bfC\u00f3mo hago para saber qu\u00e9 tan bueno es?", "tokens": [50672, 2104, 17792, 10906, 23388, 31959, 10284, 13, 3841, 28342, 38721, 1690, 12489, 8057, 7603, 11974, 785, 30, 50976], "temperature": 0.0, "avg_logprob": -0.24485694305806222, "compression_ratio": 1.458762886597938, "no_speech_prob": 0.032007183879613876}, {"id": 574, "seek": 253556, "start": 2550.2, "end": 2551.4, "text": " \u00bfS\u00ed? \u00bfC\u00f3mo lo eval\u00fao?", "tokens": [51096, 3841, 30463, 30, 3841, 28342, 450, 1073, 304, 2481, 78, 30, 51156], "temperature": 0.0, "avg_logprob": -0.24485694305806222, "compression_ratio": 1.458762886597938, "no_speech_prob": 0.032007183879613876}, {"id": 575, "seek": 253556, "start": 2555.4, "end": 2558.2799999999997, "text": " Supongo que yo, ahora vamos a hablar de cu\u00e1l es la medida, pero supongo que yo tengo una", "tokens": [51356, 9141, 25729, 631, 5290, 11, 9923, 5295, 257, 21014, 368, 44318, 785, 635, 32984, 11, 4768, 9331, 25729, 631, 5290, 13989, 2002, 51500], "temperature": 0.0, "avg_logprob": -0.24485694305806222, "compression_ratio": 1.458762886597938, "no_speech_prob": 0.032007183879613876}, {"id": 576, "seek": 255828, "start": 2558.36, "end": 2562.36, "text": " medida de performa que me dice, bueno, aplicarle tu modelo a este texto.", "tokens": [50368, 32984, 368, 2042, 64, 631, 385, 10313, 11, 11974, 11, 18221, 36153, 2604, 27825, 257, 4065, 35503, 13, 50568], "temperature": 0.0, "avg_logprob": -0.19193429766960865, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.026157815009355545}, {"id": 577, "seek": 255828, "start": 2564.92, "end": 2569.4, "text": " S\u00ed. Supongamos que la medida es el que le asigne, ahora vamos a ver por qu\u00e9, pero el que le asigne", "tokens": [50696, 12375, 13, 9141, 556, 2151, 631, 635, 32984, 785, 806, 631, 476, 382, 26341, 11, 9923, 5295, 257, 1306, 1515, 8057, 11, 4768, 806, 631, 476, 382, 26341, 50920], "temperature": 0.0, "avg_logprob": -0.19193429766960865, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.026157815009355545}, {"id": 578, "seek": 255828, "start": 2569.4, "end": 2577.7200000000003, "text": " mayor probabilidad a todo el texto, a las oraciones del texto, es el mejor. El mejor modelo es el que", "tokens": [50920, 10120, 31959, 4580, 257, 5149, 806, 35503, 11, 257, 2439, 420, 9188, 1103, 35503, 11, 785, 806, 11479, 13, 2699, 11479, 27825, 785, 806, 631, 51336], "temperature": 0.0, "avg_logprob": -0.19193429766960865, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.026157815009355545}, {"id": 579, "seek": 255828, "start": 2577.7200000000003, "end": 2587.4, "text": " la asigna probabilidad mayor a las oraciones que tengo en el texto. Si yo aplico mi m\u00e9todo,", "tokens": [51336, 635, 382, 328, 629, 31959, 4580, 10120, 257, 2439, 420, 9188, 631, 13989, 465, 806, 35503, 13, 4909, 5290, 25522, 2789, 2752, 20275, 17423, 11, 51820], "temperature": 0.0, "avg_logprob": -0.19193429766960865, "compression_ratio": 1.7864077669902914, "no_speech_prob": 0.026157815009355545}, {"id": 580, "seek": 258740, "start": 2588.04, "end": 2592.6, "text": " mi modelo, o sea, eval\u00fao mi modelo. Sobre este mismo corpus, \u00bfqu\u00e9 problema tengo?", "tokens": [50396, 2752, 27825, 11, 277, 4158, 11, 1073, 304, 2481, 78, 2752, 27825, 13, 407, 2672, 4065, 12461, 1181, 31624, 11, 3841, 16412, 12395, 13989, 30, 50624], "temperature": 0.0, "avg_logprob": -0.16379353639890823, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.0058060940355062485}, {"id": 581, "seek": 258740, "start": 2594.6, "end": 2600.52, "text": " Que me va a dar b\u00e1rbaro porque lo calcul\u00e9 ah\u00ed. Es decir, yo nunca puedo, nunca, pero nunca,", "tokens": [50724, 4493, 385, 2773, 257, 4072, 272, 20335, 5356, 78, 4021, 450, 4322, 526, 12571, 13, 2313, 10235, 11, 5290, 13768, 21612, 11, 13768, 11, 4768, 13768, 11, 51020], "temperature": 0.0, "avg_logprob": -0.16379353639890823, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.0058060940355062485}, {"id": 582, "seek": 258740, "start": 2600.52, "end": 2606.36, "text": " nunca, evaluar un modelo en el mismo corpus en el que entren\u00e9. Esto aplica siempre. Cabe que", "tokens": [51020, 13768, 11, 6133, 289, 517, 27825, 465, 806, 12461, 1181, 31624, 465, 806, 631, 45069, 526, 13, 20880, 25522, 2262, 12758, 13, 383, 4488, 631, 51312], "temperature": 0.0, "avg_logprob": -0.16379353639890823, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.0058060940355062485}, {"id": 583, "seek": 258740, "start": 2606.36, "end": 2610.6800000000003, "text": " yo utilizo un m\u00e9todo estad\u00edstico, aprendizaje autom\u00e1tico. Lo m\u00e1s importante a saber en el", "tokens": [51312, 5290, 4976, 19055, 517, 20275, 17423, 39160, 19512, 2789, 11, 21003, 590, 11153, 3553, 28234, 13, 6130, 3573, 9416, 257, 12489, 465, 806, 51528], "temperature": 0.0, "avg_logprob": -0.16379353639890823, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.0058060940355062485}, {"id": 584, "seek": 258740, "start": 2610.6800000000003, "end": 2617.08, "text": " aprendizaje autom\u00e1tico es nunca eval\u00faes tu modelo en un corpus, en el mismo corpus que entrenaste,", "tokens": [51528, 21003, 590, 11153, 3553, 28234, 785, 13768, 1073, 304, 2481, 279, 2604, 27825, 465, 517, 1181, 31624, 11, 465, 806, 12461, 1181, 31624, 631, 45069, 9079, 11, 51848], "temperature": 0.0, "avg_logprob": -0.16379353639890823, "compression_ratio": 1.7397769516728625, "no_speech_prob": 0.0058060940355062485}, {"id": 585, "seek": 261708, "start": 2617.16, "end": 2619.72, "text": " porque por definiciones est\u00e1s haciendo trampa, eso lo que se llama", "tokens": [50368, 4021, 1515, 1561, 29719, 24389, 20509, 504, 26625, 11, 7287, 450, 631, 369, 23272, 50496], "temperature": 0.0, "avg_logprob": -0.2472973527579472, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.0012424796586856246}, {"id": 586, "seek": 261708, "start": 2621.3199999999997, "end": 2627.48, "text": " sobreajuste. Vos sobreajustas a tu corpus de entrenamiento. Entonces yo lo que voy a hacer es", "tokens": [50576, 5473, 1805, 41389, 13, 691, 329, 5473, 1805, 381, 296, 257, 2604, 1181, 31624, 368, 45069, 16971, 13, 15097, 5290, 450, 631, 7552, 257, 6720, 785, 50884], "temperature": 0.0, "avg_logprob": -0.2472973527579472, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.0012424796586856246}, {"id": 587, "seek": 261708, "start": 2628.52, "end": 2635.3199999999997, "text": " dividir mi corpus en dos y voy a decir, este es el corpus de entrenamiento,", "tokens": [50936, 4996, 347, 2752, 1181, 31624, 465, 4491, 288, 7552, 257, 10235, 11, 4065, 785, 806, 1181, 31624, 368, 45069, 16971, 11, 51276], "temperature": 0.0, "avg_logprob": -0.2472973527579472, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.0012424796586856246}, {"id": 588, "seek": 261708, "start": 2637.7999999999997, "end": 2638.68, "text": " voy a poner en ingl\u00e9s,", "tokens": [51400, 7552, 257, 19149, 465, 49766, 11, 51444], "temperature": 0.0, "avg_logprob": -0.2472973527579472, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.0012424796586856246}, {"id": 589, "seek": 261708, "start": 2641.3199999999997, "end": 2642.36, "text": " y el corpus de evaluaci\u00f3n.", "tokens": [51576, 288, 806, 1181, 31624, 368, 6133, 3482, 13, 51628], "temperature": 0.0, "avg_logprob": -0.2472973527579472, "compression_ratio": 1.6145251396648044, "no_speech_prob": 0.0012424796586856246}, {"id": 590, "seek": 264236, "start": 2642.36, "end": 2658.28, "text": " Entonces lo que yo voy a hacer es entrenar y \u00bfcu\u00e1nto se para ac\u00e1? Bueno,", "tokens": [50364, 15097, 450, 631, 5290, 7552, 257, 6720, 785, 45069, 289, 288, 3841, 12032, 27525, 78, 369, 1690, 23496, 30, 16046, 11, 51160], "temperature": 0.0, "avg_logprob": -0.30272357528274124, "compression_ratio": 1.0288461538461537, "no_speech_prob": 0.011304925195872784}, {"id": 591, "seek": 264236, "start": 2661.96, "end": 2664.6, "text": " la regla m\u00e1s o menos es 80-20.", "tokens": [51344, 635, 1121, 875, 3573, 277, 8902, 785, 4688, 12, 2009, 13, 51476], "temperature": 0.0, "avg_logprob": -0.30272357528274124, "compression_ratio": 1.0288461538461537, "no_speech_prob": 0.011304925195872784}, {"id": 592, "seek": 267236, "start": 2672.36, "end": 2676.76, "text": " Pregunto, \u00bfpor qu\u00e9 me interesar\u00eda que esto fuera lo m\u00e1s grande posible?", "tokens": [50364, 430, 3375, 24052, 11, 3841, 2816, 8057, 385, 728, 18876, 2686, 631, 7433, 24818, 450, 3573, 8883, 26644, 30, 50584], "temperature": 0.0, "avg_logprob": -0.32123794555664065, "compression_ratio": 1.158273381294964, "no_speech_prob": 0.21781721711158752}, {"id": 593, "seek": 267236, "start": 2683.8, "end": 2690.76, "text": " Para que tener m\u00e1s informaci\u00f3n. \u00bfY por qu\u00e9 no abuso 90-10 o 95-5 o 97-3?", "tokens": [50936, 11107, 631, 11640, 3573, 21660, 13, 3841, 56, 1515, 8057, 572, 410, 24431, 4289, 12, 3279, 277, 13420, 12, 20, 277, 23399, 12, 18, 30, 51284], "temperature": 0.0, "avg_logprob": -0.32123794555664065, "compression_ratio": 1.158273381294964, "no_speech_prob": 0.21781721711158752}, {"id": 594, "seek": 267236, "start": 2694.52, "end": 2694.76, "text": " \u00bfC\u00f3mo?", "tokens": [51472, 3841, 28342, 30, 51484], "temperature": 0.0, "avg_logprob": -0.32123794555664065, "compression_ratio": 1.158273381294964, "no_speech_prob": 0.21781721711158752}, {"id": 595, "seek": 269476, "start": 2695.7200000000003, "end": 2697.5600000000004, "text": " \u00bfQuieres evaluarlo con una cantidad de datos?", "tokens": [50412, 3841, 8547, 811, 279, 6133, 19457, 416, 2002, 33757, 368, 27721, 30, 50504], "temperature": 0.0, "avg_logprob": -0.26953760060397064, "compression_ratio": 1.4926108374384237, "no_speech_prob": 0.02328306995332241}, {"id": 596, "seek": 269476, "start": 2697.5600000000004, "end": 2702.0400000000004, "text": " Tengo que solucionar ese balance, entretener una cantidad razonable de datos para hablar,", "tokens": [50504, 314, 30362, 631, 1404, 1311, 33639, 10167, 4772, 11, 3962, 1147, 260, 2002, 33757, 367, 6317, 712, 368, 27721, 1690, 21014, 11, 50728], "temperature": 0.0, "avg_logprob": -0.26953760060397064, "compression_ratio": 1.4926108374384237, "no_speech_prob": 0.02328306995332241}, {"id": 597, "seek": 269476, "start": 2702.0400000000004, "end": 2709.32, "text": " porque si yo le evaluo sobre una oraci\u00f3n, la varianza es muy grande, es decir, la posibilidad", "tokens": [50728, 4021, 1511, 5290, 476, 6133, 78, 5473, 2002, 420, 3482, 11, 635, 3034, 20030, 785, 5323, 8883, 11, 785, 10235, 11, 635, 1366, 33989, 51092], "temperature": 0.0, "avg_logprob": -0.26953760060397064, "compression_ratio": 1.4926108374384237, "no_speech_prob": 0.02328306995332241}, {"id": 598, "seek": 269476, "start": 2709.32, "end": 2714.92, "text": " de equivocarme es muy grande. Entonces una regla es m\u00e1s o menos 80-20.", "tokens": [51092, 368, 48726, 905, 35890, 785, 5323, 8883, 13, 15097, 2002, 1121, 875, 785, 3573, 277, 8902, 4688, 12, 2009, 13, 51372], "temperature": 0.0, "avg_logprob": -0.26953760060397064, "compression_ratio": 1.4926108374384237, "no_speech_prob": 0.02328306995332241}, {"id": 599, "seek": 271492, "start": 2715.88, "end": 2716.6800000000003, "text": " \u00bfS\u00ed?", "tokens": [50412, 3841, 30463, 30, 50452], "temperature": 0.0, "avg_logprob": -0.38588281778188854, "compression_ratio": 1.1007751937984496, "no_speech_prob": 0.02970879338681698}, {"id": 600, "seek": 271492, "start": 2723.88, "end": 2729.8, "text": " Y bueno, ah\u00ed habla de 90-10, yo tengo la regla de 80-20.", "tokens": [50812, 398, 11974, 11, 12571, 42135, 368, 4289, 12, 3279, 11, 5290, 13989, 635, 1121, 875, 368, 4688, 12, 2009, 13, 51108], "temperature": 0.0, "avg_logprob": -0.38588281778188854, "compression_ratio": 1.1007751937984496, "no_speech_prob": 0.02970879338681698}, {"id": 601, "seek": 271492, "start": 2732.92, "end": 2738.2000000000003, "text": " Va a surgir un problema adicional ac\u00e1, y es que ahora lo que voy a mover es,", "tokens": [51264, 16822, 257, 19560, 347, 517, 12395, 614, 33010, 23496, 11, 288, 785, 631, 9923, 450, 631, 7552, 257, 39945, 785, 11, 51528], "temperature": 0.0, "avg_logprob": -0.38588281778188854, "compression_ratio": 1.1007751937984496, "no_speech_prob": 0.02970879338681698}, {"id": 602, "seek": 273820, "start": 2738.52, "end": 2747.72, "text": " por ejemplo, si yo quiero saber cu\u00e1ntos eleg\u00ed el n, \u00bfno?", "tokens": [50380, 1515, 13358, 11, 1511, 5290, 16811, 12489, 44256, 329, 14459, 870, 806, 297, 11, 3841, 1771, 30, 50840], "temperature": 0.0, "avg_logprob": -0.36433294545049255, "compression_ratio": 1.219298245614035, "no_speech_prob": 0.009095550514757633}, {"id": 603, "seek": 273820, "start": 2749.72, "end": 2758.04, "text": " Yo quiero elegir el n, yo necesito, lo que puedo hacer es pruebo con un n ac\u00e1,", "tokens": [50940, 7616, 16811, 14459, 347, 806, 297, 11, 5290, 11909, 3528, 11, 450, 631, 21612, 6720, 785, 32820, 1763, 416, 517, 297, 23496, 11, 51356], "temperature": 0.0, "avg_logprob": -0.36433294545049255, "compression_ratio": 1.219298245614035, "no_speech_prob": 0.009095550514757633}, {"id": 604, "seek": 275804, "start": 2758.36, "end": 2768.36, "text": " modelo 1, n igual 2, y hago modelo 2, n igual 3.", "tokens": [50380, 27825, 502, 11, 297, 10953, 568, 11, 288, 38721, 27825, 568, 11, 297, 10953, 805, 13, 50880], "temperature": 0.0, "avg_logprob": -0.35323944091796877, "compression_ratio": 1.2276422764227641, "no_speech_prob": 0.04414935037493706}, {"id": 605, "seek": 275804, "start": 2774.36, "end": 2781.32, "text": " Esto es un poco m\u00e1s \u00fatil de ver. Y lo evaluo ac\u00e1 y digo m1 y m2, y me quedo con el que me da mejor.", "tokens": [51180, 20880, 785, 517, 10639, 3573, 49191, 368, 1306, 13, 398, 450, 6133, 78, 23496, 288, 22990, 275, 16, 288, 275, 17, 11, 288, 385, 13617, 78, 416, 806, 631, 385, 1120, 11479, 13, 51528], "temperature": 0.0, "avg_logprob": -0.35323944091796877, "compression_ratio": 1.2276422764227641, "no_speech_prob": 0.04414935037493706}, {"id": 606, "seek": 278132, "start": 2782.28, "end": 2787.0800000000004, "text": " Eso metodologicamente no est\u00e1 bien. \u00bfPor qu\u00e9?", "tokens": [50412, 27795, 1131, 378, 1132, 23653, 572, 3192, 3610, 13, 3841, 24907, 8057, 30, 50652], "temperature": 0.0, "avg_logprob": -0.23105072445339628, "compression_ratio": 1.4954954954954955, "no_speech_prob": 0.012064829468727112}, {"id": 607, "seek": 278132, "start": 2790.76, "end": 2797.2400000000002, "text": " Y esto es una de las cosas que es m\u00e1s dif\u00edcil de entender a veces. Si yo pruebo los dos modelos", "tokens": [50836, 398, 7433, 785, 2002, 368, 2439, 12218, 631, 785, 3573, 17258, 368, 20054, 257, 17054, 13, 4909, 5290, 32820, 1763, 1750, 4491, 2316, 329, 51160], "temperature": 0.0, "avg_logprob": -0.23105072445339628, "compression_ratio": 1.4954954954954955, "no_speech_prob": 0.012064829468727112}, {"id": 608, "seek": 278132, "start": 2797.2400000000002, "end": 2802.36, "text": " ac\u00e1, de alguna forma tambi\u00e9n estoy haciendo trampa, porque sup\u00f3nganse que yo tengo no dos", "tokens": [51160, 23496, 11, 368, 20651, 8366, 6407, 15796, 20509, 504, 26625, 11, 4021, 9331, 812, 872, 47661, 631, 5290, 13989, 572, 4491, 51416], "temperature": 0.0, "avg_logprob": -0.23105072445339628, "compression_ratio": 1.4954954954954955, "no_speech_prob": 0.012064829468727112}, {"id": 609, "seek": 278132, "start": 2802.36, "end": 2807.2400000000002, "text": " par\u00e1metros, porque ac\u00e1 tengo un par\u00e1metro que tiene dos valores. Supongamos que yo quiero", "tokens": [51416, 971, 842, 29570, 11, 4021, 23496, 13989, 517, 971, 842, 45400, 631, 7066, 4491, 38790, 13, 9141, 556, 2151, 631, 5290, 16811, 51660], "temperature": 0.0, "avg_logprob": -0.23105072445339628, "compression_ratio": 1.4954954954954955, "no_speech_prob": 0.012064829468727112}, {"id": 610, "seek": 280724, "start": 2807.3199999999997, "end": 2816.04, "text": " ajustar otro par\u00e1metro de mi m\u00e9todo que puede tomar 500 valores posibles. Si yo hago 500", "tokens": [50368, 41023, 289, 11921, 971, 842, 45400, 368, 2752, 20275, 17423, 631, 8919, 22048, 5923, 38790, 1366, 14428, 13, 4909, 5290, 38721, 5923, 50804], "temperature": 0.0, "avg_logprob": -0.17174388885498046, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.04994338005781174}, {"id": 611, "seek": 280724, "start": 2816.04, "end": 2825.0, "text": " entrenamientos y 500 pruebas, muy probablemente tambi\u00e9n est\u00e9 ajustando ac\u00e1, est\u00e9 sobreajustando", "tokens": [50804, 45069, 43466, 288, 5923, 32820, 16342, 11, 5323, 21759, 4082, 6407, 34584, 41023, 1806, 23496, 11, 34584, 5473, 1805, 381, 1806, 51252], "temperature": 0.0, "avg_logprob": -0.17174388885498046, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.04994338005781174}, {"id": 612, "seek": 280724, "start": 2825.0, "end": 2829.4799999999996, "text": " ac\u00e1, porque estoy eligiendo de los 500, y a veces pueden ser miles o cientos de miles,", "tokens": [51252, 23496, 11, 4021, 15796, 31089, 7304, 368, 1750, 5923, 11, 288, 257, 17054, 14714, 816, 6193, 277, 269, 20370, 368, 6193, 11, 51476], "temperature": 0.0, "avg_logprob": -0.17174388885498046, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.04994338005781174}, {"id": 613, "seek": 280724, "start": 2830.9199999999996, "end": 2835.3999999999996, "text": " el que mejor anda en este cuerpo de evaluaci\u00f3n, o sea que estoy sobreajustando el cuerpo de evaluaci\u00f3n.", "tokens": [51548, 806, 631, 11479, 21851, 465, 4065, 20264, 368, 6133, 3482, 11, 277, 4158, 631, 15796, 5473, 1805, 381, 1806, 806, 20264, 368, 6133, 3482, 13, 51772], "temperature": 0.0, "avg_logprob": -0.17174388885498046, "compression_ratio": 1.7066666666666668, "no_speech_prob": 0.04994338005781174}, {"id": 614, "seek": 283540, "start": 2835.7200000000003, "end": 2841.0, "text": " Entonces, para el ajuste de par\u00e1metros, yo usualmente lo que tengo que hacer es definir", "tokens": [50380, 15097, 11, 1690, 806, 41023, 68, 368, 971, 842, 29570, 11, 5290, 7713, 4082, 450, 631, 13989, 631, 6720, 785, 1561, 347, 50644], "temperature": 0.0, "avg_logprob": -0.2289336313966845, "compression_ratio": 1.4413793103448276, "no_speech_prob": 0.0034078555181622505}, {"id": 615, "seek": 283540, "start": 2842.28, "end": 2850.76, "text": " dividir este corpus, sacar un pedacito del cuerpo de entrenamiento,", "tokens": [50708, 4996, 347, 4065, 1181, 31624, 11, 43823, 517, 5670, 326, 3528, 1103, 20264, 368, 45069, 16971, 11, 51132], "temperature": 0.0, "avg_logprob": -0.2289336313966845, "compression_ratio": 1.4413793103448276, "no_speech_prob": 0.0034078555181622505}, {"id": 616, "seek": 283540, "start": 2854.44, "end": 2857.48, "text": " que lo llamo corpus held auto, corpus de desarrollo.", "tokens": [51316, 631, 450, 4849, 10502, 1181, 31624, 5167, 8399, 11, 1181, 31624, 368, 38295, 13, 51468], "temperature": 0.0, "avg_logprob": -0.2289336313966845, "compression_ratio": 1.4413793103448276, "no_speech_prob": 0.0034078555181622505}, {"id": 617, "seek": 285748, "start": 2858.28, "end": 2867.8, "text": " Y lo que hago es entreno sobre esta parte y evaluo sobre el held auto, y me reservo este", "tokens": [50404, 398, 450, 631, 38721, 785, 45069, 78, 5473, 5283, 6975, 288, 6133, 78, 5473, 806, 5167, 8399, 11, 288, 385, 16454, 78, 4065, 50880], "temperature": 0.0, "avg_logprob": -0.25384764023769046, "compression_ratio": 1.5380710659898478, "no_speech_prob": 0.03596499562263489}, {"id": 618, "seek": 285748, "start": 2869.32, "end": 2873.48, "text": " de evaluaci\u00f3n, solamente para cuando tengo mi modelo definitivo y quiero saber su", "tokens": [50956, 368, 6133, 3482, 11, 27814, 1690, 7767, 13989, 2752, 27825, 28781, 6340, 288, 16811, 12489, 459, 51164], "temperature": 0.0, "avg_logprob": -0.25384764023769046, "compression_ratio": 1.5380710659898478, "no_speech_prob": 0.03596499562263489}, {"id": 619, "seek": 285748, "start": 2873.48, "end": 2876.2, "text": " performance, con su medida de evaluaci\u00f3n, \u00bfde acuerdo?", "tokens": [51164, 3389, 11, 416, 459, 32984, 368, 6133, 3482, 11, 3841, 1479, 28113, 30, 51300], "temperature": 0.0, "avg_logprob": -0.25384764023769046, "compression_ratio": 1.5380710659898478, "no_speech_prob": 0.03596499562263489}, {"id": 620, "seek": 285748, "start": 2880.6, "end": 2884.84, "text": " Esto lo van a, algo como esto van a tener que presentar en el laboratorio,", "tokens": [51520, 20880, 450, 3161, 257, 11, 8655, 2617, 7433, 3161, 257, 11640, 631, 1974, 289, 465, 806, 5938, 48028, 11, 51732], "temperature": 0.0, "avg_logprob": -0.25384764023769046, "compression_ratio": 1.5380710659898478, "no_speech_prob": 0.03596499562263489}, {"id": 621, "seek": 288484, "start": 2885.8, "end": 2888.28, "text": " decir, c\u00f3mo evaluar\u00edan el m\u00e9todo, un m\u00e9todo.", "tokens": [50412, 10235, 11, 12826, 6133, 289, 11084, 806, 20275, 17423, 11, 517, 20275, 17423, 13, 50536], "temperature": 0.0, "avg_logprob": -0.2068881201989872, "compression_ratio": 1.6683673469387754, "no_speech_prob": 0.004632830619812012}, {"id": 622, "seek": 288484, "start": 2890.52, "end": 2895.08, "text": " Hay otras posibilidades que no implican un corpus held auto, por ejemplo, hacer lo que se", "tokens": [50648, 8721, 20244, 1366, 11607, 10284, 631, 572, 10629, 282, 517, 1181, 31624, 5167, 8399, 11, 1515, 13358, 11, 6720, 450, 631, 369, 50876], "temperature": 0.0, "avg_logprob": -0.2068881201989872, "compression_ratio": 1.6683673469387754, "no_speech_prob": 0.004632830619812012}, {"id": 623, "seek": 288484, "start": 2895.08, "end": 2901.0, "text": " llama cross validation, que es separo este pedacito, entreno sobre esto y evaluo sobre este,", "tokens": [50876, 23272, 3278, 24071, 11, 631, 785, 3128, 78, 4065, 5670, 326, 3528, 11, 45069, 78, 5473, 7433, 288, 6133, 78, 5473, 4065, 11, 51172], "temperature": 0.0, "avg_logprob": -0.2068881201989872, "compression_ratio": 1.6683673469387754, "no_speech_prob": 0.004632830619812012}, {"id": 624, "seek": 288484, "start": 2903.0, "end": 2909.1600000000003, "text": " si, despu\u00e9s separo otra franjita, entreno sobre el resto y evaluo sobre la franjita y as\u00ed con", "tokens": [51272, 1511, 11, 15283, 3128, 78, 13623, 431, 282, 73, 2786, 11, 45069, 78, 5473, 806, 28247, 288, 6133, 78, 5473, 635, 431, 282, 73, 2786, 288, 8582, 416, 51580], "temperature": 0.0, "avg_logprob": -0.2068881201989872, "compression_ratio": 1.6683673469387754, "no_speech_prob": 0.004632830619812012}, {"id": 625, "seek": 290916, "start": 2909.48, "end": 2916.44, "text": " en cada franca, franjas y saco el promedio, eso me sirve para no desperdiciar, digamos,", "tokens": [50380, 465, 8411, 431, 40835, 11, 431, 282, 19221, 288, 4899, 78, 806, 2234, 292, 1004, 11, 7287, 385, 4735, 303, 1690, 572, 10679, 67, 8787, 289, 11, 36430, 11, 50728], "temperature": 0.0, "avg_logprob": -0.2007507985951949, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.00837472453713417}, {"id": 626, "seek": 290916, "start": 2916.44, "end": 2922.6, "text": " esta parte del corpus, para poder utilizar todo el cuerpo de entrenamiento, se llama cross", "tokens": [50728, 5283, 6975, 1103, 1181, 31624, 11, 1690, 8152, 24060, 5149, 806, 20264, 368, 45069, 16971, 11, 369, 23272, 3278, 51036], "temperature": 0.0, "avg_logprob": -0.2007507985951949, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.00837472453713417}, {"id": 627, "seek": 290916, "start": 2922.6, "end": 2930.68, "text": " validation. Vamos a volver a hablar un poquito de cross validation, cuando hablemos de", "tokens": [51036, 24071, 13, 10894, 257, 33998, 257, 21014, 517, 28229, 368, 3278, 24071, 11, 7767, 324, 1113, 329, 368, 51440], "temperature": 0.0, "avg_logprob": -0.2007507985951949, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.00837472453713417}, {"id": 628, "seek": 290916, "start": 2930.68, "end": 2935.56, "text": " clasificaci\u00f3n, pero lo que me interesa es que le quede claro la diferencia entre estos corpus", "tokens": [51440, 596, 296, 40802, 11, 4768, 450, 631, 385, 728, 13708, 785, 631, 476, 421, 4858, 16742, 635, 38844, 3962, 12585, 1181, 31624, 51684], "temperature": 0.0, "avg_logprob": -0.2007507985951949, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.00837472453713417}, {"id": 629, "seek": 293556, "start": 2936.2, "end": 2944.2, "text": " y cuando, como dec\u00eda, cuando tengo el modelo final, uso esto solamente para evaluar las", "tokens": [50396, 288, 7767, 11, 2617, 37599, 11, 7767, 13989, 806, 27825, 2572, 11, 22728, 7433, 27814, 1690, 6133, 289, 2439, 50796], "temperature": 0.0, "avg_logprob": -0.21583102447818023, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.006021662149578333}, {"id": 630, "seek": 293556, "start": 2944.2, "end": 2951.16, "text": " performas, en una medida que determinar\u00eda ese unitaria. \u00bfC\u00f3mo evaluamos un modelo bueno?", "tokens": [50796, 2042, 296, 11, 465, 2002, 32984, 631, 3618, 6470, 2686, 10167, 517, 3981, 654, 13, 3841, 28342, 6133, 2151, 517, 27825, 11974, 30, 51144], "temperature": 0.0, "avg_logprob": -0.21583102447818023, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.006021662149578333}, {"id": 631, "seek": 293556, "start": 2951.16, "end": 2956.32, "text": " La manera correcta de evaluar un modelo deber\u00eda, ser\u00eda emp\u00edricamente, es decir, si yo quiero", "tokens": [51144, 2369, 13913, 3006, 64, 368, 6133, 289, 517, 27825, 29671, 2686, 11, 23679, 4012, 870, 1341, 3439, 11, 785, 10235, 11, 1511, 5290, 16811, 51402], "temperature": 0.0, "avg_logprob": -0.21583102447818023, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.006021662149578333}, {"id": 632, "seek": 293556, "start": 2956.32, "end": 2961.88, "text": " valorar un modelo de lenguaje y lo estoy usando para el reconocimiento del habla, deber\u00eda ser una", "tokens": [51402, 15367, 289, 517, 27825, 368, 35044, 84, 11153, 288, 450, 15796, 29798, 1690, 806, 43838, 14007, 1103, 42135, 11, 29671, 2686, 816, 2002, 51680], "temperature": 0.0, "avg_logprob": -0.21583102447818023, "compression_ratio": 1.6375545851528384, "no_speech_prob": 0.006021662149578333}, {"id": 633, "seek": 296188, "start": 2961.88, "end": 2967.88, "text": " evaluaci\u00f3n de qu\u00e9 tambi\u00e9n reconozco el habla o qu\u00e9 tambi\u00e9n reconozco la escritura, pero eso puede", "tokens": [50364, 6133, 3482, 368, 8057, 6407, 850, 8957, 89, 1291, 806, 42135, 277, 8057, 6407, 850, 8957, 89, 1291, 635, 4721, 3210, 2991, 11, 4768, 7287, 8919, 50664], "temperature": 0.0, "avg_logprob": -0.16647904007523148, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.08468109369277954}, {"id": 634, "seek": 296188, "start": 2967.88, "end": 2971.8, "text": " ser muy costoso a veces, o yo puedo estar haciendo un modelo en lenguaje y no s\u00e9 para qu\u00e9 se va a", "tokens": [50664, 816, 5323, 2063, 9869, 257, 17054, 11, 277, 5290, 21612, 8755, 20509, 517, 27825, 465, 35044, 84, 11153, 288, 572, 7910, 1690, 8057, 369, 2773, 257, 50860], "temperature": 0.0, "avg_logprob": -0.16647904007523148, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.08468109369277954}, {"id": 635, "seek": 296188, "start": 2971.8, "end": 2979.28, "text": " usar, entonces me interesa mucho o me puede interesar tener una media intr\u00ednseca de la", "tokens": [50860, 14745, 11, 13003, 385, 728, 13708, 9824, 277, 385, 8919, 728, 18876, 11640, 2002, 3021, 17467, 10973, 405, 496, 368, 635, 51234], "temperature": 0.0, "avg_logprob": -0.16647904007523148, "compression_ratio": 1.5846994535519126, "no_speech_prob": 0.08468109369277954}, {"id": 636, "seek": 297928, "start": 2979.28, "end": 2990.28, "text": " performa de mi modelo. Entonces, vamos a ver una forma de evaluar. A m\u00ed esta parte de este", "tokens": [50364, 2042, 64, 368, 2752, 27825, 13, 15097, 11, 5295, 257, 1306, 2002, 8366, 368, 6133, 289, 13, 316, 14692, 5283, 6975, 368, 4065, 50914], "temperature": 0.0, "avg_logprob": -0.25702598571777346, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.15392912924289703}, {"id": 637, "seek": 297928, "start": 2990.28, "end": 2999.5600000000004, "text": " parte en el libro est\u00e1 puesta como un tema avanzado, pero a m\u00ed me parece interesante mostrarlo porque", "tokens": [50914, 6975, 465, 806, 29354, 3192, 2362, 7841, 2617, 517, 15854, 42444, 1573, 11, 4768, 257, 14692, 385, 14120, 36396, 21487, 752, 4021, 51378], "temperature": 0.0, "avg_logprob": -0.25702598571777346, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.15392912924289703}, {"id": 638, "seek": 297928, "start": 2999.5600000000004, "end": 3006.2400000000002, "text": " porque la entrop\u00eda es un concepto que aparece muchas veces en el profesoramiento del lenguaje", "tokens": [51378, 4021, 635, 948, 1513, 2686, 785, 517, 3410, 78, 631, 37863, 16072, 17054, 465, 806, 22912, 284, 16971, 1103, 35044, 84, 11153, 51712], "temperature": 0.0, "avg_logprob": -0.25702598571777346, "compression_ratio": 1.5343915343915344, "no_speech_prob": 0.15392912924289703}, {"id": 639, "seek": 300624, "start": 3006.24, "end": 3011.9199999999996, "text": " natural y en otras cosas me parece que le vale la pena por lo menos aproximarse. Supongan que", "tokens": [50364, 3303, 288, 465, 20244, 12218, 385, 14120, 631, 476, 15474, 635, 29222, 1515, 450, 8902, 31270, 11668, 13, 9141, 556, 282, 631, 50648], "temperature": 0.0, "avg_logprob": -0.22733789331772747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.03615439683198929}, {"id": 640, "seek": 300624, "start": 3011.9199999999996, "end": 3017.04, "text": " yo tengo una variada de la aleatoria y todo esto voy a llegar a una forma de evaluar un modelo,", "tokens": [50648, 5290, 13989, 2002, 3034, 1538, 368, 635, 6775, 1639, 654, 288, 5149, 7433, 7552, 257, 24892, 257, 2002, 8366, 368, 6133, 289, 517, 27825, 11, 50904], "temperature": 0.0, "avg_logprob": -0.22733789331772747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.03615439683198929}, {"id": 641, "seek": 300624, "start": 3017.04, "end": 3023.24, "text": " \u00bfno? No hay que empezar a hablar de esto porque s\u00ed. Supongan que yo tengo una variada de la", "tokens": [50904, 3841, 1771, 30, 883, 4842, 631, 31168, 257, 21014, 368, 7433, 4021, 8600, 13, 9141, 556, 282, 631, 5290, 13989, 2002, 3034, 1538, 368, 635, 51214], "temperature": 0.0, "avg_logprob": -0.22733789331772747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.03615439683198929}, {"id": 642, "seek": 300624, "start": 3023.24, "end": 3028.64, "text": " aleatoria que tiene varios eventos posibles, en nuestro caso dijimos que eran las palabras", "tokens": [51214, 6775, 1639, 654, 631, 7066, 33665, 2280, 329, 1366, 14428, 11, 465, 14726, 9666, 47709, 8372, 631, 32762, 2439, 35240, 51484], "temperature": 0.0, "avg_logprob": -0.22733789331772747, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.03615439683198929}, {"id": 643, "seek": 302864, "start": 3028.64, "end": 3039.3599999999997, "text": " posibles. La entrop\u00eda, la entrop\u00eda es una variada de la aleatoria que es un concepto que viene de la", "tokens": [50364, 1366, 14428, 13, 2369, 948, 1513, 2686, 11, 635, 948, 1513, 2686, 785, 2002, 3034, 1538, 368, 635, 6775, 1639, 654, 631, 785, 517, 3410, 78, 631, 19561, 368, 635, 50900], "temperature": 0.0, "avg_logprob": -0.316013240814209, "compression_ratio": 1.5037037037037038, "no_speech_prob": 0.05806923657655716}, {"id": 644, "seek": 302864, "start": 3039.3599999999997, "end": 3053.3199999999997, "text": " teor\u00eda de la informaci\u00f3n, de Claude Shannon. La teor\u00eda de informaci\u00f3n lo que hablaba era, bueno,", "tokens": [50900, 40238, 2686, 368, 635, 21660, 11, 368, 12947, 2303, 28974, 13, 2369, 40238, 2686, 368, 21660, 450, 631, 26280, 5509, 4249, 11, 11974, 11, 51598], "temperature": 0.0, "avg_logprob": -0.316013240814209, "compression_ratio": 1.5037037037037038, "no_speech_prob": 0.05806923657655716}, {"id": 645, "seek": 305332, "start": 3053.32, "end": 3058.76, "text": " alguno capaz que hicieron, lo vieron en un curso, pero la teor\u00eda de informaci\u00f3n lo que trataba", "tokens": [50364, 9813, 78, 35453, 631, 23697, 14440, 11, 450, 371, 14440, 465, 517, 31085, 11, 4768, 635, 40238, 2686, 368, 21660, 450, 631, 21507, 5509, 50636], "temperature": 0.0, "avg_logprob": -0.22851949219309955, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.041669875383377075}, {"id": 646, "seek": 305332, "start": 3058.76, "end": 3062.0800000000004, "text": " era de medir cu\u00e1nto me cuesta a m\u00ed transmitir un mensaje. \u00bfC\u00f3mo puedo transmitir un mensaje", "tokens": [50636, 4249, 368, 1205, 347, 44256, 78, 385, 2702, 7841, 257, 14692, 17831, 347, 517, 10923, 11153, 13, 3841, 28342, 21612, 17831, 347, 517, 10923, 11153, 50802], "temperature": 0.0, "avg_logprob": -0.22851949219309955, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.041669875383377075}, {"id": 647, "seek": 305332, "start": 3062.0800000000004, "end": 3071.0800000000004, "text": " de forma \u00f3ptima? Digamos, es un poco la idea, o qu\u00e9 hay atr\u00e1s de una comunicaci\u00f3n. La noci\u00f3n", "tokens": [50802, 368, 8366, 11857, 662, 4775, 30, 10976, 2151, 11, 785, 517, 10639, 635, 1558, 11, 277, 8057, 4842, 22906, 368, 2002, 31710, 3482, 13, 2369, 572, 5687, 51252], "temperature": 0.0, "avg_logprob": -0.22851949219309955, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.041669875383377075}, {"id": 648, "seek": 305332, "start": 3071.0800000000004, "end": 3078.1200000000003, "text": " de entrop\u00eda, esta funci\u00f3n es, tengo el evento, quiero decir, la probabilidad del evento por el", "tokens": [51252, 368, 948, 1513, 2686, 11, 5283, 43735, 785, 11, 13989, 806, 40655, 11, 16811, 10235, 11, 635, 31959, 4580, 1103, 40655, 1515, 806, 51604], "temperature": 0.0, "avg_logprob": -0.22851949219309955, "compression_ratio": 1.560483870967742, "no_speech_prob": 0.041669875383377075}, {"id": 649, "seek": 307812, "start": 3078.12, "end": 3084.08, "text": " valorismo de esa probabilidad, \u00bfs\u00ed? La entrop\u00eda tiene como caracter\u00edstica fundamental que es una", "tokens": [50364, 15367, 6882, 368, 11342, 31959, 4580, 11, 3841, 82, 870, 30, 2369, 948, 1513, 2686, 7066, 2617, 34297, 2262, 8088, 631, 785, 2002, 50662], "temperature": 0.0, "avg_logprob": -0.18753172670091903, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.1567676067352295}, {"id": 650, "seek": 307812, "start": 3084.08, "end": 3092.52, "text": " medida que, si hay un evento que tiene toda la masa de probabilidad, la entrop\u00eda es m\u00ednima. Es decir,", "tokens": [50662, 32984, 631, 11, 1511, 4842, 517, 40655, 631, 7066, 11687, 635, 29216, 368, 31959, 4580, 11, 635, 948, 1513, 2686, 785, 33656, 4775, 13, 2313, 10235, 11, 51084], "temperature": 0.0, "avg_logprob": -0.18753172670091903, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.1567676067352295}, {"id": 651, "seek": 307812, "start": 3092.52, "end": 3098.2799999999997, "text": " si yo tengo un dado que est\u00e1 tan cargado y una forma, algo que, equivalentemente se puede decir", "tokens": [51084, 1511, 5290, 13989, 517, 29568, 631, 3192, 7603, 1032, 30135, 288, 2002, 8366, 11, 8655, 631, 11, 9052, 317, 16288, 369, 8919, 10235, 51372], "temperature": 0.0, "avg_logprob": -0.18753172670091903, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.1567676067352295}, {"id": 652, "seek": 307812, "start": 3098.2799999999997, "end": 3105.0, "text": " que la entrop\u00eda mide migrado disertidumbre sobre un evento. Si yo tengo un dado que est\u00e1 tan cargado,", "tokens": [51372, 631, 635, 948, 1513, 2686, 275, 482, 6186, 14974, 717, 911, 327, 449, 2672, 5473, 517, 40655, 13, 4909, 5290, 13989, 517, 29568, 631, 3192, 7603, 1032, 30135, 11, 51708], "temperature": 0.0, "avg_logprob": -0.18753172670091903, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.1567676067352295}, {"id": 653, "seek": 310500, "start": 3105.0, "end": 3111.16, "text": " que cabe que lo tiro, s\u00e9 que siempre va a salir seis, no tengo disertidumbre. Mi entrop\u00eda es cero.", "tokens": [50364, 631, 18893, 631, 450, 44188, 11, 7910, 631, 12758, 2773, 257, 31514, 28233, 11, 572, 13989, 717, 911, 327, 449, 2672, 13, 10204, 948, 1513, 2686, 785, 269, 2032, 13, 50672], "temperature": 0.0, "avg_logprob": -0.24897572488495798, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.005654456559568644}, {"id": 654, "seek": 310500, "start": 3113.16, "end": 3124.08, "text": " En cambio, si el dado est\u00e1 perfectamente calibrado, equilibrado, \u00bfs\u00ed? Mi entrop\u00eda es m\u00e1xima.", "tokens": [50772, 2193, 28731, 11, 1511, 806, 29568, 3192, 2176, 3439, 2104, 6414, 1573, 11, 1267, 388, 6414, 1573, 11, 3841, 82, 870, 30, 10204, 948, 1513, 2686, 785, 31031, 64, 13, 51318], "temperature": 0.0, "avg_logprob": -0.24897572488495798, "compression_ratio": 1.3288590604026846, "no_speech_prob": 0.005654456559568644}, {"id": 655, "seek": 312408, "start": 3125.08, "end": 3130.16, "text": " Es decir, \u00bfpor c\u00f3mo est\u00e1 definida la entrop\u00eda? No puedo tener", "tokens": [50414, 2313, 10235, 11, 3841, 2816, 12826, 3192, 1561, 2887, 635, 948, 1513, 2686, 30, 883, 21612, 11640, 50668], "temperature": 0.0, "avg_logprob": -0.21907604441923254, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.04332279786467552}, {"id": 656, "seek": 312408, "start": 3134.96, "end": 3140.92, "text": " entrop\u00eda m\u00e1s alta que cuando los eventos est\u00e1n equipobables. Entonces, justamente la entrop\u00eda,", "tokens": [50908, 948, 1513, 2686, 3573, 26495, 631, 7767, 1750, 2280, 329, 10368, 5037, 996, 2965, 13, 15097, 11, 41056, 635, 948, 1513, 2686, 11, 51206], "temperature": 0.0, "avg_logprob": -0.21907604441923254, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.04332279786467552}, {"id": 657, "seek": 312408, "start": 3140.92, "end": 3146.36, "text": " generalmente lo que uno mide con la entrop\u00eda es eso. \u00bfQu\u00e9 tan parecido son los resultados? \u00bfQu\u00e9", "tokens": [51206, 2674, 4082, 450, 631, 8526, 275, 482, 416, 635, 948, 1513, 2686, 785, 7287, 13, 3841, 15137, 7603, 7448, 17994, 1872, 1750, 36796, 30, 3841, 15137, 51478], "temperature": 0.0, "avg_logprob": -0.21907604441923254, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.04332279786467552}, {"id": 658, "seek": 312408, "start": 3146.36, "end": 3152.52, "text": " tan balanceados est\u00e1n de alguna forma? Cuanto m\u00e1s incertidumbre tengo, \u00bfpor qu\u00e9 tan m\u00e1s balanceados?", "tokens": [51478, 7603, 4772, 4181, 10368, 368, 20651, 8366, 30, 13205, 5857, 3573, 834, 911, 327, 449, 2672, 13989, 11, 3841, 2816, 8057, 7603, 3573, 4772, 4181, 30, 51786], "temperature": 0.0, "avg_logprob": -0.21907604441923254, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.04332279786467552}, {"id": 659, "seek": 315252, "start": 3152.52, "end": 3158.28, "text": " Si yo no tengo ni la menor idea de la palabra que sigue, mi entrop\u00eda es m\u00e1xima.", "tokens": [50364, 4909, 5290, 572, 13989, 3867, 635, 26343, 1558, 368, 635, 31702, 631, 34532, 11, 2752, 948, 1513, 2686, 785, 31031, 64, 13, 50652], "temperature": 0.0, "avg_logprob": -0.264462667353013, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.000998520408757031}, {"id": 660, "seek": 315252, "start": 3166.2, "end": 3173.04, "text": " Y adem\u00e1s tiene otra caracter\u00edstica que es que si el logaritmo es en base 2, este n\u00famero,", "tokens": [51048, 398, 21251, 7066, 13623, 34297, 2262, 631, 785, 631, 1511, 806, 41473, 270, 3280, 785, 465, 3096, 568, 11, 4065, 14959, 11, 51390], "temperature": 0.0, "avg_logprob": -0.264462667353013, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.000998520408757031}, {"id": 661, "seek": 315252, "start": 3175.12, "end": 3178.44, "text": " la entrop\u00eda me mide la cantidad de bits que yo necesito,", "tokens": [51494, 635, 948, 1513, 2686, 385, 275, 482, 635, 33757, 368, 9239, 631, 5290, 11909, 3528, 11, 51660], "temperature": 0.0, "avg_logprob": -0.264462667353013, "compression_ratio": 1.4259259259259258, "no_speech_prob": 0.000998520408757031}, {"id": 662, "seek": 317844, "start": 3179.44, "end": 3189.2400000000002, "text": " m\u00ednimo para transmitir los eventos. Esto es lo mejor forma de verlo con un ejemplo.", "tokens": [50414, 47393, 1690, 17831, 347, 1750, 2280, 329, 13, 20880, 785, 450, 11479, 8366, 368, 1306, 752, 416, 517, 13358, 13, 50904], "temperature": 0.0, "avg_logprob": -0.24349487601936637, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.03651953116059303}, {"id": 663, "seek": 317844, "start": 3191.04, "end": 3196.44, "text": " Supongamos, y es el ejemplo que aparece en el libro, supongamos que yo tengo ocho caballos.", "tokens": [50994, 9141, 556, 2151, 11, 288, 785, 806, 13358, 631, 37863, 465, 806, 29354, 11, 9331, 556, 2151, 631, 5290, 13989, 3795, 78, 5487, 336, 329, 13, 51264], "temperature": 0.0, "avg_logprob": -0.24349487601936637, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.03651953116059303}, {"id": 664, "seek": 317844, "start": 3198.04, "end": 3202.16, "text": " S\u00ed, tengo ocho caballos y quiero transmitir las apuestas que se est\u00e1n haciendo por un cable.", "tokens": [51344, 12375, 11, 13989, 3795, 78, 5487, 336, 329, 288, 16811, 17831, 347, 2439, 1882, 47794, 631, 369, 10368, 20509, 1515, 517, 8220, 13, 51550], "temperature": 0.0, "avg_logprob": -0.24349487601936637, "compression_ratio": 1.5485714285714285, "no_speech_prob": 0.03651953116059303}, {"id": 665, "seek": 320216, "start": 3202.16, "end": 3208.52, "text": " Entonces digo, bueno, una forma cantada de transmitirlo o directa de transmitir llamar al", "tokens": [50364, 15097, 22990, 11, 11974, 11, 2002, 8366, 11223, 1538, 368, 17831, 347, 752, 277, 2047, 64, 368, 17831, 347, 16848, 289, 419, 50682], "temperature": 0.0, "avg_logprob": -0.253040720204838, "compression_ratio": 1.3134328358208955, "no_speech_prob": 0.02672267146408558}, {"id": 666, "seek": 320216, "start": 3208.52, "end": 3227.72, "text": " primer caballo 001, 010, 011, 100, 101, 110, 111. \u00bfDe acuerdo? Ac\u00e1 yo uso ocho bits.", "tokens": [50682, 12595, 5487, 37104, 7143, 16, 11, 1958, 3279, 11, 1958, 5348, 11, 2319, 11, 21055, 11, 20154, 11, 2975, 16, 13, 3841, 11089, 28113, 30, 5097, 842, 5290, 22728, 3795, 78, 9239, 13, 51642], "temperature": 0.0, "avg_logprob": -0.253040720204838, "compression_ratio": 1.3134328358208955, "no_speech_prob": 0.02672267146408558}, {"id": 667, "seek": 322772, "start": 3228.04, "end": 3237.08, "text": " Cada vez que se apuesta por el caballo 01, yo pongo 001, blablabla. Entonces en total yo", "tokens": [50380, 38603, 5715, 631, 369, 1882, 25316, 1515, 806, 5487, 37104, 23185, 11, 5290, 280, 25729, 7143, 16, 11, 888, 32212, 455, 875, 13, 15097, 465, 3217, 5290, 50832], "temperature": 0.0, "avg_logprob": -0.23251914978027344, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.0011772515717893839}, {"id": 668, "seek": 322772, "start": 3237.08, "end": 3243.72, "text": " utilizo tres bits para transmitirlo por un cable. Tres bits por cada apuesta, \u00bfno? Ahora,", "tokens": [50832, 4976, 19055, 15890, 9239, 1690, 17831, 347, 752, 1515, 517, 8220, 13, 314, 495, 9239, 1515, 8411, 1882, 25316, 11, 3841, 1771, 30, 18840, 11, 51164], "temperature": 0.0, "avg_logprob": -0.23251914978027344, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.0011772515717893839}, {"id": 669, "seek": 322772, "start": 3243.72, "end": 3251.8799999999997, "text": " cuando nosotros vemos las apuestas descubrimos que la mitad de las veces se apuesta por el caballo 1.", "tokens": [51164, 7767, 13863, 20909, 2439, 1882, 47794, 32592, 5565, 329, 631, 635, 46895, 368, 2439, 17054, 369, 1882, 25316, 1515, 806, 5487, 37104, 502, 13, 51572], "temperature": 0.0, "avg_logprob": -0.23251914978027344, "compression_ratio": 1.5875706214689265, "no_speech_prob": 0.0011772515717893839}, {"id": 670, "seek": 325188, "start": 3252.88, "end": 3260.2400000000002, "text": " Un cuarto del caballo 02, un tercio, blablabla. Un octavo del caballo 03, un 16ado del caballo 04,", "tokens": [50414, 1156, 48368, 1103, 5487, 37104, 37202, 11, 517, 1796, 8529, 11, 888, 32212, 455, 875, 13, 1156, 13350, 25713, 1103, 5487, 37104, 43677, 11, 517, 3165, 1573, 1103, 5487, 37104, 50022, 11, 50782], "temperature": 0.0, "avg_logprob": -0.20695736097252887, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.022990064695477486}, {"id": 671, "seek": 325188, "start": 3260.2400000000002, "end": 3267.6800000000003, "text": " y todos estos se apuestan mucho menos. Teniendo en cuenta eso, yo lo que trato de hacer ahora es decir,", "tokens": [50782, 288, 6321, 12585, 369, 1882, 11493, 282, 9824, 8902, 13, 9380, 7304, 465, 17868, 7287, 11, 5290, 450, 631, 504, 2513, 368, 6720, 9923, 785, 10235, 11, 51154], "temperature": 0.0, "avg_logprob": -0.20695736097252887, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.022990064695477486}, {"id": 672, "seek": 325188, "start": 3267.6800000000003, "end": 3277.12, "text": " bueno, quiero proponer una codificaci\u00f3n mejor que hace que yo, los caballos que se apuestan m\u00e1s,", "tokens": [51154, 11974, 11, 16811, 2365, 32949, 2002, 17656, 40802, 11479, 631, 10032, 631, 5290, 11, 1750, 5487, 336, 329, 631, 369, 1882, 11493, 282, 3573, 11, 51626], "temperature": 0.0, "avg_logprob": -0.20695736097252887, "compression_ratio": 1.5759162303664922, "no_speech_prob": 0.022990064695477486}, {"id": 673, "seek": 327712, "start": 3277.12, "end": 3284.4, "text": " o sea que tengo que transmitir m\u00e1s seguido, los codifico con menos bits. \u00bfDe acuerdo? La", "tokens": [50364, 277, 4158, 631, 13989, 631, 17831, 347, 3573, 8878, 2925, 11, 1750, 17656, 1089, 78, 416, 8902, 9239, 13, 3841, 11089, 28113, 30, 2369, 50728], "temperature": 0.0, "avg_logprob": -0.1978900295564498, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.04616130515933037}, {"id": 674, "seek": 327712, "start": 3284.4, "end": 3290.72, "text": " mitad de los bits, el primer bit, lo utilizo solo para el caballo 01. Es decir, que si es un 0,", "tokens": [50728, 46895, 368, 1750, 9239, 11, 806, 12595, 857, 11, 450, 4976, 19055, 6944, 1690, 806, 5487, 37104, 23185, 13, 2313, 10235, 11, 631, 1511, 785, 517, 1958, 11, 51044], "temperature": 0.0, "avg_logprob": -0.1978900295564498, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.04616130515933037}, {"id": 675, "seek": 327712, "start": 3292.0, "end": 3303.8399999999997, "text": " es que transmitir el caballo 01 necesita un solo bit. Si es un 01, si es un 01 y un 0 despu\u00e9s,", "tokens": [51108, 785, 631, 17831, 347, 806, 5487, 37104, 23185, 45485, 517, 6944, 857, 13, 4909, 785, 517, 23185, 11, 1511, 785, 517, 23185, 288, 517, 1958, 15283, 11, 51700], "temperature": 0.0, "avg_logprob": -0.1978900295564498, "compression_ratio": 1.5243243243243243, "no_speech_prob": 0.04616130515933037}, {"id": 676, "seek": 330384, "start": 3303.84, "end": 3312.1200000000003, "text": " es el caballo 02. Si son 01 y un 0, despu\u00e9s es el caballo 03. Si son 01 y un 0, f\u00edjense que yo", "tokens": [50364, 785, 806, 5487, 37104, 37202, 13, 4909, 1872, 23185, 288, 517, 1958, 11, 15283, 785, 806, 5487, 37104, 43677, 13, 4909, 1872, 23185, 288, 517, 1958, 11, 283, 870, 73, 1288, 631, 5290, 50778], "temperature": 0.0, "avg_logprob": -0.154746675491333, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.026661062613129616}, {"id": 677, "seek": 330384, "start": 3312.1200000000003, "end": 3322.04, "text": " para transmitir estos caballos utilizo 1, 2, 3, 4, 5, 6 bits. Utilizo m\u00e1s bits. Pero como son", "tokens": [50778, 1690, 17831, 347, 12585, 5487, 336, 329, 4976, 19055, 502, 11, 568, 11, 805, 11, 1017, 11, 1025, 11, 1386, 9239, 13, 12555, 388, 19055, 3573, 9239, 13, 9377, 2617, 1872, 51274], "temperature": 0.0, "avg_logprob": -0.154746675491333, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.026661062613129616}, {"id": 678, "seek": 330384, "start": 3322.04, "end": 3330.2400000000002, "text": " mucho menos probables, mi entrop\u00eda me da 2 bits. O sea, el promedio de bits que yo utilizo seg\u00fan", "tokens": [51274, 9824, 8902, 1239, 2965, 11, 2752, 948, 1513, 2686, 385, 1120, 568, 9239, 13, 422, 4158, 11, 806, 2234, 292, 1004, 368, 9239, 631, 5290, 4976, 19055, 36570, 51684], "temperature": 0.0, "avg_logprob": -0.154746675491333, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.026661062613129616}, {"id": 679, "seek": 333024, "start": 3330.24, "end": 3341.04, "text": " la distribuci\u00f3n es 2 bits, que es m\u00e1s baja que los 3 bit originales. \u00bfSe entiende? Incorporando", "tokens": [50364, 635, 4400, 30813, 785, 568, 9239, 11, 631, 785, 3573, 49427, 631, 1750, 805, 857, 3380, 279, 13, 3841, 10637, 948, 45816, 30, 39120, 2816, 1806, 50904], "temperature": 0.0, "avg_logprob": -0.17903115068163192, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.049062859266996384}, {"id": 680, "seek": 333024, "start": 3341.04, "end": 3348.16, "text": " la informaci\u00f3n de la distribuci\u00f3n bajo. Podemos mejorar eso. No, no podemos mejorar eso. Nunca", "tokens": [50904, 635, 21660, 368, 635, 4400, 30813, 30139, 13, 12646, 4485, 48858, 7287, 13, 883, 11, 572, 12234, 48858, 7287, 13, 23696, 496, 51260], "temperature": 0.0, "avg_logprob": -0.17903115068163192, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.049062859266996384}, {"id": 681, "seek": 333024, "start": 3348.16, "end": 3352.16, "text": " vamos a, la entrop\u00eda lo que nos dice es eso. Nunca vas a encontrar una, porque justamente la", "tokens": [51260, 5295, 257, 11, 635, 948, 1513, 2686, 450, 631, 3269, 10313, 785, 7287, 13, 23696, 496, 11481, 257, 17525, 2002, 11, 4021, 41056, 635, 51460], "temperature": 0.0, "avg_logprob": -0.17903115068163192, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.049062859266996384}, {"id": 682, "seek": 333024, "start": 3352.16, "end": 3357.4799999999996, "text": " entrop\u00eda es 2. Como la entrop\u00eda es 2, la entrop\u00eda me da una cota inferior sobre cu\u00e1nto puedo llegar.", "tokens": [51460, 948, 1513, 2686, 785, 568, 13, 11913, 635, 948, 1513, 2686, 785, 568, 11, 635, 948, 1513, 2686, 385, 1120, 2002, 269, 5377, 24249, 5473, 44256, 78, 21612, 24892, 13, 51726], "temperature": 0.0, "avg_logprob": -0.17903115068163192, "compression_ratio": 1.7280701754385965, "no_speech_prob": 0.049062859266996384}, {"id": 683, "seek": 335748, "start": 3357.96, "end": 3362.4, "text": " Con menos de 2 bits no puedo. \u00bfDe acuerdo?", "tokens": [50388, 2656, 8902, 368, 568, 9239, 572, 21612, 13, 3841, 11089, 28113, 30, 50610], "temperature": 0.0, "avg_logprob": -0.2832626664494894, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.013971651904284954}, {"id": 684, "seek": 335748, "start": 3364.96, "end": 3366.92, "text": " Entonces se preguntar\u00e1n para qu\u00e9 sirve esto.", "tokens": [50738, 15097, 369, 19860, 289, 7200, 1690, 8057, 4735, 303, 7433, 13, 50836], "temperature": 0.0, "avg_logprob": -0.2832626664494894, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.013971651904284954}, {"id": 685, "seek": 335748, "start": 3371.2, "end": 3375.56, "text": " De hecho no, la entrop\u00eda es una cota de lo que dec\u00eda, una cota m\u00ednima para el n\u00famero de bits", "tokens": [51050, 1346, 13064, 572, 11, 635, 948, 1513, 2686, 785, 2002, 269, 5377, 368, 450, 631, 37599, 11, 2002, 269, 5377, 33656, 4775, 1690, 806, 14959, 368, 9239, 51268], "temperature": 0.0, "avg_logprob": -0.2832626664494894, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.013971651904284954}, {"id": 686, "seek": 335748, "start": 3375.56, "end": 3382.12, "text": " necesarios. A partir de la entrop\u00eda yo puedo calcular la entrop\u00eda de una secuencia.", "tokens": [51268, 11909, 9720, 13, 316, 13906, 368, 635, 948, 1513, 2686, 5290, 21612, 2104, 17792, 635, 948, 1513, 2686, 368, 2002, 907, 47377, 13, 51596], "temperature": 0.0, "avg_logprob": -0.2832626664494894, "compression_ratio": 1.4918032786885247, "no_speech_prob": 0.013971651904284954}, {"id": 687, "seek": 338212, "start": 3382.12, "end": 3393.3199999999997, "text": " La entrop\u00eda de una secuencia es de todas las combinaciones posibles,", "tokens": [50364, 2369, 948, 1513, 2686, 368, 2002, 907, 47377, 785, 368, 10906, 2439, 38514, 9188, 1366, 14428, 11, 50924], "temperature": 0.0, "avg_logprob": -0.2118599923808923, "compression_ratio": 1.8907103825136613, "no_speech_prob": 0.0028420360758900642}, {"id": 688, "seek": 338212, "start": 3395.3599999999997, "end": 3399.72, "text": " de una secuencia la probabilidad de esa combinaci\u00f3n es lo mismo para aplicado a secuencia.", "tokens": [51026, 368, 2002, 907, 47377, 635, 31959, 4580, 368, 11342, 38514, 3482, 785, 450, 12461, 1690, 18221, 1573, 257, 907, 47377, 13, 51244], "temperature": 0.0, "avg_logprob": -0.2118599923808923, "compression_ratio": 1.8907103825136613, "no_speech_prob": 0.0028420360758900642}, {"id": 689, "seek": 338212, "start": 3399.72, "end": 3404.2799999999997, "text": " Este si lo ven es un n\u00famero muy complicado porque es la sumatoria de una cantidad impresionante del", "tokens": [51244, 16105, 1511, 450, 6138, 785, 517, 14959, 5323, 49850, 4021, 785, 635, 2408, 1639, 654, 368, 2002, 33757, 35672, 313, 2879, 1103, 51472], "temperature": 0.0, "avg_logprob": -0.2118599923808923, "compression_ratio": 1.8907103825136613, "no_speech_prob": 0.0028420360758900642}, {"id": 690, "seek": 338212, "start": 3404.2799999999997, "end": 3410.56, "text": " n\u00famero, porque son todas las combinaciones posibles de secuencia. Eso es lo que me", "tokens": [51472, 14959, 11, 4021, 1872, 10906, 2439, 38514, 9188, 1366, 14428, 368, 907, 47377, 13, 27795, 785, 450, 631, 385, 51786], "temperature": 0.0, "avg_logprob": -0.2118599923808923, "compression_ratio": 1.8907103825136613, "no_speech_prob": 0.0028420360758900642}, {"id": 691, "seek": 341056, "start": 3410.56, "end": 3418.56, "text": " dice es la entrop\u00eda de la secuencia. \u00bfQu\u00e9 tanta incertidumbre hay en una secuencia?", "tokens": [50364, 10313, 785, 635, 948, 1513, 2686, 368, 635, 907, 47377, 13, 3841, 15137, 40864, 834, 911, 327, 449, 2672, 4842, 465, 2002, 907, 47377, 30, 50764], "temperature": 0.0, "avg_logprob": -0.26090276652369004, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.006545554380863905}, {"id": 692, "seek": 341056, "start": 3427.92, "end": 3438.52, "text": " Y la tasa entrop\u00eda ser\u00eda eso dividido de n, es decir el promedio, porque si no la secuencia", "tokens": [51232, 398, 635, 8023, 64, 948, 1513, 2686, 23679, 7287, 4996, 2925, 368, 297, 11, 785, 10235, 806, 2234, 292, 1004, 11, 4021, 1511, 572, 635, 907, 47377, 51762], "temperature": 0.0, "avg_logprob": -0.26090276652369004, "compression_ratio": 1.3953488372093024, "no_speech_prob": 0.006545554380863905}, {"id": 693, "seek": 343852, "start": 3438.52, "end": 3444.32, "text": " m\u00e1s larga o no la entrop\u00edamos antes. El promedio por palabra de la entrop\u00eda.", "tokens": [50364, 3573, 1613, 3680, 277, 572, 635, 948, 1513, 16275, 11014, 13, 2699, 2234, 292, 1004, 1515, 31702, 368, 635, 948, 1513, 2686, 13, 50654], "temperature": 0.0, "avg_logprob": -0.2883087566920689, "compression_ratio": 1.3923076923076922, "no_speech_prob": 0.003284008940681815}, {"id": 694, "seek": 343852, "start": 3449.84, "end": 3462.92, "text": " Entonces, la entrop\u00eda de un lenguaje que ser\u00eda como la medida de qu\u00e9 tanta incertidumbre hay en un", "tokens": [50930, 15097, 11, 635, 948, 1513, 2686, 368, 517, 35044, 84, 11153, 631, 23679, 2617, 635, 32984, 368, 8057, 40864, 834, 911, 327, 449, 2672, 4842, 465, 517, 51584], "temperature": 0.0, "avg_logprob": -0.2883087566920689, "compression_ratio": 1.3923076923076922, "no_speech_prob": 0.003284008940681815}, {"id": 695, "seek": 346292, "start": 3462.92, "end": 3472.84, "text": " lenguaje. \u00bfQu\u00e9 tanto puedo yo llegar a predecir lo que va a seguir diciendo el lenguaje?", "tokens": [50364, 35044, 84, 11153, 13, 3841, 15137, 10331, 21612, 5290, 24892, 257, 24874, 23568, 450, 631, 2773, 257, 18584, 42797, 806, 35044, 84, 11153, 30, 50860], "temperature": 0.0, "avg_logprob": -0.2659942115225443, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.024054113775491714}, {"id": 696, "seek": 346292, "start": 3472.84, "end": 3478.2000000000003, "text": " Ese al l\u00edmite, pero como valor\u00f3, no en un contexto en general en el lenguaje,", "tokens": [50860, 462, 405, 419, 287, 14569, 642, 11, 4768, 2617, 15367, 812, 11, 572, 465, 517, 47685, 465, 2674, 465, 806, 35044, 84, 11153, 11, 51128], "temperature": 0.0, "avg_logprob": -0.2659942115225443, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.024054113775491714}, {"id": 697, "seek": 346292, "start": 3478.2000000000003, "end": 3485.36, "text": " es una medida para el lenguaje. Ese al l\u00edmite cuando la secuencia tiene infinito de la tasa", "tokens": [51128, 785, 2002, 32984, 1690, 806, 35044, 84, 11153, 13, 462, 405, 419, 287, 14569, 642, 7767, 635, 907, 47377, 7066, 7193, 3528, 368, 635, 8023, 64, 51486], "temperature": 0.0, "avg_logprob": -0.2659942115225443, "compression_ratio": 1.4943181818181819, "no_speech_prob": 0.024054113775491714}, {"id": 698, "seek": 348536, "start": 3485.36, "end": 3488.1200000000003, "text": " entrop\u00eda.", "tokens": [50364, 948, 1513, 2686, 13, 50502], "temperature": 0.0, "avg_logprob": -0.3589209739607994, "compression_ratio": 1.5368421052631578, "no_speech_prob": 0.02268926426768303}, {"id": 699, "seek": 348536, "start": 3498.0, "end": 3502.4, "text": " Y que s\u00e9 que ac\u00e1 es la suma, como dec\u00edamos, es la suma de todas las secuencias posibles.", "tokens": [50996, 398, 631, 7910, 631, 23496, 785, 635, 2408, 64, 11, 2617, 979, 16275, 11, 785, 635, 2408, 64, 368, 10906, 2439, 907, 7801, 12046, 1366, 14428, 13, 51216], "temperature": 0.0, "avg_logprob": -0.3589209739607994, "compression_ratio": 1.5368421052631578, "no_speech_prob": 0.02268926426768303}, {"id": 700, "seek": 348536, "start": 3502.4, "end": 3508.36, "text": " Es decir, que es una cosa imposible, calcular. Pero hay un teorema que es el de Llano Muam\u00ed", "tokens": [51216, 2313, 10235, 11, 631, 785, 2002, 10163, 38396, 964, 11, 2104, 17792, 13, 9377, 4842, 517, 535, 418, 1696, 631, 785, 806, 368, 32717, 3730, 15601, 335, 870, 51514], "temperature": 0.0, "avg_logprob": -0.3589209739607994, "compression_ratio": 1.5368421052631578, "no_speech_prob": 0.02268926426768303}, {"id": 701, "seek": 348536, "start": 3508.36, "end": 3514.6400000000003, "text": " Lambrayman que dice que el lenguaje es estacionario y erg\u00f3dico. Estacionario y erg\u00f3dico quiere", "tokens": [51514, 18825, 1443, 320, 1601, 631, 10313, 631, 806, 35044, 84, 11153, 785, 871, 18803, 4912, 288, 1189, 70, 17081, 2789, 13, 4410, 18803, 4912, 288, 1189, 70, 17081, 2789, 23877, 51828], "temperature": 0.0, "avg_logprob": -0.3589209739607994, "compression_ratio": 1.5368421052631578, "no_speech_prob": 0.02268926426768303}, {"id": 702, "seek": 351464, "start": 3514.64, "end": 3521.8799999999997, "text": " decir que no importa d\u00f3nde yo est\u00e9 parado en una secuencia, todas las posiciones, las probabilidades", "tokens": [50364, 10235, 631, 572, 33218, 34264, 5290, 34584, 971, 1573, 465, 2002, 907, 47377, 11, 10906, 2439, 1366, 29719, 11, 2439, 31959, 10284, 50726], "temperature": 0.0, "avg_logprob": -0.23119607338538536, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0067977444268763065}, {"id": 703, "seek": 351464, "start": 3521.8799999999997, "end": 3528.24, "text": " son las mismas de continuidad. Lo cual no es as\u00ed en el lenguaje, porque lo que yo digo ahora", "tokens": [50726, 1872, 2439, 23220, 296, 368, 2993, 4580, 13, 6130, 10911, 572, 785, 8582, 465, 806, 35044, 84, 11153, 11, 4021, 450, 631, 5290, 22990, 9923, 51044], "temperature": 0.0, "avg_logprob": -0.23119607338538536, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0067977444268763065}, {"id": 704, "seek": 351464, "start": 3528.24, "end": 3533.68, "text": " incide dentro de lo que estoy diciendo entre un minuto m\u00e1s. No, no es aleatorio, digamos. Pero", "tokens": [51044, 834, 482, 10856, 368, 450, 631, 15796, 42797, 3962, 517, 923, 8262, 3573, 13, 883, 11, 572, 785, 6775, 48028, 11, 36430, 13, 9377, 51316], "temperature": 0.0, "avg_logprob": -0.23119607338538536, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0067977444268763065}, {"id": 705, "seek": 351464, "start": 3533.68, "end": 3540.52, "text": " suponiendo eso es una simplificaci\u00f3n, lo que me permite es simplemente para calcular la entrop\u00eda,", "tokens": [51316, 9331, 266, 7304, 7287, 785, 2002, 6883, 40802, 11, 450, 631, 385, 31105, 785, 33190, 1690, 2104, 17792, 635, 948, 1513, 2686, 11, 51658], "temperature": 0.0, "avg_logprob": -0.23119607338538536, "compression_ratio": 1.5934959349593496, "no_speech_prob": 0.0067977444268763065}, {"id": 706, "seek": 354052, "start": 3541.52, "end": 3548.08, "text": " la tasa de entrop\u00eda del lenguaje es simplemente uno sobre n dividido en logaritmo. F\u00edjense que", "tokens": [50414, 635, 8023, 64, 368, 948, 1513, 2686, 1103, 35044, 84, 11153, 785, 33190, 8526, 5473, 297, 4996, 2925, 465, 41473, 270, 3280, 13, 479, 870, 73, 1288, 631, 50742], "temperature": 0.0, "avg_logprob": -0.17101737780448717, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.017510823905467987}, {"id": 707, "seek": 354052, "start": 3548.08, "end": 3552.96, "text": " perd\u00ed las probabilidades de cada una de las de la secuencia. Es como que si yo tomo una secuencia", "tokens": [50742, 12611, 870, 2439, 31959, 10284, 368, 8411, 2002, 368, 2439, 368, 635, 907, 47377, 13, 2313, 2617, 631, 1511, 5290, 2916, 78, 2002, 907, 47377, 50986], "temperature": 0.0, "avg_logprob": -0.17101737780448717, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.017510823905467987}, {"id": 708, "seek": 354052, "start": 3552.96, "end": 3561.04, "text": " suficientemente larga del lenguaje, voy a incluir a todas las subsecuencias. O sea que si yo una", "tokens": [50986, 459, 1786, 1196, 16288, 1613, 3680, 1103, 35044, 84, 11153, 11, 7552, 257, 25520, 347, 257, 10906, 2439, 1422, 8159, 7801, 12046, 13, 422, 4158, 631, 1511, 5290, 2002, 51390], "temperature": 0.0, "avg_logprob": -0.17101737780448717, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.017510823905467987}, {"id": 709, "seek": 354052, "start": 3561.04, "end": 3566.16, "text": " secuencia suficientemente larga, puede ser el cuerpo de evaluaci\u00f3n. Yo puedo calcular la entrop\u00eda", "tokens": [51390, 907, 47377, 459, 1786, 1196, 16288, 1613, 3680, 11, 8919, 816, 806, 20264, 368, 6133, 3482, 13, 7616, 21612, 2104, 17792, 635, 948, 1513, 2686, 51646], "temperature": 0.0, "avg_logprob": -0.17101737780448717, "compression_ratio": 1.7422222222222221, "no_speech_prob": 0.017510823905467987}, {"id": 710, "seek": 356616, "start": 3566.16, "end": 3583.3199999999997, "text": " sobre el cuerpo de evaluaci\u00f3n. Entonces esto es un n\u00famero, hasta ahora lo que dije ac\u00e1 es un", "tokens": [50364, 5473, 806, 20264, 368, 6133, 3482, 13, 15097, 7433, 785, 517, 14959, 11, 10764, 9923, 450, 631, 39414, 23496, 785, 517, 51222], "temperature": 0.0, "avg_logprob": -0.21483516693115234, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.03273877128958702}, {"id": 711, "seek": 356616, "start": 3583.3199999999997, "end": 3590.2, "text": " n\u00famero, no sabemos por qu\u00e9 tengo esto. Pero f\u00edjense que si yo puedo calcular lo que se llama la", "tokens": [51222, 14959, 11, 572, 27200, 1515, 8057, 13989, 7433, 13, 9377, 283, 870, 73, 1288, 631, 1511, 5290, 21612, 2104, 17792, 450, 631, 369, 23272, 635, 51566], "temperature": 0.0, "avg_logprob": -0.21483516693115234, "compression_ratio": 1.3379310344827586, "no_speech_prob": 0.03273877128958702}, {"id": 712, "seek": 359020, "start": 3590.2, "end": 3597.72, "text": " entrop\u00eda cruzada, porque yo que tengo, yo tengo un lenguaje que genera las palabras con una cierta", "tokens": [50364, 948, 1513, 2686, 5140, 89, 1538, 11, 4021, 5290, 631, 13989, 11, 5290, 13989, 517, 35044, 84, 11153, 631, 1337, 64, 2439, 35240, 416, 2002, 39769, 1328, 50740], "temperature": 0.0, "avg_logprob": -0.19417938819298378, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.07375507801771164}, {"id": 713, "seek": 359020, "start": 3597.72, "end": 3603.72, "text": " distribuci\u00f3n de probabilidad, que es lo que queremos averiguar, que es tan lo que es lo que es", "tokens": [50740, 4400, 30813, 368, 31959, 4580, 11, 631, 785, 450, 631, 26813, 18247, 16397, 289, 11, 631, 785, 7603, 450, 631, 785, 450, 631, 785, 51040], "temperature": 0.0, "avg_logprob": -0.19417938819298378, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.07375507801771164}, {"id": 714, "seek": 359020, "start": 3603.72, "end": 3609.08, "text": " nuestro problema original, c\u00f3mo da las palabras anteriores y genera la siguiente. Eso es algo", "tokens": [51040, 14726, 12395, 3380, 11, 12826, 1120, 2439, 35240, 364, 34345, 2706, 288, 1337, 64, 635, 25666, 13, 27795, 785, 8655, 51308], "temperature": 0.0, "avg_logprob": -0.19417938819298378, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.07375507801771164}, {"id": 715, "seek": 359020, "start": 3609.08, "end": 3613.72, "text": " que he desconocido, no sabemos c\u00f3mo es, porque es el del lenguaje espa\u00f1ol el que yo quiero", "tokens": [51308, 631, 415, 49801, 905, 2925, 11, 572, 27200, 12826, 785, 11, 4021, 785, 806, 1103, 35044, 84, 11153, 31177, 806, 631, 5290, 16811, 51540], "temperature": 0.0, "avg_logprob": -0.19417938819298378, "compression_ratio": 1.7488584474885844, "no_speech_prob": 0.07375507801771164}, {"id": 716, "seek": 361372, "start": 3614.04, "end": 3621.8799999999997, "text": " pero yo tengo un modelo M, que es el modelo de negramas. La entrop\u00eda cruzada lo que dice es bueno", "tokens": [50380, 4768, 5290, 13989, 517, 27825, 376, 11, 631, 785, 806, 27825, 368, 408, 861, 19473, 13, 2369, 948, 1513, 2686, 5140, 89, 1538, 450, 631, 10313, 785, 11974, 50772], "temperature": 0.0, "avg_logprob": -0.225653467299063, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.13543835282325745}, {"id": 717, "seek": 361372, "start": 3621.8799999999997, "end": 3632.9599999999996, "text": " calculamos esta H utilizando la probabilidad original por el logaritmo de la probabilidad", "tokens": [50772, 4322, 2151, 5283, 389, 19906, 1806, 635, 31959, 4580, 3380, 1515, 806, 41473, 270, 3280, 368, 635, 31959, 4580, 51326], "temperature": 0.0, "avg_logprob": -0.225653467299063, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.13543835282325745}, {"id": 718, "seek": 361372, "start": 3632.9599999999996, "end": 3638.7999999999997, "text": " asignada por el modelo. La probabilidad de la secuencia es la que ten\u00eda el lenguaje general,", "tokens": [51326, 382, 788, 1538, 1515, 806, 27825, 13, 2369, 31959, 4580, 368, 635, 907, 47377, 785, 635, 631, 23718, 806, 35044, 84, 11153, 2674, 11, 51618], "temperature": 0.0, "avg_logprob": -0.225653467299063, "compression_ratio": 1.5932203389830508, "no_speech_prob": 0.13543835282325745}, {"id": 719, "seek": 363880, "start": 3638.8, "end": 3646.96, "text": " que no la conozco, y el logaritmo s\u00ed, o sea esa distancia, esa largo env\u00edcese del modelo.", "tokens": [50364, 631, 572, 635, 416, 15151, 1291, 11, 288, 806, 41473, 270, 3280, 8600, 11, 277, 4158, 11342, 1483, 22862, 11, 11342, 31245, 2267, 870, 887, 68, 1103, 27825, 13, 50772], "temperature": 0.0, "avg_logprob": -0.3845081115036868, "compression_ratio": 1.4375, "no_speech_prob": 0.0809158906340599}, {"id": 720, "seek": 363880, "start": 3648.5600000000004, "end": 3653.2400000000002, "text": " Seg\u00fan el teorema otra vez, ya no manmilan, yo puedo sacar esta probabilidad simplific\u00e1ndola,", "tokens": [50852, 21595, 9453, 806, 535, 418, 1696, 13623, 5715, 11, 2478, 572, 587, 28674, 282, 11, 5290, 21612, 43823, 5283, 31959, 4580, 6883, 1089, 18606, 4711, 11, 51086], "temperature": 0.0, "avg_logprob": -0.3845081115036868, "compression_ratio": 1.4375, "no_speech_prob": 0.0809158906340599}, {"id": 721, "seek": 363880, "start": 3653.2400000000002, "end": 3662.2000000000003, "text": " suponiendo que es ergodico, y digo bueno, la entrop\u00eda cruzada depende solo del logaritmo", "tokens": [51086, 9331, 266, 7304, 631, 785, 1189, 21787, 2789, 11, 288, 22990, 11974, 11, 635, 948, 1513, 2686, 5140, 89, 1538, 47091, 6944, 1103, 41473, 270, 3280, 51534], "temperature": 0.0, "avg_logprob": -0.3845081115036868, "compression_ratio": 1.4375, "no_speech_prob": 0.0809158906340599}, {"id": 722, "seek": 366220, "start": 3662.2, "end": 3673.8799999999997, "text": " de la probabilidad asignada por el modelo. Y esto es lo interesante, cualquier entrop\u00eda cruzada", "tokens": [50364, 368, 635, 31959, 4580, 382, 788, 1538, 1515, 806, 27825, 13, 398, 7433, 785, 450, 36396, 11, 21004, 948, 1513, 2686, 5140, 89, 1538, 50948], "temperature": 0.0, "avg_logprob": -0.19888087185946376, "compression_ratio": 1.3897058823529411, "no_speech_prob": 0.052294373512268066}, {"id": 723, "seek": 366220, "start": 3673.8799999999997, "end": 3680.48, "text": " que yo obtenga, que yo calcule con un modelo, va a ser mayor necesariamente que la entrop\u00eda", "tokens": [50948, 631, 5290, 28326, 3680, 11, 631, 5290, 2104, 66, 2271, 416, 517, 27825, 11, 2773, 257, 816, 10120, 11909, 45149, 631, 635, 948, 1513, 2686, 51278], "temperature": 0.0, "avg_logprob": -0.19888087185946376, "compression_ratio": 1.3897058823529411, "no_speech_prob": 0.052294373512268066}, {"id": 724, "seek": 368048, "start": 3680.64, "end": 3693.12, "text": " d\u00e9 lenguaje. Cualquier modelo va a asignarme una entrop\u00eda mayor a la de lenguaje, esto es la cota inferior.", "tokens": [50372, 2795, 35044, 84, 11153, 13, 383, 901, 16622, 27825, 2773, 257, 382, 788, 35890, 2002, 948, 1513, 2686, 10120, 257, 635, 368, 35044, 84, 11153, 11, 7433, 785, 635, 269, 5377, 24249, 13, 50996], "temperature": 0.0, "avg_logprob": -0.35551416551744613, "compression_ratio": 1.101010101010101, "no_speech_prob": 0.3738986849784851}, {"id": 725, "seek": 369312, "start": 3693.12, "end": 3718.08, "text": " Entonces f\u00edjense que como son todas mayores, cuanto m\u00e1s parecido sea mi modelo, al modelo", "tokens": [50364, 15097, 283, 870, 73, 1288, 631, 2617, 1872, 10906, 815, 2706, 11, 36685, 3573, 7448, 17994, 4158, 2752, 27825, 11, 419, 27825, 51612], "temperature": 0.0, "avg_logprob": -0.26465525993934047, "compression_ratio": 1.058139534883721, "no_speech_prob": 0.06724435091018677}, {"id": 726, "seek": 371808, "start": 3718.08, "end": 3723.24, "text": " del lenguaje, cuanto m\u00e1s aparecido, asigne probabilidad m\u00e1s parecida de las de ac\u00e1,", "tokens": [50364, 1103, 35044, 84, 11153, 11, 36685, 3573, 15004, 17994, 11, 382, 26341, 31959, 4580, 3573, 7448, 37200, 368, 2439, 368, 23496, 11, 50622], "temperature": 0.0, "avg_logprob": -0.2573118209838867, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.5108886957168579}, {"id": 727, "seek": 371808, "start": 3723.24, "end": 3734.44, "text": " por como est\u00e1 definido, va a ser mejor. Entonces, cuanto menor sea la entrop\u00eda cruzada de mi modelo,", "tokens": [50622, 1515, 2617, 3192, 1561, 2925, 11, 2773, 257, 816, 11479, 13, 15097, 11, 36685, 26343, 4158, 635, 948, 1513, 2686, 5140, 89, 1538, 368, 2752, 27825, 11, 51182], "temperature": 0.0, "avg_logprob": -0.2573118209838867, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.5108886957168579}, {"id": 728, "seek": 371808, "start": 3734.44, "end": 3739.04, "text": " evaluado sobre una secuencia suficientemente larga, es decir, sobre el corpo de evaluaci\u00f3n,", "tokens": [51182, 6133, 1573, 5473, 2002, 907, 47377, 459, 1786, 1196, 16288, 1613, 3680, 11, 785, 10235, 11, 5473, 806, 23257, 368, 6133, 3482, 11, 51412], "temperature": 0.0, "avg_logprob": -0.2573118209838867, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.5108886957168579}, {"id": 729, "seek": 371808, "start": 3739.04, "end": 3746.56, "text": " mejor va a ser mi aproximaci\u00f3n. Y justamente, la medida de esa intr\u00ednseca que est\u00e1bamos buscando", "tokens": [51412, 11479, 2773, 257, 816, 2752, 31270, 3482, 13, 398, 41056, 11, 635, 32984, 368, 11342, 17467, 10973, 405, 496, 631, 3192, 65, 2151, 46804, 51788], "temperature": 0.0, "avg_logprob": -0.2573118209838867, "compression_ratio": 1.598326359832636, "no_speech_prob": 0.5108886957168579}, {"id": 730, "seek": 374656, "start": 3746.56, "end": 3761.44, "text": " era esto, que es dos, \u00bfpor qu\u00e9 es dos? No lo s\u00e9, porque es lo mismo, es para sacarlos", "tokens": [50364, 4249, 7433, 11, 631, 785, 4491, 11, 3841, 2816, 8057, 785, 4491, 30, 883, 450, 7910, 11, 4021, 785, 450, 12461, 11, 785, 1690, 4899, 39734, 51108], "temperature": 0.0, "avg_logprob": -0.2326817890954396, "compression_ratio": 1.3357142857142856, "no_speech_prob": 0.03330569714307785}, {"id": 731, "seek": 374656, "start": 3761.44, "end": 3769.0, "text": " logaritmos nada m\u00e1s, es dos a la entrop\u00eda cruzada, a este valor, y esto se llama perplejidad. La", "tokens": [51108, 41473, 270, 3415, 8096, 3573, 11, 785, 4491, 257, 635, 948, 1513, 2686, 5140, 89, 1538, 11, 257, 4065, 15367, 11, 288, 7433, 369, 23272, 680, 781, 73, 4580, 13, 2369, 51486], "temperature": 0.0, "avg_logprob": -0.2326817890954396, "compression_ratio": 1.3357142857142856, "no_speech_prob": 0.03330569714307785}, {"id": 732, "seek": 376900, "start": 3769.0, "end": 3786.88, "text": " perplejidad es lo que mide qu\u00e9 tan bueno es intr\u00ednseamente mi modelo sobre mi cuerpo de", "tokens": [50364, 680, 781, 73, 4580, 785, 450, 631, 275, 482, 8057, 7603, 11974, 785, 17467, 10973, 405, 3439, 2752, 27825, 5473, 2752, 20264, 368, 51258], "temperature": 0.0, "avg_logprob": -0.19416416815991672, "compression_ratio": 1.4015151515151516, "no_speech_prob": 0.014469150453805923}, {"id": 733, "seek": 376900, "start": 3786.88, "end": 3792.0, "text": " entrenamiento, sobre mi cuerpo de evaluaci\u00f3n. Es decir, si yo tengo dos modelos, el que asigne", "tokens": [51258, 45069, 16971, 11, 5473, 2752, 20264, 368, 6133, 3482, 13, 2313, 10235, 11, 1511, 5290, 13989, 4491, 2316, 329, 11, 806, 631, 382, 26341, 51514], "temperature": 0.0, "avg_logprob": -0.19416416815991672, "compression_ratio": 1.4015151515151516, "no_speech_prob": 0.014469150453805923}, {"id": 734, "seek": 379200, "start": 3792.0, "end": 3799.08, "text": " mayor probabilidad, menor perplejidad, mayor probabilidad al cuerpo de evaluaci\u00f3n, es mejor", "tokens": [50364, 10120, 31959, 4580, 11, 26343, 680, 781, 73, 4580, 11, 10120, 31959, 4580, 419, 20264, 368, 6133, 3482, 11, 785, 11479, 50718], "temperature": 0.0, "avg_logprob": -0.17659803118024553, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.13837173581123352}, {"id": 735, "seek": 379200, "start": 3799.08, "end": 3803.6, "text": " desde ese punto de vista, lo consideramos mejor. \u00bfPor qu\u00e9? Porque tiene menos dudas de c\u00f3mo se", "tokens": [50718, 10188, 10167, 14326, 368, 22553, 11, 450, 1949, 2151, 11479, 13, 3841, 24907, 8057, 30, 11287, 7066, 8902, 38512, 296, 368, 12826, 369, 50944], "temperature": 0.0, "avg_logprob": -0.17659803118024553, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.13837173581123352}, {"id": 736, "seek": 379200, "start": 3803.6, "end": 3813.2, "text": " comporta, porque la perplejidad es como la incertidumbre que yo tengo ante... Dada una palabra,", "tokens": [50944, 25883, 64, 11, 4021, 635, 680, 781, 73, 4580, 785, 2617, 635, 834, 911, 327, 449, 2672, 631, 5290, 13989, 23411, 485, 413, 1538, 2002, 31702, 11, 51424], "temperature": 0.0, "avg_logprob": -0.17659803118024553, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.13837173581123352}, {"id": 737, "seek": 379200, "start": 3813.2, "end": 3816.92, "text": " cuando yo me paro una palabra, \u00bfcu\u00e1l es mi incertidumbre? Mi branching factor,", "tokens": [51424, 7767, 5290, 385, 971, 78, 2002, 31702, 11, 3841, 12032, 11447, 785, 2752, 834, 911, 327, 449, 2672, 30, 10204, 9819, 278, 5952, 11, 51610], "temperature": 0.0, "avg_logprob": -0.17659803118024553, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.13837173581123352}, {"id": 738, "seek": 381692, "start": 3817.32, "end": 3822.2400000000002, "text": " en cuanto se puede abrir la siguiente palabra en promedio? Un poco eso es lo que captura la", "tokens": [50384, 465, 36685, 369, 8919, 27446, 635, 25666, 31702, 465, 2234, 292, 1004, 30, 1156, 10639, 7287, 785, 450, 631, 3770, 2991, 635, 50630], "temperature": 0.0, "avg_logprob": -0.19544855753580728, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.10678761452436447}, {"id": 739, "seek": 381692, "start": 3822.2400000000002, "end": 3829.16, "text": " perplejidad. Mi lenguaje va a tener un branching factor, es decir, no es que es cero, pero mi modelo", "tokens": [50630, 680, 781, 73, 4580, 13, 10204, 35044, 84, 11153, 2773, 257, 11640, 517, 9819, 278, 5952, 11, 785, 10235, 11, 572, 785, 631, 785, 269, 2032, 11, 4768, 2752, 27825, 50976], "temperature": 0.0, "avg_logprob": -0.19544855753580728, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.10678761452436447}, {"id": 740, "seek": 381692, "start": 3829.16, "end": 3833.6800000000003, "text": " siempre va a calcular algo mayor o igual a ese branching factor. Cuanto m\u00e1s bajo sea,", "tokens": [50976, 12758, 2773, 257, 2104, 17792, 8655, 10120, 277, 10953, 257, 10167, 9819, 278, 5952, 13, 13205, 5857, 3573, 30139, 4158, 11, 51202], "temperature": 0.0, "avg_logprob": -0.19544855753580728, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.10678761452436447}, {"id": 741, "seek": 381692, "start": 3833.6800000000003, "end": 3838.44, "text": " es que quiere decir que yo no estoy acercando m\u00e1s a la perplejidad posta, por eso la perplejidad", "tokens": [51202, 785, 631, 23877, 10235, 631, 5290, 572, 15796, 696, 2869, 1806, 3573, 257, 635, 680, 781, 73, 4580, 2183, 64, 11, 1515, 7287, 635, 680, 781, 73, 4580, 51440], "temperature": 0.0, "avg_logprob": -0.19544855753580728, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.10678761452436447}, {"id": 742, "seek": 383844, "start": 3838.44, "end": 3850.6, "text": " es la medida de que tambi\u00e9n hace las cosas. \u00bfDe acuerdo? Bueno, no, eso es cuentas.", "tokens": [50364, 785, 635, 32984, 368, 631, 6407, 10032, 2439, 12218, 13, 3841, 11089, 28113, 30, 16046, 11, 572, 11, 7287, 785, 46414, 296, 13, 50972], "temperature": 0.0, "avg_logprob": -0.32161597485812204, "compression_ratio": 1.2826086956521738, "no_speech_prob": 0.1719743311405182}, {"id": 743, "seek": 383844, "start": 3853.2400000000002, "end": 3860.2400000000002, "text": " Por ejemplo, si nosotros entrenamos unigramas, bigramas y triramas en un corpo de art\u00edculo", "tokens": [51104, 5269, 13358, 11, 1511, 13863, 45069, 2151, 517, 33737, 296, 11, 955, 2356, 296, 288, 504, 347, 19473, 465, 517, 23257, 368, 1523, 34365, 51454], "temperature": 0.0, "avg_logprob": -0.32161597485812204, "compression_ratio": 1.2826086956521738, "no_speech_prob": 0.1719743311405182}, {"id": 744, "seek": 386024, "start": 3860.24, "end": 3868.56, "text": " de Wall Street Journal de 38 millones de palabras, probaron el cuerpo sobre un modelo de un", "tokens": [50364, 368, 9551, 7638, 16936, 368, 12843, 22416, 368, 35240, 11, 1239, 6372, 806, 20264, 5473, 517, 27825, 368, 517, 50780], "temperature": 0.0, "avg_logprob": -0.16682323595372642, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.2622162699699402}, {"id": 745, "seek": 386024, "start": 3868.56, "end": 3875.3199999999997, "text": " cuerpo de prueba de 1,5 millones de palabras y calcularon la perplejidad. Y f\u00edjense que la", "tokens": [50780, 20264, 368, 48241, 368, 502, 11, 20, 22416, 368, 35240, 288, 2104, 17792, 266, 635, 680, 781, 73, 4580, 13, 398, 283, 870, 73, 1288, 631, 635, 51118], "temperature": 0.0, "avg_logprob": -0.16682323595372642, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.2622162699699402}, {"id": 746, "seek": 386024, "start": 3875.3199999999997, "end": 3884.64, "text": " perplejidad con los unigramas es de 962. \u00bfNo sabemos cu\u00e1l es el m\u00ednimo de esto? No sabemos cu\u00e1nto", "tokens": [51118, 680, 781, 73, 4580, 416, 1750, 517, 33737, 296, 785, 368, 24124, 17, 13, 3841, 4540, 27200, 44318, 785, 806, 47393, 368, 7433, 30, 883, 27200, 44256, 78, 51584], "temperature": 0.0, "avg_logprob": -0.16682323595372642, "compression_ratio": 1.5159574468085106, "no_speech_prob": 0.2622162699699402}, {"id": 747, "seek": 388464, "start": 3884.64, "end": 3890.56, "text": " puede bajar, pero sabemos que con bigrama llega a 170 y con triramas a 109. Es decir, si yo tengo", "tokens": [50364, 8919, 23589, 289, 11, 4768, 27200, 631, 416, 955, 29762, 40423, 257, 27228, 288, 416, 504, 347, 19473, 257, 1266, 24, 13, 2313, 10235, 11, 1511, 5290, 13989, 50660], "temperature": 0.0, "avg_logprob": -0.28716251585218644, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.3513285219669342}, {"id": 748, "seek": 388464, "start": 3890.56, "end": 3895.3599999999997, "text": " dos palabras antes, puedo predecir con mejor, porque ac\u00e1 es con unigrama, es la probabilidad", "tokens": [50660, 4491, 35240, 11014, 11, 21612, 24874, 23568, 416, 11479, 11, 4021, 23496, 785, 416, 517, 33737, 64, 11, 785, 635, 31959, 4580, 50900], "temperature": 0.0, "avg_logprob": -0.28716251585218644, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.3513285219669342}, {"id": 749, "seek": 388464, "start": 3895.3599999999997, "end": 3901.68, "text": " que a palabra no dice mucho. Si yo tengo el anterior, lo r\u00e1pidamente baja. Y si se fija,", "tokens": [50900, 631, 257, 31702, 572, 10313, 9824, 13, 4909, 5290, 13989, 806, 22272, 11, 450, 18213, 49663, 49427, 13, 398, 1511, 369, 283, 20642, 11, 51216], "temperature": 0.0, "avg_logprob": -0.28716251585218644, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.3513285219669342}, {"id": 750, "seek": 388464, "start": 3901.68, "end": 3914.24, "text": " cuando agrega un tercero baja, pero no tanto, ni cerca tanto. Bueno, lo \u00faltimo que nos queda", "tokens": [51216, 7767, 623, 3375, 64, 517, 38103, 78, 49427, 11, 4768, 572, 10331, 11, 3867, 26770, 10331, 13, 16046, 11, 450, 21013, 631, 3269, 23314, 51844], "temperature": 0.0, "avg_logprob": -0.28716251585218644, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.3513285219669342}, {"id": 751, "seek": 391424, "start": 3914.24, "end": 3923.7999999999997, "text": " hablar es muy bien. \u00bfQu\u00e9 pas\u00f3 con las probabilidades nuladas? \u00bfSe acuerdan que nos quedaban las", "tokens": [50364, 21014, 785, 5323, 3610, 13, 3841, 15137, 41382, 416, 2439, 31959, 10284, 297, 425, 6872, 30, 3841, 10637, 696, 5486, 10312, 631, 3269, 13617, 18165, 2439, 50842], "temperature": 0.0, "avg_logprob": -0.2391815185546875, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.01726708374917507}, {"id": 752, "seek": 391424, "start": 3923.7999999999997, "end": 3928.8799999999997, "text": " probabilidades nuladas cuando no hab\u00eda conteo? Bueno, uno de los problemas es la palabra que", "tokens": [50842, 31959, 10284, 297, 425, 6872, 7767, 572, 16395, 34444, 78, 30, 16046, 11, 8526, 368, 1750, 20720, 785, 635, 31702, 631, 51096], "temperature": 0.0, "avg_logprob": -0.2391815185546875, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.01726708374917507}, {"id": 753, "seek": 391424, "start": 3928.8799999999997, "end": 3934.9199999999996, "text": " no existen. La palabra que no existen lo \u00fanico que podemos hacer o lo que t\u00edpicamente se hace es", "tokens": [51096, 572, 2514, 268, 13, 2369, 31702, 631, 572, 2514, 268, 450, 26113, 631, 12234, 6720, 277, 450, 631, 256, 28236, 23653, 369, 10032, 785, 51398], "temperature": 0.0, "avg_logprob": -0.2391815185546875, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.01726708374917507}, {"id": 754, "seek": 391424, "start": 3934.9199999999996, "end": 3942.9599999999996, "text": " crear un vocabulario fijo y sustituyo las palabras de conocida por una especial. Esto es t\u00edpicamente", "tokens": [51398, 31984, 517, 2329, 455, 1040, 1004, 283, 24510, 288, 5402, 6380, 8308, 2439, 35240, 368, 15871, 2887, 1515, 2002, 15342, 13, 20880, 785, 256, 28236, 23653, 51800], "temperature": 0.0, "avg_logprob": -0.2391815185546875, "compression_ratio": 1.676595744680851, "no_speech_prob": 0.01726708374917507}, {"id": 755, "seek": 394296, "start": 3943.12, "end": 3947.64, "text": " lo que se hace. Es decir, todas las palabras de conocida las considero una sola palabra que nos", "tokens": [50372, 450, 631, 369, 10032, 13, 2313, 10235, 11, 10906, 2439, 35240, 368, 15871, 2887, 2439, 1949, 78, 2002, 34162, 31702, 631, 3269, 50598], "temperature": 0.0, "avg_logprob": -0.25376057912068195, "compression_ratio": 1.5842696629213484, "no_speech_prob": 0.04820508509874344}, {"id": 756, "seek": 394296, "start": 3947.64, "end": 3955.84, "text": " equivalece. Y cuando aparecen enigramas que no ocurren, este es el caso de come que no aparec\u00eda,", "tokens": [50598, 48726, 1220, 384, 13, 398, 7767, 15004, 13037, 465, 328, 2356, 296, 631, 572, 26430, 1095, 11, 4065, 785, 806, 9666, 368, 808, 631, 572, 15004, 31298, 11, 51008], "temperature": 0.0, "avg_logprob": -0.25376057912068195, "compression_ratio": 1.5842696629213484, "no_speech_prob": 0.04820508509874344}, {"id": 757, "seek": 394296, "start": 3957.04, "end": 3961.12, "text": " pero puede ser que el enigrama no ocurra, lo que voy a hacer son t\u00e9cnicas de suavizado.", "tokens": [51068, 4768, 8919, 816, 631, 806, 465, 328, 29762, 572, 26430, 424, 11, 450, 631, 7552, 257, 6720, 1872, 25564, 40672, 368, 459, 706, 27441, 13, 51272], "temperature": 0.0, "avg_logprob": -0.25376057912068195, "compression_ratio": 1.5842696629213484, "no_speech_prob": 0.04820508509874344}, {"id": 758, "seek": 396112, "start": 3961.12, "end": 3977.92, "text": " Yo tengo, \u00bfse acuerdan? Tengo el contador de, por ejemplo, ac\u00e1 es un enigrama, \u00bfno?", "tokens": [50364, 7616, 13989, 11, 3841, 405, 696, 5486, 10312, 30, 314, 30362, 806, 660, 5409, 368, 11, 1515, 13358, 11, 23496, 785, 517, 465, 328, 29762, 11, 3841, 1771, 30, 51204], "temperature": 0.0, "avg_logprob": -0.32774339873215247, "compression_ratio": 1.348148148148148, "no_speech_prob": 0.0032519777305424213}, {"id": 759, "seek": 396112, "start": 3979.6, "end": 3985.12, "text": " Contador de la palabra, el cantidad de veces de la palabra, dividido el total de token que hay.", "tokens": [51288, 4839, 5409, 368, 635, 31702, 11, 806, 33757, 368, 17054, 368, 635, 31702, 11, 4996, 2925, 806, 3217, 368, 14862, 631, 4842, 13, 51564], "temperature": 0.0, "avg_logprob": -0.32774339873215247, "compression_ratio": 1.348148148148148, "no_speech_prob": 0.0032519777305424213}, {"id": 760, "seek": 398512, "start": 3985.44, "end": 3997.44, "text": " Y as\u00ed calculo las probabilidades. La t\u00e9cnica de la plaz, lo que dice es, bueno, le agrego uno", "tokens": [50380, 398, 8582, 4322, 78, 2439, 31959, 10284, 13, 2369, 45411, 368, 635, 499, 921, 11, 450, 631, 10313, 785, 11, 11974, 11, 476, 4554, 1571, 8526, 50980], "temperature": 0.0, "avg_logprob": -0.31669115552715227, "compression_ratio": 1.4368932038834952, "no_speech_prob": 0.014036804437637329}, {"id": 761, "seek": 398512, "start": 3997.44, "end": 4001.0, "text": " a cada contador, o sea que nunca me va a dar cero, lo hago a lo bestia, digamos, \u00bfno? Compare", "tokens": [50980, 257, 8411, 660, 5409, 11, 277, 4158, 631, 13768, 385, 2773, 257, 4072, 269, 2032, 11, 450, 38721, 257, 450, 1151, 654, 11, 36430, 11, 3841, 1771, 30, 6620, 543, 51158], "temperature": 0.0, "avg_logprob": -0.31669115552715227, "compression_ratio": 1.4368932038834952, "no_speech_prob": 0.014036804437637329}, {"id": 762, "seek": 398512, "start": 4001.0, "end": 4006.04, "text": " que no me d\u00e9 cero, le sumo uno. Y le sumo B, \u00bfse acuerdan? Que lo he vivido en la clase pasada. Le sumo", "tokens": [51158, 631, 572, 385, 2795, 269, 2032, 11, 476, 2408, 78, 8526, 13, 398, 476, 2408, 78, 363, 11, 3841, 405, 696, 5486, 10312, 30, 4493, 450, 415, 23603, 78, 465, 635, 44578, 1736, 1538, 13, 1456, 2408, 78, 51410], "temperature": 0.0, "avg_logprob": -0.31669115552715227, "compression_ratio": 1.4368932038834952, "no_speech_prob": 0.014036804437637329}, {"id": 763, "seek": 400604, "start": 4006.04, "end": 4021.12, "text": " B para que esto me siga dando una distribuci\u00f3n de probabilidad. Esto simplemente lo que hace", "tokens": [50364, 363, 1690, 631, 7433, 385, 4556, 64, 29854, 2002, 4400, 30813, 368, 31959, 4580, 13, 20880, 33190, 450, 631, 10032, 51118], "temperature": 0.0, "avg_logprob": -0.21091911839503868, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.09219591319561005}, {"id": 764, "seek": 400604, "start": 4021.12, "end": 4029.24, "text": " es calcular un contador ajustado, multiplica por T y divide por T m\u00e1s B, es decir, multiplica", "tokens": [51118, 785, 2104, 17792, 517, 660, 5409, 41023, 1573, 11, 12788, 2262, 1515, 314, 288, 9845, 1515, 314, 3573, 363, 11, 785, 10235, 11, 12788, 2262, 51524], "temperature": 0.0, "avg_logprob": -0.21091911839503868, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.09219591319561005}, {"id": 765, "seek": 402924, "start": 4029.24, "end": 4045.08, "text": " por el junial y divide por esto, \u00bfno? Por el PWBI. Por ejemplo, si yo digo, si este es mi corpo", "tokens": [50364, 1515, 806, 8156, 831, 288, 9845, 1515, 7433, 11, 3841, 1771, 30, 5269, 806, 46375, 11291, 13, 5269, 13358, 11, 1511, 5290, 22990, 11, 1511, 4065, 785, 2752, 23257, 51156], "temperature": 0.0, "avg_logprob": -0.31005051176426773, "compression_ratio": 1.3013698630136987, "no_speech_prob": 0.08122918009757996}, {"id": 766, "seek": 402924, "start": 4045.08, "end": 4050.68, "text": " entrenamiento, esta es la historia de un hombre de la ciudad que creo, f\u00edjense que me cont\u00e9", "tokens": [51156, 45069, 16971, 11, 5283, 785, 635, 18385, 368, 517, 26102, 368, 635, 24329, 631, 14336, 11, 283, 870, 73, 1288, 631, 385, 660, 526, 51436], "temperature": 0.0, "avg_logprob": -0.31005051176426773, "compression_ratio": 1.3013698630136987, "no_speech_prob": 0.08122918009757996}, {"id": 767, "seek": 405068, "start": 4050.68, "end": 4064.56, "text": " o da uno, y quiso me da cero. Perd\u00f3n, este es el conteo. Ah\u00ed va, el conteo de esta es uno,", "tokens": [50364, 277, 1120, 8526, 11, 288, 421, 19227, 385, 1120, 269, 2032, 13, 47633, 1801, 11, 4065, 785, 806, 34444, 78, 13, 49924, 2773, 11, 806, 34444, 78, 368, 5283, 785, 8526, 11, 51058], "temperature": 0.0, "avg_logprob": -0.23640213390388112, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.5614582896232605}, {"id": 768, "seek": 405068, "start": 4064.56, "end": 4071.12, "text": " de la es dos y de quiso es cero. La probabilidad de esta es uno dividido trece. En total de palabras,", "tokens": [51058, 368, 635, 785, 4491, 288, 368, 421, 19227, 785, 269, 2032, 13, 2369, 31959, 4580, 368, 5283, 785, 8526, 4996, 2925, 2192, 384, 13, 2193, 3217, 368, 35240, 11, 51386], "temperature": 0.0, "avg_logprob": -0.23640213390388112, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.5614582896232605}, {"id": 769, "seek": 405068, "start": 4071.12, "end": 4077.0, "text": " una es esta y es cero, cero, ocho. La es dos dividido trece y quiso me da cero en la probabilidad", "tokens": [51386, 2002, 785, 5283, 288, 785, 269, 2032, 11, 269, 2032, 11, 3795, 78, 13, 2369, 785, 4491, 4996, 2925, 2192, 384, 288, 421, 19227, 385, 1120, 269, 2032, 465, 635, 31959, 4580, 51680], "temperature": 0.0, "avg_logprob": -0.23640213390388112, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.5614582896232605}, {"id": 770, "seek": 407700, "start": 4077.0, "end": 4085.24, "text": " de que no queremos que no de cero. Si nosotros aplicamos la plaza, lo que me da es sumo veinticinco,", "tokens": [50364, 368, 631, 572, 26813, 631, 572, 368, 269, 2032, 13, 4909, 13863, 18221, 2151, 635, 499, 12257, 11, 450, 631, 385, 1120, 785, 2408, 78, 1241, 686, 299, 259, 1291, 11, 50776], "temperature": 0.0, "avg_logprob": -0.26369409000172334, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.09787806868553162}, {"id": 771, "seek": 407700, "start": 4085.24, "end": 4093.52, "text": " \u00bfno? Son doce palabras en el vocabulario porque la \u00fanica que est\u00e1 repetida es la. O sea que tengo", "tokens": [50776, 3841, 1771, 30, 5185, 360, 384, 35240, 465, 806, 2329, 455, 1040, 1004, 4021, 635, 30104, 631, 3192, 13645, 2887, 785, 635, 13, 422, 4158, 631, 13989, 51190], "temperature": 0.0, "avg_logprob": -0.26369409000172334, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.09787806868553162}, {"id": 772, "seek": 407700, "start": 4093.52, "end": 4102.52, "text": " doce en el vocabulario, no trece, trece es T y doce es B. Entonces, se hago dos dividido veinticinco y", "tokens": [51190, 360, 384, 465, 806, 2329, 455, 1040, 1004, 11, 572, 2192, 384, 11, 2192, 384, 785, 314, 288, 360, 384, 785, 363, 13, 15097, 11, 369, 38721, 4491, 4996, 2925, 1241, 686, 299, 259, 1291, 288, 51640], "temperature": 0.0, "avg_logprob": -0.26369409000172334, "compression_ratio": 1.5510204081632653, "no_speech_prob": 0.09787806868553162}, {"id": 773, "seek": 410252, "start": 4102.52, "end": 4109.6, "text": " as\u00ed me da las nuevas probabilidades. Y ac\u00e1 quiso deja de ser cero. El contador ajustado", "tokens": [50364, 8582, 385, 1120, 2439, 42817, 31959, 10284, 13, 398, 23496, 421, 19227, 38260, 368, 816, 269, 2032, 13, 2699, 660, 5409, 41023, 1573, 50718], "temperature": 0.0, "avg_logprob": -0.19926192210270807, "compression_ratio": 1.385185185185185, "no_speech_prob": 0.07363267987966537}, {"id": 774, "seek": 410252, "start": 4109.6, "end": 4113.72, "text": " de lo que nos permite es comparar lo que ten\u00edamos antes con lo que ten\u00edamos ahora. Por ejemplo,", "tokens": [50718, 368, 450, 631, 3269, 31105, 785, 6311, 289, 450, 631, 2064, 16275, 11014, 416, 450, 631, 2064, 16275, 9923, 13, 5269, 13358, 11, 50924], "temperature": 0.0, "avg_logprob": -0.19926192210270807, "compression_ratio": 1.385185185185185, "no_speech_prob": 0.07363267987966537}, {"id": 775, "seek": 411372, "start": 4113.72, "end": 4124.96, "text": " esta val\u00eda uno y baja a cero noventa y seis. \u00bfDe acuerdo? La val\u00eda dos y baja a uno cuarenta y cuatro.", "tokens": [50364, 5283, 1323, 2686, 8526, 288, 49427, 257, 269, 2032, 572, 2475, 64, 288, 28233, 13, 3841, 11089, 28113, 30, 2369, 1323, 2686, 4491, 288, 49427, 257, 8526, 2702, 1898, 64, 288, 28795, 13, 50926], "temperature": 0.0, "avg_logprob": -0.2190890039716448, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.2737380266189575}, {"id": 776, "seek": 411372, "start": 4124.96, "end": 4136.64, "text": " Y quiso va de cero a cero cuarenta y ocho. Si se fijan ac\u00e1, lo que se llama descuento,", "tokens": [50926, 398, 421, 19227, 2773, 368, 269, 2032, 257, 269, 2032, 2702, 1898, 64, 288, 3795, 78, 13, 4909, 369, 42001, 282, 23496, 11, 450, 631, 369, 23272, 7471, 84, 15467, 11, 51510], "temperature": 0.0, "avg_logprob": -0.2190890039716448, "compression_ratio": 1.3785714285714286, "no_speech_prob": 0.2737380266189575}, {"id": 777, "seek": 413664, "start": 4136.64, "end": 4145.4800000000005, "text": " que es el cociente entre los dos valores, me permite ver que le estoy sacando m\u00e1s masa de", "tokens": [50364, 631, 785, 806, 598, 537, 1576, 3962, 1750, 4491, 38790, 11, 385, 31105, 1306, 631, 476, 15796, 4899, 1806, 3573, 29216, 368, 50806], "temperature": 0.0, "avg_logprob": -0.24269020557403564, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.4659610092639923}, {"id": 778, "seek": 413664, "start": 4145.4800000000005, "end": 4154.4800000000005, "text": " probabilidad a la que a esta, que queda casi igual. Es decir, le ten\u00eda a la plaza el problema,", "tokens": [50806, 31959, 4580, 257, 635, 631, 257, 5283, 11, 631, 23314, 22567, 10953, 13, 2313, 10235, 11, 476, 23718, 257, 635, 499, 12257, 806, 12395, 11, 51256], "temperature": 0.0, "avg_logprob": -0.24269020557403564, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.4659610092639923}, {"id": 779, "seek": 413664, "start": 4154.4800000000005, "end": 4161.0, "text": " porque \u00bfqu\u00e9 es lo que est\u00e1 pasando ac\u00e1? Esto es lo que me muestra, es que yo le tengo que sacar", "tokens": [51256, 4021, 3841, 16412, 785, 450, 631, 3192, 45412, 23496, 30, 20880, 785, 450, 631, 385, 2992, 32036, 11, 785, 631, 5290, 476, 13989, 631, 43823, 51582], "temperature": 0.0, "avg_logprob": -0.24269020557403564, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.4659610092639923}, {"id": 780, "seek": 416100, "start": 4161.0, "end": 4167.4, "text": " masa de probabilidad a los que aparecen, porque todo me tiene que sumar uno, todas las probabilidades", "tokens": [50364, 29216, 368, 31959, 4580, 257, 1750, 631, 15004, 13037, 11, 4021, 5149, 385, 7066, 631, 2408, 289, 8526, 11, 10906, 2439, 31959, 10284, 50684], "temperature": 0.0, "avg_logprob": -0.2256349224910558, "compression_ratio": 1.9310344827586208, "no_speech_prob": 0.27989915013313293}, {"id": 781, "seek": 416100, "start": 4167.4, "end": 4172.76, "text": " me tienen que sumar uno. Si yo iba a agregar diagramas que antes estaban en cero, tengo que", "tokens": [50684, 385, 12536, 631, 2408, 289, 8526, 13, 4909, 5290, 33423, 257, 4554, 2976, 10686, 296, 631, 11014, 36713, 465, 269, 2032, 11, 13989, 631, 50952], "temperature": 0.0, "avg_logprob": -0.2256349224910558, "compression_ratio": 1.9310344827586208, "no_speech_prob": 0.27989915013313293}, {"id": 782, "seek": 416100, "start": 4172.76, "end": 4178.0, "text": " sacarle probabilidad a los que est\u00e1n, pues no me suma m\u00e1s que uno. Entonces, esto es lo que tiene", "tokens": [50952, 4899, 36153, 31959, 4580, 257, 1750, 631, 10368, 11, 11059, 572, 385, 2408, 64, 3573, 631, 8526, 13, 15097, 11, 7433, 785, 450, 631, 7066, 51214], "temperature": 0.0, "avg_logprob": -0.2256349224910558, "compression_ratio": 1.9310344827586208, "no_speech_prob": 0.27989915013313293}, {"id": 783, "seek": 416100, "start": 4178.0, "end": 4185.96, "text": " que castiga mucho a los m\u00e1s frecuentes. Les sacan mucho probabilidad a los m\u00e1s frecuentes y como", "tokens": [51214, 631, 4193, 9900, 9824, 257, 1750, 3573, 2130, 12032, 9240, 13, 6965, 4899, 282, 9824, 31959, 4580, 257, 1750, 3573, 2130, 12032, 9240, 288, 2617, 51612], "temperature": 0.0, "avg_logprob": -0.2256349224910558, "compression_ratio": 1.9310344827586208, "no_speech_prob": 0.27989915013313293}, {"id": 784, "seek": 418596, "start": 4185.96, "end": 4194.68, "text": " que premia demasiado a los que no aparecen. Hay otras t\u00e9cnicas, no vamos a entrar en eso que tratan", "tokens": [50364, 631, 5624, 654, 39820, 257, 1750, 631, 572, 15004, 13037, 13, 8721, 20244, 25564, 40672, 11, 572, 5295, 257, 20913, 465, 7287, 631, 21507, 282, 50800], "temperature": 0.0, "avg_logprob": -0.22722035576315486, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.03844424709677696}, {"id": 785, "seek": 418596, "start": 4194.68, "end": 4203.56, "text": " de ajustarlo un poco mejor, pues ahora vamos a mover algunas, perd\u00f3n. Mueve demasiada probabilidad.", "tokens": [50800, 368, 41023, 19457, 517, 10639, 11479, 11, 11059, 9923, 5295, 257, 39945, 27316, 11, 12611, 1801, 13, 376, 622, 303, 35259, 1538, 31959, 4580, 13, 51244], "temperature": 0.0, "avg_logprob": -0.22722035576315486, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.03844424709677696}, {"id": 786, "seek": 418596, "start": 4203.56, "end": 4214.64, "text": " Otra posibilidad es usar un delta en lugar de uno. Ese delta tengo que calcularlo, se acuerdan lo", "tokens": [51244, 12936, 424, 1366, 33989, 785, 14745, 517, 8289, 465, 11467, 368, 8526, 13, 462, 405, 8289, 13989, 631, 2104, 17792, 752, 11, 369, 696, 5486, 10312, 450, 51798], "temperature": 0.0, "avg_logprob": -0.22722035576315486, "compression_ratio": 1.5025125628140703, "no_speech_prob": 0.03844424709677696}, {"id": 787, "seek": 421464, "start": 4214.72, "end": 4219.320000000001, "text": " calamos del cuerpo de, siempre que yo tengo esos par\u00e1metros para calcular, los calculo sobre el", "tokens": [50368, 2104, 2151, 1103, 20264, 368, 11, 12758, 631, 5290, 13989, 22411, 971, 842, 29570, 1690, 2104, 17792, 11, 1750, 4322, 78, 5473, 806, 50598], "temperature": 0.0, "avg_logprob": -0.2707891715200324, "compression_ratio": 1.5, "no_speech_prob": 0.015183191746473312}, {"id": 788, "seek": 421464, "start": 4219.320000000001, "end": 4234.04, "text": " cuerpo de desarrollo. Finalmente, hay otra, esa es una aproximaci\u00f3n, es decir, con t\u00e9cnicas sobre", "tokens": [50598, 20264, 368, 38295, 13, 13443, 4082, 11, 4842, 13623, 11, 11342, 785, 2002, 31270, 3482, 11, 785, 10235, 11, 416, 25564, 40672, 5473, 51334], "temperature": 0.0, "avg_logprob": -0.2707891715200324, "compression_ratio": 1.5, "no_speech_prob": 0.015183191746473312}, {"id": 789, "seek": 421464, "start": 4234.04, "end": 4241.08, "text": " el contenci. Hay otra posibilidad que son un poco m\u00e1s avanzadas, digamos que es cuando yo quiero", "tokens": [51334, 806, 21795, 537, 13, 8721, 13623, 1366, 33989, 631, 1872, 517, 10639, 3573, 42444, 6872, 11, 36430, 631, 785, 7767, 5290, 16811, 51686], "temperature": 0.0, "avg_logprob": -0.2707891715200324, "compression_ratio": 1.5, "no_speech_prob": 0.015183191746473312}, {"id": 790, "seek": 424108, "start": 4241.08, "end": 4250.24, "text": " estimar, por ejemplo, en t\u00e9cnicas de trigrama, una palabra a partir de las dos anteriores y", "tokens": [50364, 8017, 289, 11, 1515, 13358, 11, 465, 25564, 40672, 368, 35386, 29762, 11, 2002, 31702, 257, 13906, 368, 2439, 4491, 364, 34345, 2706, 288, 50822], "temperature": 0.0, "avg_logprob": -0.2891596187244762, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00930751208215952}, {"id": 791, "seek": 424108, "start": 4252.48, "end": 4259.92, "text": " no existen casos de las dos anteriores en el texto, de las dos anteriores seguidas a W,", "tokens": [50934, 572, 2514, 268, 25135, 368, 2439, 4491, 364, 34345, 2706, 465, 806, 35503, 11, 368, 2439, 4491, 364, 34345, 2706, 8878, 11382, 257, 343, 11, 51306], "temperature": 0.0, "avg_logprob": -0.2891596187244762, "compression_ratio": 1.565217391304348, "no_speech_prob": 0.00930751208215952}, {"id": 792, "seek": 425992, "start": 4260.92, "end": 4272.24, "text": " ac\u00e1 es WN, perd\u00f3n. S\u00ed, lo que hago es hacer lo que se llama BACOV, calcularlo a trav\u00e9s de la", "tokens": [50414, 23496, 785, 343, 45, 11, 12611, 1801, 13, 12375, 11, 450, 631, 38721, 785, 6720, 450, 631, 369, 23272, 363, 4378, 46, 53, 11, 2104, 17792, 752, 257, 24463, 368, 635, 50980], "temperature": 0.0, "avg_logprob": -0.27983567931435327, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.18034139275550842}, {"id": 793, "seek": 425992, "start": 4272.24, "end": 4275.84, "text": " probabilidad de la anterior. Bueno, si no tengo la anterior, pruebo con la anterior, se entiende,", "tokens": [50980, 31959, 4580, 368, 635, 22272, 13, 16046, 11, 1511, 572, 13989, 635, 22272, 11, 32820, 1763, 416, 635, 22272, 11, 369, 948, 45816, 11, 51160], "temperature": 0.0, "avg_logprob": -0.27983567931435327, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.18034139275550842}, {"id": 794, "seek": 425992, "start": 4275.84, "end": 4284.88, "text": " eso se llama hacer BACOV. El BACOV, ten\u00e9s que resolver tambi\u00e9n que ahora otra vez se", "tokens": [51160, 7287, 369, 23272, 6720, 363, 4378, 46, 53, 13, 2699, 363, 4378, 46, 53, 11, 2064, 2191, 631, 34480, 6407, 631, 9923, 13623, 5715, 369, 51612], "temperature": 0.0, "avg_logprob": -0.27983567931435327, "compression_ratio": 1.4946808510638299, "no_speech_prob": 0.18034139275550842}, {"id": 795, "seek": 428488, "start": 4284.88, "end": 4289.56, "text": " est\u00e1 introduciendo nuevas, luego caso que no ten\u00edas antes. Estas probabilidades tengo que", "tokens": [50364, 3192, 2814, 16830, 42817, 11, 17222, 9666, 631, 572, 2064, 10025, 11014, 13, 4410, 296, 31959, 10284, 13989, 631, 50598], "temperature": 0.0, "avg_logprob": -0.2184469537827575, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.05750058963894844}, {"id": 796, "seek": 428488, "start": 4289.56, "end": 4292.92, "text": " calcularlo y darle m\u00e1s a la probabilidad. O sea, otra vez tengo que mover probabilidad.", "tokens": [50598, 2104, 17792, 752, 288, 37666, 3573, 257, 635, 31959, 4580, 13, 422, 4158, 11, 13623, 5715, 13989, 631, 39945, 31959, 4580, 13, 50766], "temperature": 0.0, "avg_logprob": -0.2184469537827575, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.05750058963894844}, {"id": 797, "seek": 428488, "start": 4297.04, "end": 4304.400000000001, "text": " Cuando los corpus son muy, muy, muy grandes, una forma alternativa y es un m\u00e9todo muy nuevo,", "tokens": [50972, 21907, 1750, 1181, 31624, 1872, 5323, 11, 5323, 11, 5323, 16640, 11, 2002, 8366, 5400, 18740, 288, 785, 517, 20275, 17423, 5323, 18591, 11, 51340], "temperature": 0.0, "avg_logprob": -0.2184469537827575, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.05750058963894844}, {"id": 798, "seek": 428488, "start": 4304.400000000001, "end": 4309.84, "text": " se llama STUPID BACOV, que es, como mi cuerpo es muy grande, t\u00edpicamente el cuerpo de Google,", "tokens": [51340, 369, 23272, 4904, 22917, 2777, 363, 4378, 46, 53, 11, 631, 785, 11, 2617, 2752, 20264, 785, 5323, 8883, 11, 256, 28236, 23653, 806, 20264, 368, 3329, 11, 51612], "temperature": 0.0, "avg_logprob": -0.2184469537827575, "compression_ratio": 1.550420168067227, "no_speech_prob": 0.05750058963894844}, {"id": 799, "seek": 430984, "start": 4310.0, "end": 4318.32, "text": " no normalizo nada las probabilidades, conteo nom\u00e1s como me fu\u00e9 y ya est\u00e1. Si una no me da", "tokens": [50372, 572, 2710, 19055, 8096, 2439, 31959, 10284, 11, 34444, 78, 5369, 2490, 2617, 385, 8536, 526, 288, 2478, 3192, 13, 4909, 2002, 572, 385, 1120, 50788], "temperature": 0.0, "avg_logprob": -0.3752015861305031, "compression_ratio": 1.4530386740331491, "no_speech_prob": 0.013926247134804726}, {"id": 800, "seek": 430984, "start": 4318.32, "end": 4327.08, "text": " pruebo con la anterior, si es igual tengo un mont\u00f3n de edad. O tambi\u00e9n se puede hacer", "tokens": [50788, 32820, 1763, 416, 635, 22272, 11, 1511, 785, 10953, 13989, 517, 45259, 368, 1257, 345, 13, 422, 6407, 369, 8919, 6720, 51226], "temperature": 0.0, "avg_logprob": -0.3752015861305031, "compression_ratio": 1.4530386740331491, "no_speech_prob": 0.013926247134804726}, {"id": 801, "seek": 430984, "start": 4327.08, "end": 4332.12, "text": " interpolaci\u00f3n, es decir, la probabilidad de una palabra dada a las dos anteriores", "tokens": [51226, 44902, 3482, 11, 785, 10235, 11, 635, 31959, 4580, 368, 2002, 31702, 274, 1538, 257, 2439, 4491, 364, 34345, 2706, 51478], "temperature": 0.0, "avg_logprob": -0.3752015861305031, "compression_ratio": 1.4530386740331491, "no_speech_prob": 0.013926247134804726}, {"id": 802, "seek": 433212, "start": 4332.12, "end": 4342.84, "text": " es la probabilidad de la palabra, la probabilidad nueva, es la probabilidad original de la palabra", "tokens": [50364, 785, 635, 31959, 4580, 368, 635, 31702, 11, 635, 31959, 4580, 28963, 11, 785, 635, 31959, 4580, 3380, 368, 635, 31702, 50900], "temperature": 0.0, "avg_logprob": -0.22215561866760253, "compression_ratio": 2.0347222222222223, "no_speech_prob": 0.15639568865299225}, {"id": 803, "seek": 433212, "start": 4342.84, "end": 4348.36, "text": " dada a las dos anteriores por un cierto lambda, m\u00e1s un cierto lambda 2 por la probabilidad de la", "tokens": [50900, 274, 1538, 257, 2439, 4491, 364, 34345, 2706, 1515, 517, 28558, 13607, 11, 3573, 517, 28558, 13607, 568, 1515, 635, 31959, 4580, 368, 635, 51176], "temperature": 0.0, "avg_logprob": -0.22215561866760253, "compression_ratio": 2.0347222222222223, "no_speech_prob": 0.15639568865299225}, {"id": 804, "seek": 433212, "start": 4348.36, "end": 4355.68, "text": " palabra dada solo en el bigrama, m\u00e1s la probabilidad del unigrama. Y combino las tres a la vez,", "tokens": [51176, 31702, 274, 1538, 6944, 465, 806, 955, 29762, 11, 3573, 635, 31959, 4580, 1103, 517, 328, 29762, 13, 398, 2512, 2982, 2439, 15890, 257, 635, 5715, 11, 51542], "temperature": 0.0, "avg_logprob": -0.22215561866760253, "compression_ratio": 2.0347222222222223, "no_speech_prob": 0.15639568865299225}, {"id": 805, "seek": 435568, "start": 4355.84, "end": 4363.4400000000005, "text": " es como combino las tres t\u00e9cnicas a la vez, \u00bfde acuerdo? Es decir, le doy un cierto peso a las", "tokens": [50372, 785, 2617, 2512, 2982, 2439, 15890, 25564, 40672, 257, 635, 5715, 11, 3841, 1479, 28113, 30, 2313, 10235, 11, 476, 360, 88, 517, 28558, 28149, 257, 2439, 50752], "temperature": 0.0, "avg_logprob": -0.2360703503644025, "compression_ratio": 1.7038327526132404, "no_speech_prob": 0.15589284896850586}, {"id": 806, "seek": 435568, "start": 4363.4400000000005, "end": 4369.200000000001, "text": " probabilidades que yo quiero. De esta forma, porque ac\u00e1 podr\u00eda ser que existiera el bigrama anterior,", "tokens": [50752, 31959, 10284, 631, 5290, 16811, 13, 1346, 5283, 8366, 11, 4021, 23496, 27246, 816, 631, 2514, 10609, 806, 955, 29762, 22272, 11, 51040], "temperature": 0.0, "avg_logprob": -0.2360703503644025, "compression_ratio": 1.7038327526132404, "no_speech_prob": 0.15589284896850586}, {"id": 807, "seek": 435568, "start": 4369.200000000001, "end": 4374.72, "text": " pero existiera una vez sola, entonces yo no le tengo mucha confianza a esa. Puedes usarme y no", "tokens": [51040, 4768, 2514, 10609, 2002, 5715, 34162, 11, 13003, 5290, 572, 476, 13989, 25248, 49081, 2394, 257, 11342, 13, 430, 5827, 279, 505, 35890, 288, 572, 51316], "temperature": 0.0, "avg_logprob": -0.2360703503644025, "compression_ratio": 1.7038327526132404, "no_speech_prob": 0.15589284896850586}, {"id": 808, "seek": 435568, "start": 4374.72, "end": 4378.8, "text": " le tenga mucha confianza, entonces le doy un cierto peso a este tambi\u00e9n, capaz que le doy un", "tokens": [51316, 476, 36031, 25248, 49081, 2394, 11, 13003, 476, 360, 88, 517, 28558, 28149, 257, 4065, 6407, 11, 35453, 631, 476, 360, 88, 517, 51520], "temperature": 0.0, "avg_logprob": -0.2360703503644025, "compression_ratio": 1.7038327526132404, "no_speech_prob": 0.15589284896850586}, {"id": 809, "seek": 435568, "start": 4378.8, "end": 4384.16, "text": " poquito m\u00e1s alto a este. O sea, el 7 existe, est\u00e1 todo bien, pero este siempre me ayuda. Y de esa", "tokens": [51520, 28229, 3573, 21275, 257, 4065, 13, 422, 4158, 11, 806, 1614, 16304, 11, 3192, 5149, 3610, 11, 4768, 4065, 12758, 385, 30737, 13, 398, 368, 11342, 51788], "temperature": 0.0, "avg_logprob": -0.2360703503644025, "compression_ratio": 1.7038327526132404, "no_speech_prob": 0.15589284896850586}, {"id": 810, "seek": 438416, "start": 4384.16, "end": 4389.44, "text": " forma va balanceo. \u00bfC\u00f3mo calculo estos lambda y con el cuerpo de... tengo que", "tokens": [50364, 8366, 2773, 4772, 78, 13, 3841, 28342, 4322, 78, 12585, 13607, 288, 416, 806, 20264, 368, 485, 13989, 631, 50628], "temperature": 0.0, "avg_logprob": -0.3568051488775956, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.032852500677108765}, {"id": 811, "seek": 438416, "start": 4392.08, "end": 4397.2, "text": " de alguna forma calcularlo sobre el cuerpo de desarrollo o el cuerpo gel dado?", "tokens": [50760, 368, 20651, 8366, 2104, 17792, 752, 5473, 806, 20264, 368, 38295, 277, 806, 20264, 4087, 29568, 30, 51016], "temperature": 0.0, "avg_logprob": -0.3568051488775956, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.032852500677108765}, {"id": 812, "seek": 438416, "start": 4402.32, "end": 4403.76, "text": " Tambi\u00e9n hay interpolaci\u00f3n", "tokens": [51272, 25682, 4842, 44902, 3482, 51344], "temperature": 0.0, "avg_logprob": -0.3568051488775956, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.032852500677108765}, {"id": 813, "seek": 438416, "start": 4405.76, "end": 4411.36, "text": " condicionada por el contexto, o sea, hay un lambda, ac\u00e1 ya lo que pasa es un poco m\u00e1s raro,", "tokens": [51444, 2224, 18899, 1538, 1515, 806, 47685, 11, 277, 4158, 11, 4842, 517, 13607, 11, 23496, 2478, 450, 631, 20260, 785, 517, 10639, 3573, 367, 9708, 11, 51724], "temperature": 0.0, "avg_logprob": -0.3568051488775956, "compression_ratio": 1.443298969072165, "no_speech_prob": 0.032852500677108765}, {"id": 814, "seek": 441136, "start": 4411.44, "end": 4416.5599999999995, "text": " y un poco m\u00e1s moderno. Digamos que es que m\u00e1s de estas \u00e9pocas, digamos, donde a m\u00ed ya no me", "tokens": [50368, 288, 517, 10639, 3573, 4363, 78, 13, 10976, 2151, 631, 785, 631, 3573, 368, 13897, 21018, 905, 296, 11, 36430, 11, 10488, 257, 14692, 2478, 572, 385, 50624], "temperature": 0.0, "avg_logprob": -0.17813918102218443, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.001885940320789814}, {"id": 815, "seek": 441136, "start": 4416.5599999999995, "end": 4421.92, "text": " preocupa tanto tener muchos par\u00e1metros. Ac\u00e1 estoy definiendo un par\u00e1metro para cada combinaci\u00f3n de palabras.", "tokens": [50624, 23080, 64, 10331, 11640, 17061, 971, 842, 29570, 13, 5097, 842, 15796, 1561, 7304, 517, 971, 842, 45400, 1690, 8411, 38514, 3482, 368, 35240, 13, 50892], "temperature": 0.0, "avg_logprob": -0.17813918102218443, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.001885940320789814}, {"id": 816, "seek": 441136, "start": 4430.5599999999995, "end": 4437.04, "text": " Y hasta aqu\u00ed llegamos hoy. Esto es el... es este cap\u00edtulo que tengo ac\u00e1, cap\u00edtulo 4 del", "tokens": [51324, 398, 10764, 6661, 11234, 2151, 13775, 13, 20880, 785, 806, 485, 785, 4065, 1410, 30389, 631, 13989, 23496, 11, 1410, 30389, 1017, 1103, 51648], "temperature": 0.0, "avg_logprob": -0.17813918102218443, "compression_ratio": 1.4634146341463414, "no_speech_prob": 0.001885940320789814}, {"id": 817, "seek": 443704, "start": 4437.12, "end": 4440.8, "text": " libro de Juraski. Tiene algunas cositas m\u00e1s, pero esencialmente es eso.", "tokens": [50368, 29354, 368, 508, 12907, 2984, 13, 314, 10174, 27316, 3792, 14182, 3573, 11, 4768, 785, 26567, 4082, 785, 7287, 13, 50552], "temperature": 0.0, "avg_logprob": -0.29819324493408206, "compression_ratio": 1.291044776119403, "no_speech_prob": 0.016005007550120354}, {"id": 818, "seek": 443704, "start": 4443.92, "end": 4450.72, "text": " Y es lo que vamos a hablar de en este curso de Enigrama. La clase que viene presentamos laboratorio.", "tokens": [50708, 398, 785, 450, 631, 5295, 257, 21014, 368, 465, 4065, 31085, 368, 2193, 328, 29762, 13, 2369, 44578, 631, 19561, 1974, 2151, 5938, 48028, 13, 51048], "temperature": 0.0, "avg_logprob": -0.29819324493408206, "compression_ratio": 1.291044776119403, "no_speech_prob": 0.016005007550120354}], "language": "es"}