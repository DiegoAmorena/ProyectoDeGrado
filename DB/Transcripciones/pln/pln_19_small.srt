WEBVTT

00:00.000 --> 00:23.920
Una vez que elegí con el paso 1, elegí cuántas palabras en español le voy a usar, en el paso

00:23.920 --> 00:27.960
2 lo que voy a elegir es una alineación, una función de alineación que me dice cada

00:27.960 --> 00:31.600
palabra con cuál se va a corresponder, cada palabra del lado del español con qué palabra

00:31.600 --> 00:37.920
en inglés se va a corresponder. Este modelo asume de manera muy naiv que todas las alineaciones

00:37.920 --> 00:45.080
que yo puedo tener son equiprobables. O sea, asume que yo voy a tener un conjunto de alineaciones

00:45.080 --> 00:50.160
posibles y todas van a tener la misma probabilidad. Bien, entonces, la probabilidad de elegir

00:50.160 --> 00:55.440
una alineación en particular, si yo tengo un montón de alineaciones, digamos, la probabilidad

00:55.440 --> 01:00.560
de elegir una alineación en particular, va a ser uno sobre la cantidad de alineaciones

01:00.560 --> 01:06.080
que tengo, porque en realidad todas van a ser equiprobables. Bien, entonces, ¿cuántas

01:06.080 --> 01:10.040
alineaciones puedo tener entre dos oraciones, una oración en inglés que tiene largo I y

01:10.040 --> 01:13.720
una oración en español que tiene largo J? ¿Cómo puedo calcular cuántas alineaciones

01:13.720 --> 01:30.480
existen? Más o menos, sí, casi la J. Recuerden que del lado de inglés yo tenía ciertas

01:30.480 --> 01:41.520
palabras, en inglés tenía la palabra E1, E2 hasta, subí, y en español tenía las palabras

01:41.520 --> 01:52.120
F1, F2 hasta F subj. Entonces, yo podía trazar líneas para alinear, pero además en inglés

01:52.120 --> 01:57.040
yo siempre he considerado que tenía un token nul, entonces todas las palabras que no estaban

01:57.040 --> 02:03.000
alineadas del lado del español iban a parar ahí. Así que en inglés en realidad no tengo

02:03.000 --> 02:07.480
i posibilidades, tengo una más, tengo i más uno. Entonces, ¿cuántas formas tengo yo

02:07.480 --> 02:14.040
de mapear estas J posibilidades en español con las I de inglés? Exacto, i más uno a

02:14.040 --> 02:17.920
la J, porque yo tengo i más un opciones para la primera y más una opciones para la segunda,

02:17.920 --> 02:26.920
etcétera, hasta que llevo al final. Así que son i más uno a las J alineaciones posibles.

02:26.920 --> 02:43.280
Ojo, el nul es como una pisadita que hago yo para alinear cosas que no tienen un correspondiente.

02:43.280 --> 02:51.400
O sea, yo tenía una palabra en español que… Varias de las Fes pueden estar alineadas

02:51.400 --> 02:59.080
de nul, no importa en qué orden están. Eso. Bien, entonces eran i más uno a las J posibles

02:59.080 --> 03:09.320
alineaciones, por lo tanto, la probabilidad de elegir una alineación A dada la oración

03:09.320 --> 03:14.640
en inglés, la probabilidad de elegir una alineación cualquiera dada la oración en

03:14.640 --> 03:21.040
inglés va a ser el producto de la probabilidad de haber sorteado un valor J primero, que era

03:21.040 --> 03:26.280
epsilon por la probabilidad de elegir una alineación cualquiera para ese J, que es

03:26.280 --> 03:33.560
uno sobre i más uno a la J. Bien, entonces esto lo resumimos como epsilon sobre i más

03:33.560 --> 03:45.160
uno a la J. Epsilon sobre i más uno a la J es la probabilidad de, dada una oración

03:45.160 --> 03:51.560
en inglés, elegir cierta alineación que yo voy a utilizar. Bien, ese fue el segundo

03:51.560 --> 03:58.160
paso. El tercer paso es una vez que ya tengo la alineación, voy mirando cada palabra del

03:58.160 --> 04:03.880
lado en inglés y le voy poniendo una palabra correspondiente del lado español. Para acá

04:03.880 --> 04:07.960
voy a asumir que yo tengo una tabla de traducción, una tabla de traducción que me dice que tiene

04:07.960 --> 04:11.360
de un lado todas las palabras en español y del otro lado todas las palabras en inglés,

04:11.360 --> 04:18.200
entonces mi tabla va a tener una forma como, por ejemplo, hacer una tabla así que de un

04:18.200 --> 04:27.360
lado va a decir las palabras en español como banco, perro, gato y más cosas y del otro

04:27.360 --> 04:35.000
lado va a tener las correspondientes en inglés como bank, bench, cut, tree y más cosas.

04:35.000 --> 04:39.400
Y entonces esta tabla va a decir la probabilidad de traducir una cosa en la gota. Entonces banco

04:39.400 --> 04:44.400
probablemente tenga cierta probabilidad para bank y cierta probabilidad para bench, 0.4

04:44.400 --> 04:53.000
y 0.6, 0.06 puse. Y para cut no va a tener ninguna probabilidad y para tree tampoco

04:53.000 --> 04:58.720
y después perro no va a tener nada de esto, pero sí después y cut va a ser, no sé 0.8

04:58.720 --> 05:03.000
en este caso, etcétera. Voy a tener una tabla bastante grande que tiene todas las posibilidades

05:03.000 --> 05:12.040
de traducir una palabra como otra. Entonces si yo tengo esa tabla, lo que puedo decir

05:12.040 --> 05:20.440
es que la forma de calcular la probabilidad de esa oración final que yo traduje va a

05:20.440 --> 05:23.520
depender de cuáles son las palabras que yo elija, va a depender de cuáles son las palabras

05:23.520 --> 05:31.080
que yo haya puesto dentro de mi oración para traducir. Entonces esa tabla que está

05:31.080 --> 05:38.320
ahí definida, le llamamos acá en la slide, aparece como t de f sub x sub y dice que la

05:38.320 --> 05:48.280
probabilidad de traducir la palabra es sub y como f sub x. Entonces saca de una cosa importante.

05:48.280 --> 05:57.120
Si tenemos la oración en inglés, la oración en inglés recuerdan que tenía las palabras

05:57.120 --> 06:03.560
f sub 1, f sub 2 hasta f sub n, la oración en español tenía las palabras f sub 1, f

06:03.560 --> 06:10.080
sub 1, f sub 2 hasta f sub j. Y yo tenía en el medio una función de alineación que

06:10.080 --> 06:19.600
me decía qué palabra se correspondía con cuál. Entonces no era f sub n ni f sub j.

06:19.600 --> 06:32.200
Era f sub i y f sub j grande. Esto era f sub i y esto era f sub j grande. Entonces si yo

06:32.200 --> 06:39.560
tengo una palabra cualquiera dentro de la oración en español, tengo un f sub j chica

06:39.560 --> 06:45.160
dentro de la oración en español, esto se va a corresponder con algún f sub i chica

06:45.160 --> 06:49.720
en la oración en inglés, digamos. Yo sé que esto se cumple por la función de alineación

06:49.720 --> 06:52.560
porque agarra y mapea todas las palabras que están en español con algo que está del

06:52.560 --> 06:57.160
lado del inglés, potencialmente con el token vacío nul.

06:57.160 --> 07:02.240
Bien, entonces tengo una palabra del lado del español que es f sub j y una palabra del

07:02.240 --> 07:07.520
lado del inglés que es f sub i. ¿Cuál es la relación entre f sub j y f sub i? ¿Cómo

07:07.520 --> 07:20.000
es la relación entre sí, digamos? Yo puedo decir que el i es igual a algo de j. ¿De alguna

07:20.000 --> 07:27.920
manera? La función de alineación, ahí está. O sea, el i es igual a la función de alineación

07:27.920 --> 07:35.080
aplicada j. Como la i, el índice de este de acá es igual a la función de alineación

07:35.080 --> 07:43.160
aplicada j. Entonces, yo puedo decir que la palabra f sub i es igual a la palabra e sub

07:43.160 --> 07:48.600
a sub j. Así que puedo decir que, en realidad, los que están alineados son la palabra f

07:48.600 --> 07:55.240
sub j está alineada con la palabra e sub a sub j. Y ahí me saqué el i de encima, digamos.

07:55.240 --> 08:01.840
Simplemente, iterando sobre las palabras, iterando sobre la j puedo establecer la correspondencia

08:01.960 --> 08:10.960
entre las dos palabras. Y eso es un poco lo que dice acá para terminar de armar lo que

08:10.960 --> 08:13.800
es el modelo de traducción. Para terminar de armar el modelo de traducción dicen que

08:13.800 --> 08:17.720
en el tercer paso yo voy a elegir cuáles son las palabras. Entonces, lo que voy a hacer

08:17.720 --> 08:25.120
es iterar sobre todas las palabras y haciendo el producto de todas las probabilidades. O

08:25.120 --> 08:31.520
sea, el producto de dado que yo tenía la palabra f sub j, perdón, dado que yo tenía la palabra

08:31.600 --> 08:37.520
e sub a sub j en inglés, entonces elegir la palabra f sub j en español. Eso hago una

08:37.520 --> 08:45.760
productoria con todos los valores de las distintas palabras. Bien, entonces ahí llegué

08:45.760 --> 08:55.040
a el último de los valores que quería calcular, que es la probabilidad de f dado que conozco

08:55.040 --> 09:05.560
ahí es igual a la productoria con j igual a 1 hasta j grande de el valor de la tabla

09:05.560 --> 09:19.720
de traducción, que es t sub f sub j, t de f sub j e sub a sub j. Bueno, ahí tengo cómo

09:19.720 --> 09:25.320
en cada paso fui calculando cosas, este se correspondía al paso uno del modelo, paso

09:25.320 --> 09:29.040
uno, este se corresponde con el paso dos del modelo, en realidad este ya tiene el paso

09:29.040 --> 09:32.360
uno y el paso dos juntos porque ya tengo el éxilón acá y este se corresponde con el

09:32.360 --> 09:41.840
paso tres del modelo. El paso tres de la historia de generación. Mi objetivo con todos estos

09:41.840 --> 09:52.280
valores que están acá es calcular p de f dado e. ¿Qué parámetros introduce? ¿Qué

09:52.280 --> 09:56.440
parámetros fueron surgiendo a medida que yo iba iterando sobre estos pasos? Bueno, en

09:56.440 --> 10:00.320
primer lugar el éxilón aquel que estábamos viendo, este es un valor que yo tendría que

10:00.320 --> 10:06.440
estimar a partir de mirar en los corpus como son los largos de las oraciones relativos

10:06.440 --> 10:09.920
y el otro parámetro importante es aquella tabla de allá, aquella tabla de traducción

10:09.920 --> 10:13.800
es que me dice banco, con qué probabilidad lo puedo traducir como bank, con qué probabilidad

10:13.800 --> 10:18.760
lo puedo traducir como bench, etcétera, etcétera. Esa tabla en realidad es un parámetro del

10:18.760 --> 10:22.040
modelo, es un parámetro del sistema que si yo lo tuviera me alcanzaría con eso para

10:22.040 --> 10:27.360
poder construirme este modelo y calcular la probabilidad de cualquier par de oraciones.

10:27.360 --> 10:39.200
Bien, y entonces antes de continuar vamos a terminar de armar cuál es la imagen de esto,

10:39.200 --> 10:44.240
que es decir yo en realidad lo que quería calcular era p de f dado e, que eso va a ser

10:44.240 --> 10:49.640
mi modelo de traducción y de hecho va a ser el encargado de medir la adecuación de una

10:49.640 --> 10:55.560
frase. P de f dado e lo puedo calcular con esta descomposición de pasos que hice acá

10:55.560 --> 11:19.000
en realidad porque lo hago de la siguiente manera. Yo quiero calcular p de f dado e y

11:19.000 --> 11:23.760
entonces voy a mirar lo que dice acá, p de f dado e es igual a la sumatoriana de p de f

11:23.760 --> 11:30.960
dado e. ¿Qué significa eso? Que para traducir entre una oración en español y una oración

11:30.960 --> 11:35.560
en inglés, o más bien para traducir entre una oración en inglés y una oración en

11:35.560 --> 11:40.760
español hay muchas formas de alinear las palabras entre el inglés y en español y una

11:40.760 --> 11:44.440
vez que yo elegí una forma de alinear hay muchas formas de elegir las palabras que vienen

11:44.440 --> 11:49.120
después digamos yo miro la tarjeta de traducción y capaz que hay varias maneras de elegir distintas

11:49.120 --> 11:54.320
palabras. Entonces lo que eso significa es que no existe una sola manera de traducir una

11:54.320 --> 11:57.800
oración en inglés a una oración en español. Yo puedo encontrar varias formas de alinear

11:57.800 --> 12:01.880
las palabras y varias formas de elegir las palabras de manera que muchas alineaciones son

12:01.880 --> 12:11.000
posibles. Entonces para saber cuál es la probabilidad de traducir p de f dado e, entonces

12:11.000 --> 12:14.320
yo voy a tener que sumar sobre todas las alineaciones posibles sobre todas las formas

12:14.320 --> 12:20.080
de alinear las dos oraciones f y e, voy a tener que iterar sobre eso y para cada una

12:20.080 --> 12:25.720
voy a tener que calcular la probabilidad parcial. Entonces digamos yo tengo cinco formas de

12:25.720 --> 12:30.000
alinear las dos oraciones, cinco es un número un poco raro pero digamos tengo n formas de

12:30.000 --> 12:34.800
alinear las dos oraciones, voy a tener que mirar bueno para la primera alineación cuál

12:34.800 --> 12:39.920
es la probabilidad de encontrar la oración f para la segunda alineación cuál es la

12:39.920 --> 12:42.840
probabilidad de encontrar la oración f para la tercera oración y así hasta llegar al

12:42.840 --> 12:48.840
final y agarro y sumo todo eso. Eso lo puedo hacer porque las alineaciones son una descomposición

12:48.840 --> 12:52.520
del espacio de probabilidades. En realidad yo puedo descomponer el espacio de probabilidades

12:52.520 --> 12:57.480
en pedacitos disjuntos y cada alineación va a ser uno de ellos. Así que digamos que

12:57.480 --> 13:02.040
para calcular el modelo de traducción p de f dado e necesito sumar sobre todas las alineaciones

13:02.040 --> 13:08.960
posibles. Ahora lo que me falta es saber cómo calculo este valor de acá. Así que lo que

13:09.040 --> 13:14.840
estoy diciendo es que la probabilidad de f dado e es la suma sobre las alineaciones

13:14.840 --> 13:20.840
de la probabilidad de f y esa alineación dado e. Eso es simplemente lo que dice ahí

13:20.840 --> 13:25.560
en la slide. Lo que me falta calcular entonces es esta parte de acá y esa parte de acá la

13:25.560 --> 13:31.800
calculo de esta manera. Yo digo que la probabilidad de f dado e es igual, ahí está más o menos

13:31.800 --> 13:38.800
el resultado final pero podemos sacar qué es lo que tendría que poner de este lado.

13:39.440 --> 13:46.440
Ahora sí me acuerdo bien. Ah, ahí está. Por definición de probabilidad condicional.

13:46.440 --> 13:53.440
Eso. p de f dado e, le voy a dar varias maneras a hacerlo pero esto se puede definir como

14:03.480 --> 14:10.480
p de f a e sobre p de e. No? Por definición de probabilidad condicional. Pero además

14:11.480 --> 14:18.480
esto si quiero podría llegar a decir esto es lo mismo que p de f a e sobre p de e por

14:21.320 --> 14:28.320
la cualidad que me faltaba. No. A e. Por p de a e sobre p de a e. Era esto lo quería.

14:40.480 --> 14:47.480
O sea, yo puedo agarrar esta probabilidad que está acá y multiplicarla y dividirla por

14:48.120 --> 14:51.920
el mismo número, que sé que son mayores que cero, así que en definitiva esa división

14:51.920 --> 14:57.920
me va a dar uno. Y ahí yo puedo tomar y asigno este con este y este con este. En definitiva

14:57.920 --> 15:04.920
lo que me queda es si asocio estos dos me va a quedar p de f dado a e y si asocio estos

15:04.920 --> 15:11.920
dos de acá me va a quedar p de a dado e. ¿Qué es lo que dice allá? La probabilidad

15:16.160 --> 15:22.680
de p de f a dado e, bueno, sí, de los dos, de f y a dado e es igual a la probabilidad

15:22.680 --> 15:28.960
de f dado a e por la probabilidad de a dado e. Bien, y estos dos valores que están acá

15:28.960 --> 15:32.600
no los elegí por casualidad sino que son los valores que tenía antes en el modelo.

15:32.600 --> 15:39.600
O sea, yo tenía que el p de a dado e era igual a epsilon sobre y más uno a la j y el

15:41.520 --> 15:48.520
otro era la productoria desde j igual a 1 hasta j grande de las valores de traducción,

15:50.320 --> 15:57.320
el f sub j y el e sub a sub j. Entonces, en definitiva puedo calcular p de f a dado e

15:58.000 --> 16:01.400
y además puedo calcular haciendo una suma sobre todas las alineaciones posibles, puedo

16:01.400 --> 16:08.400
calcular el p de f dado e. Bien, con eso y con todo ese montón de cocciones llegamos

16:09.920 --> 16:14.720
a construir lo que es un modelo de traducción, o sea, solamente teniendo una tabla de traducciones

16:14.720 --> 16:19.400
que me diga cuál es la probabilidad de traducir una palabra. Como otra palabra, yo puedo llegar

16:19.400 --> 16:26.400
a definirme cuál es la probabilidad de traducir una oración dada otra oración. Bien, y hay

16:26.400 --> 16:33.400
una cosa más, bueno, esto ya lo estuvimos viendo que aplicamos en cada paso, y hay una

16:34.240 --> 16:41.240
cosa más que es si yo tuviera las dos oraciones, digamos, la oración en inglés y la oración

16:42.120 --> 16:47.080
en español y además tuviera la tabla esta con todas las probabilidades, yo podría hacer

16:47.080 --> 16:50.800
un algoritmo de programación dinámica, un algoritmo estilo Viterbi que vaya recorriendo

16:50.800 --> 16:55.400
alineaciones y me diga cuál es la alineación más probable. No vamos a ver los detalles

16:55.400 --> 16:58.800
del algoritmo, pero hay una forma de decir, bueno, voy recorriendo las dos oraciones y

16:58.800 --> 17:04.000
me voy quedando con las subsecciones más probables y al final me termina devolviendo

17:04.000 --> 17:09.280
cuál es la alineación más probable dada esas oraciones. O sea, que si yo tuviera ya

17:09.280 --> 17:14.520
esa tabla de traducciones, esa tabla de probabilidad de traducción, podría construirme las alineaciones

17:14.520 --> 17:21.520
del corpus. Así que bueno, hasta el momento decíamos, bueno, suponemos que tenemos esta

17:21.600 --> 17:27.360
tabla de traducción que me dice para bank si se traduce, perdón, para banco si se traduce

17:27.360 --> 17:33.400
como bank o como bench, etc. Estaba diciendo que tenía esa tabla, pero en realidad la

17:33.400 --> 17:38.040
realidad es que no tengo esa tabla y me gustaría poder construirla. Entonces, nos gustaría

17:38.040 --> 17:42.560
poder estimar esas probabilidades para poder construirme esa tabla. Si yo tuviera un corpus

17:42.560 --> 17:46.040
paralelo, simplemente podría ir recorriendo el corpus y contando cuántas veces aparece

17:46.040 --> 17:50.720
banco alineado con bench y cuántas veces aparece alineado con bank y ahí sacaría

17:50.800 --> 17:58.280
una probabilidad, pero no tengo las alineaciones. Y por lo que vimos, digamos, recién, si yo

17:58.280 --> 18:02.320
tuviera la tabla, entonces yo además podría ir recorriendo el corpus y construirme las

18:02.320 --> 18:07.560
alineaciones. Así que si yo tuviera las alineaciones podría contar y sacar la tabla, si yo tuviera

18:07.560 --> 18:12.800
la tabla podría pasarle un algoritmo y construir las alineaciones. Pero la verdad que no tengo

18:12.800 --> 18:17.120
ninguna de las dos cosas. Entonces se vuelve un problema de huevo y la gallina. O sea,

18:17.120 --> 18:21.160
si yo tuviera las alineaciones construiría el modelo, construiría la tabla de probabilidades,

18:21.160 --> 18:25.720
si yo tuviera la tabla de probabilidades podría construir las alineaciones. Para este tipo

18:25.720 --> 18:31.360
de problemas, en los cuales yo tengo como dos variables interdependientes y no conozco

18:31.360 --> 18:34.960
exactamente el valor de ninguna de las dos, se utiliza lo que se conoce como el algoritmo

18:34.960 --> 18:40.760
expectation maximization o maximización de la esperanza. Y bueno, es un algoritmo que

18:40.760 --> 18:45.680
sirve exactamente para este tipo de problemas. En realidad lo que va a hacer el algoritmo

18:45.840 --> 18:51.040
iterar es un algoritmo iterativo que va tratando de converger una solución y lo que hace es

18:51.040 --> 18:59.320
decir, bueno, yo no tengo ninguno de los dos valores. O sea, si yo tuviera mi tabla de

18:59.320 --> 19:03.520
probabilidad de traducción me podría calcular las alineaciones y tuviera mis alineaciones

19:03.520 --> 19:07.280
me podría calcular la probabilidad de traducción. Entonces lo que hace es decir, bueno, asumo

19:07.280 --> 19:12.360
que mi tabla de traducción va a ser uniforme, digamos. Cualquier palabra se puede traducir

19:12.480 --> 19:17.520
como cualquier otra palabra con la misma probabilidad. A partir de eso calculo alineaciones y a partir

19:17.520 --> 19:24.520
de esas nuevas alineaciones calculo otra vez la tabla. Y de vuelta, con esa tabla que calculé,

19:24.960 --> 19:29.360
vuelvo a medir las alineaciones y de vuelta con esas nuevas alineaciones vuelvo a calcular

19:29.360 --> 19:34.960
la tabla. Entonces, aunque no me crean, esto después de muchas iteraciones va convergiendo

19:34.960 --> 19:39.440
a algo. Y parece mágico, ¿no? Parece como que, en realidad si yo no tengo ninguno de

19:39.520 --> 19:46.000
dos valores, no debería nada, debería como dar fruta. Pero voy a tratar de comenzarlos

19:46.000 --> 19:53.000
de que, en realidad, esto sí funciona, con un ejemplito. Bien, tenemos. Entonces, vamos

19:54.400 --> 19:59.680
a construir un sistema que es de traducción entre francés y el inglés donde hay un cuerpo

19:59.680 --> 20:03.000
muy grande, pero bueno, nos vamos a concentrar solo en tres pequeñas oraciones citas que

20:03.000 --> 20:07.000
dicen la mesón se traduce como de House, la mesón blue se traduce como de Blue House

20:07.160 --> 20:12.160
y la flea se traduce como de Flower. Entonces, al principio lo que hago es decir, bueno,

20:12.160 --> 20:16.760
todas las traducciones entre todas las palabras son equiprobables, así que lo que me va a

20:16.760 --> 20:21.240
quedar es cuando reparten entre las alineaciones, todas van a tener el mismo peso. Entre la y

20:21.240 --> 20:25.760
mesón, la probabilidad de que la se traduzca como D o que se traduzca como House va a ser

20:25.760 --> 20:30.720
la misma, en realidad porque todas las alineaciones son equiprobables. En la mesón blue también

20:30.720 --> 20:34.200
pasa lo mismo, la probabilidad de traducir la como D como Blue o como House va a ser

20:34.360 --> 20:44.360
la misma y en la flea pasa igual. Entonces, eso es la primera, el primer paso, digamos,

20:44.360 --> 20:49.640
en el primer paso yo voy a tener todas las alineaciones equiprobables y todas las valores

20:49.640 --> 20:50.760
de las palabras iguales.

21:03.760 --> 21:10.920
Entonces, en mi algoritmo yo empecé con una tabla de traducción que era toda uniforme,

21:10.920 --> 21:14.520
digamos, yo tenía la probabilidad de traducir cualquier palabra en cualquier otra, era la

21:14.520 --> 21:20.240
misma. A partir de eso yo me construí estas alineaciones que también parece que son

21:20.240 --> 21:23.760
todas equiprobables y parece que no tienen como mucha información. Entonces lo que voy

21:23.760 --> 21:28.280
a hacer ahora, a partir de esto, es tratar de construirme de vuelta la tabla de traducciones

21:28.280 --> 21:32.160
pero mirando estas nuevas alineaciones que hay. Entonces lo que voy a construir es una

21:32.160 --> 21:42.800
tabla que tiene todas las palabras del lado de francés, tiene la mesón blue flower y

21:42.800 --> 21:48.680
de House blue flower.

21:48.680 --> 21:57.520
Y para llenar esta nueva tabla, lo que tengo que hacer es iterar sobre las alineaciones,

21:57.520 --> 22:00.920
mirar cada una de las palabras cuantas veces está alineada con las otras y contar, o sea,

22:01.680 --> 22:08.120
y sumar los pesos de cada una de las alineaciones. Entonces la alineación entre la y de. En total,

22:08.120 --> 22:12.160
mirando ese ejemplo de corpus, ¿cuánto me daría? ¿Cuál sería el peso de esa alineación?

22:14.800 --> 22:19.640
Para verlo, en realidad lo que hago es contar, miro cuantas veces la y de están alineados.

22:19.640 --> 22:26.240
Entonces tengo 0.5 de peso en la primera, en la segunda tengo 0.33 y en la última tengo

22:26.240 --> 22:34.320
0.5 de vuelta. Así que en total tengo como 1.33 de peso entre la y de. Después miro,

22:34.320 --> 22:40.680
entre la y House, ¿cuánto peso tengo? ¿cuánta masa de probabilidad tengo? Bueno, tengo 0.5

22:40.680 --> 22:48.160
en la primera relación, 0.33 en la segunda y nada en la tercera. Por lo tanto en total tengo 0.83

22:48.880 --> 22:55.280
de probabilidades entre la y House. Después miro, entre la y blue, ¿cuánto peso tengo?

22:59.600 --> 23:05.480
Solamente 0.33 solo está en la del medio y entre la y flair, ¿cuánto tengo? No, entre la y

23:05.480 --> 23:11.280
flower, ¿cuánto tengo? 0.5 solo aparece en la del final. Bien, completemos la siguiente,

23:11.280 --> 23:21.480
entre mesón y de, ¿cuánto tendría? 0.83, está en la primera y en la segunda, entre

23:21.480 --> 23:34.720
mesón y House, entre mesón y House y 0.83 porque aparece en las dos. Bien, entre mesón y blue,

23:34.720 --> 23:39.360
solamente aparece en la segunda, así que voy a tener 0.33 y entre mesón y flower no tengo nada.

23:40.000 --> 23:46.360
Después entre blue y de, solamente aparece en la segunda, así que voy a tener 0.33, entre blue

23:46.360 --> 23:53.040
y House, creo que de vuelta tengo 0.33 y entre blue y blue también 0.33 y no aparece junto con

23:53.040 --> 24:01.800
flower. Y para después, para flair tengo 0.5 con de, 0 con House, 0 con blue y 0.5 con flower.

24:02.800 --> 24:07.920
Bien, entonces hice una pasada por todas las alineaciones y me calculé cuáles son los

24:07.920 --> 24:12.320
pesos relativos de cada una de estos pares. Lo siguiente que hago, como esto va a ser una

24:12.320 --> 24:16.440
probabilidad, es normalizar. Entonces me voy a construir una tabla, digamos normalizando por,

24:16.440 --> 24:22.040
digamos, voy a sumar en cada fila y voy a dividir entre la cantidad que aparece para cada fila,

24:22.040 --> 24:32.120
así que de vuelta me construyo la tabla, que me queda la mesón blue flower y de este lado de

24:35.560 --> 24:46.600
House acá, de House y blue flower. Y lo que voy a hacer es normalizar, entonces si yo sumo estos

24:46.600 --> 24:55.240
de acá, creo que me da 2 en total, no, 3 en total. Tengo los valores acá, no tengo que

24:55.240 --> 25:00.080
hacer los cálculos, pero sí, me da 3 en total. Entonces lo que pasa cuando yo normalizo es que

25:00.080 --> 25:09.440
acá me queda 0.44, acá me queda 0.28, acá me queda 0.11 y acá me queda 0.17. Pues el segundo,

25:09.440 --> 25:18.720
también lo normalizo esta vez entre 2 y me queda 0.42, 0.42, 0.16, 0. El tercero ya suma 1,

25:18.720 --> 25:31.160
así que me queda 0.23, 0.33, 0.33, 0 y el último también queda igual, 0.5, 0, 0, 0.5. Bien,

25:31.440 --> 25:39.240
entonces me construí una nueva tabla de probabilidad de traducción, dado que ahora las

25:39.240 --> 25:44.560
alineaciones serían estas. Y noten lo que pasó acá, si yo miro la fila correspondiente a la,

25:44.560 --> 25:57.520
¿qué es lo que pasa ahora con esa fila? Recuerda que yo empecé teniendo todas las

25:57.520 --> 26:01.680
probabilidades de traducción de que parecen palabras, eran equiprobables. Si yo ahora miro la

26:01.680 --> 26:16.360
fila de la, ¿qué es lo que pasa? Exacto, aparece claramente que la asociación entre la y de es

26:16.360 --> 26:22.160
más fuerte, tengo un 0.44 de probabilidad de traducir la como de y tengo bastante menos en

26:22.160 --> 26:27.160
los otros, tengo 0.28, 0.11, 0.17. Y yo había empezado diciendo que eran equiprobables, entonces yo

26:27.200 --> 26:35.080
probablemente tenía 0.25, 0.25, 0.25, 0.25 en cada una. Y después de un paso de la iteración,

26:35.080 --> 26:42.800
descubrió que la ID tiene más chance de ser una traducción de la otra, en vez de traducir la

26:42.800 --> 26:48.080
como House o la como Blue o la como Flower. Eso pasa en el primer paso, en la primera iteración

26:48.080 --> 26:53.720
el tipo descubre, el algoritmo descubre que la asociación entre la ID es bastante más fuerte.

26:54.440 --> 27:01.160
Como pasa eso, lo que va a pasar es que cuando yo reparta de vuelta en las alineaciones estas

27:01.160 --> 27:05.760
líneas que se corresponden a la asociación entre la ID van a estar más fuertes, van a tener un

27:05.760 --> 27:12.560
poco más de peso y como esto es una distribución de probabilidades, esa masa que ganó la asociación

27:12.560 --> 27:16.360
entre la ID se va a tener que sacar de otras alineaciones posibles, o sea si la está asociada

27:16.360 --> 27:21.880
con D, entonces no está asociada con las otras que están alrededor. Entonces esa masa que se pierde,

27:21.880 --> 27:29.720
digamos, o sea que gana en la D se tiene que repartir en las otras alineaciones posibles,

27:29.720 --> 27:36.200
o sea en las que no son entre la ID. Entonces después de una iteración la asociación entre

27:36.200 --> 27:43.880
la ID empieza a ser más fuerte y como pasa eso en la siguiente iteración va a empezar a descubrir

27:43.880 --> 27:48.280
que como la estaba alineado con D, entonces mesón tiene que estar alineado con House

27:48.280 --> 27:55.760
y como mesón estaba alineado con House, digamos, esa misma masa de probabilidad se va a traducir,

27:55.760 --> 28:01.120
a transferir a la segunda y lo mismo, como la estaba alineado con D, entonces Fleur tiene

28:01.120 --> 28:07.560
que estar alineado con Flour. Entonces si yo sigo iterando en estos pasos, en cada paso lo que va

28:07.560 --> 28:11.520
a pasar es que se va a mover un poco más de probabilidad hasta que al final va a terminar

28:11.520 --> 28:17.120
descubriendo cuál es la alineación real de las palabras, o sea va a descubrir que la va,

28:17.120 --> 28:23.080
o sea con D, mesón con House, Blue con Blue, Fleur con Flour. ¿Cómo es que va a descubrir eso?

28:23.080 --> 28:27.160
Porque en cada paso lo que va pasando es que algunas de las asociaciones como están, como

28:27.160 --> 28:32.640
aparecen, que ocurren digamos en más oraciones, tienen más fuerza que otras, entonces el peso

28:32.640 --> 28:37.360
que esas asociaciones ganan lo va sacando de otro lado y eso hace que de otro lado se empicen

28:37.360 --> 28:44.160
a generar otras alineaciones diferentes. Entonces al final esto termina convergiendo y termina

28:44.200 --> 28:49.120
revelando lo que es la estructura suyacente de las palabras y cómo se alinean unas con otras.

28:49.120 --> 28:54.920
Bueno, una vez que yo termine de hacer esto puedo agarrar y construirme efectivamente la tabla

28:54.920 --> 29:00.640
final de traducciones que es simplemente busco cada una de las posibles traducciones, digamos de los

29:00.640 --> 29:08.760
posibles pares y saco las probabilidades. ¿Y qué pasó acá? Mientras yo estaba construyendo mi

29:08.760 --> 29:14.000
modelo de traducción, mientras yo estaba construyendo la tabla de traducciones además de como

29:14.000 --> 29:19.080
efectos secundarios se construyó un corpus alineado, un corpus que está alineado a nivel de palabras.

29:24.000 --> 29:32.120
Así que bueno, el algoritmo de Spectation Maximization funciona de esa manera, tiene siempre dos

29:32.120 --> 29:41.640
pasos, un paso de Spectation y un paso de Maximization. En este caso el paso de Spectation se

29:41.640 --> 29:48.560
trataba de agarro la tabla de probabilidad de traducción que tengo y con eso me armo alineaciones y

29:48.560 --> 29:52.360
después el de Maximization es al revés agarro las alineaciones que acabo de construir y me

29:52.360 --> 29:56.520
armo una nueva tabla y voy iterando todos esos pasos hasta que eventualmente converge.

29:56.520 --> 30:03.800
Bien, dijimos que eran cinco modelos de IBM, no vamos a ver muy en detalle los otros, o sea,

30:03.800 --> 30:09.360
solo mencionar que empiezan a crear complejidad. En este modelo uno habíamos dicho que todas las

30:09.360 --> 30:14.960
alineaciones eran equiprobables, en el modelo dos abandonan esa noción y dicen bueno, en vez de

30:14.960 --> 30:20.440
alineaciones equiprobables, yo voy a tener un modelo de reordenamiento de las palabras para decir,

30:20.440 --> 30:25.320
bueno, tengo cierta probabilidad de que las palabras que están, si yo tengo I palabras en inglés,

30:25.320 --> 30:30.520
J palabras en español, tengo cierta probabilidad de mover la palabra I y la palabra J y bueno,

30:30.520 --> 30:35.800
y así siguen subiendo en complejidad hasta llegar al modelo cinco, que modelo cinco es el que anda

30:35.800 --> 30:42.640
mejor, pero de todas maneras estos son modelos que ya no se usan, digamos, esto es del año 93 y en

30:42.640 --> 30:48.640
general se han obtenido mejores resultados abandonando estos modelos. Entonces el que vamos a pasar a

30:48.640 --> 30:53.600
ver a continuación es un modelo bastante más moderno que es lo que sí se utiliza hoy en día

30:53.600 --> 31:11.040
en traductores como los de Google. Sí. Es que en realidad, claro, a ver, estos modelos

31:11.040 --> 31:16.160
estadísticos no utilizan ningún tipo de analizador morfológico y nada para sacarlo. Hay otros modelos

31:16.160 --> 31:20.280
que sí lo hacen, no vamos a dar ninguno en esta clase pero hay otros modelos que sí hacen

31:20.280 --> 31:26.120
uso de esa información. Igual son como refinamientos, creo que ninguno lo tiene como en la base del

31:26.120 --> 31:32.480
modelo, el uso de parto speech, pero sí cuando vos no sabes una palabra, digamos una palabra que

31:32.480 --> 31:37.880
es desconocida, en realidad utilizar información sobre parto speech y eso probablemente te ayude.

31:37.880 --> 31:43.360
En estos modelos por lo menos no lo habían tenido en cuenta. Bien, entonces sí, lo que vamos a ver

31:43.360 --> 31:47.880
ahora es el modelo de frases que es algo más moderno y es, o sea, el Google Translate o Bing

31:48.040 --> 31:52.280
Translate se basan en modelos de este estilo. Y bueno, y antes de ver cómo se modelo de frases,

31:52.280 --> 31:56.760
volvamos un poco a lo que era la alineación entre palabras. Yo tenía esta frase clásica,

31:56.760 --> 32:03.440
¿no? María no dio una bofetada de la bruja verde, en inglés era Mary did not slap de Greenwich y una

32:03.440 --> 32:07.760
alineación entre esas dos oraciones en realidad se vería como algo así. Yo tengo que María se

32:07.760 --> 32:13.440
alinea con Mary, no se alinea con did not, slap se alinea con daba una bofetada, de se alinea

32:13.440 --> 32:18.760
con ala, podría ser solamente con la y el a que no está alineado nada. Green se alinea con verde

32:18.760 --> 32:25.040
y bruja con wedge. ¿Qué diferencia tiene esto con la otra alineación que habíamos visto hoy?

32:26.400 --> 32:30.600
A ver si se les ocurre algo distinto que tiene esta alineación y la que habíamos visto hoy.

32:34.800 --> 32:39.800
Era not con no, sí. ¿Y qué es lo que cambia acá para que pase eso?

32:43.440 --> 32:51.720
Lo que estaba pasando hoy era que yo partía de las palabras en español,

32:51.720 --> 32:55.080
iba las palabras en inglés y yo tenía una función que me mapeaba las palabras en

32:55.080 --> 32:59.000
español con las palabras en inglés. Entonces yo a cada palabra en español como máximo le

32:59.000 --> 33:03.720
podía hacer corresponder una palabra en inglés. Entonces me quedaba que yo podía expresar cosas

33:03.720 --> 33:08.760
como que daba una bofetada, daba, está asociado a slap, una está asociado a slap, bofetada está

33:08.760 --> 33:13.840
asociado a slap, eso lo podía expresar, pero no podía expresar algo como esto, que no está

33:13.840 --> 33:18.360
asociado did not, porque no sería una función. Yo no puedo asociar uno de los valores de la función

33:18.360 --> 33:25.480
con dos cosas del lado del codominio. Y acá en realidad no puedo hacerlo ni en este sentido ni

33:25.480 --> 33:29.000
en el otro sentido, con una función no me sirve porque de vuelta me pasa que slap está asociado

33:29.000 --> 33:34.880
tres cosas. Entonces con una función de alineación yo no puedo construir este tipo de expresiones,

33:34.880 --> 33:40.760
en realidad necesito algo como un poco más poderoso. Esto es lo que decíamos, los modelos

33:40.760 --> 33:44.920
de IBM siempre usan un mapeo de uno a muchos, usan una función de alineación, mapeo uno a muchos,

33:44.920 --> 33:49.400
pero en realidad lo que necesito para poder capturar realmente cómo funciona en el lenguaje es

33:49.400 --> 33:53.080
mapeo de muchos a muchos. Yo voy a tener que un conjunto de palabras se va a traducir en otro

33:53.080 --> 33:58.480
conjunto de palabras. En definitiva lo que pasa es que pequeñas frases se traducen como otras

33:58.480 --> 34:04.960
pequeñas frases, por eso necesito un mapeo de muchos a muchos. Entonces bueno hay algoritmos que

34:04.960 --> 34:11.280
agarran estos mapeos que como construimos recién, el mapeo de uno a muchos en las dos

34:11.280 --> 34:15.520
direcciones digamos y a partir de eso construyen este mapeo de muchos a muchos. Por ejemplo,

34:15.520 --> 34:19.960
el algoritmo de la herramienta quizá más más, lo que hace es decir bueno yo tengo un corpus en

34:19.960 --> 34:26.840
inglés y en español alineo utilizando los los modelos de IBM digamos voy alineo por un lado

34:26.840 --> 34:31.880
de inglés español y por otro lado de español inglés y acá me quedan dos mapeos de uno a

34:31.880 --> 34:37.040
n digamos dos mapeos con funciones y después lo que hago es intersectar esos dos esas dos

34:37.040 --> 34:43.000
alineaciones que me quedaron y unirlas. Cuando las intersecto obtengo lo que se conoce como

34:43.000 --> 34:50.680
puntos de alta confianza, los puntos negros son los puntos de alta confianza que son los de la

34:50.680 --> 34:55.700
intersección y los puntos grises son los que están en la unión, o sea los que pertenecían a algunos

34:55.700 --> 34:59.020
de los dos modelos. Entonces la herramienta de lo que hace es decir bueno una vez que yo tengo la

34:59.020 --> 35:04.500
intersección y la unión hago crecer los puntos que están en la intersección, colonizando otros

35:04.500 --> 35:08.740
puntos que estén en la unión, hasta que al final termino completando digamos toda la imagen. Este

35:08.740 --> 35:14.320
punto que quedó solito ahí ese no sería parte de la alineación al final, sólo los que podéis

35:14.320 --> 35:23.160
llegar moviéndote a través de puntos ya conocidos. Entonces bueno eso es una forma que utiliza se

35:23.160 --> 35:28.960
llama el algoritmo de Oginey que partiendo alineaciones unidireccionales digamos me permite

35:28.960 --> 35:34.640
construir una alineación completa muchos a muchos entre las palabras. Bien eso le quería mencionar

35:34.640 --> 35:38.800
acerca de las alineaciones entre palabras y ahora sí vamos a ver cómo funciona un modelo

35:38.800 --> 35:45.120
basado en frases. Un modelo basado en frases tiene cierta semejanza con el modelo anterior que

35:45.120 --> 35:49.840
habíamos visto pero es un poco más expresivo en realidad yo parto de una oración por ejemplo en

35:49.840 --> 35:54.920
alemán que decía Morgan Fligge y Gnaskana de la Sur Conference. Lo primero que hace el modelo

35:54.920 --> 35:59.920
cuando quiere traducir digamos en este caso es decir bueno yo voy a segmentar esa oración de

35:59.920 --> 36:05.760
origen en cierta cantidad de frases. Después voy a traducir cada una de esas frases usando una

36:05.760 --> 36:09.280
tabla de traducción y esta vez no es una tabla de traducción de palabras sino que es una tabla

36:09.280 --> 36:15.040
de traducción de frases que me dice para acá frases con que otra frase se corresponde y una vez

36:15.040 --> 36:19.640
que yo traduje cada una de esas frases las voy a reordenar de alguna manera buscando que suene

36:19.640 --> 36:25.080
lo más natural posible buscando aumentar la fluidez de esa oración. Entonces como que la

36:25.080 --> 36:28.760
historia de generación es un poco más simple que la otra no tenía que ir sorteando cosas simplemente

36:28.760 --> 36:36.560
digo separo mi oración en segmentos que les voy a llamar frases los traduzco y los reordeno.

36:36.560 --> 36:43.160
Esa segmentación en frases no tiene porque tener un significado lingüístico yo no voy a separarlas

36:43.160 --> 36:48.240
en grupo nominal, grupo verbal, grupo profesional, etcétera. No tengo por qué o sea capaz que yo

36:48.240 --> 36:53.200
segmento las frases y justo me queda un grupo preposicional capaz que no. Lo único que tiene que

36:53.200 --> 36:57.800
pasar es que estos segmentos que yo construyo tienen que estar en mi tabla de traducción de frases

36:57.800 --> 37:02.080
alcanza con eso como para que yo pueda utilizarlos en mi traducción pero no tienen por qué tener

37:02.080 --> 37:09.680
una motivación lingüística. Bueno entonces un modelo basado en frases tiene estos componentes

37:09.680 --> 37:14.400
parecido al anterior porque de vuelta yo lo que quiero hacer es encontrar la probabilidad de

37:14.400 --> 37:20.200
pdf dado e digamos sigo teniendo la misma ecuación fundamental de la traducción automática estadística

37:20.200 --> 37:26.240
la quiero resolver necesito pdf dado e y pdf solo que ahora el pdf dado e lo voy a calcular de una

37:26.240 --> 37:30.960
manera distinta voy a decir que para calcular esto tengo un modelo de traducción de frases y un modelo

37:30.960 --> 37:35.600
de reordenamiento un modelo de una gran tabla de frases que me dice cada frase con qué probabilidad

37:35.600 --> 37:41.240
la traduzco en otra y después una forma de decir cómo reordeno esas frases para tener mejores

37:41.240 --> 37:47.720
oraciones y bueno y como siempre voy a tener otro componente que es el que mide la la fluidez que es

37:47.720 --> 37:53.920
el modelo del lenguaje porque los modelos de frases funcionan mejor que los modelos basados en

37:53.920 --> 38:00.280
palabras porque la frase ya tiene cierto contexto la frases en realidad son como pequeños grupos de

38:00.280 --> 38:08.040
palabras que yo puedo traducir uno uno en el otro entonces cosas como dar la mano dar una

38:08.040 --> 38:12.720
bofetada a tomar el pelo etcétera todas esas cosas como expresiones son mucho más fáciles de traducir

38:12.720 --> 38:16.360
si en realidad yo ya sé que esta expresión que son tres cuatro palabras la puedo traducir en esta

38:16.360 --> 38:20.200
otra expresión que son tres cuatro palabras es como más expresivo entonces se puede aprender más

38:20.200 --> 38:25.440
cosas y bueno obviamente cuanto más cuanto más datos tenga cuanto más largo sea el cuerpo que yo

38:25.440 --> 38:29.400
tengo yo puedo aprender frases más largas mejores probabilidades y mejores frases

38:31.400 --> 38:35.920
bueno acá hay un ejemplo de cómo sería una tabla de traducción de frases o sea es parecido a la

38:35.920 --> 38:40.240
tabla de traducción de palabras o es lo que acá tengo de en borschlag o sea si yo busco la fila

38:40.240 --> 38:45.160
asociada en borschlag o sea encontraría todas estas traducciones de propósal con 62 por ciento

38:45.160 --> 38:51.400
de probabilidad posesivo propósal con 10 por ciento a propósal con 3 por ciento etcétera o

38:51.400 --> 38:56.680
sea como ven se traducen frases en frases bueno y cómo hago para aprender una tabla de traducción

38:56.680 --> 39:04.240
de frases yo parto de esta alineación de palabras digamos esta alineación completa que ya no es

39:04.240 --> 39:09.880
una función sino que es digamos una alineación de muchos a muchos y voy a tratar de encontrar todos

39:09.880 --> 39:15.560
los todas las frases todos los pares de frases que son consistentes con la alineación a qué me

39:15.560 --> 39:24.760
refiero con que son consistentes acá hay ejemplos yo quiero decir que mariano y maría did not son

39:24.760 --> 39:30.440
es son un par de frases que son consistentes con esta alineación en cambio mariano y maría did

39:30.840 --> 39:36.240
como es que miro esto lo que pasa es que cuando yo tengo mariano y maría did la palabra no está

39:36.240 --> 39:41.400
alineada con did not y el did not digamos el no no pertenece hasta alineación que yo estoy

39:41.400 --> 39:46.720
tratando de decir entonces digo que es no consistente lo mismo pasa con si yo tato alinear mariano

39:46.720 --> 39:53.040
daba y maría did not lo que pasa ahí es que daba no está digamos los puntos de alineación de daba

39:53.040 --> 39:56.920
no están dentro de este cuadrante que estoy tratando de buscar entonces en definitiva digo que no es

39:56.920 --> 40:01.280
consistente las alineaciones consistentes correctas son las que consideran todos los

40:01.280 --> 40:06.080
puntos dentro de ese cuadrante entonces mariano está asociado con maría did not y es así es

40:06.080 --> 40:16.160
consistente así que como aprendo frases consistentes empiezo por las alineaciones digamos

40:16.160 --> 40:19.400
empiezo por la alineación de palabra después busco de una palabra y digo bueno me quedo

40:19.400 --> 40:24.320
con todas esas traducciones de palabras y las pongo en mi tabla de frases y después voy

40:24.320 --> 40:28.840
tomando de a dos y me quedo con todas esas otras frases y las voy agregando mi tabla de frases

40:28.840 --> 40:35.120
después me puedo avanzar en uno y tomar de a tres tomar de a cuatro y llegar a tomar incluso

40:35.120 --> 40:40.160
toda la oración como frases entonces a partir de estas oraciones que tenían no sé este 1 2 3

40:40.160 --> 40:46.120
4 5 6 7 8 9 palabras yo termino aprendiendo como 17 frases digamos cada vez más grandes

40:47.760 --> 40:52.800
y bueno hoy voy sacando esto de todo el corpus y calculando mi tabla de probabilidades

40:54.360 --> 40:59.480
de qué manera calculo esas probabilidades yo lo que puedo hacer es como siempre ver cuántas

40:59.480 --> 41:05.560
veces aparece en el corpus y contar o si no si yo tenía construido el modelo anterior el modelo

41:05.560 --> 41:10.400
de la tabla de traducciones de palabra a palabra en realidad lo que puedo hacer es aprovechar ese

41:10.400 --> 41:14.920
modelo de traducción de palabra a palabra y decir bueno me armo una traducción entre un par de

41:14.920 --> 41:19.120
frases basándome en las traduciones palabra a palabra son como dos formas distintas de

41:19.120 --> 41:27.920
construirlo y a veces hasta complementarias bien eso fue el modelo de frases los modelos

41:27.920 --> 41:33.200
de frases son los más usados hoy en día en realidad en lo que es la traducción automática son los

41:33.200 --> 41:39.160
que han dado mejores resultados y bueno y nos faltaba una cosa para terminar el toda la imagen de

41:39.160 --> 41:42.200
lo que es la traducción automática estadística que es la decodificación

41:42.200 --> 41:48.160
entonces damos un resumen de lo que teníamos hasta ahora

41:49.360 --> 41:54.920
hasta ahora yo partí de yo quería resolver la cocción fundamental de la traducción automática

41:54.920 --> 42:00.720
estadística y yo tenía un corpus paralelo que tenía texto en el idioma origen y el idioma

42:00.720 --> 42:04.960
destino y a partir de ciento análisis estadístico yo me construí un modelo de traducción que es

42:04.960 --> 42:10.920
lo que vimos en esta clase además yo tenía cierto cierta cantidad de texto en el idioma

42:10.920 --> 42:15.360
destino y a partir de cierto análisis estadístico me construí un modelo de lenguaje que me dice

42:15.360 --> 42:22.440
que tan fluido es una oración en el lenguaje destino entonces ahora lo que me falta recuerden

42:22.440 --> 42:26.920
que yo lo que tenía que hacer era iterar sobre todas las oraciones del lenguaje destino y pasarlas

42:26.920 --> 42:30.320
a través del modelo de traducción y del modelo de lenguaje para que me dé la probabilidad de esa

42:30.320 --> 42:36.440
oración bueno lo que me falta es el algoritmo de codificación que en vez de probar con todas

42:36.440 --> 42:40.680
las oraciones del lenguaje destino me va a decir unas cuantas oraciones para probar capa que me

42:40.680 --> 42:46.120
dice 150 oraciones para probar sobre las cuales utilizar el modelo de traducción y el modelo

42:46.120 --> 42:51.000
de lenguaje entonces esto es como un diagrama de de módulos en los cuales el algoritmo de

42:51.000 --> 42:55.640
codificación utiliza los dos módulos tanto el de traducción como el de lenguaje

42:57.640 --> 43:04.560
bueno cómo funciona el algoritmo de codificación el que vamos a ver es un algoritmo de codificación

43:04.560 --> 43:11.200
de tipo beam search y bueno funciona de la siguiente manera yo tengo la oración maría no

43:11.200 --> 43:16.440
dio una bofetada a la bruja verde y la quiero traducir al inglés y tengo una tabla de traducción de

43:16.440 --> 43:23.960
frases entonces mi oración maría no dio una bofetada a la bruja verde yo busco en la tabla

43:23.960 --> 43:30.200
de frases cuáles de esas de digamos cuáles segmentos cuáles subsegmentos de esa oración yo

43:30.200 --> 43:34.280
puedo encontrar en la tabla de traducción de frases entonces voy a encontrar por ejemplo que maría lo

43:34.280 --> 43:39.240
puedo traducir como mary no lo busco en la tabla y lo puedo traducir como not como did not o como

43:39.240 --> 43:45.640
no dio lo puedo traducir como git pero además no dio esa frase entera yo lo busco en la tabla y

43:45.640 --> 43:50.720
me parece que la puedo traducir como did not give dio una bofetada toda esa frase lo puedo traducir

43:50.720 --> 43:58.840
como slap una bofetada lo puedo decir como a slap y bueno y otras cosas bruja lo puedo decir como

43:58.840 --> 44:02.160
witch verde como green pero además en algún lado de la tabla tengo que bruja verde lo puedo

44:02.160 --> 44:08.560
traducir como green witch y así digamos yo puedo encontrar tengo diferentes maneras de segmentar

44:08.560 --> 44:12.560
la oración y además para cada uno de esos segmentos puedo encontrar distintas formas de

44:12.560 --> 44:20.280
traducirlo en el lenguaje destino con mi tabla de frases entonces el algoritmo de codificación

44:20.280 --> 44:25.040
funciona de la siguiente manera empezamos teniendo en cada paso del algoritmo vamos a tener un conjunto

44:25.040 --> 44:30.480
de hipótesis de traducción se llega a ver ahí lo que dice ahi ojo más o menos

44:37.160 --> 44:37.660
bien

44:40.000 --> 44:45.520
acá quedaron mal los cuadraditos bueno en cada uno de los pasos yo voy a tener un conjunto de hipótesis

44:45.520 --> 44:52.440
de traducción al principio del algoritmo voy a empezar con lo con una hipótesis vacía como

44:52.440 --> 44:56.880
se le está hipótesis dice que lo importante de leer es la parte de la f que tiene un montón de

44:56.880 --> 45:01.560
guiones significa que no hay ninguna palabra del español cubierta esas son todas las 9 creo 9

45:01.560 --> 45:07.160
palabras en español ninguna está cubierta y esta hipótesis tiene probabilidad 1 entonces en

45:07.160 --> 45:12.800
cada paso del algoritmo lo que voy a hacer es elegir un par de frases tal que una es traducción de

45:12.800 --> 45:17.840
la otra y voy a crear una hipótesis nueva a partir de una que ya tengo entonces en este paso lo que

45:17.840 --> 45:25.720
hice fue decir el hijo el par de frases maría mary y ahí me creo una nueva hipótesis que cubre

45:25.720 --> 45:30.840
la primera palabra por eso parece una serie con este caso elige la frase en inglés mary y ahora

45:30.840 --> 45:36.840
tiene una probabilidad de 0.564 ese número de esa probabilidad va a servir para guiar un poco en el

45:36.840 --> 45:40.680
algoritmo pero vamos a ver después cómo es que se calcula por ahora quédense solamente con el número

45:42.040 --> 45:46.840
bien pero entonces yo tenía otra opción en realidad yo podía haber elegido empezar en

45:46.840 --> 45:50.760
vez de traducir maría por mary podía haber elegido empezar por traducir bruja por witch

45:51.760 --> 46:00.280
y ahí me crearía otra hipótesis de traducción donde cubro la penúltima de las de las palabras

46:00.280 --> 46:06.120
en español agarro la palabra witch delijo la palabra witch y tiene una probabilidad de 0.182

46:07.880 --> 46:12.480
entonces en cada paso del algoritmo lo que hace es elegir una hipótesis que tiene elegir un par

46:12.480 --> 46:18.840
de frases y expandir así que lo siguiente que puedo hacer es elegir la frase did not expandirla a

46:18.840 --> 46:23.920
partir de la hipótesis que tenía con mary y bueno eso me cubre ahora dos palabras en español y me

46:23.920 --> 46:31.320
tiene me me dio otra probabilidad y después sigo avanzando y sigo avanzando hasta que llegó a cubrir

46:31.320 --> 46:35.480
en algún momento si yo sigo avanzando y sigo agregando hipótesis en algún momento voy a

46:35.480 --> 46:41.640
llegar a cubrir todas las palabras del idioma español todas las palabras de la oración en idioma

46:42.040 --> 46:46.960
entonces hay una vez que yo cubrí todas las palabras digo bueno esto es una hipótesis completa

46:46.960 --> 46:52.880
y esto lo devuelvo como una potencial candidata digamos una oración candidata a traducción

46:52.880 --> 46:58.680
pero claro a medida que yo fui avanzando una cosa que pasó es que fui dejando hipótesis colgadas

46:58.680 --> 47:04.360
y esas hipótesis podrían tener otras traducciones posibles yo acá lo que devolí era una posible

47:04.360 --> 47:08.200
traducción pero a medida que yo tenía las otras hipótesis si yo hubiera seguido por las otras

47:08.200 --> 47:14.200
hipótesis hubiera podido devolver otras cosas entonces yo necesito hacer un backtracking para

47:14.200 --> 47:19.160
poder devolver todas las posibilidades poder volver a ver las hipótesis a revisitar las hipótesis

47:19.160 --> 47:24.160
que había dejado colgadas y volver a explorar los otros caminos entonces necesitaría hacer un

47:24.160 --> 47:31.400
backtracking para recorrerlas todas y si hago un backtracking lo que va a pasar es que voy a

47:32.360 --> 47:38.520
va a ocurrir una explosión de exponencial del espacio de búsqueda porque en realidad todas las

47:38.520 --> 47:44.760
las posibilidades que se abren son exponenciales y ahí esto como que se vuelve bastante lento entonces

47:45.920 --> 47:50.280
yo quería un decodificador para volver este problema un problema tratable en vez de agarrar

47:50.280 --> 47:55.080
las infinitas oraciones del idioma me quedo con algunas que sean más probables con este algoritmo

47:55.080 --> 48:01.080
de codificación logré reducir de infinito a algo finito pero aún así es demasiado lento porque

48:01.080 --> 48:06.440
hay una explosión combinación combinatoria digamos de la hipótesis y me queda una cantidad

48:06.440 --> 48:13.160
exponencial de hipótesis entonces como es tan grande este problema digamos como la cantidad

48:13.160 --> 48:17.800
de hipótesis exponencial y este es un problema NP completo entonces se utilizan técnicas para

48:17.800 --> 48:23.280
reducir el espacio de búsqueda y hay como dos tipos de técnicas algunas son con riesgo y otras

48:23.280 --> 48:28.440
son sin riesgo las técnicas sin riesgo lo que quiere decir es que si yo aplica una técnica de

48:28.440 --> 48:34.640
reducción de hipótesis sin riesgo la solución ideal que yo tenía dentro de mi búsqueda no la

48:34.640 --> 48:39.280
voy a perder utilizando una técnica sin riesgo en cambio en la con riesgo si yo podría llegar a

48:39.280 --> 48:44.840
perder la solución óptima bien entonces la técnica sin riesgo que conocemos es la de recombinación de

48:44.840 --> 48:50.280
hipótesis que dice que si yo tengo dos hipótesis voy avanzando por dos caminos dentro del algoritmo

48:50.280 --> 48:55.440
y llevo a dos hipótesis iguales por lo menos dos hipótesis que cubren las mismas palabras entonces

48:55.440 --> 49:00.880
me puedo quedar con la que tiene mayor probabilidad de las dos y descartar la otra porque porque a

49:00.880 --> 49:04.160
medida que yo voy a seguir avanzando en el algoritmo lo que va a pasar es que van a bajar las

49:04.160 --> 49:08.560
probabilidades digamos yo eligiendo más palabras y eligiendo más frases me va a bajar la probabilidad

49:08.560 --> 49:14.400
y nunca me va a pasar que la una de las hipótesis que tenía menos probabilidad vaya a subir en

49:14.400 --> 49:20.280
realidad siempre va a tener menos entonces en definitiva yo puedo con seguridad descartar la

49:20.280 --> 49:26.000
que tiene menos probabilidad bueno esa es recombinación de hipótesis pero ni siquiera con

49:26.000 --> 49:30.960
eso alcanza digamos para reducir el espacio de búsqueda lo suficiente aún queda muchísimas hipótesis

49:30.960 --> 49:36.360
entonces suele utilizar técnicas de podado con riesgo la técnica del histograma la técnica del

49:36.360 --> 49:40.840
umbral el histograma significa que a cada paso digamos en cada paso del algoritmo yo me quedo

49:40.840 --> 49:46.560
con los n las n hipótesis de traducción más probables y descarto las otras y la técnica con

49:46.560 --> 49:52.000
un umbral dice que a cada paso del algoritmo me quedó con la hipótesis de mayor probabilidad y

49:52.000 --> 49:59.400
las que estén a una distancia alfa máximo de esa cuál es el riesgo de las las técnicas de podado

49:59.400 --> 50:04.600
que si la mejor traducción y la traducción óptima tenía algunas frases muy poco probables al

50:04.600 --> 50:10.880
principio entonces probablemente yo descarte esa solución en los primeros pasos y no llegan

50:10.880 --> 50:13.880
a encontrar la solución óptima digamos la perdí por el hecho de haber podado

50:15.440 --> 50:20.160
sin embargo bueno tiene como como ventaja que en realidad reduce muchísimo el espacio de búsqueda

50:20.160 --> 50:28.120
y vuelve vuelve este problema un problema tratable bueno y ahora sí qué significaba esa probabilidad

50:28.120 --> 50:33.080
que estaba viendo en cada una de las hipótesis o sea el podado necesita tener las mejores hipótesis

50:33.080 --> 50:38.600
y bueno y para la recombinación también necesito saber la probabilidad de la hipótesis bueno la forma

50:38.600 --> 50:43.360
de calcular la probabilidad de hipótesis se divide en dos digamos tengo lo que encontré hasta el

50:43.360 --> 50:47.840
momento la hipótesis lleva cubierta cierta cantidad de palabras entonces para esa cantidad de palabras

50:47.840 --> 50:53.080
que ya llevo cubiertas utilizo los tres modelos el modelo de traducción el modelo de reordenamiento

50:53.080 --> 50:58.000
del modelo de lenguaje utilizo los tres modelos para calcular la probabilidad de la frase hasta

50:58.000 --> 51:03.760
el momento pero para lo que me falta traducir yo no puedo utilizar todo porque no tengo toda la

51:03.760 --> 51:07.880
información de traducción entonces lo que hago es utilizar solamente el modelo de traducción y el

51:07.920 --> 51:12.840
modelo de lenguaje descarto el modelo de reordenamiento y bueno entonces hago calcular una

51:12.840 --> 51:16.480
probabilidad que es una parte con todos los tres modelos y otra parte sin el modelo de

51:16.480 --> 51:23.600
reordenamiento bien este algoritmo que acabamos de escribir que hace esta búsqueda basándose en

51:23.600 --> 51:28.880
hipótesis que utiliza recombinación hipodado hipótesis y bueno el calcula de las probabilidades

51:28.880 --> 51:33.760
de esta manera se conoce como algoritmo búsqueda asterisco es un algoritmo de bin search que se

51:33.760 --> 51:41.040
usa muchísimo en lo que es traducción automática estadística por ejemplo el sistema mouses acá

51:41.040 --> 51:47.680
tenemos este ejemplos de herramientas open source o gratuitas que sirven para construcción de de

51:47.680 --> 51:53.200
traductores automáticos el sistema mouses es un sistema open source para desarrollar este tipo

51:53.200 --> 51:59.280
de traductores automáticos estadísticos e implementa este algoritmo de codificación de

51:59.280 --> 52:05.160
búsqueda asterisco y bueno lo que tiene el sistema mouses de bueno es que en realidad lo que hace

52:05.160 --> 52:10.600
además de implementar el decodificadores utiliza a los otros sistemas y los integra de alguna manera

52:10.600 --> 52:16.160
entonces integra este otro sistema el irs tlm que es una herramienta para crear modelos de lenguaje

52:16.160 --> 52:20.800
basados en n en n gramas y el otro sistema se quiza más más que lo habíamos mencionado hoy que es

52:20.800 --> 52:28.640
el sistema que me permite alinear corpus de oraciones en los distintos idiomas llegando

52:28.640 --> 52:33.840
los modelos del 1 al 5 de traducción de bm bueno entonces estas tres herramientas sirven si uno

52:33.840 --> 52:38.000
quiere construir un traductor automático estadístico entre cualquier par de idiomas puede utilizar

52:38.000 --> 52:44.480
estas tres herramientas y teniendo un corpus paralelo y un corpus monolingüe puede construirse un

52:44.480 --> 52:50.560
traductor pero bueno además otra cosa que mencionamos en la clase pasada pero este eran los

52:50.560 --> 52:55.320
sistemas basados en reglas los sistemas basados en reglas han caído un poco este digamos no tienen

52:55.320 --> 53:00.160
tanta popularidad como antes sin embargo algunos se siguen usando y el sistema apertium es un sistema

53:00.160 --> 53:04.960
opensource para construir sistema de traducción basados en reglas que tiene como un montón de

53:04.960 --> 53:10.080
pares de lenguajes y bueno ya anda relativamente bien digamos entonces se sigue desarrollando

53:10.080 --> 53:15.960
hasta hoy entonces es una alternativa opensource que está basada en reglas en vez de estar basado en

53:15.960 --> 53:24.400
estadísticas y bueno esta es un resumen de lo que vimos así que dejamos por acá

