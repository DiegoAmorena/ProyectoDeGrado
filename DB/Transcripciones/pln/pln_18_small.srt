WEBVTT

00:00.000 --> 00:23.200
La clase de hoy y la clase que viene vamos a ver el tema de traducción automática y bueno vamos a

00:23.200 --> 00:30.480
arrancar por esto que se conoce como la nota de weaver o el memorando de weaver warren weaver

00:30.480 --> 00:37.600
era un matemático norteamericano de primera mitad de siglo 20 y el tipo trabajó durante la guerra

00:37.600 --> 00:42.440
especialmente en cosas de criptografía en análisis estadístico de códigos etcétera entonces en un

00:42.440 --> 00:47.720
momento dijo lo siguiente dijo es muy tentador decir que un libro escrito en chino es simplemente un

00:47.720 --> 00:52.560
libro escrito en inglés que ha sido codificado en el código chino si tenemos métodos útiles para

00:52.560 --> 00:56.360
resolver casi cualquier problema criptográfico no será que con la interpretación apropiada

00:56.360 --> 01:06.200
ya tendríamos métodos útiles para traducción el opinaba digamos en este memorándum que los códigos

01:06.200 --> 01:10.440
o los métodos que se utilizan para romper códigos criptográficos que son métodos estadísticos se

01:10.440 --> 01:15.240
podían aplicar al problema de la traducción automática y bueno esto introduce algunas

01:15.240 --> 01:21.760
ideas clave como que puede existir un mapeo automático entre un lenguaje y otro y que codificar

01:21.760 --> 01:28.400
de codificar en un lenguaje es análogo a codificar de codificar en un algoritmo criptográfico y

01:28.400 --> 01:35.480
bueno el tiro esa idea en 1949 tomó como 50 años para que esa idea madurara digamos y después

01:35.480 --> 01:41.440
de 50 años los métodos más utilizados hoy en día son métodos estadísticos que bueno que se

01:41.440 --> 01:46.600
basan un poco en estos principios pero claro en esa época era como muy difícil ver qué era lo que

01:46.600 --> 01:52.640
iba a ocurrir entonces bueno vamos a ver un poco esta esta es la agenda de lo que vamos a mirar

01:52.640 --> 01:58.360
vamos a llegar más o menos hasta la mitad hoy y después la clase siguiente y empecemos con un poco

01:58.360 --> 02:02.800
de historia de lo que es la traducción automática esto empezó como muchas otras tecnologías como

02:02.800 --> 02:08.400
una tecnología militar con fines militares inicialmente era durante la guerra fría era resultado

02:08.400 --> 02:14.320
de interés traducir rápidamente y a bajo costo traducir entre el ruso y el inglés digamos a los

02:14.320 --> 02:19.240
norteamericanos les convenía poder traducir entre el inglés y el ruso y bueno en aquella época se

02:19.240 --> 02:23.120
imaginan lo que era los inicios de la computación las computadoras eran caras en las lentas no tenía

02:23.120 --> 02:27.440
mucho poder de computó pero igual había como mucho optimismo de que en poco tiempo si va a poder

02:27.440 --> 02:33.760
resolver todos los problemas íbamos a tener sistemas que iban a traducir bárbaro y bueno era

02:33.760 --> 02:37.880
más o menos la época del desarrollo de la lingüística computacional inspirado un poco en las teorías

02:37.880 --> 02:42.400
de chonsky estaba la idea que se podía escribir reglas para todo y que a partir de eso se podría

02:42.400 --> 02:50.240
llegar a hacer cosas muy muy buenas en particular para traducción hasta que en 1964 apareció el

02:50.240 --> 02:55.240
reporte al pac al pac que era un comité que estaba estudiando cuál eran los avances en

02:55.240 --> 02:58.760
lingüística computacional porque se estaba poniendo se estaba poniendo mucha plata en muchas

02:58.760 --> 03:03.720
esas cosas y eso se mostraron escépticos acerca de la traducción automática acerca de los logros

03:03.720 --> 03:08.600
que se habían logrado después de todos esos años de meter plata y decía bueno pero se puso

03:08.600 --> 03:12.680
mucho dinero pasó en pasar muchos años pero todavía los humanos lo hacen más barato con

03:12.680 --> 03:18.000
mayor precisión más rápido entonces como que para qué estamos gastando en esto como resultado

03:18.000 --> 03:21.920
de eso hubo un recorte de fondos especialmente en estados unidos para todo lo que es traducción

03:21.920 --> 03:26.680
automática y esto fue parte de lo que se conoció como el invierno de la inteligencia artificial que

03:26.680 --> 03:30.720
un montón de proyectos de inteligencia artificial también no tenía buenos resultados entonces

03:30.720 --> 03:35.960
separó la financiación que había para todo eso durante unos cuantos años entonces se detuvo el

03:35.960 --> 03:40.600
desarrollo de unas cuantas cosas durante unos cuantos años y bueno después empezaron a resurgir de

03:40.600 --> 03:48.440
a poco pero después de esto digamos en los 70 y hasta los 90 más o menos eso logró que la

03:48.440 --> 03:52.280
investigación se frenara un poco en estados unidos pero empezara a aparecer en otros lados del mundo

03:52.280 --> 03:57.320
como por ejemplo en europa o en japón y ahí empezó llano con con files bélicos sino más bien con

03:57.320 --> 04:03.920
fines comerciales entonces había necesidad de tener traducciones o por lo menos dar soporte a los

04:03.920 --> 04:08.680
traductores humanos con algunas traducciones aunque no estuvieran del todo bien pero bueno dar

04:08.680 --> 04:12.080
algunas traducciones de inicio para que los doctores pudieran los doctores humanos pudieran

04:12.080 --> 04:16.080
continuar además las computadoras empezaron a bajar de precio a tener mayor poder de cómputo

04:16.080 --> 04:22.640
y ésta fue como la era de oro de los sistemas de traducción basados en reglas y vamos a caer unos

04:22.640 --> 04:27.280
ejemplos sistemas distrán que todavía se desarrolla aunque ya no está completamente basado en reglas y

04:27.280 --> 04:34.680
bueno y sistemas que se realizaron en japón y en europa y bueno o sea estos sistemas tenían fines

04:34.680 --> 04:41.960
comerciales y no tanto fines militares pero bueno fines de los 90 y después del 2000 en adelante

04:41.960 --> 04:47.600
empezaron a dejarse de usar un poco los sistemas basados en reglas porque porque empezó a ver mayor

04:47.600 --> 04:51.840
poder de cómputo y mayor cantidad de datos disponibles especialmente con la aparición de

04:51.840 --> 04:58.800
internet empezaron a ver muchísimos datos de texto disponibles y eso permitía construir buenos

04:58.800 --> 05:03.760
modelos estadísticos que pudieran explotar las regularidades de los idiomas entonces aparecieron

05:03.760 --> 05:07.440
distintos tipos de modelos estadísticos los primeros los que llamamos traducciones automáticas

05:07.440 --> 05:11.280
estadísticas el otro traducción basado en ejemplos y aparecían las primeras aplicaciones

05:11.280 --> 05:15.560
comerciales que funcionaban bien que utilizaban modelos estadísticos la primera fue lengua

05:15.560 --> 05:21.040
y luego los traductores que más conocemos hoy en día el bing translate de microsoft y bueno el

05:21.120 --> 05:26.520
translate que probablemente lo conozcan lo hayan usado en algún momento y son traductores que la

05:26.520 --> 05:31.760
verdad que hoy en día se puede decir que funcionan bastante bien entonces bueno los métodos

05:31.760 --> 05:36.200
estadísticos empezaron su boom alrededor del año 2000 y siguen siendo el estado del arte

05:38.280 --> 05:42.360
pero bueno primero vamos a ver un poco de lo que son los sistemas basados en reglas que eran estos

05:42.360 --> 05:49.920
primeros sistemas que mencionamos antes en 1968 un investigador de traducción automática se

05:49.960 --> 05:55.480
llamaba pernar bocua hizo un relevamiento de todos los sistemas que se habían construido más o

05:55.480 --> 06:01.720
menos por la época y los clasificó todos dentro de este diagrama el dibujó un triángulo que ahora

06:01.720 --> 06:06.200
se llama el triángulo de bocua y bueno y en este triángulo se ubican los distintos tipos de

06:06.200 --> 06:12.720
sistemas de traducción basados en reglas se ponen como escalones dentro de este triángulo y los

06:12.720 --> 06:16.880
lados del triángulo tienen como distinta interpretación el lado izquierdo si yo voy subiendo por

06:16.920 --> 06:21.600
este lado en realidad lo que aumenta es la cantidad o el esfuerzo de análisis que tengo que

06:21.600 --> 06:25.160
hacer del lenguaje origen yo siempre quiero traducirlo en lenguaje origen o lenguaje destino

06:25.160 --> 06:29.240
bueno entonces de este lado aumenta el esfuerzo de traducción del lenguaje origen y si voy

06:29.240 --> 06:33.800
bajando del lado derecho aumenta bueno si voy subiendo del lado derecho quiero decir aumenta

06:33.800 --> 06:39.000
el esfuerzo de generación en el lenguaje destino entonces qué quiere decir esto yo ubico distintos

06:39.000 --> 06:46.840
sistemas de traducción la traducción directa es simplemente buscar en el diccionario las palabras

06:46.840 --> 06:52.000
y traducir palabra palabra con poca información más entonces eso casi no necesita ningún tipo

06:52.000 --> 06:57.080
de análisis y casi no necesita generación pero para que son de bien yo necesito ponerle muchas

06:57.080 --> 07:02.280
ganas a las reglas o sea las reglas de traducción tienen que ser muy buenas y tienen que tomar en

07:02.280 --> 07:07.080
cuenta muchos casos para que esa traducción llegue a ser buena entonces es como que la flecha de la

07:07.080 --> 07:11.520
transferencia la flecha de la traducción es mucho más larga en cambio si yo hago un poco de análisis

07:11.520 --> 07:16.240
por ejemplo llegó hasta el nivel de análisis intactico tengo un parcer puedo escribir otro

07:16.240 --> 07:20.720
tipo de reglas que pueden ser un poco más expresivas me resulta un poco más fácil y después si tengo

07:20.720 --> 07:26.320
un generador puedo llegar a traducir entonces si sigo subiendo de vuelta voy a necesitar mayor

07:26.320 --> 07:29.880
esfuerzo de análisis de generación pero las reglas pueden ser más expresivas y más fácil de

07:29.880 --> 07:35.480
escribir y probablemente la traducción sea mejor hasta que si llegamos al al vértice del

07:35.480 --> 07:40.880
triángulo llegamos a la interlingua que es una especie de noción en la cual no necesito ningún

07:40.880 --> 07:47.000
tipo de transferencia vamos a ver un poco dentro de un rato de que se trata eso pero bueno empecemos

07:47.000 --> 07:51.720
a ver los distintos niveles de este triángulo de bocua el de más abajo era la traducción directa

07:51.720 --> 07:56.720
es el enfoque más simple lo único que necesito para este para este enfoque es un diccionario

07:56.720 --> 08:01.760
bilingüe yo quiero traducir entre los idiomas y necesito un diccionario que tenga la correspondencia

08:01.760 --> 08:06.160
entre palabras de un idioma y palabras del otro y lo que voy a hacer es traducir palabra-palabra

08:06.160 --> 08:11.680
o sea puedo agregarle alguna cosa extra como por ejemplo algún reordenamiento local yo que

08:11.680 --> 08:16.320
sé para traducir entre español inglés yo diría que en español el nombre se sigue al adjetivo y

08:16.320 --> 08:19.880
en inglés en realidad lo hacen al revés ponen el adjetivo seguido el nombre entonces ese tipo de

08:19.880 --> 08:27.080
reglas simples se las puedo agregar al sistema y bueno el sistema funcionaría un poco así yo tengo

08:27.080 --> 08:32.200
una oración de entrada en el idioma origen mary didn't slap de greenwich le pasa un analizador

08:32.200 --> 08:37.200
morfológico bastante de superficie que no hace mucho en realidad simplemente me dice que esto era

08:37.200 --> 08:43.520
el verbo du en pasado y seguido por un not y bueno el resto de los tokens siguen igual y acá viene

08:43.520 --> 08:47.920
la parte de diccionario digamos lo siguiente que tengo que hacer es buscar en mi diccionario cada

08:47.920 --> 08:52.760
una de las palabras y poner la palabra correspondiente del otro lado entonces mary queda maría du en

08:52.760 --> 08:58.080
pasado como en español no se usa el du usamos simplemente el marcador de pasado no es no slap es

08:58.080 --> 09:05.760
dar una ufetada de es la green es verde witch es bruja con el diccionario voy poniendo todas las

09:05.760 --> 09:12.000
traducciones y después puedo usar mis reglas de reordenamiento local reordenamiento simple como

09:12.000 --> 09:17.080
por ejemplo que el adjetivo seguido en nombre en inglés en realidad en español se corresponde con

09:17.080 --> 09:21.520
nombre seguido adjetivo entonces verdad de bruja lo cambió por bruja verde acá hay otro reordenamiento

09:21.520 --> 09:27.080
digamos donde tengo una marca de pasado y se la pasó para adelante a lo largo y finalmente lo que

09:27.080 --> 09:33.480
hago es una pequeña generación morfológica con estas marcas y digo bueno este dar en pasado se

09:33.480 --> 09:39.720
transforma en dio entonces me queda maría no dio una ufetada a la bruja verde así que partí de

09:39.720 --> 09:45.680
el texto en el idioma origen merited en slap de green witch y llegué a una oración en el idioma

09:45.680 --> 09:49.920
estino maría no dio una ufetada la bruja verde que parece está bastante bien digamos bastante

09:49.920 --> 09:55.480
bien la traducción entonces así es como funcionaría un poco un sistema de traducción directa como les

09:55.480 --> 10:00.400
parece que funcionan estos sistemas en la práctica digamos que también se comportan en la práctica

10:00.400 --> 10:05.480
este tipo de sistemas pues acá vimos un ejemplo que anda bastante bien digamos pero no sé que

10:05.480 --> 10:15.840
claro y hay otro problema más y es

10:18.680 --> 10:21.800
que no tenga todas las palabras pero además que palabras que se pueden traducir de más de

10:21.800 --> 10:27.120
una manera entonces necesitas saber qué palabra tenés que usar entonces bueno

10:28.280 --> 10:33.160
la web está llena de ejemplos de lo que puede salir mal si yo utilizo un sistema de traducción

10:33.160 --> 10:38.800
directa como éste entonces lo que estábamos viendo recién era los sistemas de traducción directa

10:38.800 --> 10:44.680
vamos a subir un poco en la complejidad de los sistemas y llegar a la transferencia sintáctica

10:44.680 --> 10:50.400
entonces para transferencia sintáctica yo lo que voy a necesitar primero es tener un pársar del

10:50.400 --> 10:56.560
lenguaje origen que me lleva a una una análisis sintáctico y además voy a necesitar un generador

10:56.560 --> 11:00.720
del lenguaje destino que agarra un árbol sintáctico del lenguaje destino y genera una oración

11:01.480 --> 11:07.640
entonces yo lo que puedo hacer es escribir reglas que transforma un árbol en el otro y esas reglas

11:07.640 --> 11:11.320
son un poco más fáciles digamos que lo que necesitaría para un sistema de traducción directa

11:11.320 --> 11:14.840
entonces para el inglés por ejemplo para tu siguiente el inglés y el español yo diría que

11:14.840 --> 11:19.560
si tengo un nominal que es un adjetivo nombre un adjetivo sería un nombre en inglés lo transformaría

11:19.560 --> 11:25.880
en un nombre seguir un adjetivo en español y la regla se escribiría algo así diría tengo

11:25.880 --> 11:29.320
un nominal adjetivo nombre entonces lo cambio por nominal nombre adjetivo

11:31.640 --> 11:38.000
entonces ahora que sabemos cómo funciona esto tratemos de hacer el ejemplo en japonés digamos

11:38.000 --> 11:43.240
cómo serían las reglas para transformar el árbol en inglés de giador soliciendo music a el

11:43.240 --> 11:50.000
japonés kareha ongaku uokiku no kadaizuki desu donde está tenemos la correspondencia de cada una de

11:50.000 --> 11:57.200
las palabras pero claro los árboles son un poco distintos el inglés y el español se caracterizan

11:57.200 --> 12:02.320
por ser lenguajes de tipo no sé si esto lo hemos visto ya en el curso pero son lenguajes de tipo

12:02.320 --> 12:07.160
sbo que significa que habitualmente yo suelo escribir un sujeto se dio un verbo seguido de

12:07.160 --> 12:12.320
un objeto el japonés en cambio es un lenguaje de tipo sb porque habitualmente se escribió

12:12.320 --> 12:16.520
el sujeto seguido del objeto seguido del verbo hay muchos lenguajes que pertenecen a esta otra

12:16.520 --> 12:23.560
categoría entonces bueno queremos escribir reglas de transferencia para transformar este árbol en

12:23.560 --> 12:31.200
aquel otro árbol cómo escribiríamos esas reglas que les parece que reglas utilizaría yo para

12:31.200 --> 12:45.400
transformar un árbol en el otro ahí está una de esas en inglés yo escribo

12:50.400 --> 12:56.280
una frase verbal un grupo verbal como un verbo seguido de un grupo precaucional esta es la que

12:57.000 --> 12:59.280
está y la cambio por qué otra cosa

13:04.840 --> 13:13.000
la cambio por un grupo preposicional que sigue un verbo esa es una qué otra regla tendría que

13:13.000 --> 13:21.440
agregar cuál la elaboración que tiene la operación la operación según esto en inglés es un

13:21.440 --> 13:28.920
pronombre seguido de un verbo seguido de un grupo verbal por qué tendría a cambiarlo

13:30.720 --> 13:38.680
ahora en japonés la operación va a ser el pronombre seguido del verse seguido del verbo

13:38.680 --> 13:40.000
bien alguna otra

13:42.440 --> 13:47.840
ahí está el grupo preposicional que está formado por un tú seguido de un nombre

13:47.840 --> 13:55.160
eso es en inglés y en japonés que va a pasar voy a tener un grupo preposicional que es un nombre

13:55.160 --> 14:01.720
seguido de tú bien entonces con eso más o menos creo que tendría las reglas suficientes para

14:01.720 --> 14:05.760
transformar un árbol en el otro los sistemas de traducción vamos a ver si está bien

14:07.760 --> 14:10.840
son los que escribimos esta es la solución del ejercicio

14:11.840 --> 14:18.520
los sistemas de traducción basados en sintaxis en realidad los sistemas de reglas basados en

14:18.520 --> 14:23.720
sintaxis hacen esto a alto nivel digamos tienen un montón de pares de árboles hay gente que

14:23.720 --> 14:28.640
los analiza y escribe reglas de cómo se transforma uno en el otro a veces las reglas son complicadas

14:28.640 --> 14:35.320
porque se pueden super poner entonces hay que definir prioridades y ese tipo de cosas bueno

14:35.320 --> 14:40.520
esos transferencias sintácticas si seguimos subiendo en la en el triángulo de bocua llegamos

14:40.520 --> 14:45.480
a lo que es la transferencia semántica transferencia semántica uno puede pensarla un poco como lo

14:45.480 --> 14:49.720
que habíamos en la clase pasada utilizando roles semánticos yo tengo un etiquetador de

14:49.720 --> 14:55.000
roles semánticos que agarra la oración juan fue a la tienda y me devuelve los roles de los

14:55.000 --> 15:00.680
constituyentes me dice que juan es el agente y a la tienda es el objetivo o gol digamos es el nombre

15:00.680 --> 15:07.000
del rol entonces yo para ciertos idiomas podría escribir reglas más específicas por ejemplo en

15:07.000 --> 15:12.200
chino ocurre que los sintamas preposicionales que son de tipo objetivo se escriben antes del verbo

15:12.200 --> 15:16.360
pero los demás sintamas preposicionales escriben después o sea el chino es un lenguaje de tipo

15:16.360 --> 15:23.800
sbo igual que el inglés o el español pero cuando el objeto es de tipo gol lo que hacen es ponerlo

15:23.800 --> 15:30.280
antes del verbo entonces yo podría escribir una regla un poco más expresiva para este caso del

15:30.280 --> 15:37.480
chino si yo tuviera los roles semánticos yo diría que un grupo verbal es un verbo seguido de esto no

15:37.480 --> 15:42.720
está tachado sino que era la barrita que quedó arriba es un verbo seguido de una de un grupo

15:42.720 --> 15:48.440
preposicional de tipo gol en chino lo cambiaría por un verbo seguido de perdón por un grupo

15:48.440 --> 15:55.200
producción de tipo gol seguido de un verbo es más costoso para generar y para parcer digamos

15:55.200 --> 15:58.960
necesito tener más esfuerzo de par sin más esfuerzo de generación pero puede escribir mejores

15:58.960 --> 16:05.560
reglas que capturan ciertas particularidades de los lenguajes y si yo sigo subiendo en el

16:05.560 --> 16:09.680
triángulo llego a lo que se conoce como interlingua cuál es la gracia del interlingua cuál es la

16:09.680 --> 16:14.000
idea estos sirve si nosotros estamos en un contexto multicultural estamos trabajando por

16:14.000 --> 16:19.960
ejemplo en la ONU o en el parlamento europeo o algo de eso donde se hablan muchos idiomas si

16:19.960 --> 16:24.720
yo quiero mantener un montón de documentos que estén en todos los idiomas a la vez voy a necesitar

16:24.880 --> 16:28.840
para los sistemas que estuve viendo hasta el momento voy a necesitar tener n parsers uno

16:28.840 --> 16:33.440
para acá de idioma n generadores también uno para acá de idioma y después para cada

16:33.440 --> 16:37.720
parte de idiomas voy a necesitar reglas de transferencia entonces voy a necesitar tener

16:37.720 --> 16:43.920
en total n por n menos un set de transferencia yo tengo 20 idiomas voy a necesitar 380 conjuntos

16:43.920 --> 16:48.320
de reglas de transferencia y esos conjuntos de referencia son largos son grandes son complejos

16:48.320 --> 16:53.640
hay que mantenerlos pueden tener errores entonces esto claramente no escala es como muy difícil

16:54.080 --> 16:58.480
poder mantener un entorno de todos esos idiomas y poder mantener la traducción en base a reglas

16:58.480 --> 17:05.800
entonces la idea del interlingua es decir qué tal si pudiéramos parsear lo suficiente o analizarlo

17:05.800 --> 17:10.000
lo suficiente como para llevar a una representación común una representación que capture el

17:10.000 --> 17:14.520
significado de todos los idiomas a la vez y además tuviéramos un generador para cada uno de los

17:14.520 --> 17:20.640
idiomas si eso pasara si nosotros pudiéramos capturar con una representación el significado de

17:20.640 --> 17:25.600
los idiomas a la vez no necesitaríamos transferencia simplemente parseamos y llevamos a esa interlingua

17:25.600 --> 17:32.840
y después generamos en el otro idioma esto está muy bien digamos del punto de vista ideal pero es

17:32.840 --> 17:38.720
muy difícil de obtener en la práctica que se podría usar como representación de interlingua que

17:38.720 --> 17:43.520
podría ser un candidato bueno podríamos usar la lógica de primer orden que era lo que veíamos

17:43.520 --> 17:47.680
en las primeras clases de semántica como representar veraciones en los primer orden o alguna de sus

17:47.680 --> 17:52.320
variantes que dan cuenta mejor de lo que es la lógica del lenguaje natural como la mínima

17:52.320 --> 17:57.120
recurso semánticos o la whole semántics o si no algo más parecido lo que veíamos en la clase

17:57.120 --> 18:01.880
anterior de frames construirme frames con el estado de las cosas como por ejemplo esta era la

18:01.880 --> 18:06.160
misma oración de hoy mary didn't slap de green wish pero escrita como un frame es hay un evento de

18:06.160 --> 18:12.480
slapping el agente es mary ocurre en pasado la polaridad negativa el tema de ese evento es la

18:12.480 --> 18:16.640
bruja y la bruja de más es verde yo podría construirme este tipo de frames y usarlos como

18:16.640 --> 18:26.320
representaciones pero bueno el problema que tiene crear o pensar en crear una interlingua es que

18:26.320 --> 18:31.400
esa interlingua seguro que va a ser muy compleja y seguro que va a tener que modelar las características

18:31.400 --> 18:37.640
de todos los idiomas al mismo tiempo y hay características que son complicadas en los

18:37.640 --> 18:44.000
distintos idiomas y algunas que ni nos ni nos imaginamos o sea por ejemplo en chino existen

18:44.000 --> 18:47.280
palabras distintas para decir hermano mayor y hermano menor y no hay una palabra para decir

18:47.280 --> 18:51.840
hermano o sea no hay una palabra que quiere decir solamente hermano en español si y en inglés

18:51.840 --> 18:56.640
también en inglés puede decir brother pero en chino no en chino tenés que elegir cuando vas a decir

18:56.640 --> 19:01.560
hermano si es hermano mayor o hermano menor entonces imagínense que si yo estoy traduciendo del español

19:01.560 --> 19:07.720
al inglés y estoy utilizando una interlingua la interlingua en su parcer necesita poder distinguir

19:07.720 --> 19:11.600
en algún momento si estoy hablando de un hermano mayor o un hermano menor porque tiene que lograr

19:11.600 --> 19:16.360
la representación suficiente como para poder traducir al chino entonces necesita esa información y

19:16.360 --> 19:20.000
no sé dónde la va a sacar la puede sacar de contexto lo puede sacar inventar de algún lado

19:20.000 --> 19:24.480
pero en algún momento va a tener que averiguar el hermano que se está hablando en español si es un

19:24.480 --> 19:28.680
hermano mayor o menor como para poder tener la representación y después de información se va

19:28.680 --> 19:33.320
a perder porque cuando baja de vuelta al lado del inglés de vuelta vuelve a ser brother y no importa si

19:33.320 --> 19:38.600
es mayor o menor y esto solamente un caso de un fenómeno que ocurre en chino pero digamos imagínense

19:38.600 --> 19:43.600
los fenómenos que ocurren en el idioma en en en todo el tiempo digamos y todas las pequeñas

19:43.600 --> 19:50.120
variantes que hay y como en realidad no es cierto que podamos traducir exactamente los mismos conceptos

19:50.120 --> 19:53.920
como que es muy difícil encontrar conceptos que se correspondan 100 por ciento de un idioma y otro

19:53.920 --> 19:58.360
hay una cosa que llama el principio de incertidumbre la traducción y dice eso que en realidad cuando

19:58.360 --> 20:02.520
yo tengo un idioma y otro los conceptos no siempre se van a traducir 100 por ciento bien o sea no

20:02.520 --> 20:07.400
siempre la traducción es exacta sino que hay cierto solopamiento y a veces va a funcionar y a veces

20:07.400 --> 20:16.600
no bien pero a pesar de que es una utopía tener una interlingua que funcione para todo para todos

20:16.600 --> 20:21.240
los lenguajes bien este tipo de tecnología si se utilizan para dominios más acotados para dominios

20:21.240 --> 20:26.960
pequeños como por ejemplo el de meteorología yo puedo escribir perfectamente puedo construir una

20:26.960 --> 20:30.600
representación de todos los estados meteorológicos que hay si hay viento si hay lluvias y nieve hacia

20:30.600 --> 20:36.080
y granizo la temperatura la presión etcétera y traducir los distintos las distintas palabras

20:36.080 --> 20:40.440
que se usan los distintos idiomas para dar cuenta de estos conceptos entonces ese dominio acotado

20:40.440 --> 20:46.600
es bastante bien manejable con una interlingua y otro ejemplo son los manuales técnicos hay empresas

20:46.600 --> 20:52.480
que tienen un montón de documentación técnica o describen las apis de sus productos etcétera y

20:52.480 --> 20:57.800
uno suele dar cuando cuando mira la página web digamos que aparece como que con su fijo es porque

20:57.800 --> 21:01.680
está en español pero si se lo cambia por n automáticamente te genera otra página exactamente

21:01.680 --> 21:05.760
igual pero en inglés en realidad lo que hacen es como mantener una representación abstracta de

21:05.760 --> 21:08.200
lo que están escribiendo y generarla en los distintos idiomas

21:11.400 --> 21:15.240
bien entonces hasta ahí lo que vimos era como un paneo de lo que son los distintos

21:15.240 --> 21:20.080
sistemas basados en reglas ahora vamos a pasar a hablar de lo que es la traducción estadística

21:20.080 --> 21:25.840
que es el estado del arte hoy en día y vamos a empezar con un ejemplo un ejemplo de una frase

21:25.840 --> 21:32.160
en hebreo que es adona y roi que la traducción sería el señor es mi pastor o del or y es

21:32.160 --> 21:39.200
my shepherd y esta frase en realidad funciona bien porque nosotros conocemos que son las ovejas

21:39.200 --> 21:44.160
digamos la cultura en la que surgió esta frase conocía que eran las ovejas tenían pastores

21:44.160 --> 21:47.800
los pastores este cuidaban las ovejas la llevaban a donde estaban los mejores pastos etcétera

21:47.800 --> 21:54.600
entonces esta esta metáfora funcionaba bien digamos la gente describía como se sentía

21:54.600 --> 22:00.720
en respecto a dios utilizando esta metáfora pero qué tal si quisiéramos expresar esta misma frase

22:01.200 --> 22:06.280
a una cultura que no conoce a las ovejas como por ejemplo los primeros misioneros que vendrían

22:06.280 --> 22:13.120
de europa y tendrían contacto con los indígenas americanos no conocían ovejas entonces cómo

22:13.120 --> 22:20.720
hacemos para expresarles el concepto de adona y roi una forma de expresarlo es decir bueno

22:20.720 --> 22:26.200
traduzco la metáfora el significado de la metáfora digo significa el señor me cuidará que en

22:26.200 --> 22:31.680
definitiva es un poco la metáfora quiere decir eso aunque pierda un poco del contenido o si no lo

22:31.680 --> 22:37.120
que lo otro que puedo hacer es tratar de ser más fiel al significado original y tratar de traducirlo

22:37.120 --> 22:41.320
más literalmente y decir bueno el señor será para mí como un hombre que cuida de animales que tiene

22:41.320 --> 22:49.560
el pelo como algodón que es bastante más fiel al original pero sin embargo se entiende mucho menos

22:49.560 --> 22:56.360
como que te van a mirar y decirte qué me estás hablando y bueno un poco este es el problema que

22:56.360 --> 23:02.920
hay que se enfrentan los traductores humanos todos los días o sea es muy difícil tener las dos

23:02.920 --> 23:09.360
cosas ser fiel al original y sonar natural que suene bien en el lenguaje destino una traducción

23:09.360 --> 23:14.360
queremos que tenga esas dos propiedades pero muy difícil lograrlo a la vez entonces los traductores

23:14.360 --> 23:19.320
humanos saben que esto es imposible en la práctica lo que hacen es tratar de traducir de manera

23:19.320 --> 23:23.960
de encontrar un punto intermedio en el cual bueno suene bastante bien pero además sea fiel al

23:23.960 --> 23:32.120
significado original entonces esto significa que lo que estamos tratando de hacer al traducir es que

23:32.120 --> 23:38.360
estamos tratando de maximizar dos cosas a la vez como dos medidas que queremos maximizar una medida

23:38.360 --> 23:42.960
es que tan fiel es mi oración traducida a la oración original a esa medida le vamos a llamar

23:42.960 --> 23:50.840
adecuación o fidelidad y en inglés es adecuación fidelidad y la otra medida es que tan natural suena

23:50.840 --> 23:55.360
la oración que yo traduje en el lenguaje destino y a esa medida le voy a llamar fluidez o en inglés

23:55.360 --> 24:03.480
fluency entonces esta idea de que estoy tratando de maximizar dos medidas a la vez después vamos a

24:03.480 --> 24:06.720
ver que en realidad lo que vamos a total maximizar es el producto de las dos medidas porque eso

24:06.720 --> 24:13.440
significa maximizar ambas al mismo tiempo es una idea que sirve para poder inferir o para poder

24:13.440 --> 24:17.880
construir mecanismos para crear los traductores automáticos y también mecanismo para testarlos

24:17.880 --> 24:25.120
y vamos a ver un poco cómo es que funciona eso yo voy a intentar traducir a partir de ahora del

24:25.120 --> 24:29.000
resto de la clase y la clase que viene vamos a hablar siempre de que voy a traducir de un lenguaje

24:29.000 --> 24:35.160
origen f a un lenguaje destino e vamos a ponerlo acá si no nos olvidamos

24:41.600 --> 24:43.480
f es el lenguaje origen

24:48.480 --> 24:50.520
y es el lenguaje destino

24:51.000 --> 25:00.400
esos nombres surgen porque el paper inicial en donde se empezó a hablar de estas cosas

25:00.400 --> 25:03.720
de los métodos estadísticos traducía del francés al inglés entonces sacó los nombres

25:03.720 --> 25:10.200
ahí dijo en francés f el inglés e entonces traducimos del origen al destino bueno yo quiero

25:10.200 --> 25:16.600
traducir una frase del idioma f a otra frase del idioma e lo que quiero tratar de encontrar es el mejor

25:16.600 --> 25:23.000
etecho que maximice a la vez la adecuación y la fluidez o sea de todos los e posibles del lenguaje

25:23.000 --> 25:28.640
destino quiero encontrar el que maximice la fluidez de o sea que suene natural y además la

25:28.640 --> 25:37.440
adecuación entre la oración origen f y ese que estoy buscando esto esta fórmula así escrita

25:37.440 --> 25:41.440
de esa manera estás a acordar algo que hayamos visto ya en el curso en algún momento les suena

25:41.440 --> 25:44.560
a algún lado entropía si

25:47.280 --> 25:52.600
valles si o sea viene por ese lado se parece al modelo de valles porque esto es otra aplicación

25:52.600 --> 25:56.120
del modelo de canal ruidoso el modelo de canal ruidoso lo habíamos visto en el curso cuando

25:56.120 --> 26:00.960
vimos correcciones de errores hace ya bastante tiempo y también es una aplicación de lo que es la

26:00.960 --> 26:07.280
regla de valles entonces el modelo de canal ruidoso aplicado acá funciona de la siguiente manera yo

26:07.280 --> 26:13.640
tengo una oración origen en el lenguaje f que es f chica que tiene m palabras y es bueno f sub 1

26:13.640 --> 26:19.920
f sub 2 hasta f sub m y quiero encontrar la mejor oración en el lenguaje destino etecho que es

26:19.920 --> 26:26.440
es sub 1 hasta vez es su bene hasta es su bene que maximiza y en realidad lo que yo quiero maximizar

26:26.440 --> 26:32.400
originalmente como todos esperaríamos es decir bueno yo quiero encontrar la oración e que maximice

26:32.400 --> 26:36.880
la probabilidad de e dado f digamos eso es lo que uno se le ocurriría primero diría bueno yo quiero

26:36.880 --> 26:41.480
estoy traduciendo la oración f quiero encontrar la e que me de máximo la probabilidad de e dado

26:41.480 --> 26:47.520
f bien pero en realidad yo esto lo puedo descomponer por valles digamos y por definición de probabilidad

26:47.520 --> 26:52.600
condicional puede decir que la probabilidad de e dado f es igual a la probabilidad de f dado e por la

26:52.600 --> 26:59.360
probabilidad de divido la probabilidad de f digamos esa equivalencia es directa por definición de

26:59.360 --> 27:05.120
probabilidad condicional y además como estoy maximizando en e esta f se mantiene constante porque

27:05.120 --> 27:11.480
lo que voy variando es la e entonces la tacho o sea maximizar sobre una constante no no hace ningún

27:11.480 --> 27:18.960
cambio entonces lo que me queda el final es que yo busco un etecho que es el e que hace máximo la

27:18.960 --> 27:26.160
probabilidad de f dado e por la probabilidad de y eso que tenemos escrito ahí se parece mucho a la

27:26.160 --> 27:32.680
otra ecuación que teníamos antes digamos se parece mucho a esta ecuación de adecuación de f e y

27:32.680 --> 27:43.080
fluidez de entonces esto se conoce como la ecuación fundamental de la traducción automática estadística

27:43.080 --> 27:49.640
la vamos a ver unas cuantas veces en estas dos clases la vamos a estar refrescando y funciona

27:49.640 --> 27:55.200
de la siguiente manera yo quiero encontrar el etecho que es el e que maximiza el producto de estas

27:55.200 --> 28:00.480
dos probabilidades la primera probabilidad pdf dado e es la que se encarga de medir qué tal la

28:00.480 --> 28:05.960
adecuación digamos de la frase que tan adecuada es la frase f para la frase e la segunda probabilidad

28:05.960 --> 28:12.600
la pd es la que se encarga de la fluidez que tan natural suena esa frase en el lenguaje destino

28:12.600 --> 28:18.080
y se calculan con modelos distintos la primera se calcula con lo que se conoce como modelo de traducción

28:18.080 --> 28:22.440
y la segunda con lo que se conoce como modelo de lenguaje de hecho los modelos del lenguaje ya

28:22.440 --> 28:28.960
los hemos visto en el curso vamos a dar un breve repaso de qué se trataba bueno porque esto es

28:28.960 --> 28:34.480
una aplicación de canal ruidoso es una aplicación de canal ruidoso por lo siguiente nosotros estamos

28:34.480 --> 28:40.320
tratando de traducir del lenguaje f efe la lenguaje origen al lenguaje que es el lenguaje destino y lo

28:40.320 --> 28:45.240
estamos pensando al revés estamos pensando como que alguien emitió los sonidos de la oración e

28:45.240 --> 28:50.400
la oración del lenguaje destino eso pasó a través de un canal ruidoso y cuando llegó hasta mí yo

28:50.400 --> 28:55.480
escuché los sonidos de la oración efe estoy pensando como esa especie de metáfora alguien emitió

28:55.480 --> 29:00.120
e pasó por un canal ruidoso y llegaron los ruidos de efe entonces lo que yo trato de hacer como

29:00.120 --> 29:05.120
proceso de traducción es encontrar cuál tiene que haber sido esa e original para que yo haya

29:05.120 --> 29:10.280
escuchado la efe cuál es la e original que me da probabilidad máxima de que yo haya escuchado esta efe

29:13.000 --> 29:17.920
y bueno por eso es una aplicación de canal ruidoso y bueno la realidad es que en realidad

29:17.920 --> 29:22.600
damos vuelta esta probabilidad porque nos da toda otra forma de calcular lo que no podríamos

29:22.600 --> 29:27.880
hacerlo si calculamos la probabilidad directa es como que hay mejores herramientas para hacer eso

29:27.880 --> 29:32.320
bueno de vuelta esto es la ecuación fundamental de la traducción automática estadística e

29:32.320 --> 29:39.200
techo es el argumento que hace máximo la probabilidad de efe dado e por la probabilidad e y para poder

29:39.200 --> 29:44.160
resolver esta ecuación necesitamos tres cosas necesitamos un modelo de lenguaje pde que es el

29:44.160 --> 29:52.000
que se va a encargar de la fluidez esto se calcula mediante la técnica de negramas en general

29:53.720 --> 29:57.600
los engramas son bastante fáciles de construir digamos porque yo necesito texto en un solo

29:57.600 --> 30:05.880
idioma solo en el idioma destino pdf dado e es la componente que se encarga de la adecuación y

30:05.880 --> 30:09.640
se resuelve mediante el modelo de traducción el modelo de traducción no es tan fácil de

30:09.640 --> 30:12.880
construir como el modelo de lenguaje porque para el modelo de traducción voy a necesitar

30:12.880 --> 30:17.280
texto bilingüe de hecho voy a necesitar un corpus paralelo que sea texto en dos idiomas que además

30:17.280 --> 30:22.960
tengan su correspondencia y además necesito una tercera componente esta tercera componente se

30:22.960 --> 30:28.320
llama decodificador y se trata de lo siguiente yo cuando estoy buscando cuando se resuelve esta

30:28.320 --> 30:34.000
ecuación yo veo la oración efe y quiero buscar la mejor e que maximice esa ecuación pero en

30:34.000 --> 30:38.400
realidad lo que tendría que hacer es probar con todas las oraciones e del idioma destino todas las

30:38.400 --> 30:44.720
oraciones posibles que cuántas son las oraciones del idioma estino son infinitas oraciones posibles

30:44.720 --> 30:48.680
en el idioma estino entonces yo estaría probando con infinitas oraciones hasta que una de ellas me

30:48.680 --> 30:52.840
dé el máximo obviamente esto no es un problema tratable yo no puedo probar con infinitas oraciones

30:52.840 --> 30:58.080
lo que necesito es un proceso que me limites a cantidad de búsqueda de infinitas oraciones a

30:58.080 --> 31:04.240
algo tratable entonces el decodificador va a ser un algoritmo de búsqueda que va a agarrar la oración

31:04.240 --> 31:11.040
en origen y va me va a devolver las 100 200 mil oraciones destino candidatas más probable que

31:11.040 --> 31:16.560
a veces lo ocurra para que yo pueda resolver y calcular esa ecuación para esas para esas oraciones

31:16.560 --> 31:21.440
en vez de para todas las posibles entonces lo que hace es volver este problema tratable vamos a ver

31:21.440 --> 31:30.480
también un algoritmo de codificación que se llama beam search bueno entonces un poco más sobre

31:30.480 --> 31:35.120
modelos de lenguaje la componente pde de la ecuación era la que medida la fluidez y se

31:35.120 --> 31:39.000
calculaba mediante un modelo de lenguaje los modelos de lenguaje son relativamente fáciles de

31:39.000 --> 31:43.440
construir porque necesitamos información monolingua información solamente en el lenguaje destino

31:43.440 --> 31:49.840
entonces en la web tenemos montón toneladas información de muchos idiomas entonces como

31:49.840 --> 31:55.480
solo necesitamos información idiomas sacamos texto web noticias blogs etcétera y compilamos un gran

31:55.480 --> 32:01.640
corpus del lenguaje destino los modelos que se utilizan para traducción automática en general

32:01.640 --> 32:06.200
son modelos basados en engramas que ya hemos visto en el curso cómo funcionaban se suele usar orden

32:06.200 --> 32:12.880
de 4 o 5 en otras tareas de pdn se suelen usar órdenes más chicos pero para acá da buenos

32:12.880 --> 32:18.120
resultados en 4 o 5 y bueno lo importante es tener una gran cantidad de material de entrenamiento o

32:18.120 --> 32:24.360
sea los mejores modelos que usan google translate y otras empresas usan trillones de palabras y bueno

32:24.360 --> 32:29.800
son necesitan hardware especial especialmente diseñado para poder ir rápido y recuperar la

32:29.800 --> 32:35.040
información o si no bueno si estoy hablando un dominio acotado puedo usar datos de dominio para

32:35.040 --> 32:42.800
entrenar que también va a ser buenos resultados las técnicas de mutin es cuando vos haya alguna

32:42.800 --> 32:47.360
engrama que no viste lo que te va a pasar es que la probabilidad cero y ahí te va a dar todo cero

32:47.360 --> 32:51.360
en realidad las mejores técnicas de mutin significa darle una buena probabilidad a eso a pesar de que

32:51.360 --> 32:57.360
nunca lo has visto se dice que las mejores mejoras digamos las más grandes mejoras en los modelos

32:57.360 --> 33:01.120
en la traducción automática de los últimos años se han dado porque hay mejor en modelo

33:01.120 --> 33:07.320
el lenguaje que me dan traducciones que son más fluidas y y bueno y usualmente hay como

33:07.320 --> 33:12.840
cierta cierta correlación o cierta inclinación hacia las fluidez la gente prefiere cuando las

33:12.840 --> 33:18.480
oraciones son sonan más naturales acá un ejemplo esto era sacado un sistema de traducción del

33:18.480 --> 33:24.400
chino al inglés el sistema estadístico basado en sintaxis que cuando no utilizaba modelo

33:24.400 --> 33:30.120
lenguaje tenía un puntaje de 25 con 2 al incorporar modelo lenguaje subió como un 20

33:30.120 --> 33:36.120
por ciento su su performance y llegó a 31 con 2 como 6 puntos esos puntos corresponden a una

33:36.120 --> 33:40.040
medida que vamos a ver dentro un rato que se llama medida blu que es una medida muy utilizada en lo

33:40.040 --> 33:47.880
que es traducción estadística traducción automática en general pero bueno ahora solamente

33:47.880 --> 33:54.720
saber que 6 puntos es una mejora que es muchísimo y como es que mejora esto mejora haciendo que

33:54.720 --> 33:59.320
las traducciones que devuelve en general sean más fluidas son más natural en el lenguaje

33:59.320 --> 34:04.000
destino y acá hay un ejemplo de traducciones de ese mismo sistema yo tenía una traducción de

34:04.000 --> 34:09.760
referencia que era I don't have enough money with me to buy a new airplane ticket el sistema sin el

34:09.760 --> 34:14.680
modelo lenguaje devolvía esta traducción decía don't have enough bag on me change please go a

34:14.680 --> 34:20.600
new by plane que no nos entiende mucho que lo que dice no es gramatical pero al agregar el modelo

34:20.600 --> 34:25.800
de traducción su traducción es la siguiente I have enough money to buy a new one by air que

34:25.800 --> 34:36.320
suena mucho mejor que les parece acerca del significado el significado se lo puesto digamos

34:36.320 --> 34:40.080
acá está diciendo que tiene suficiente plata para comprar uno por aire y acá dice que no tiene

34:40.080 --> 34:45.280
suficiente plata para comprar un pasaje de avión o sea este suena muchísimo mejor porque

34:45.280 --> 34:49.040
está ni siquiera gramatical pero está por lo menos mantenía la negación digamos mantenía que

34:49.040 --> 34:55.160
era una oración negativa entonces hay cuidado con esto la traducción suena mucho mejor pero

34:55.160 --> 34:58.920
a veces podemos estar sacrificando fidelidad sacrificando adecuación de la traducción

35:02.120 --> 35:06.640
bien esos son los modelos de lenguaje ahora pasemos a la otra los modelos de traducción

35:07.280 --> 35:14.640
la componente pdf dado de la ecuación mide lo que es la ecuación o fidelidad de una traducción

35:14.640 --> 35:20.680
y la otra y para esto necesito corpus paralelos o corpus bilingües que para poder entrenar

35:20.680 --> 35:24.120
estos modelos los corpus bilingües son bastante más difíciles de construir que los corpus

35:24.120 --> 35:28.800
monolingües digamos no alcanza con hacer una pasada por la web y obtener texto de un idioma

35:28.800 --> 35:36.040
y bueno los modelos que vamos a ver son los propuestos por brown brown y su equipo en 1993

35:36.040 --> 35:40.400
que trabajan en ibm ellos construyeron cinco modelos de cómo construir cinco modelos digamos

35:40.400 --> 35:45.760
en creciente complejidad de cómo construir un modelo de traducción para traducción estadística

35:47.440 --> 35:51.920
y bueno los modelos la diferencia entre cada modelo se es en la historia de generación de

35:51.920 --> 35:56.080
las de las oraciones candidatas y bueno después vamos a ver también otro modelo un poco más

35:56.080 --> 36:02.040
moderno pero bueno vamos a empezar viendo más bien los modelos de brown a qué me refiero con

36:02.120 --> 36:07.960
historia de generación de las oraciones candidatas una historia de generación esto lo digo ahora

36:07.960 --> 36:11.120
pero en realidad lo vamos a profundizar después una historia de generación en realidad es como

36:11.120 --> 36:15.360
una especie de proceso mental que seguiría un traductor cuando quiere pasar de una oración a la

36:15.360 --> 36:20.880
otra entonces estas historias se basan en decir bueno un traductor agarra una oración en el idioma

36:20.880 --> 36:26.560
origen y después elige la cantidad de palabras que voy a tener el idioma destino reordena palabras

36:26.560 --> 36:30.880
después va traduciendo una a una según un diccionario después agrega palabras nuevas que

36:30.880 --> 36:35.880
no estaban en la oración ese tipo de cosas digamos el tipo de pasos me lo voy a escribir en

36:35.880 --> 36:40.560
la historia de generación y para qué sirve eso sirve para que a cada uno de esos pasos yo le

36:40.560 --> 36:45.000
puedo dar un valor numérico un valor en cuanto a probabilidades y después lo que voy a hacer

36:45.000 --> 36:49.600
cuando entreno mi sistema es tunear esos valores numéricos tunear todas esas probabilidades para

36:49.600 --> 36:54.720
darme el cálculo de probabilidad total vamos a profundizar más de en esto después

36:56.440 --> 37:00.040
pero antes de pasar a lo que son los modelos de traducción vamos a hablar un poco de cómo

37:00.040 --> 37:05.440
se evaluan estos sistemas en general siempre es importante evaluar todo en el pln digamos porque

37:05.440 --> 37:11.000
no hay soluciones perfectas entonces voy a tener sistemas que andan mejor o peor que otros y bueno

37:11.000 --> 37:16.320
y la traducción automática obviamente no es la excepción entonces me sirve poder evaluar los

37:16.320 --> 37:19.600
sistemas para poder saber qué sistema mejor que el otro y además si yo hago cambios en mi

37:19.600 --> 37:24.680
sistema poder evaluar de vuelta a ver si mejoré o no entonces qué puedo considerar una buena

37:24.720 --> 37:28.280
traducción para empezar eso es una pregunta que es abierto en su

37:31.440 --> 37:36.160
digamos es abierto en su respuesta no o sea yo tenía en un sistema de traducción tenía una

37:36.160 --> 37:40.600
referencia un candidato de referencia que era de katsat on the mat digamos esa era una traducción

37:40.600 --> 37:47.360
de referencia y un sistema me dio seis posibles candidatos para esa traducción o sea originalmente

37:47.360 --> 37:53.480
había una frase por ejemplo en chino la traducción de referencia de katsat on the mat y mi sistema

37:53.480 --> 37:59.200
a traducir el chino me dio estas opciones tengo de katsat on mat de on the mat de cat de cat on

37:59.200 --> 38:04.920
the floor a katsat on the mat de katsat on the mat con minúscula o de katsat on the straw mat

38:06.920 --> 38:11.640
cuáles les parecen que son buenas traducciones de estos candidatos que me dio el sistema

38:11.640 --> 38:19.440
cuáles les gustan más la e que es de katsat on the mat pero con minúscula en vez de comayúscula

38:19.440 --> 38:27.400
que otra la b on the mat de cat que otra la de les gusta también a katsat on the mat

38:34.120 --> 38:38.760
capaz que no calienta tanto dependiendo del uso que le vas a dar esa frase en contexto capaz que no

38:38.760 --> 38:43.840
calienta tanto y bueno si la verdad no se ve nada cuando están las cosas marcadas en rojo pero bueno

38:43.840 --> 38:50.640
en fin créanme acá la cosa macaza en rojo son las que acaban de decir una buena traducción podemos

38:50.640 --> 38:53.800
decir que es una traducción que le gusta a la gente que la gente dice si es una buena traducción

38:53.800 --> 39:00.200
entonces acá se elige on the mat sat de cat a katsat on the mat y de katsat on the mat en

39:00.200 --> 39:05.960
minúscula y bueno como como decimos es le preguntamos a la gente a ver qué traducciones le gustan y

39:05.960 --> 39:11.120
bueno y ahí ponemos cuáles son las mejores traducciones o si no le damos a un conjunto de

39:11.120 --> 39:16.600
jurados las traducciones y le decimos que hagan un análisis un poco más preciso y nos digan bueno

39:16.600 --> 39:20.560
cuánto le dan en uno al diez de adecuación y cuánto le dan en uno al diez de fluidez

39:23.560 --> 39:29.600
esa es otra forma de valor digamos y ahí ya nos están dando las dos medidas en general a los

39:29.600 --> 39:33.760
humanos nos cuesta realizar esta evaluación en general tenemos una preferencia de la fluidez

39:33.760 --> 39:40.200
como pasaba hoy con el caso de traducción del chino al inglés por los pasajes de avión

39:41.120 --> 39:45.880
además la gente no se pone de acuerdo además hay un problema que es que hacer este tipo de

39:45.880 --> 39:50.040
evaluaciones con usuarios humanos lleva tiempo digamos hay que pagarles a los usuarios por

39:50.040 --> 39:55.760
hora para que estén evaluando sistemas y después yo les di un conjunto de traducciones ellos me

39:55.760 --> 40:00.440
las evaluaron y si hay un cambio en mi sistema para mejorarlo y de vueltas le tengo que darle

40:00.440 --> 40:04.240
conjunto de traducciones a los humanos y de vuelta lo tienen que evaluar y de vuelta tengo que pagar

40:05.480 --> 40:10.200
horas de usuarios humanos para que lo evaluen entonces es difícil de reutilizar yo estar

40:10.200 --> 40:14.760
haciendo cambios constantemente en mi sistema y bueno y necesito tener una forma más rápida de

40:14.760 --> 40:19.560
evaluar a ver si estoy haciendo las cosas mejor entonces como este proceso de evaluación es largo

40:19.560 --> 40:24.080
es engorroso es caro lo que se ha vuelto más popular son los métodos automáticos de evaluación

40:24.080 --> 40:30.920
y a continuación vamos a ver uno que es muy utilizado en lo que es la traducción automática

40:33.640 --> 40:39.560
bueno cómo funciona un método de evaluación en realidad lo que hace alguien alguien que

40:39.560 --> 40:47.000
está diseñando un sistema es crearse un conjunto de oraciones con una traducción cada

40:47.000 --> 40:50.640
uno con una traducción de referencia que está bien digamos una traducción hecha a mano entonces

40:50.640 --> 40:55.000
yo quiero evaluar un sistema que va del español al inglés lo que tengo es un conjunto de oraciones

40:55.000 --> 41:00.360
en español y alguien algún traductor humano me tradujo todas esas oraciones en español y

41:00.360 --> 41:05.200
me dio un candidato o más candidato tal vez para cada una digamos a eso le voy a llamar referencias

41:05.200 --> 41:09.800
traducciones de referencia lo siguiente que tengo que hacer es poder diseñar una métrica de

41:09.800 --> 41:15.040
similitud para que cuando mi sistema me da un candidato a traducción yo puedo establecer una

41:15.040 --> 41:19.160
similitud entre ese candidato y alguna de las referencias y bueno después lo que voy a hacer

41:19.160 --> 41:25.760
es aplicar esa métrica para los pares candidato y referencias y bueno y sacar como un promedio de

41:25.760 --> 41:32.040
todos los valores de similitud que tengo entonces se han inventado muchos métodos de este estilo

41:32.040 --> 41:37.120
muchos métodos automáticos que vamos a ver en particular se llama blue que es este una

41:37.120 --> 41:43.040
métrica muy difundida en lo que es la traducción automática estadística y bueno primero algunas

41:43.040 --> 41:48.240
definiciones le vamos a llamar referencia a una traducción que está traducida manualmente

41:48.240 --> 41:52.520
o sea consideramos que es una oración correcta eso es una referencia y le vamos a llamar candidato

41:52.520 --> 41:58.040
a una traducción que no tiene porque estar correcta porque le tradujo el sistema automático y le

41:58.040 --> 42:03.160
vamos a llamar documento al conjunto de todas las oraciones candidatas al conjunto de todas las

42:03.160 --> 42:08.000
oraciones traducidas por el sistema que es lo que vamos a estar evaluando así que recuerden tenemos

42:08.000 --> 42:14.760
referencia candidato y documento y bueno qué es lo primero que se nos puede ocurrir hacer cuando

42:14.760 --> 42:20.880
queremos saber si un candidato es bueno para la referencia o no lo primero que podemos hacer es

42:20.880 --> 42:25.560
tratar de contar las palabras que ocurren en ambos entonces yo puedo tratar de contar

42:28.280 --> 42:33.120
palabras que ocurren en el candidato y palabras que ocurren en la referencia y ahí diría que la

42:33.120 --> 42:36.920
elección de las palabras del candidato si están las palabras del candidato si están también la

42:36.920 --> 42:41.160
referencia yo diría que eso se acerca un poco la adecuación se acerca que bueno por lo menos

42:41.160 --> 42:46.840
usó palabras que son fieles a la traducción de referencia pero si además esas palabras están

42:46.840 --> 42:51.280
usadas en el mismo orden ahí se acerca un poco más a la fluidez o sea si están usadas en ese

42:51.280 --> 42:57.600
mismo orden puede sonar tan natural como la referencia y esto se puede hacer automáticamente haciendo

42:57.600 --> 43:06.160
conteos de n gramas acá yo tengo un una referencia que es de cazad mi sistema me tenía que haber

43:06.160 --> 43:13.240
devuelto de cazad y tenía dos candidatos candidato a era de caz y el candidato b era cazad de entonces

43:13.400 --> 43:16.620
en éldogs вел extraordinary lo que puedo hacer es prevailer de n grams cuáles en gramas de los

43:16.620 --> 43:23.360
candidatos pertenecen a la referencia entonces para el caso de deidad en la en grama de pertenece

43:23.360 --> 43:28.020
la referencia en el en grama cat pertenece a referencia al en grama de cad o sea el bigama de

43:28.020 --> 43:34.460
que también pertenece a la referencia para el caso del candidato de el Unic 1999 pertenece el

43:34.460 --> 43:40.560
pertenece, el unigrama D pertenece, pero SatCat este bigrama no pertenece la referencia

43:40.560 --> 43:45.600
y CatD tampoco pertenece a la referencia. Y además el único trigrama que hay, SatCatD

43:45.600 --> 43:49.880
tampoco está en la referencia. Entonces lo que aparece a la derecha son los engramas

43:49.880 --> 43:57.000
que sí pertenecen tanto al candidato como a la referencia. Así que bueno, resumiendo,

43:57.000 --> 44:02.360
yo puedo contar la cantidad de hits de unigramas, de bigramas, de trigramas y para el candidato

44:02.360 --> 44:05.880
hace cumple que todos los unigramas que hay pertenece a la referencia, así que voy a

44:05.880 --> 44:11.440
tener dos de dos hits, para los bigramas voy a tener uno de uno, pero para el candidato

44:11.440 --> 44:17.240
B los unigramas me dan tres de tres, digamos tres hits, los bigramas no, o sea tengo dos

44:17.240 --> 44:20.680
bigramas posibles si ninguno estaba bien y los trigramas tampoco, tengo un trigrama

44:20.680 --> 44:26.440
posible y no estaba bien. Entonces por ahora parece que le va ganando de Cat, el candidato

44:26.440 --> 44:33.160
A de Cat le va ganando a SatCatD como traducción. Bien, ¿qué puedo hacer con los conteos de

44:33.160 --> 44:39.480
engramas? Lo que hago habitualmente, o sea contar engramas, contar unigramas, bigramas

44:39.480 --> 44:44.720
y gramas, se acerca un poco a lo que es la noción de una precisión de algo. Entonces

44:44.720 --> 44:48.600
lo que voy a hacer es contarlos por separado, voy a decir voy a contar todos los unigramas

44:48.600 --> 44:52.360
por un lado, todos los bigramas por otro, todos los trigramas por otro y para cada uno

44:52.360 --> 44:59.320
de esos me voy a armar una precisión. Voy a decir que tengo el candidato CSUB, digamos

44:59.320 --> 45:05.280
un candidato que voy a considerar, voy a contar los hits de orden N de CSUB, digamos los hits

45:05.280 --> 45:10.760
de unigrama de CSUB, le voy a llamar H de CSUB y voy a contar la cantidad de unigramas

45:10.760 --> 45:16.320
totales que hay, le voy a llamar T de CSUB. Pero además voy a hacer esto en vez de hacerlo

45:16.320 --> 45:21.080
para una sola oración, para un candidato y su referencia, le voy a hacer para todo

45:21.080 --> 45:26.480
el documento, voy a contar todos los unigramas que estaban en mis candidatos, voy a ver cuánto

45:26.480 --> 45:31.240
de esos estaban bien y voy a hacer esta división, entonces me va a dar que cuál es la precisión

45:31.240 --> 45:38.040
en unigramas. Que va a ser, bueno, tanta cantidad de unigramas estaban bien dividido, toda la

45:38.040 --> 45:42.880
cantidad de unigramas que genero en los candidatos. Después voy a hacer eso para bigramas, voy

45:42.880 --> 45:46.160
a contar toda la cantidad de bigramas que estaban bien, porque estaban en el candidato

45:46.160 --> 45:49.680
en la referencia, dividido toda la cantidad de bigramas que hay en el candidato. Voy

45:49.760 --> 45:53.520
a hacer lo mismo para trigramas y voy a hacer lo mismo para 4g, en general se suele llegar

45:53.520 --> 45:58.400
hasta 4, digamos en traducción automática estadística, la medida blue llega a calcular

45:58.400 --> 46:03.880
hasta 4. Entonces bueno, lo que me defino ahí es lo que se llama probabilidad de orden

46:03.880 --> 46:08.880
n, la probabilidad, perdón, precisión de orden n, la precisión para unigrama, la precisión

46:08.880 --> 46:16.080
para bigramas, la precisión para trigramas, etcétera. Bien, esta métrica que estamos

46:16.080 --> 46:20.960
construyendo es bastante fácil de engañar, en realidad yo me definí una probabilidad,

46:20.960 --> 46:24.600
por ejemplo la probabilidad de orden 1 y la puedo engañar muy fácil, porque yo me

46:24.600 --> 46:29.800
puedo construir un candidato que tiene siempre la misma palabra. Puedo decir, bueno, un candidato

46:29.800 --> 46:37.080
para la referencia de katsato nemat es el candidato DDDDD. Como yo justo le envoqué

46:37.080 --> 46:40.480
a una palabra que está en la referencia, entonces cuento los unigramas y me da que

46:40.480 --> 46:46.400
hay 6 hits de 6, a pesar de que la traducción es horrible. Entonces como hago para evitar

46:46.400 --> 46:50.960
esto, lo que se suele hacer es clipping, lo que significa que cuento cuánto es la cantidad

46:50.960 --> 46:54.840
máxima de palabras en la referencia y no permito que haya más de eso, entonces yo acá tengo

46:54.840 --> 47:01.320
hasta dos palabras D, entonces no puedo contar 6 de 6, tendría que contar máximo 2 de 6.

47:01.320 --> 47:06.200
Entonces ahí evitamos ese problema de que bueno alguien se haga el vivo y genere simplemente

47:06.200 --> 47:13.400
una sola palabra. Bien, entonces hasta ahora vimos dos cosas, calculamos la precisión

47:13.400 --> 47:18.960
de orden n, la precisión de cada uno de los unigramas o bigramas o trigramas, lo segundo

47:18.960 --> 47:22.640
que vimos es que vamos a hacer clipping para evitar pasarnos de conteo en las palabras

47:22.640 --> 47:31.600
que aparecen más de una vez. Lo tercero que pasa es, veíamos en este ejemplo de acá,

47:31.600 --> 47:37.600
acá tenemos dos candidatos de CAT y SAT-CAT-D y lo que pasaba acá era que le estaba yendo

47:37.600 --> 47:44.600
mejor a la traducción de de CAT porque tenía todos los unigramas que están en la traducción,

47:44.600 --> 47:47.960
están también en la referencia y todos los bigramas también, en cambio el candidato

47:47.960 --> 47:52.080
B no, el candidato B tiene unigramas que están pero bigramas y trigramas que no están,

47:52.080 --> 47:57.360
entonces en cuanto a precisión el candidato A va bastante mejor. ¿Por qué va bastante

47:57.360 --> 48:02.840
mejor el candidato A? Porque es un candidato que es más corto que la referencia, o sea

48:02.840 --> 48:07.600
es un candidato que tiene menos palabras. Como venimos definiendo la métrica, si yo

48:07.600 --> 48:12.800
tengo una referencia y después tengo un candidato que es justo un prefijo de la referencia,

48:12.800 --> 48:16.160
entonces va a cumplir que ese prefijo anda bien en todas las medidas de precisión porque

48:16.160 --> 48:21.040
todos los enigramas que tiene van a pertenecer a la referencia. Así que lo que hace la medida

48:21.040 --> 48:30.680
blue es penalizar ese tipo de comportamiento, penaliza los candidatos que son muy cortos

48:30.680 --> 48:37.200
para que digamos le dé menos puntaje. Entonces, ¿por qué se penalizan los candidatos cortos

48:37.200 --> 48:44.280
y no los candidatos largos? ¿Por qué les parece? Candidatos que son demasiado cortos

48:44.280 --> 48:51.000
se penalizan pero los demasiado largos no. La respuesta está en la slide, pero bueno.

48:51.000 --> 48:55.120
Se penaliza los candidatos cortos porque los candidatos largos, si yo genero un candidato

48:55.120 --> 48:58.880
que es mucho más largo que la referencia, lo que va a pasar es que ese candidato tiene

48:58.880 --> 49:03.400
enigramas, seguramente tiene enigramas que no pertenecen a la referencia. Entonces, en

49:03.400 --> 49:08.040
el conteo de precisión me va a dar un puntaje más bajo. Candidatos largos ya están penalizados

49:08.040 --> 49:13.100
por la precisión, candidatos cortos no están penalizados por la precisión. Entonces, necesito

49:13.100 --> 49:19.380
otro tipo de penalización para evitar eso. Bien, entonces, lo que vamos a dar es una cosa

49:19.380 --> 49:23.980
que se llama penalización por brevedad o brevity penalty, que es un puntaje que se

49:23.980 --> 49:29.900
le da en referencia a que tan corto es un candidato respecto a la referencia y bueno,

49:29.900 --> 49:33.020
se calcula teniendo en cuenta todo el largo del documento, todo el largo del documento

49:33.020 --> 49:38.900
traducido. Entonces acá yo defino que R' es el largo total de todas las referencias,

49:38.900 --> 49:46.660
R' es el largo total de todos los candidatos y entonces si el largo de los candidatos es

49:46.660 --> 49:51.860
mayor a largo de las referencias, no hay penalización, le pongo un 1, si el largo total de los candidatos

49:51.860 --> 49:58.620
es menor a largo de las referencias, entonces lo calculo como e a la 1 menos la división

49:58.620 --> 50:05.820
entre los largos. Esto es una definición de probabilidad exponencial, digamos, no es

50:05.820 --> 50:10.700
más que eso y en realidad lo que trata de hacer es penalizar traducciones que son

50:10.700 --> 50:16.020
muy cortas. Entonces, si yo tenía un candidato que tenía 5 palabras, mientras la referencia

50:16.020 --> 50:21.660
tenía 10, lo voy a penalizar fuertemente, le voy a dar un 0.37 de penalización. Si yo

50:21.660 --> 50:26.340
tenía un candidato que estaba que era menor pero era más cercano, entonces la penalización

50:26.340 --> 50:31.820
no es tanta de 0.78 y después si los largos son iguales o si el candidato es más largo,

50:31.820 --> 50:37.820
no penalizo nada, le doy un 1 de puntaje. Bueno, entonces la métrica Blue, que es una

50:37.820 --> 50:43.660
métrica muy usada en traducción automática, pone todos estos juntos, digamos, todos estos

50:43.660 --> 50:47.980
pedacitos que estuvimos viendo los pone juntos en un solo cálculo. Blue se calcula como

50:47.980 --> 50:56.540
la penalización por probabilidad, el brevite penalti, por e a la suma de las precisiones

50:56.540 --> 51:18.060
que se ordenen. ¿Qué palabra es ruido? Por ejemplo, Stro. Bueno, esta palabra es un

51:18.060 --> 51:22.220
unigrama que le va a dar 0 de precisión, digamos, porque no está. Además, participan

51:22.220 --> 51:25.380
un unigrama que también le va a dar mala precisión porque tampoco está el unigrama.

51:25.380 --> 51:30.300
Entonces lo que resta en realidad porque no está sumando la precisión. Acá yo tengo

51:30.300 --> 51:37.540
1, 2, 3, 4, 5, 6, 7 unigramas de los cuales 6 están bien pero hay uno que no. En cambio,

51:37.540 --> 51:42.220
en este tengo 6 unigramas de los cuales los 6 están bien. Entonces acá el hecho de agregar

51:42.220 --> 51:49.460
palabras que no están bien, que no están en la referencia ya te penaliza. La diferencia

51:49.460 --> 51:53.620
es cuando yo tengo una traducción que es más corta. Si yo diría solo de cut-sat-on,

51:53.620 --> 51:57.060
entonces ahí es más corta y no tengo forma de penalizarlo solo con la precisión. Entonces

51:57.060 --> 52:03.820
tengo el otro penalizador que es porque la traducción es muy corta. Bien, entonces,

52:03.820 --> 52:13.620
les estaba comentando. Acá. La medida Blue se define como una media geométrica, definición

52:13.620 --> 52:18.740
de media geométrica, de las precisiones de orden N. También tienes un peso por precisión

52:18.740 --> 52:24.220
que se puede variar pero en general se utiliza el mismo peso para todos. Multiplicado por

52:24.220 --> 52:35.900
la penalización por brevedad. Bien, eso. O sea, esa es la definición de la métrica

52:35.900 --> 52:41.100
Blue que es una métrica que se utiliza muchísimo. Esos puntajes que vemos hoy de 25,2 y 31 con

52:41.100 --> 52:48.660
algo eran ejemplos de métrica Blue aplicados a un sistema. Y bueno, una cosa importante,

52:48.660 --> 52:52.100
algunos comentarios importantes sobre la métrica Blue es que en general cuando un

52:52.100 --> 52:56.020
sistema le da mejor, digamos, un conjunto de traducciones le va mejor en métrica Blue,

52:56.020 --> 52:59.940
también le va mejor con un conjunto de humanos que evalúen el sistema. O sea, que tiene una

52:59.940 --> 53:05.220
correlación bastante buena con lo que es la evaluación subjetiva humana. Pero como

53:05.220 --> 53:09.900
contra, es difícil de interpretar estos puntajes. O sea, si yo tengo un puntaje de, como nos

53:09.900 --> 53:13.620
pasaba hoy, que tenía un puntaje de 31, en realidad un 31 es un número que puede ser

53:13.620 --> 53:20.540
muy bueno, muy malo, dependiendo del idioma. Pero, o sea, si todo saliera bien y yo tradujer

53:20.540 --> 53:24.780
exactamente lo mismo que están las referencias, por construcción la medida me daría uno.

53:24.780 --> 53:28.220
Pero en realidad es muy difícil traducir exactamente lo que están las referencias,

53:28.220 --> 53:35.020
porque no es cierto que exista una única traducción posible en la traducción, digamos, humana.

53:35.020 --> 53:39.020
Oraciones se pueden traducir de manera distinta y estar igualmente bien. Entonces es muy difícil

53:39.020 --> 53:42.580
tener un conjunto de referencias que contemple todas las posibilidades. Así que mi traductor,

53:42.580 --> 53:48.060
no es el papás que anda bárbaro, pero el puntaje aún no es uno, no es 100, digamos,

53:48.060 --> 53:52.580
porque está eligiendo palabras distintas o eligiendo formas de escribir las oraciones

53:52.580 --> 54:00.340
distintas. Entonces bueno, por eso es difícil interpretar. Yo tengo un puntaje blue de 30

54:00.340 --> 54:09.580
o de 50, o sea, de 0.3 o de 0.5, y puede ser buenísimo para ese sistema. Pero para algo

54:09.580 --> 54:14.020
que sí me sirve muchísimo el porcentaje, digamos, el puntaje de blue es para decir,

54:14.020 --> 54:19.580
yo tengo mi sistema, lo evaluo, después hago algunos cambios, evaluo de vuelta, y si subió

54:19.580 --> 54:22.980
la performance con el puntaje blue, entonces estoy seguro de que mejoró porque hay una

54:22.980 --> 54:25.060
correlación con la evaluación subjetiva.

54:25.060 --> 54:41.460
Para pasar el español inglés, en realidad lo que pasa es que entrenás otro traductor.

54:41.460 --> 54:47.900
No, acá estoy hablando uno solo. Acá estoy hablando solamente en un sentido. Yo tenía

54:47.900 --> 54:57.340
un sistema en español, por ejemplo, digo, una oración en español, el gato se sentó,

54:57.340 --> 55:01.820
y alguien me dijo, bueno, la traducción de referencia de eso es de cat-sat, y mi sistema

55:01.820 --> 55:06.740
me dijo, bueno, pero mis traducciones posibles son de cat y sat-cat-de. Entonces yo tenía

55:06.740 --> 55:10.180
un sistema en español, pero que traduce al inglés, digamos, un sistema de traducción

55:10.180 --> 55:16.220
de español al inglés, pero no estoy traduciendo en el otro sentido. No, no es como las canciones.

55:16.220 --> 55:20.060
Acá, partí del español y llegué al inglés, y estoy tratando de evaluar comparando las

55:20.060 --> 55:24.740
frases en inglés esperadas con las frases en inglés generadas. Claro. Probablemente…

55:24.740 --> 55:29.460
Acá está el mismo idioma, se entendí. Claro, pero está en el mismo idioma, o sea, lo

55:29.460 --> 55:33.780
que nos mostramos acá era cuál era la oración o origen, porque para evaluar no nos importa

55:33.780 --> 55:37.860
en realidad, para evaluar nos importa que comparar solamente la oración candidato con

55:37.860 --> 55:43.140
la referencia, y la origen nos olvidamos. Sabemos que los dos intentaron traducir de la misma

55:43.140 --> 55:51.220
oración, y bueno, y alguno le fue mejor que a otro. Bien, esos son comentarios de

55:51.220 --> 55:56.700
Blue, esto era evaluación de los sistemas. Lo siguiente que vamos a ver es el problema

55:56.700 --> 56:00.660
de los corpus paralelos. Antes de pasar a lo que son modelos de traducción, vamos a

56:00.660 --> 56:05.140
hablar un poco de lo que son los corpus paralelos, que son necesarios para construir un modelo

56:05.140 --> 56:11.020
de traducción. Un corpus paralelo consiste en pares de textos en dos idiomas, por ejemplo,

56:11.100 --> 56:15.660
tener textos en español y en inglés, pero además yo tengo que tener algún nivel, tengo

56:15.660 --> 56:20.060
que tener una correspondencia entre esos textos. De alguna forma, yo tengo que saber cómo

56:20.060 --> 56:26.740
se corresponde un texto con el otro. Entonces, bueno, tiene que estar con conjuntos, digamos,

56:26.740 --> 56:31.380
ordenados de textos en el lenguaje origen, en el lenguaje destino, y bueno, existen,

56:31.380 --> 56:35.660
en el mundo existen corpus paralelos para algunos idiomas, o sea, hay muchos idiomas

56:35.660 --> 56:39.780
en el mundo, pero no todos los pares de idiomas tienen corpus paralelo construido, entonces

56:39.860 --> 56:44.660
existen paralela de inglés, el chino inglés para la mayoría de los lenguajes europeos,

56:44.660 --> 56:50.380
debido a su uso en la Unión Europea, digamos, existen también corpus paralelos para ellos,

56:50.380 --> 56:56.300
pero para la gran mayoría de pares de lenguas no hay, digamos, no tengo un par que traduzca

56:56.300 --> 57:01.020
entre el chino y el guaraní, por ejemplo, o sea, es poco probable que se construya

57:01.020 --> 57:07.540
un par de estilos. Bien, ¿qué es un corpus paralelo? Ya que no se ve nada, de vuelta.

57:07.540 --> 57:14.100
Acá hay un ejemplo, que no sé si lo conocen, es un ejemplo famoso de corpus paralelo.

57:17.620 --> 57:24.260
Tiene idea de lo que es, lo han visto alguna vez, ¿sí? La piedra de Rosetta. La piedra de Rosetta

57:25.700 --> 57:33.900
fue una piedra que la construyeron, o por lo menos la tallaron en el año 196 a.C. y hablaba

57:33.900 --> 57:42.540
sobre la coronación de Tolomeo V y su adoración como semi-dios, etcétera, etcétera. Y bueno,

57:42.540 --> 57:49.500
estuvo perdida un montón de años hasta que durante las campañas napoleónicas 1799 la

57:49.500 --> 57:55.700
encontraron en Egipto, en lugar Rosetta, casualmente, y se la llevaron para Francia y ahí la empezaron

57:55.700 --> 58:00.260
a analizar lingüistas, empezaron a tratar de entender qué es lo que decía. Y bueno, descubrieron

58:00.260 --> 58:07.140
que tiene tres textos, vieron que tiene como tres regiones, tres textos y después de estudiarla un

58:07.140 --> 58:12.420
rato se dieron cuenta que en realidad lo que tiene es el mismo texto en tres idiomas distintos. Y los

58:12.420 --> 58:16.860
idiomas eran, el de arriba eran jeroglíficos egipcios, del estilo de lo que uno encuentra dentro de las

58:16.860 --> 58:22.740
pirámides, el del medio era egipcio demótico, que era el egipcio vulgar que se usaba digamos en el

58:22.740 --> 58:28.780
día a día, y el de abajo el todo era griego antiguo. Entonces, si bien ninguno de los tres idiomas se

58:28.820 --> 58:35.460
hablaban, el momento que se encontró la piedra, los tres idiomas antiguos, el griego antiguo por lo

58:35.460 --> 58:40.140
menos sí se sabía, digamos, se conocía como idioma, se sabía qué significaba y digamos, había gente

58:40.140 --> 58:44.220
que lo estudiaba, los otros dos no, los otros dos eran lenguas completamente perdidas que nadie sabía

58:44.220 --> 58:50.220
identificarlas. Pero gracias al hecho de que en realidad se descubrió que los tres textos hablan de

58:50.220 --> 58:55.300
lo mismo, son el mismo texto en tres idiomas, entonces ahí se empezó a hacer un trabajo de

58:55.340 --> 58:59.860
alineación, digamos, los arqueólogos empezaron a decir, bueno, esta porción de texto acá se

58:59.860 --> 59:03.540
corresponde con esta de acá, se corresponde con esta de acá, y etcétera, y a tratar de encontrar

59:03.540 --> 59:08.660
correspondencias en los idiomas, y como sabían qué quería decir en griego antiguo, empezaron a poder

59:08.660 --> 59:13.300
descubrir qué querían decir en los otros idiomas. Entonces, a raíz de eso, es que empezó, digamos,

59:13.300 --> 59:18.740
la egiptología moderna, se pudo empezar a descifrar, que dicen, por ejemplo, los jeroglíficos están en

59:18.740 --> 59:23.900
las pirámides y bueno, un montón de cultura egipcia antiguas se conoce gracias a que se pudo descifrar

59:23.900 --> 59:28.020
lo que decía esta piedra. Y en definitiva, esto es un ejemplo de corpus paralelos, o sea, tengo el mismo

59:28.020 --> 59:35.580
texto en tres idiomas y con un poco de esfuerzo logro alinear cuáles son cada uno de los elementos

59:35.580 --> 59:43.780
de mis lenguajes y logro saber la traducción de los tres. Bueno, entonces, eso no llega al concepto

59:43.780 --> 59:49.660
de alineación, los corpus paralelos tienen distintos niveles de alineación, lo más fácil de encontrar

59:49.660 --> 59:53.180
son corpus que están alineados a nivel de documentos, yo tengo una colección de documentos en

59:53.180 --> 59:57.100
español y una colección de documentos en chino y yo sé qué documento se corresponde con qué

59:57.100 --> 01:00:03.020
otro, pero no sé nada más. Sería mejor incluso que estuvieran alineados a nivel de alineación,

01:00:03.020 --> 01:00:07.580
además de conocer los documentos, yo sé cuál es la relación en español o con cuál es la

01:00:07.580 --> 01:00:12.420
relación en chino, digamos, tengo una correspondencia entre esas dos, pero sería aún mejor y esto es lo

01:00:12.420 --> 01:00:16.660
que más nos serviría si estuvieran alineados a nivel de palabra. Cada uno de los caracteres que

01:00:16.660 --> 01:00:20.260
están en chino se corresponde con qué palabra en español o qué grupo de palabras y cada una de las

01:00:20.300 --> 01:00:25.540
palabras en español, con qué grupo de caracteres se corresponde en chino. Esto es el ideal, pero claro,

01:00:25.540 --> 01:00:30.220
o sea, si ya es difícil conseguir cosas que estén alineadas a nivel documento, se imaginan que

01:00:30.220 --> 01:00:37.660
nadie va a ir a mano alinear a nivel de palabra cada uno de las palabras de los idiomas. Entonces,

01:00:37.660 --> 01:00:42.820
en la práctica nunca vamos a encontrar un corpus alineado a nivel de palabra, pero vamos a ver que,

01:00:42.820 --> 01:00:47.900
como resultado de la construcción de los modelos de lenguaje, se produce también como un producto

01:00:47.900 --> 01:00:53.100
secundario, se produce la alineación de los corpus, entonces obtenés las dos cosas a la vez.

01:00:55.660 --> 01:01:01.820
Bueno, y otra cosa es que a diferencia del texto monolingua que yo usaba para los modelos

01:01:01.820 --> 01:01:07.420
de lenguaje, es muy raro que naturalmente se produzcan textos en dos idiomas a la vez, o sea,

01:01:07.420 --> 01:01:15.260
hay que buscarlos bastante, digamos, bastante cuidadosamente. Existen algunos contextos en

01:01:15.260 --> 01:01:19.300
donde eso se produce. Por ejemplo, en algunos portales de noticias puede pasar que tengan

01:01:19.300 --> 01:01:23.660
versiones en distintos idiomas y lo que hagan es traducir las noticias en distintos idiomas.

01:01:23.660 --> 01:01:27.540
Entonces, si yo puedo encontrar uno de esos, es una buena fuente para construirme un corpus

01:01:27.540 --> 01:01:32.260
paralelo alineado a nivel de documento. Yo sé, esta noticia se corresponde con esta otra en el otro

01:01:32.260 --> 01:01:38.660
idioma. Pero un lugar en donde se producen naturalmente este tipo de textos es en los países que

01:01:38.660 --> 01:01:44.340
son bilingües o multilingües. Por ejemplo, en Canadá, que hablan inglés y francés,

01:01:44.340 --> 01:01:49.740
las discusiones del Parlamento canadiense siempre por ley tienen que transcribirse en los dos

01:01:49.740 --> 01:01:53.540
idiomas, tienen que traducirse, si están en inglés se traducen en francés, si están en francés se

01:01:53.540 --> 01:01:59.380
traducen en inglés, y guardan una correspondencia entre eso, guardan los documentos de todas las

01:01:59.380 --> 01:02:04.020
discusiones del Parlamento en los dos idiomas. Entonces, ahí, naturalmente se produce un corpus

01:02:04.020 --> 01:02:08.340
paralelo en el nivel de documentos para el inglés y el francés, ese se conoce como el corpus

01:02:08.340 --> 01:02:14.500
Hansard. Eso también ocurre en Hong Kong, en Hong Kong se habla inglés y chino, son los idiomas

01:02:14.500 --> 01:02:18.780
oficiales. Entonces, el corpus más grande que se tiene para inglés y chino está hecho como una

01:02:18.780 --> 01:02:22.660
compilación de lo que son las discusiones del Parlamento de Hong Kong. Y también pasa en la

01:02:22.660 --> 01:02:28.780
Unión Europea, en el Parlamento Europeo también tienen la costumbre de traducir todas las discusiones

01:02:28.780 --> 01:02:32.780
a todos los idiomas o a muchos de los idiomas que se usan en la Unión Europea. Entonces,

01:02:32.940 --> 01:02:38.420
hay corpus paralelos para casi todos los idiomas de la Unión Europea. Pero claro, todos estos

01:02:38.420 --> 01:02:43.100
están alineados a nivel de documentos. Yo sé qué documento se corresponde con cuál es otro en el

01:02:43.100 --> 01:02:51.340
otro idioma, pero no a nivel de oraciones y mucho menos a nivel de palabras. Pero bueno, partiendo

01:02:51.340 --> 01:02:57.340
de un corpus alineado a nivel de documentos, yo puedo llegar a construirme por lo menos una

01:02:57.340 --> 01:03:03.900
alineación a nivel de oraciones. Si en un proceso relativamente sencillo, esto se conoce como el

01:03:03.900 --> 01:03:14.820
algoritmo de Gale y Church, que es un algoritmo relativamente fácil para alinear corpus, o sea,

01:03:14.820 --> 01:03:18.060
para pasar corpus que están alineados a nivel de documentos, pasarlos a que estén alineados a

01:03:18.060 --> 01:03:24.020
nivel de oración. Y bueno, esto es un algoritmo que funciona, está un poco basado en lo que era el

01:03:24.020 --> 01:03:29.540
algoritmo de distancia de edición de Levenstein, que vimos hace bastante tiempo en el curso.

01:03:34.620 --> 01:03:39.860
Es como muy parecido, también es un algoritmo de programación dinámica, similar a ese, funciona de

01:03:39.860 --> 01:03:43.780
la siguiente manera. O sea, no vamos a dar lo mucho en detalle, pero vamos a dar una idea de cómo

01:03:43.780 --> 01:03:49.780
es que funciona. El algoritmo de Gale y Church dice, yo voy a tener un conjunto de oraciones en un idioma

01:03:49.780 --> 01:04:01.500
y otro conjunto de oraciones en el otro idioma. Entonces considero que un traductor para cada

01:04:01.500 --> 01:04:06.700
oración pudo haber tenido tres opciones, digamos, para pasarlas al otro idioma. Un traductor,

01:04:06.700 --> 01:04:12.180
supongan un traductor humano, agarró oraciones que estaban en español y oraciones que estaban en

01:04:12.180 --> 01:04:18.660
francés. Vamos a no ponerles EIF porque lo que puede confundir con las otras cosas. Así que vamos

01:04:18.660 --> 01:04:25.860
a decir, el lenguaje origen era F, francés y el lenguaje destino era español. Bien, entonces un

01:04:25.860 --> 01:04:30.740
traductor humano cada vez que se enfrentaba una oración tenía tres posibilidades. O bien traducía

01:04:30.740 --> 01:04:37.140
una oración por otra oración, o bien parte esta oración en dos y traduce una oración por dos,

01:04:37.140 --> 01:04:42.780
o bien borra esta oración. Decide que no es tan importante y agarra y borra la oración. Entonces

01:04:42.780 --> 01:04:48.140
las tres operaciones que se hacen a nivel de oración son la de transformarla en cero, una o dos

01:04:48.140 --> 01:04:58.660
oraciones del otro lado. Eso es una cosa. Lo otro es el costo relativo de alinear estas dos oraciones

01:04:58.660 --> 01:05:03.020
depende del largo relativo de las oraciones. Entonces, si yo tengo dos oraciones que tienen un

01:05:03.020 --> 01:05:10.780
largo muy parecido, le voy a dar un costo menor para alinearlos, era menor o mayor, si menor. Si

01:05:10.780 --> 01:05:15.340
tiene un largo muy parecido le voy a dar un valor menor para alinear, si tiene un largo muy

01:05:15.380 --> 01:05:20.500
distinto, una es muy corta y la otra es muy larga, entonces le doy un valor mayor para alinear. Entonces

01:05:20.500 --> 01:05:26.740
lo que ellos hacen es pensando en todo este tipo de operaciones que hay, todas las combinaciones

01:05:26.740 --> 01:05:35.260
de operaciones posibles, o sea, partir esta operación en dos o no partirla o eliminarla o dejarla

01:05:35.260 --> 01:05:39.700
como está. Entonces, con programación dinámica ven todas las posibilidades, ven todas las posibilidades

01:05:39.700 --> 01:05:44.980
de operar distinto para llegar al otro lado y calculan las que le da un costo menor. O sea,

01:05:45.020 --> 01:05:49.780
para cada una de las posibilidades calcula cuál es el costo de cada par de oraciones,

01:05:49.780 --> 01:05:57.860
suman todos los costos del documento y se quedan con el caso que les dé un costo menor en alineación,

01:05:57.860 --> 01:06:02.300
eso se puede hacer eficientemente usando programación dinámica, lo mismo que

01:06:02.300 --> 01:06:16.300
hacíamos con la distancia de edición de Levenstein. Bueno, y este algoritmo que es relativamente

01:06:16.300 --> 01:06:22.580
sencillo, digamos, es una solución bastante simple, logra una tasa de error muy buena,

01:06:22.580 --> 01:06:27.940
que es de un 4%, digamos, sobre todo para idiomas relacionados, para idiomas que se

01:06:27.940 --> 01:06:32.580
parecen como el inglés y el español, etcétera, logra una tasa bastante baja de error de un 4%,

01:06:32.580 --> 01:06:36.460
hay algunas mejoras que se pueden hacer, pero en realidad un 4% es algo que está bastante bien.

01:06:37.460 --> 01:06:42.780
Hay un catch que es que para sistemas de traducción distintos o traducciones no literales,

01:06:42.780 --> 01:06:46.740
esto se rompe un poco, por ejemplo, para traducir entre inglés y chino, que en chino

01:06:46.740 --> 01:06:50.420
ni siquiera está claro cuáles son los límites de las palabras y eso es más difícil de ver.

01:06:50.420 --> 01:06:54.780
Entonces, bueno, este tipo de algoritmos no funcionan tan bien. Y bueno,

01:06:54.780 --> 01:06:57.980
hay variantes que funcionan un poco mejor. Así que bueno.

01:07:01.180 --> 01:07:05.300
Hoy vamos a dejar por acá y vamos a continuar la próxima con modelos de traducción.

