start	end	text
0	7000	En la clase pasado entonces lo que estuvimos viendo es fundamentalmente lo que es recuperación
7000	13120	de información como una aplicación en donde tendremos a utilizar procesamiento del
13120	23120	lenguaje natural en alguna de las tareas, que se hacen sobre todo antes o durante el proceso
23120	28440	de recuperación, los algoritmos que implementan el proceso de recuperación. Y después
28440	34160	cometamos también lo que es la extracción de información como otra disciplina diferente
34160	39240	a la recuperación que a veces entre mezclan este o se confunden y que se está hablando
39240	43680	de lo mismo y en realidad son como complementarias, ¿no? Si yo tengo un proceso de recuperación
43680	49280	de información que me recupera documentos donde se supone que está la información que
49280	55240	yo estoy buscando usuario y el proceso de extracción de información lo que hace es a partir
55240	62160	de un conjunto de documentos que se supone que son de interés extrae aquellas partes que efectivamente
62160	67320	hablan de lo que yo estoy queriendo, nada que incluso alguno comentaba que hoy se
67320	74080	pensamos en Google que solamente pone las palabras y ya te trae la porción de texto
74080	83080	donde están las palabras que vos en este ministerucán. Decíamos la extracción de información
83080	90880	es una disciplina que, típicamente, lo que hace es extraer a tributos, relaciones, el
90880	99400	perdón entidades, relaciones y eventos. Y como un mente lo que se hace es se trata de generar
99400	107520	una suerte de plantilla con pares a tributo valor donde ahí se cargan los valores de
107520	112280	los valores de los atributos, de los nombres de los atributos y el valor que tienen.
112280	119280	En función de lo que yo quiero extraer, eso genera una estructura que es, después, mucho
119280	127760	más manipulable por un usuario experto o digamos o algún sistema que después permita
127760	132680	a hacer otras cosas. Dentro de las tareas de extracción
132680	138520	de información y quedamos más o menos en eso, y este tenemos el reconocimiento de entidades
138520	144600	con nombre, la resolución de co-referencias, extracción de relaciones semánticas, entrentidades,
144600	148600	resolución y reconocimiento de expresiones temporales, asignación de relaciones semánticos
148600	157080	entre otras tareas. Lo que queríamos hoy es ver, algún ejemplo de que consiste, por ejemplo,
157080	164600	la extracción de reconocimiento de entidades con nombre.
164600	171280	Nosotros esencialmente en entidades con nombre, lo que tenemos que pensar es que,
171280	179000	típicamente, lo que uno quiere extraer, son tres grandes conjuntos. Organizaciones, personas
179000	184960	y lugares. Después uno puede seguir queriendo poniendo, poniendo el nombre de una
184960	190400	otra cosa, pero esencialmente la sentidades que tienen nombre, son algún tipo de organización,
190480	197640	algún tipo de lugar, o algún nombre de personas. Entonces, un poco acabemos el ejemplo.
197640	204840	Y vemos el ejemplo y vemos que lo que se pretenden mostrar es que ya dificultades
204840	212040	que se pueden presentar. Parcelón, autorizó, noticias viajanos, autorizó a Luis Ores,
212040	217440	a viajar el lunes a Montevideo para estar a la orden de la selección, para los partidos
217520	224240	de este herivinatorias. Pa, pa, pa. ¿Qué entidades con nombre ustedes reconoce en
224240	231280	ahí o que el sistema debería detectar? Pensamos, ¿de vuelta, ¿no? Organizaciones, lugares,
231280	247160	personas. En piecena. Luis Ores, Barcelona, Argentina, Paraguay, Montevideo, Liga española, bien, lo
247160	258400	vemos como una organización. El eliminatorias. ¿Qué sería el eliminatorias?
258400	266840	El eliminatorias, como el, el partido, el partido, el eliminatorias, vamos a ir al saludo.
266840	274720	Ve, si, bien, me interesa saberlo, pero es una organización, es una persona, es un lugar.
274720	280080	Capa que a mí se me interesa después hacer cosas, por un poco, justamente, el chiste de
280080	286000	borre conocés en día de con nombre y después lo que vas a querer reconocer, son relaciones
286000	290920	entre esas sentidades o cosas por el estilo. Pero es un paso que viene después, después de que yo
290920	295160	detecto las sentidades, en piezo jugar, en piezo, bueno, el que no trajo aquí lo hacer con esas
295160	300840	sentidades. Es como un primer paso, correcto. Capa que el eliminatorias me puede servir
300920	306240	porque quiero saber, para que, por ejemplo, para preguntar, para qué es que va, este licorio
306240	312800	es que día venir a Montevideo, porque quería venir a jugar las eliminatorias, pero eso ya
312800	315760	entra en la siguiente etapa que sería la detección de relaciones.
315760	326160	Es en dos, históricos, es un de la comunidad. Bien, es una de el medio dido, la segunda
326160	330720	guerra mundial, como lo vas a, como lo vas a encasillar, es eso.
330720	337040	Podría ser un evento, después vamos a hablar de eventos y de las dificultades de eventos.
337040	343240	Pero bueno, es algo que, como un mente uno lo que tiene, por lo menos para arrancar o podría
343240	347360	llegar a tener son listas de palabras que tienen todas las organizaciones, todos los
347360	351840	no hombres o que se yo. Ahí yo podría usar esas listas eventualmente para desambiguar y
351840	357760	segunda guerra mundial, ahí yo lo tomo, todo como una sola entidad y es, pero que es una
357760	367000	persona, es un organización, es un nombre. No, entonces, ver cómo lo categorizas. Eso es algo
367000	373560	que me va a interesar tenerlo determinado, pero no en principio no es una entidad con nombre.
373560	378880	Si viene por lado lo que dice compañero después lo de eliminatorias o las relaciones o los
378880	390100	eventos. Exacto. Bueno, ahí están, ¿no? Este barcelona, suales, está se nota más como
390100	398940	ar, vamos a usar ellos. Ahí, en negritas. Barcelona, montevide, argentina, paraguay. Bien, en
398940	404480	negritas están un poquito en las sentidades que se encontraron.
404560	410200	Después está, en contra las sentidades, tratamos de, acá, bueno, el desdito valor. El
410200	415580	cuáles son nombres, cuáles son lugares y cuáles son organizaciones. En rojo organizaciones,
415580	423700	en verde lugares y en azul nombres. No, no, no, no es de luz, no. Perfecto. Eso quería llegar,
423700	429500	qué Barcelona, Barcelona es un lugar, es una ciudad preciosa que queda allá en el, el, no de
429500	437060	no eres de España, pero no es un club. De hecho, acá está siendo referencia, un club, con
437060	441900	a la vez pasa lo mismo, bueno, en España pasa mucho, porque hay las ciudades, los equipos
441900	448540	de fútbol tienen nombre de ciudades, muchos de ellos. Entonces, acá se tenemos un problema,
448540	454620	cómo vas a, digamos, potencialmente tenemos un problema, es decir, cómo vas a tratar
454620	461300	esa entidad, como un nombre de una persona o como un nombre de lugar.
461300	471180	Entonces, lo podemos acá, como en realidad nosotros sabemos que es un club o que
471180	477580	acá en el texto está recibiendo referencia a club, lo ponemos en rojo. Pero es algo
477580	488460	que yo lo hago o lo debería hacer a posterior y de una primera reconocimiento, ok.
488460	493380	Y después está lo que interesa de bien, yo tengo la sentidad de con nombre y me puedo
493380	501500	querer, me pueden querer encontrar relaciones entre esas sentidades, cómo se combina
501500	508500	esas sentidades. Entonces, esa parece ahí, ahí, con un color medio rozadito, autorizar,
508500	514500	la organización, Barcelona, autorizó a Luis Ares, a viajar. Entonces, ahí tenemos más tenemos
514500	519860	lo autorizó a viajar, tenemos dos relaciones, o autorizar a viajar, podría tratar como uno,
519860	525260	todo depende como uno, lo interpreto, lo que quiere hacer. Y ahí aparece, no se no
525260	529900	tan mucho, porque hablamos de las tareas de tracción de información y hablamos de la sentidad
529900	535460	de con nombre. También dijimos el tema de las referencias. Fíjense acá, no sé si se nota
535460	543700	que está con otro colorcito. Pese a que el jugador no fue incluido, ¿Quién es el jugador?
543700	551700	El Luis Ares, o sea, tengo que de alguna forma también determinar que ese término
551700	558020	hace referencia en este caso del Luis Ares. Lo mismo acá, lo de Club Catalán, hace referencia
558020	564900	a Barcelona. ¿O qué estas son todas cosas, otareas, que uno hace en ese proceso de
564900	574740	extracción de entidades con nombre? Estrader, nombres, estrader relaciones, bueno, lo que
574740	578980	está diciendo. La mayor parte de los trabajos estrader en relaciones, entrentidades
578980	587020	mencionadas en la misma oración, siempre se trata uno ya debo cuando analiza con referencias,
587020	593940	el texto analizar es un poco más o puede ser un poco más largo, las referencias pueden
593940	601540	ser en esa misma oración, mas complicado es cuando la referencia está en otra oración después,
601540	612700	¿verdad? Bueno, esto es una un desafío. La mayor reedación es predeterminada, dirección
612700	619060	de la empresa, clunde jugar el jugador, etcétera. Por relaciones de más de dos argumentos,
619060	625900	donde muchas veces se habrá de extracción de eventos. Ahora vamos a hablar un poquito
625940	631940	de eventos. Entonces, relación, lo que decía es, bueno, la relación autorizar, que requiere
631940	639260	dos argumentos, a autoriza a ver. Y pues, pues, bueno, podemos agregar cuando que aquí para
639260	646420	qué el autorizo etcétera. Entonces, ahí apareció otro concepto que quizás no está puesto
646420	655420	acá, acá el evento podría ser viajar que el autorizo habíajar, no sé si se cuándo,
655460	662660	dice para qué, para estar a la orden, en fin, hay una serie de textos ahí que uno podría
662660	670740	o de presiones que uno podría quedar llegar a determinar. Se tiene entonces la idea, bien,
670740	676060	viajar a estar a la orden incluido, como decía en Marrassión, en general se procede
676060	681340	por etapas, primero en las sentidades y luego después que entencó las sentidades cuáles
681340	690820	son las relaciones. Entonces, otra cosa y otro desafío importante es lo que podríamos
690820	695580	decir la extracción de eventos. Un evento es una actividad en el mundo real que ocurre durante
695580	703680	cierto periodo de tiempo en un cierto espacio geográfico, una definición. Y para eso, yo lo
703680	710140	que tengo muchas veces tengo, alguna vez en lo puedo reconocer, puedo sacar por lo que decíamos
710140	714420	recién, por ejemplo, el evento de las eliminatorias podríamos determinar que es un evento
714420	723220	que a lo que hablábamos hoy. Pero a veces es una tarea en sí misma la la detección
723220	728620	de eventos, donde yo tengo un conjunto de también determinos o de palabras disparadoras
728620	736820	de un evento. Y por ahí me puede llegar a querer interesar encontrar. Fíjense
736820	744420	en la primera. Primero es ejemplos, una tormenta de arriba, perdón. Una tormenta de arriba
744420	750140	centenares de árboles en un video. ¿Cómo yo puedo detectar con la cada monte de video?
750140	757020	No, sería con nombre. Pero tengo algo que me indica que se dio un evento que es.
757940	768780	Tormenta. Tormenta, me da la idea de que hubo algo, pasó algo. Un motociclista
768780	776860	de 38 años, falleció en un accidente de trancito. Tal vez la palabra accidente, sea
776860	782060	el evento. También bueno, que falleció. Pero accidente es una palabra disparadora que
782060	790060	me dice, bueno, acá hay un evento. Esta es un desafío más grande que se está. Colóñe
790060	797740	Requena es una mugre. A priori, por qué va a ser un evento. Pero en realidad sí me
797740	805460	está marcando un evento de que hay un problema de limpieza en Colóñe Requena. Entonces,
805460	814060	a veces yo tengo palabras disparadoras que me ayúna de detectar eventos y a veces tengo
814060	825140	que encontrar alguna otra técnica para detectar esos eventos. De acuerdo. Bien, arquitectura
825140	833260	generica, esto es una propuesta aquí, o jobs en la década en lochenta, si más no recuerdo,
833260	839860	que plantea cuál es una arquitectura en general de un sistema de extracción de información.
839860	846500	Como bueno, parece un montón de cosas y determinos que estuvimos haciendo. Analisis
846500	851380	lexico gráfico, nos basamos en diccionarios, analisis sintáticos, reconocimientos
851380	856100	de entidades, reconociendo de patrones, siempre acá en realidad todos estos reconocimientos
856100	862340	de patrones de alguna manera. Analisis sintáticos, referencias y acabajo, lo que decíamos
862340	870140	generación de plantillas, donde se van a cargar esos altos.
870140	874420	Y lo sé enfoque para la construcción de un sistema de extracción de información, tengo
874420	883020	por un lado reglas, o por otro lado, los sistemas mediante aprendizaje automático.
883020	891060	No voy a entrar a hacer juicio de valor, yo creo que los dos sombalidos, el término de
891060	899820	método genera reglas, reciere un conocimiento lingüístico, sin duda, técnicas reconocimientos
899820	904780	de patrones, voy a tener que generar esas listas que me permitan a mí, pues yo lo puedo
904780	908220	hacer todas estas cosas que estoy moviendo, lo puedo hacer con grandes listas y no necesito
908220	919020	entrenar nada. Pero tengo que tener claro este tipo de cosas, ¿no? Como Barcelona,
919100	927820	Uruguay, que es, a que estoy haciendo referencia, es un lugar, es el Rio Uruguay, es el país Uruguay,
927820	934220	es la selección Uruguay, se entiende, entonces, tengo dificultades que por ahí las tengo
934220	942460	que resolver más adelante. Con el sistema, bueno, la contra que puede llegar a tener
942460	948500	los sistemas de reglas es en algún caso que no tengo las capacidades ni los recursos, como
948500	954020	para poder hacer todo eso. Además, si yo le quiero incorporar, después, nuevos documentos por
954020	959860	ahí, tengo que entrar a la redefinir reglas y esas reglas nuevas que agregó, capaz que me
959860	967620	repercute en las que ya tenía, es un proceso que es muy bueno, que funciona, pero tiene
967620	971580	algunas limitaciones, por lado de los recursos y por lo lado de las escrituras de las reglas.
971580	989580	Para esto, la clave, que lo que yo necesito, que es. Paitos.
989580	992620	Dato, corpus.
992940	1000900	En los sistemas de Machine Learning, de aprendizaje automático, si no tengo datos, prácticamente
1000900	1007860	seguramente tenga problemas a la hora de resolver un desafío. La clave está en la cantidad
1007860	1014260	de datos que yo tengo para entrenar mi modelo.
1014260	1021020	Bueno, lo está muy diciendo, los criterios para decidir un enfoque de ponida de recursos,
1021020	1026140	por disposibilidad de escriturar reglas, la datos, los datos de entrenamiento, cambios posibles
1026140	1031020	en la especificación y la performance, no, capaz que algún algoritmo puede ser un poco
1031020	1034380	más eficiente que otro.
1034380	1041900	Bien. La idea es, ahora, hablar de un par de temitas más, en donde también, el
1041900	1052380	pensamiento del lenguaje natural, tiene una participación porque en su cuando estamos
1052380	1058000	manejando texto, estas técnicas que estamos hablando se aplican a muchas otras, a muchos
1058000	1065140	otros temas, a los que nosotros nos interesa esa procesamiento de texto.
1065140	1071340	Uno es cláctering, y el otro es la detección del modelo de tópicos, ¿no? Entonces, lo primero
1071340	1081020	que, de gustaría hacer una cierta precisión es porque nosotros hasta ahora vimos, creo que
1081020	1087620	no sé si lo vieron con allá, creo que con Luis, el tema de clasificación, ¿no? Entonces,
1087620	1096140	muchas veces o, o el, o hacer cláctering implica que yo, en definitiva, estoy haciendo clasificación,
1096140	1102500	lo que yo estoy haciendo es, o que significa cláctering es agrupar, dado un conjunto de datos,
1102500	1109780	ir agrupando sendatos que tengan un comportamiento similar, o sea, en similares en algún sentido.
1110700	1118500	Cuando yo hago clasificación, es un método en donde yo ya sé qué es lo que yo pretendo
1118500	1124980	clasificar, recibir que se yo hago, autos de determinado tipo o determinada marca, entonces
1124980	1131380	los tengo un montón de autos y los clasificos, por si algo, mientras que es y además
1131380	1137020	está asociado a técnicas de aprendizaje supervisados, yo tengo un conjunto de datos en donde
1137020	1143620	yo ya sé, y cuando hay un nuevo dato, sea donde lo mando, o debería saber, ya está
1143620	1152380	prestablecido, cuáles son los términos de clasificación. En cláctering está más asociado
1152380	1158900	a lo que sería en técnicas de aprendizaje, no supervisado, donde en general no necesariamente
1158900	1165740	dependiendo el algoritmo que yo utilice, sea la cantidad de conjuntos o clácter que yo voy a
1165740	1174300	determinar. La estrategia es de poder en base a qué es que yo genero sus clácter, esos
1174300	1184860	agrupamientos, qué es lo que hace de qué dos datos o dos textos sean similares, y ese
1184860	1191420	justamente es el desafío, entonces simplemente presentar el presentar el tema, presentar
1191420	1198220	dos modelos, un poquito distintos o dos enfoques de agoritmo de clácterización, y en
1198220	1209340	una donde yo, a priori digo, bueno quiero que tenga equiscan que cá clácter, entonces
1209340	1215460	en función de eso, no sé cuáles son, pero lo que hace el algoritmo es tratar de
1215460	1223220	encontrarlos, es esa agrupamiento, tienen su propio contra, ¿no? Entonces, el clácter
1223220	1227860	y nés, como decía por decir, una tarea que tiene como finalidad, lograr agrupamiento
1227860	1234060	de conjuntos de objetos que están no etiquetados, y esa agrupación es esa agrupamiento recibe
1234060	1240780	en el nombre de clácter. Los elementos de cada uno de cada uno de esos conjuntos
1240780	1245220	poseen algunas características que los distinguen de otros, esto es importante, porque
1245220	1250260	la idea es que cada uno de los elementos pertenecan a uno y sólo uno de los conjuntos
1250260	1255660	determinados.
1255660	1265900	Y esa última oración acá queda nuestro criterio de la zona interpretación semántica,
1265900	1271540	por ahí yo no sé por qué lo estoy agrupando de esa manera, y muchas veces se despoze
1271540	1276580	de que los agrupe, ellos trato de ver y de ponerlo un nombre a cada uno de esos conjuntos.
1276580	1284140	Se entiende, a priori, no necesariamente tengo por qué conocer de que trata cada uno
1284140	1292780	de esos clácter, simplemente los agrupo, y después le pongo un nombre.
1292780	1298900	Algunos susos de técnicas de clácter, algunos son más conocidos seguramente o enseguida
1298900	1308140	les suelen, la biología en el estudio de la célula, en un medio ambiente en marketing.
1308140	1316820	En marketing, segmentación de mercado, muchas veces se habla de hacer clácter en marketing,
1316820	1321860	lo que estamos haciendo es segmentar, tratar de hacer agrupaciones de clientes, con determinado
1321860	1327020	perfil, determinado comportamiento, y eso justamente es un determinado clácter, a donde yo
1327020	1332700	le voy a mandar, o mi empresa le va a mandar, eso que se está lo buen información.
1332700	1338300	En sociología, bueno, en análisis de redes sociales, eso se hace mucho cuando se estudian
1338300	1349420	los perfiles de los que actúan en redes sociales, y bueno, en función de eso, te tiran,
1349500	1354740	Twitter, por ejemplo, y te tiran qué, qué, ¿cómo es? Que Twitter, promocionado, determinado
1354740	1360180	producto, te puedes llegar a interesar, eso está relacionado en las dos, es algo de segmentación
1360180	1364620	de mercado, pero también implica a análisis de redes sociales.
1364620	1369780	Bueno, que no que veis tipo de nombre social, no va a hacer autóist, por ejemplo, te va
1369780	1374260	a hacer un autóist primero, y ahí también según la segmentación en clácter, los que
1374580	1378300	se pueden salvar a los usuarios muy intereses, ¿no?
1378300	1383900	Pero eso es lo que haría voz después, tal, eso lo hace es voz después, cuando tenés
1383900	1387100	los tweets, yo me sé que cuando empezaste ahora, pensé que ahora va a de cuando
1387100	1391100	vos entrar a Twitter, y veis lo que te aparece, yo me refería a que vos entrar a
1391100	1397180	Twitter, y de repente te aparece algo, un tweet que no sabes por qué te lo ponen, y eso es
1397180	1400900	porque alguien sabe, a este le gusta al futuro, entonces seguramente le va a pasar en un
1400900	1407460	tweet en la final de la Copa esta que está naciendo ahora, porque detectan que hay un
1407460	1411900	interés en vos, entonces ese tipo de cosas adrupan, claro, el tweet no te lo mandan
1411900	1418260	a vos, te lo mandan a todos aquella personas que tienen un perfil similar, entonces es un
1418260	1435380	poco en ese sentido, bien, hay como dos clases de algoritmos principales, por decirlo
1435380	1444420	alguna manera, es uno es el que se ya denomina camins, que es el que en el que yo sea
1444420	1452860	a priori, como decía, quiero conseguir cárter distintos, ese algoritmo de camins en donde
1452860	1461940	yo prefijo un cárter, es trato de terminar en un diujito para que se entienda más fácil en dos
1461940	1471500	dimensiones, ahí hay un montón de piensas en que pueden ser documentos, pueden ser
1471500	1477820	importa que demasiado representados por mundos, entonces que el algoritmo de camins lo que dice
1477820	1486860	bueno, cuánto va a lecar, tres, entonces trata de terminar, tres puntos que son van a ser
1486860	1499500	los centrógiles de esos clases, de esos conjuntos, cada clases se representan mediante un
1499500	1506420	punto en el espacio, tengo cada esos puntos, los puntos que queden más cerca del centroide,
1506420	1513620	se subí, que de cualquier otro centroide corresponden a el clastar, se subí, y eso es un proceso
1513620	1524740	iterativo, es decir, yo agarro y pongo ahí el hijo, tres puntos a priori cual es quiera y
1524740	1529540	empiezo calcular las distancias y ahí está el clave, que es lo que utilizo para que
1529540	1535540	formula es la que utilizo, para calcular la distancia de cada uno de los puntos a esos
1535540	1540460	que constituirían mis centrógiles, esos centrógiles en definitiva por eso que dice que es un
1540460	1549540	proceso iterativo, yo voy a cambiarlo, es decir, yo tiro una vez y empiezo agrupar y después
1549540	1557940	eventualmente, en función de lo que me da, puedo determinar nuevos centrógiles, porque algunos
1557940	1561440	me quedaron, medios, lejos, o lo que se ha digo, capaz que hay otra agrupación, que es un
1561440	1569940	poco mejor, acá es como en el ejemplo, este es como bastante, bastante obvio, que en definitiva
1569940	1578500	si yo eligiera un punto acá, un punto acá y un punto por acá, enseguida esos grupos
1578500	1583900	a deciría que están cercadas esos puntos, pero si yo hubiera puesto una de la séquis por
1583900	1592940	acá arriba o por acá, ¿verdad? ¿verdad? Capazquilos agrupamientos hubieran sido otros, y entonces
1592940	1599060	necesito más de una iteración para armarlos los conjuntitos que aparecen ahí, ¿ok?
1599060	1611460	Entonces, como decía recién, acá todo depende de cuántos conjuntos o cuánto vale acá,
1611460	1617980	acá yo podía decir, bueno yo tengo todo estos puntos y quiero hacer dos clastras, entonces
1617980	1626780	parece intuitivo que están agrupado de esa manera y así podía elegir seis clastras, entonces
1626780	1631140	definitiva los puntos que están más cerca, o sea, no está marcado acá cuál es el centro
1631140	1640660	vide, pero un poco podemos introduir en función de los de los de los colores, ¿ok? Bueno,
1640660	1648380	dos clastras, seis clastras, cuatro clastras, lo que fuera, para el cálculo de la distancia entre
1648380	1654740	los puntos, lo que se utiliza es la distancia en clínea, ¿ok? También se podría utilizar
1654740	1661220	el coseno del ángulo, entre eso que se forman tres dos puntos, en general es un algoritmo
1661220	1669260	muy rápido que convergen pocas iteraciones, y esto es una cosa importante, es los clastras
1669260	1674340	no hay solamente los objetos, es decir, cada uno de los elementos va a partencer a un
1674340	1687660	conjunto solo, el desafío obviamente va a hacer elegir los mejores casen troides, acá
1687660	1693260	hay un ejemplo de este justamente que hitera al más de un caso que muestra lo que decíamos
1693260	1700580	hace un ratito, yo tengo un conjunto de puntos, ahí los verdes y el hijo estos dos, como
1700580	1709340	son troides, está 12x en azul y en rojo, entonces en una primera pasada de algoritmo lo que
1709340	1718180	me dice es, divido así y así, ese agrupamiento, algunos son azules y otros, pero será la
1718180	1727660	mejor iteración vuelvo a iterarlo, el hijo cálculo de estos puntos que yo ahora están todos
1727700	1736020	azules, a ver si no hay algún otro x, no sé si se ve ahí, acá y otro, acá está la x y acá
1736020	1744060	ahí está la x en rojo, entonces, si yo defino esos otros, centros y des, el agrupamiento es
1744060	1757060	distinto y tero de vuelta, centros y acá centros y acá y el agrupamiento algunos cambian, pero
1757060	1765620	después de acá muestra que después de un par de iteraciones ya no cambia más, entonces la
1765620	1770500	partición final sería este, o sea, tiende a converger después de un cierto número de pasos,
1776500	1780980	no pero esto es como esto es un desemplito no más, debe de visualización, acá los estamos
1780980	1784980	mostrando en dos dimensiones, volo que puede estenar si pueden ser en dimension de ellos que
1784980	1790620	es ellos, el espacio en edimensional, en principio, pues vamos a traermas más que enjada el ejemplo,
1793620	1802140	entonces un modelo de clasterin es el camins y otro modelo, otro esquema es el modelo jerarquico,
1802140	1808380	entonces al revés del camins donde yo conocí a los cá, sabía que yo quería ser ca con juntos,
1808380	1814940	en el gerarquico, yo no tengo pre definido priori, cuáles son esos cá con juntos que yo quiero
1814940	1826100	determinar, entonces, yo se plantea como que los datos o las observaciones o los textos,
1826100	1832900	si fueran textos, serían las hojas y en principio trato de ver alguna forma en que estén
1832900	1838700	correlacionadas, ciertas similitudes y ahí tendremos que ver cuáles pueden ser las distancias
1838700	1845340	de similitudes entre si son documentos o si son este que si yo cualquier otro caso esto,
1845340	1850740	a ver como decíamos hoy, esto se aplica a lo que sea, a nosotros nos interesa ver cómo
1850740	1856940	estas cosas las aplicamos a los documentos, a los textos, pero en principio son algoritmos
1856940	1865740	de clacering genéricos, cada hoja representa un elemento de observación repito para nuestro
1865740	1873100	caso serían documentos, y a medida de que se sube alguna de esas hojas se van funcionando
1873100	1883340	en función de cierto grado de similitud, algunas características comunes, y la idea en este
1883340	1889940	ejemplito que está puesto acá es que a nivel horizontal yo voy marcando hoy, yo voy
1889940	1896780	marcando si acá sería en la posibilidad de la izquierda sería un solo clázca, son dos
1896780	1906580	iguales, pero los cortes estos horizontales acá en las ramas es como que yo digo bueno,
1906580	1912040	acá marco estos tengo dos clastas, tengo dos conjuntos elementos que se parecen y este
1912040	1918440	de la izquierda tengo tres, dependiendo aquí a altura corto es donde yo agrupo conjuntos
1918440	1929680	elementos que se consideren parecidos, que tengan algún grado de similitud, hay otros
1929760	1940440	pero hay otro otro modelo, también que se llama de bescán que también se utiliza y se
1940440	1949560	utiliza en clázter indetestos, es un algoritmo que también se basan la densidad de puntos
1949560	1955280	en la representación como veamos hoy en el camins, pero también es un modelo que no conoce
1955280	1965040	de priori, los cá, sino que yo voy tratando de agrupar con juntos que tengan algunas similitud,
1965040	1970880	el problema que puede llegar a tener es que yo lo que hago es para cada uno de los puntitos
1970880	1979000	misos heraciones, mis textos trato de generar un cierto círculo digamos un
1979000	1984760	un cierto epsilom de cercanía, de correlación y en función de eso voy agrupando a que ellos
1984760	1990600	que se queden cerca, está el concepto de lo que están adentro, lo que están en la frontera
1990600	1997080	o lo que están quedando muy lejos y en función de eso yo voy viendo cuáles son los
1997080	2005080	que puedo ir agrupando de alguna manera, lo que pasa ahí es que como en cualquiera de estos
2005080	2010320	otros casos yo puedo tener documentos que nos aparezcan en nada y que me quedan muy aslados
2010320	2020100	y entonces, también en cualquiera de estos algoritmos, eso puede generarme si son muy
2020100	2026640	dispersos, los documentos, muy distintos, documentos digo documentos o elementos, puede
2026640	2034680	generarme algunos elementos que no estén relacionados con ninguno de los clases, hay que
2034680	2049280	ver qué tratamiento se hace con eso, preguntas, seguimos bien, y en otro tema es que
2049280	2063360	queríamos comentar, bueno es el modelado de tópicos, que es un tópico, que es un tópico,
2063440	2069560	está en lo que es tópico, no sé si hay que estar acá, vamos a ver así, si no le haya
2069560	2077680	un rapido, que es un tópico, que le llaman tópico, escuchando en el tema modelado de tópicos,
2077680	2093200	tópico modeling, no le suena, bien, que es un tópico, tema, ¿qué es una palabra tópico?
2093280	2100800	¿Cuál es una circunstancia, para que os haya temas, bien? Claro, se utiliza, alguna
2100800	2108760	circunstanó, se habló de determinado tópico, y eso es, se habló de determinado tema, correcto,
2108760	2116560	es que es un poco esa idea, lo que pasa es que no necesariamente y esa es un poco, vamos a
2116560	2121160	primero vamos a ver un par de funicciones de la rai, fíjense en la cinta, es la que
2121160	2129200	o es eso, tema, el elemento un enunciado, no fíjense acá, esta está buena también, el elemento
2129200	2133160	un enunciado normalmente es lado entre pausas que introducia algunos de los elementos
2133160	2137560	de la radiación, o bien aporta el marco, el punto de vista pertinente para la
2137560	2149920	renunciación, en definitiva la pregunta o lado es, tópico es igual a tema, si es como yo
2149920	2165120	de término o como debería yo tener la forma de identificar los tópicos o los temas, es
2165120	2177680	decir, cuando yo hago, modelé este modelado de tópicos, lo que trato a hacer y ahora
2177680	2183800	nos vamos a concentrar directamente en textos, pensemos en textos en palabras, yo trato
2183800	2195920	de ver o de agrupar, tratar de detectar de qué tópico habla tal o cuál documento en función
2195920	2200760	de las palabras que estén en ese documento, pensemos en un texto que no tenemos, no sabemos
2200760	2207120	nada y que me decir de terminar de qué tópico habla, para eso lo que hago es analizó
2207120	2219440	las palabras que contiene, analizó las palabras que contiene, y después hay algunas
2219440	2225000	discusiones, no, porque bueno, claro, las palabras que contenga, si son palabras que
2225000	2232000	hablan, están siempre aparecen medio relacionadas en todos los tópicos, en perdón, en
2232000	2239400	todos los documentos, capaz que están hablando de lo mismo, universidad, estudiante, clase,
2239400	2246960	materia, profesor, capaz que todo eso está relacionado a algo que podemos ir tópico
2246960	2257720	educación, se entiende y le estamos dando, le estamos dando como un justamente un tema
2257720	2266960	semántico, pero si retrocedemos un casillo y lo pensamos como conjunto de palabras,
2266960	2272480	hay un ejemplo que está muy lindo y yo digo bueno, en primer lugar, en segundo lugar,
2272480	2278200	en tercer lugar, finalmente, son ciertos marcadores o palabras que también suelen aparecer
2278200	2282600	juntas en un montón de documentos, pero en realidad de qué están hablando, cuál es el
2282680	2288360	tópico, que está hablando, son palabras que si están relacionadas en algún sentido,
2288360	2292080	porque aparecen siempre juntas, lo que sea por cierto, aparecen siempre juntas, pero en realidad
2292080	2300600	no tienen un tema semántico, entonces hay que saber discriminar ese tipo de cosas, se ve la
2300600	2311960	dificultad o se ve el tema, el origen de todo esto es lo que se conoce con el nombre de las
2311960	2319480	colocaciones, o podríamos decir que uno de los origenes, que es una combinación, que son
2319480	2332720	las colocaciones, es una combinación de palabras, cerrar una ventana, cometer una roar, que
2332720	2345440	tienen aparecer juntas, mientras estas otras términos que aparecen acá, meter la pata,
2345440	2352760	tomar el pelo, cortar por los anos, son palabras que aparecen juntas, pero que en realidad
2352760	2360760	tienen significado en sí mismo, o sea, todas juntas constituyen un solo elemento o un término,
2361080	2375520	si meter la pata que es, cuando es y meter la pata, y si te agoma el cometí es una roar, entonces,
2375520	2382560	yo tendría que mi algoritmo tendría que determinar que meter la, si aparece, meter la pata,
2382560	2389480	o cometer una roar, deberían de estar juntos, por decir algo, ¿Tá? Entonces,
2390840	2395840	eso son el tipo de cosas o los desafíos que uno puede llegar a encontrar cuando está siendo estas cosas,
2404520	2412760	topicos, el definitiva es el, o debería de ser el asunto principal del que se habla,
2412760	2420200	del que se predica o del que se comunica alguna cuestión, y el tema es que ha dado un documento
2420840	2431120	no necesariamente fácil determinar el topico, y es justamente el desafío que se que convoca
2431120	2438520	cuando uno hace modelado de topicos, o topismo de ahí, tratar de encontrar o determinar
2438520	2448840	el tema o un determinado tema del que hable un documento, fíjense este ejemplo, muy lindo,
2451440	2458840	leamos arriba, a partir de este martes cada club solo podrás sumar 9 puntos, unidades que
2458840	2464440	solo definirán el último modelo del campeonato roguallo, sino que también decidirán
2464440	2478600	quiénes se mantienen en primera, de qué hablas eso, ahora tiene un montón de palabras,
2479560	2484840	enseguía de este cuenta de la verdad de futuro, cambió a club por estudiante,
2484840	2491400	campeonato roguallo por curso, y primera por carrera, y leamos la segunda agracción,
2492520	2497640	a partir de este martes cada estudiante solo podrás sumar 9 puntos, unidades que solo definirán
2497640	2502440	el último modelo del curso actual, sino que también decidirán quiénes se mantienen en carrera,
2502440	2516200	y aquí estamos hablando acá, la puntada, estudio, educación, entonces la clave está en ver
2516200	2524040	cuáles son las palabras que en definitiva son las que me marca en el topico y hay un montón de
2524040	2535800	palabras que pueden aparecer en varios textos y en varios tópicos, porque si capaz que la
2535800	2543240	palabra martes aparece tanto en los tópicos de carrera como en el topico de fútbol, se entiende,
2543240	2551040	entonces, pero que pasa, en alguna va a aparecer o más frecuentemente o menos frecuentemente,
2551040	2563200	y ahí la estrategia o el modelo que más se adecúa a este tema es trabajar con provenidades,
2563200	2574480	y hacer distribuciones de probabilidad. Entonces, y ya vamos a eso, el modelo de tópicos nos
2574480	2582000	permito organizar, entender y resumir grandes colecciones de documentos, intenta detectar patrones
2582000	2587440	de ocurrencia de las palabras, agrupando las envase a distribuciones de esas palabras en un conjunto
2587440	2594480	de documentos, un poco lo que estábamos comentando con ese ejemplo, está, es útil y identificado
2594480	2602000	las temas para poder ocupar, eso está claro. Entonces, en qué consiste el modelo de tópicos
2602000	2606800	en construir un modelo justamente que busque y encuentre las palabras que están relacionadas
2606800	2616080	de alguna manera. Esa agrupación de palabras lo que van a conformar justamente son clatas,
2618080	2624240	y esa o sea que lo que estuvimos viendo antes está, y precisamente relacionado con esto que estamos
2624240	2634640	en dos horas. Y la estrategia claramente es que mis tópicos, los distintos claster que yo
2634640	2641520	vas a juntar, sean los más distintos que pueda, entre sí. Pero eso no necesariamente lo
2641520	2650600	puedo, es porque lo que nos va a estar pasando es que palabras muchas palabras pueden aparecer
2650600	2657480	en muchos tópicos, lo que va a tener, lo que van a tener o lo que deberían detener son
2657480	2662440	distintas frecuencias de aparición o distintas probabilidades que ocurran en tal o cual
2662440	2666760	palabra, en tal o cual tópico.
2666760	2674800	¿Pero por lo cual tan alto que vale dos tópicos? Y yo porque es la estrategia y uno de las
2674880	2680640	tópicos. Sí. Exacto. Y ese es todo un desafío porque justamente lo que va a
2680640	2691200	atener no solamente un documento va a pertenecer, ahora lo vamos a ver el agorismo tradicional
2691200	2698080	de esto es el idea que lo que hace es justamente una distribución de dónde este documento
2698080	2704640	puede quedar en este tópico, en este tópico, o en este tópico. Entonces, pero con distinta
2704640	2713120	probabilidad y ese justamente el desafío. No solamente tengo palabras que pueden pertenecer
2713120	2718680	a más de un documento y a más de un tópico, sino documentos que pueden pertenecer a más
2718680	2725560	un tópico y ese es todo un problema. Sí, lo que pasa es que lo que yo trato de hacer es generar
2725560	2736120	un modelo en base distribuciones de probabilidad. En el modelo tópico yo tengo que cada tópico
2736120	2743280	es una bolsa de palabras y que cada documento es una mezcla de tópicos, que era un poco
2743280	2751240	la pregunta que vos hacía. Cada documento puede tener ciertos porcentaje de palabras
2751240	2758720	que con mayor o menor frecuencia aparecen en más de un tópico. Y eso justamente es la estrategia
2758720	2764560	que hacen los algoritmos de tópicos de língua.
2764560	2778560	Tengo un conjunto de documentos y lo que trato a hacer es agruparlos bajo un determinado
2778560	2786120	tópico. Cada uno me dirán, pero pensemos y pensamos, noticia del prensa.
2786120	2792400	La papa, porque yo por lo general tengo, ya metagatos, no que me dice este, a que de hecho
2792400	2797840	pasa, esto pertenece a economía o esta es una noticia de fútbol o esta es una noticia
2797840	2803880	de ahí pueden haber tópicos que están prácticamente determinados. Pero no necesariamente
2803880	2814120	tengo sometagatos, en donde yo me pueda basar para aplicar mi tópico de línguamos,
2814120	2825280	mismo del lado. Y no necesariamente, o sea, acá yo le estoy diciendo esto, a que?
2825280	2833400	T1, T2 y T3, yo después a este T1, T2 y T3, le voy a poner una etiqueta. Y el desafío
2833400	2838280	va a ser después, bueno, y cuando yo le incorporo un nuevo texto, a ver si encaja en alguno
2838280	2844640	de esos tres que definía ahí, o tengo que hacer un nuevo, una nueva pasada para determinar
2844640	2852160	capas otra cosa. Tampoco es una cuestión de que yo diga, bueno, hago un malado tópico,
2852160	2860880	voy a seleccionar en diéstópicos, porque 10, capa que son 5, capa que son 20, capa que son
2860880	2866960	20, capa que son 20, o sea, tampoco necesariamente se conoce en aprior y cuáles son los tópicos
2866960	2880600	o la cantidad de tópicos que existen en un corpus. Y ahí dos enfogues, por un lado, me vuelve,
2880600	2888720	lista de palabras y por otro lado es tratar de detectar patrones de aquella sucurrencias
2888720	2895440	de palabras que se agrupen en base a ciertas distribuciones dentro del conjunto de documentos.
2895440	2902560	Tampoco son 12 enfogues distintos. Y uno podía hacer este, hace un tiempo había un
2902560	2909360	setchón, un trabajo con la gente 16 económicas, entonces justamente trataban era para otra
2909360	2913960	cosa, no, el estudio de un indicador. Y que se basaba en cosas de este estilo. Trataba
2913960	2924920	de ver cuál son aquellas palabras que hablan de determinado tópico o determinado tema.
2924920	2932520	Hay economía económico, económista, comercio, inflación, entonces el tópico es economía.
2932520	2939240	Es artido, un brinciar, tu insierta riesgo, país, insartido, un hombre. Fíjese que riesgo
2939240	2945440	país lo toman con un token. No estamos necesariamente hablando de palabras, sino que estamos
2945440	2954760	hablando de tokens. Esto también les da la pauta, hoy no lo vimos en el ejemplo, que
2954760	2964000	entonces estas cosas, yo cada vez que vaya a aplicar. Y ahí ya me temo pelenes, antes de
2964000	2969200	aplicar estas cosas. Fíjelo que tengo que hacer con los textos, que yo les dije que
2969200	2978040	está minimizada esa carea cuando hacemos pelenes. Depurar, pre-procesar, sacar limpiar
2978040	2984140	el texto, sacar un reele, ver que hacer con las fechas, normalizar, ver que hacer con los
2984140	2989680	puntos, es decir, toda esa tarea de pre-procesamiento, la tengo que hacer antes. ¿Qué
2989680	2997560	hubo las palabras? Las estopuores. Las limpios, las considero, no las considero.
2997560	3005980	Sentiendes, esas palabras, las estopuores, estos temas, ¿no? Algunos agurimos las dejanadas
3005980	3010140	dentro. Pero claro, esas me van a aparecer en todos los tópicos, se aparecen casi todos
3010140	3016000	de momento, con funciones, artígulos, esas van a aparecer en todos los momentos, esas
3016000	3021900	no son palabras que me identifique en un tema. De hecho, algunas veces, uno lo que hace, algunos
3021900	3026700	agurimos, dice, bueno, genero todo un tópico con las estopuores y algunas palabras que
3026700	3037980	no agren en contenido y te hacen un tópico con eso. ¿Tá? Para este tipo de cosas, cuando uno
3037980	3043500	trabaja con listas de palabras, hay lo que se requiere es el conocimiento de un juicio
3043500	3047540	experto, ¿no? También, de que diga, bueno, ¿cuáles son las palabras asociadas
3047540	3054900	a tópico? O sea, hay un trabajo, no solamente de algoritmos, que se trata de identificar,
3054900	3061300	si no un trabajo de arranque, que me identifique, ¿cuáles son aquellos asociadas a tántópicos?
3061300	3073060	Bueno, y por otro lado, tenemos algoritmos, un enfoque basado en distribución de las palabras.
3073060	3085140	El idea es un algoritmo bastante de los más utilizados, el idea y algunas variantes en esto
3085140	3091900	hemos de lado del tópicos, sobre todo en este último tiempo. Pero, fíjense que aparecen,
3091900	3097620	son trabajos que aparecen ya en la década de 2000, ¿no? Y leyes uno de los que hecho
3097620	3106380	el que propone el algoritmo de el idea. El idea genera tópicos proponiendo una distribución
3106380	3111780	de todas las palabras del corpus y calcula una distribución de estos tópicos en cada
3111780	3121500	documento. Entonces, cada documento en ese corpus es distribuible con una cierta probabilidad
3121500	3129500	a alguno de los tópicos. O sea, un poco la pregunta a cosas días, un documento puede
3129500	3136620	pertenecer ser del tópico T1 con un 95% de probabilidad, pero tiene un 5% de probabilidad
3136620	3142060	de que ese tópico también pertene, ese documento también pertene y salto pico T2, que es un poco
3142060	3150060	lo que hace de la idea, juega con eso, pero un documento puede tener más de todo, es acto.
3150060	3159660	Bueno, ese es otro tema, pero vos quieres incasillarlo en uno de los tópicos,
3159660	3172740	es decir, este habla de 95, 50% de economía y 50% de política, política, exacto y
3172740	3177580	todo, es así, después vos después tendrás que ver qué es lo que hace con eso, pero
3177580	3185940	sí, exacto, puede pasar. Bueno, un poco lo que decíamos recién, cada tópico es una distribución
3185940	3191460	probabilítica de palabra, no, entonces tengo el tópico turismo, educación, economía,
3191460	3201260	y entonces, como ven, hay palabras que aparecen, estos son números truchos, pero palabras
3201260	3215540	que aparecen o que pueden aparecer en más tópico. Turismo argentinos, bilateral, blú,
3215540	3225100	educación, bueno, ven acá en economía también, aparece el blú peso, dólar. Entonces,
3225100	3232260	hay palabras que capaz que blú, cuando tengas que precisar un documento, bueno, donde
3232260	3238580	lo pongo, y tiene la palabra blú muchas veces, y bueno, capaz que lo pongo en el tópico turismo,
3238580	3247980	es más probable que el tópico que no mía, pero bueno, es parte de las cosas que yo tengo
3247980	3255580	que decidir cuando pico te tipos de goses. Entonces, decíamos, cada tópico es una distribución
3255580	3262740	probabilítica de palabras, y cada documento es una distribución probabilítica de tópicos,
3262740	3267780	de vuelta lo que decíamos es un rato. Entonces, si yo tengo este texto que está acá,
3267780	3273500	lo con base a lo que preguntaba a vos, y bueno, en función de lo que aparece ahí,
3273500	3278500	sí, vas a cifrar mi misterio de turismo y los solatores, por el economista, la verdad es,
3278500	3284500	señaló que el primer trimestre este año, el gasto de Uruguay, el cancer, no sé cuánto, tanto de
3284500	3290780	los Uruguayos, millones. Bueno, parece acá, el tema no parece la palabra dola, la
3290780	3296940	parecen sí, no un poco lo que decíamos hoy del prepresasamiento. En fin, aparece acá sí,
3296940	3303820	la parecen la palabra dola, la parecen blú, aparece pezos, en fin, el proceso me podría decir
3303820	3312060	que este documento tiene un 25% de que sea de turismo, un 7% de educación, porque capa que
3312060	3317100	tiene algunas palabras del tópico educación y un 19% de economía, por decir algo.
3318060	3322060	Y otra vez que por ahí no aparece ahí, ok?
3325260	3331900	Bien, se ha sido inicialmente una probabilidad y lo que la de es de Dirichlet, porque lo que
3331900	3337340	Dirichlet es la distribución de Dirichlet, no es distribución de Dirichlet.
3337340	3347140	Permite que un documento sea parte de varios tópicos cada uno con un peso diferente y lo
3347140	3354020	interesante es esto, que son las métricas, como yo mido, si mi algoritmo es bueno, malo,
3354020	3361780	se comporta bien, se comporta mal, es lo puedo medir con coherencia y perplegidad, perplegidades
3361780	3372980	como se comporta cuando yo le agrego un documento, sabe dónde ir, se encajan en uno de
3372980	3378180	los tópicos que ya definimos o no, entonces una medida de perplegidad me dice a mí,
3378180	3382900	cuán efectivo es el acorismo que socabo de aplicar y coherencia de bueno, que haya una
3382900	3393020	coherencia, se ha completo en su globalidad, que se ha corriente lo que acabo de mi distribución
3393020	3399020	de documentos a lo largo de todo el corpus, sabe de que todo se estén dentro de algunos
3399020	3402340	de los tópicos que he estado trabajando.
3402340	3411740	Hay algunas variantes de la idea CTM, BTM, la CTM es una variante que lo que hace es cambiada
3411740	3418300	la distribución de probabilidad por una normal logística, BTM está bueno, es una variante
3418300	3423940	porque que pasa, el idea estamos acostumbrados a trabajar con textos largos, donde tienen
3423940	3427700	una gran cantidad de palabras, entonces bueno, eso juego con la frecuencia de las palabras
3427700	3428700	y del dotete.
3428700	3439460	Y BTM lo que hace es incluir el concepto de BTM y es de ver si utiliza es como una versión
3439460	3445980	de desdear aplicada a textos cortos, como podrían ser textos de Twitter o cosas por el estilo,
3445980	3452540	en donde yo puedo tratar de encontrar pequeñas palabras que ocurren en un texto, es la misma
3452540	3463220	idea pero para textos mucho más cortitos, es interesante, que si yo son ejemplitos,
3463260	3474020	hay literatura que hables de estos de todos de los agolíte, quería llegar a este, esta
3474020	3483020	es una idea extendido con embeddings, es una propuesta bastante reciente, en donde yo hago
3483020	3494940	una representación de mi conjuntos de documentos, es vectorial, entonces un vector de dimensiones
3494940	3503740	de las palabras de un vocabulario, de conjunto de todas las palabras del vocabulario.
3503740	3510300	Y lo interesante es que utiliza abectores para determinar o sea para representar a los
3510300	3516260	documentos y para representar a los tópicos, los documentos están representados por palabras
3516260	3524140	y los tópicos están representados por palabras, entonces para saber cuando un nuevo documento
3524140	3530780	entre tal o cual tópico, calcula la distancia o clídeo, la distancia cocino, entre los
3530780	3538820	aspectores del tópico de el documento, que estoy agregando, o sea, lo que le agrega,
3538820	3553660	este, este M, es alele de A, vectores, embeddings, entonces yo tengo hay ciertos, y
3553660	3559700	perparámetros, no, cuál es el número de tópicos que yo quiero inferir, cuál es el espacio,
3559700	3565300	la dimensión de los sectores, está y la cantidad de vocabularios, entonces tengo una
3565300	3572140	matriz, bueno, de embeddings con dimensión de por V, una matriz de tópicos, una red neuronal,
3572140	3580300	con entrada de tamaño V y salida de tamaño V, entonces un esquemita simplemente de lo que
3580300	3591020	como haría para un nuevo documento entre la red y metida, cuál es son los tópicos inferidos
3591020	3599500	por la red con su porcentaje de probabilidad, y cuál va a hacer la distribución de las palabras
3599500	3606260	de ese texto en esos tópicos, o sea, las dos cosas, es más probable que tenga sea de economía
3606260	3611900	o de, este, política, en tal probabilidad, y bueno, y el porcentaje estas palabras, y yo
3611900	3621260	después de, pues veo, si lo depa adelante, si sigo, si no, ya queda en función del usuario,
3621260	3628580	entonces simplemente un ejemplo para que para bajar a tierras tus conceptos, no?
3628580	3636580	Yo tengo estas palabras, no? Club, campeonato, primera, tantos medios por acá, este cláster
3636580	3643520	de palabras, también juntos, por acá tengo estudiante, carrera, curso, creo que son
3643520	3652620	los mismos ejemplos que estaban en el anterior, no? Y tengo esta noticia, ¿eh? Que quiero
3652620	3665580	ver a dónde va? Tengo el tópico 1, osea que está acá, tópico 1, fíjense del centro
3665580	3675360	y el que decíamos hoy, tengo el tópico 2, yo lo que tengo que haber es calcular la distancia del
3675360	3683240	vector de esta noticia con respecto a cada uno de los tópicos, de los efectores de los
3683240	3690720	tópicos, y bueno, esto simplemente ha modo de ejemplo, me dio que esta noticia, fíjense
3690720	3694800	hablamos del texto, no hablamos de multimedia, acá está propósito para mostrarles
3694800	3701920	de que apareció una foto que probablemente sea deportes de esa noticia, pero bueno, en
3701920	3708500	función de las palabras que tiene el texto, esto dice que pertenece al tópico 1, 90 y al
3708500	3716660	tópico 2, 10, con esa probabilidad, y esta es la distinución de probabilidad de las palabras
3716660	3724380	de la noticia que aparecía ahí, está, esto simplemente ha modo de ejemplo, ¿qué está
3724380	3731380	de la distinución, osea la probabilidad de la palabra del tópico?
3731380	3745100	Sí, sí, bien, se entendió, alguna pregunta, obviamente, de vuelta, donde engancha
3745100	3750880	belleña acá, en particularmente en toda la setra paz, porque realmente en toda la
3750880	3754720	setra paz, esto ya estoy aplicando técnicas de presentamiento de lenguaje natural, porque
3754720	3758080	trabajó con las palabras, trabajó con los momentos, osea en cualquiera de estas dos
3758080	3762840	casos, más allá de clasters lo vimos con algunos ejemplitos míos aislados, en lo
3762840	3769120	mismo, acá aparecen mismo con el mismo concepto, de agrupamiento, de agrupamiento de palabras,
3769120	3776360	de agrupamiento de documentos, y bueno, pues está la manera de cómo yo represento esos documentos,
3776440	3785360	para luego procesamos, bien, no hay preguntas, ¿dale?
3785360	3795800	Estos sabrimos son, osea, no os realizados, osea, no, no, no, no, exacto, exacto, es más,
3795800	3800720	hoy lo, en este ejemplito, ¿no?
3800720	3810360	Osea, los tópicos son 1, 2 y 3, después, yo humano puedo decir, bueno, mira, al
3810360	3817640	te uno, me fijo en las palabras y digo economía, al te dole pongo deportes, que si pensamos
3817640	3824600	en noticias, no, pensamos en noticias de un diario, no necesariamente un diario que lo
3824600	3834320	coloque en el tópico política, capaz que en realidad para mí es el tópico de cormía, osea,
3834320	3839480	también me puede servir tener esos metadatos, si fueran, si estuvieran analizando texto en
3839480	3847320	prensa y tengo los metadatos, me puede servir como para validar o no validar, pero a priori,
3847320	3854400	el tipo de tira, te uno, te dos, te tres, te cuatro, te cinco, los que vos quieras,
3854400	3860560	o digamos, de vuelta, esto se va refinando, en llega un punto donde vos deciste, no, llevo
3860560	3866640	hasta 10 tópicos o llevo hasta cuatro tópicos o llevo hasta 20 tópicos, porque después
3866640	3872580	ya la distribución en la misma, no, no cambia, no, no, no, no, por más que agrande el número
3872580	3881020	tópico, se esto no cambia, o sea que no, no, no, no agaría la ecuera, pero bueno, después se
3881020	3888020	requiere de un juicio de perto que te diga bueno, te uno está, te dos está, y cuando venga
3888020	3893540	un nuevo documento entre ese agorimo y ves, si enganchó en el te uno, que era de economía
3893540	3899540	y ahí, como es que validas si estaba bien, está mal.
3907540	3919540	Bueno, entonces dejamos por acá, fin del curso, y seguimos ahora semana que viene libre y luego empezamos con las presentaciones.
3919540	3929540	En el foro, tienen para preguntar por la taría laboratorio, vamos a tratar de estar atentos a las preguntas.
3931540	3940540	Y ahí después le digo hoy publicamos en un rato, publicamos la nomina de artículos de cada uno de los grupos.
